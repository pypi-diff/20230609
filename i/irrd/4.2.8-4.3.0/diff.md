# Comparing `tmp/irrd-4.2.8.tar.gz` & `tmp/irrd-4.3.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "irrd-4.2.8.tar", last modified: Wed Apr 19 11:33:40 2023, max compression
+gzip compressed data, was "irrd-4.3.0.tar", last modified: Tue Jun  6 08:43:24 2023, max compression
```

## Comparing `irrd-4.2.8.tar` & `irrd-4.3.0.tar`

### file list

```diff
@@ -1,275 +1,307 @@
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:40.007154 irrd-4.2.8/
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.775033 irrd-4.2.8/.circleci/
--rw-r--r--   0 sasha      (501) staff       (20)     6602 2023-04-19 11:28:23.000000 irrd-4.2.8/.circleci/config.yml
--rw-r--r--   0 sasha      (501) staff       (20)      859 2023-04-19 11:28:23.000000 irrd-4.2.8/.coveragerc
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.746613 irrd-4.2.8/.github/
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.776923 irrd-4.2.8/.github/ISSUE_TEMPLATE/
--rw-r--r--   0 sasha      (501) staff       (20)      560 2018-12-20 15:44:20.000000 irrd-4.2.8/.github/ISSUE_TEMPLATE/bug_report.md
--rw-r--r--   0 sasha      (501) staff       (20)      592 2018-12-20 15:44:20.000000 irrd-4.2.8/.github/ISSUE_TEMPLATE/new-feature-requests.md
--rw-r--r--   0 sasha      (501) staff       (20)      193 2022-06-22 14:54:59.000000 irrd-4.2.8/.gitignore
--rw-r--r--   0 sasha      (501) staff       (20)      127 2018-05-07 13:20:05.000000 irrd-4.2.8/.pyup.yml
--rw-r--r--   0 sasha      (501) staff       (20)     3184 2022-06-22 14:54:59.000000 irrd-4.2.8/CONTRIBUTING.md
--rw-r--r--   0 sasha      (501) staff       (20)     3780 2023-04-19 10:20:23.000000 irrd-4.2.8/LICENSE
--rw-r--r--   0 sasha      (501) staff       (20)     2912 2023-04-19 11:33:40.007454 irrd-4.2.8/PKG-INFO
--rw-r--r--   0 sasha      (501) staff       (20)     2209 2022-06-22 14:54:59.000000 irrd-4.2.8/README.rst
--rw-r--r--   0 sasha      (501) staff       (20)     2254 2022-06-22 14:54:59.000000 irrd-4.2.8/SECURITY.rst
--rw-r--r--   0 sasha      (501) staff       (20)       78 2018-12-11 13:18:23.000000 irrd-4.2.8/alembic.ini
--rw-r--r--   0 sasha      (501) staff       (20)     2195 2023-04-19 10:20:23.000000 irrd-4.2.8/conftest.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.782940 irrd-4.2.8/docs/
--rw-r--r--   0 sasha      (501) staff       (20)      619 2021-06-28 13:39:26.000000 irrd-4.2.8/docs/Makefile
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.783898 irrd-4.2.8/docs/_static/
--rw-r--r--   0 sasha      (501) staff       (20)    63722 2021-06-28 13:39:26.000000 irrd-4.2.8/docs/_static/logo.png
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.793893 irrd-4.2.8/docs/admins/
--rw-r--r--   0 sasha      (501) staff       (20)    12688 2023-02-23 13:04:37.000000 irrd-4.2.8/docs/admins/availability-and-migration.rst
--rw-r--r--   0 sasha      (501) staff       (20)    37400 2023-04-19 11:28:23.000000 irrd-4.2.8/docs/admins/configuration.rst
--rw-r--r--   0 sasha      (501) staff       (20)    13000 2023-04-19 11:28:23.000000 irrd-4.2.8/docs/admins/deployment.rst
--rw-r--r--   0 sasha      (501) staff       (20)     1619 2022-06-22 14:54:59.000000 irrd-4.2.8/docs/admins/faq.rst
--rw-r--r--   0 sasha      (501) staff       (20)     5147 2022-06-22 14:54:59.000000 irrd-4.2.8/docs/admins/migrating-legacy-irrd.rst
--rw-r--r--   0 sasha      (501) staff       (20)     5833 2023-04-19 11:28:23.000000 irrd-4.2.8/docs/admins/object-validation.rst
--rw-r--r--   0 sasha      (501) staff       (20)     8830 2023-04-19 11:28:23.000000 irrd-4.2.8/docs/admins/rpki.rst
--rw-r--r--   0 sasha      (501) staff       (20)     3677 2023-04-19 11:28:23.000000 irrd-4.2.8/docs/admins/scopefilter.rst
--rw-r--r--   0 sasha      (501) staff       (20)     3218 2022-06-22 14:54:59.000000 irrd-4.2.8/docs/admins/status_page.rst
--rwxr-xr-x   0 sasha      (501) staff       (20)     5487 2023-04-19 11:28:23.000000 irrd-4.2.8/docs/conf.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.797260 irrd-4.2.8/docs/development/
--rw-r--r--   0 sasha      (501) staff       (20)    19639 2022-06-22 14:54:59.000000 irrd-4.2.8/docs/development/architecture.rst
--rw-r--r--   0 sasha      (501) staff       (20)     5383 2023-04-19 11:28:23.000000 irrd-4.2.8/docs/development/development-setup.rst
--rw-r--r--   0 sasha      (501) staff       (20)     9604 2021-06-28 13:39:26.000000 irrd-4.2.8/docs/development/storage.rst
--rw-r--r--   0 sasha      (501) staff       (20)     3155 2023-04-19 11:28:23.000000 irrd-4.2.8/docs/index.rst
--rw-r--r--   0 sasha      (501) staff       (20)       49 2021-06-28 13:39:26.000000 irrd-4.2.8/docs/license.rst
--rw-r--r--   0 sasha      (501) staff       (20)      780 2021-06-28 13:39:26.000000 irrd-4.2.8/docs/make.bat
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.829248 irrd-4.2.8/docs/releases/
--rw-r--r--   0 sasha      (501) staff       (20)      618 2021-06-28 13:39:26.000000 irrd-4.2.8/docs/releases/4.0.1.rst
--rw-r--r--   0 sasha      (501) staff       (20)      957 2021-06-28 13:39:26.000000 irrd-4.2.8/docs/releases/4.0.2.rst
--rw-r--r--   0 sasha      (501) staff       (20)     1099 2021-06-28 13:39:26.000000 irrd-4.2.8/docs/releases/4.0.3.rst
--rw-r--r--   0 sasha      (501) staff       (20)      710 2021-06-28 13:39:26.000000 irrd-4.2.8/docs/releases/4.0.4.rst
--rw-r--r--   0 sasha      (501) staff       (20)      502 2021-06-28 13:39:26.000000 irrd-4.2.8/docs/releases/4.0.5.rst
--rw-r--r--   0 sasha      (501) staff       (20)      420 2021-06-28 13:39:26.000000 irrd-4.2.8/docs/releases/4.0.6.rst
--rw-r--r--   0 sasha      (501) staff       (20)      404 2021-06-28 13:39:26.000000 irrd-4.2.8/docs/releases/4.0.7.rst
--rw-r--r--   0 sasha      (501) staff       (20)      353 2021-06-28 13:39:26.000000 irrd-4.2.8/docs/releases/4.0.8.rst
--rw-r--r--   0 sasha      (501) staff       (20)     8974 2023-04-19 11:28:23.000000 irrd-4.2.8/docs/releases/4.1.0.rst
--rw-r--r--   0 sasha      (501) staff       (20)      342 2021-06-28 13:39:26.000000 irrd-4.2.8/docs/releases/4.1.1.rst
--rw-r--r--   0 sasha      (501) staff       (20)     1259 2021-06-28 13:39:26.000000 irrd-4.2.8/docs/releases/4.1.2.rst
--rw-r--r--   0 sasha      (501) staff       (20)     1327 2021-06-28 13:39:26.000000 irrd-4.2.8/docs/releases/4.1.3.rst
--rw-r--r--   0 sasha      (501) staff       (20)      564 2021-06-28 13:39:26.000000 irrd-4.2.8/docs/releases/4.1.4.rst
--rw-r--r--   0 sasha      (501) staff       (20)     1429 2021-08-23 13:43:15.000000 irrd-4.2.8/docs/releases/4.1.5.rst
--rw-r--r--   0 sasha      (501) staff       (20)      349 2021-08-30 18:30:20.000000 irrd-4.2.8/docs/releases/4.1.6.rst
--rw-r--r--   0 sasha      (501) staff       (20)      755 2021-09-07 17:56:33.000000 irrd-4.2.8/docs/releases/4.1.7.rst
--rw-r--r--   0 sasha      (501) staff       (20)      712 2022-03-31 09:45:31.000000 irrd-4.2.8/docs/releases/4.1.8.rst
--rw-r--r--   0 sasha      (501) staff       (20)     8090 2022-06-22 14:54:59.000000 irrd-4.2.8/docs/releases/4.2.0.rst
--rw-r--r--   0 sasha      (501) staff       (20)      763 2022-03-31 09:45:31.000000 irrd-4.2.8/docs/releases/4.2.1.rst
--rw-r--r--   0 sasha      (501) staff       (20)     2914 2022-06-22 14:54:59.000000 irrd-4.2.8/docs/releases/4.2.2.rst
--rw-r--r--   0 sasha      (501) staff       (20)     4449 2023-04-19 11:28:23.000000 irrd-4.2.8/docs/releases/4.2.3.rst
--rw-r--r--   0 sasha      (501) staff       (20)      827 2022-06-22 14:54:59.000000 irrd-4.2.8/docs/releases/4.2.4.rst
--rw-r--r--   0 sasha      (501) staff       (20)     1099 2022-06-24 08:51:21.000000 irrd-4.2.8/docs/releases/4.2.5.rst
--rw-r--r--   0 sasha      (501) staff       (20)      610 2023-04-19 11:28:23.000000 irrd-4.2.8/docs/releases/4.2.6.rst
--rw-r--r--   0 sasha      (501) staff       (20)     1015 2023-02-23 13:04:37.000000 irrd-4.2.8/docs/releases/4.2.7.rst
--rw-r--r--   0 sasha      (501) staff       (20)      366 2023-04-19 11:33:22.000000 irrd-4.2.8/docs/releases/4.2.8.rst
--rw-r--r--   0 sasha      (501) staff       (20)      156 2022-06-22 14:54:59.000000 irrd-4.2.8/docs/releases/index.rst
--rw-r--r--   0 sasha      (501) staff       (20)       29 2022-06-22 14:54:59.000000 irrd-4.2.8/docs/security.rst
--rw-r--r--   0 sasha      (501) staff       (20)      642 2023-04-19 11:28:23.000000 irrd-4.2.8/docs/spelling_wordlist.txt
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.831081 irrd-4.2.8/docs/users/
--rw-r--r--   0 sasha      (501) staff       (20)    15100 2023-04-19 11:28:23.000000 irrd-4.2.8/docs/users/database-changes.rst
--rw-r--r--   0 sasha      (501) staff       (20)    14439 2022-06-22 14:54:59.000000 irrd-4.2.8/docs/users/mirroring.rst
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.834221 irrd-4.2.8/docs/users/queries/
--rw-r--r--   0 sasha      (501) staff       (20)    28712 2023-04-19 11:28:23.000000 irrd-4.2.8/docs/users/queries/graphql.rst
--rw-r--r--   0 sasha      (501) staff       (20)     4186 2022-06-22 14:54:59.000000 irrd-4.2.8/docs/users/queries/index.rst
--rw-r--r--   0 sasha      (501) staff       (20)    14712 2023-04-19 11:28:23.000000 irrd-4.2.8/docs/users/queries/whois.rst
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.834951 irrd-4.2.8/irrd/
--rw-r--r--   0 sasha      (501) staff       (20)       69 2023-04-19 11:33:22.000000 irrd-4.2.8/irrd/__init__.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.844792 irrd-4.2.8/irrd/conf/
--rw-r--r--   0 sasha      (501) staff       (20)    19785 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/conf/__init__.py
--rw-r--r--   0 sasha      (501) staff       (20)     2586 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/conf/default_config.yaml
--rw-r--r--   0 sasha      (501) staff       (20)      188 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/conf/defaults.py
--rw-r--r--   0 sasha      (501) staff       (20)    14923 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/conf/test_conf.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.846225 irrd-4.2.8/irrd/daemon/
--rw-r--r--   0 sasha      (501) staff       (20)        0 2020-11-10 14:53:47.000000 irrd-4.2.8/irrd/daemon/__init__.py
--rwxr-xr-x   0 sasha      (501) staff       (20)     7232 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/daemon/main.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.849507 irrd-4.2.8/irrd/integration_tests/
--rw-r--r--   0 sasha      (501) staff       (20)        0 2019-01-16 10:28:28.000000 irrd-4.2.8/irrd/integration_tests/__init__.py
--rw-r--r--   0 sasha      (501) staff       (20)      302 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/integration_tests/constants.py
--rw-r--r--   0 sasha      (501) staff       (20)     2709 2020-11-10 14:53:47.000000 irrd-4.2.8/irrd/integration_tests/mailserver.tac
--rw-r--r--   0 sasha      (501) staff       (20)    45365 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/integration_tests/run.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.858152 irrd-4.2.8/irrd/mirroring/
--rw-r--r--   0 sasha      (501) staff       (20)        0 2018-08-22 10:47:06.000000 irrd-4.2.8/irrd/mirroring/__init__.py
--rw-r--r--   0 sasha      (501) staff       (20)     3838 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/mirroring/mirror_runners_export.py
--rw-r--r--   0 sasha      (501) staff       (20)    16245 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/mirroring/mirror_runners_import.py
--rw-r--r--   0 sasha      (501) staff       (20)     3891 2023-04-19 11:28:27.000000 irrd-4.2.8/irrd/mirroring/nrtm_generator.py
--rw-r--r--   0 sasha      (501) staff       (20)     5085 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/mirroring/nrtm_operation.py
--rw-r--r--   0 sasha      (501) staff       (20)    19716 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/mirroring/parsers.py
--rw-r--r--   0 sasha      (501) staff       (20)     6461 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/mirroring/scheduler.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.865334 irrd-4.2.8/irrd/mirroring/tests/
--rw-r--r--   0 sasha      (501) staff       (20)        0 2018-08-22 10:47:06.000000 irrd-4.2.8/irrd/mirroring/tests/__init__.py
--rw-r--r--   0 sasha      (501) staff       (20)     1908 2021-09-07 16:54:18.000000 irrd-4.2.8/irrd/mirroring/tests/nrtm_samples.py
--rw-r--r--   0 sasha      (501) staff       (20)     7139 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/mirroring/tests/test_mirror_runners_export.py
--rw-r--r--   0 sasha      (501) staff       (20)    25267 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/mirroring/tests/test_mirror_runners_import.py
--rw-r--r--   0 sasha      (501) staff       (20)     7237 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/mirroring/tests/test_nrtm_generator.py
--rw-r--r--   0 sasha      (501) staff       (20)     8398 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/mirroring/tests/test_nrtm_operation.py
--rw-r--r--   0 sasha      (501) staff       (20)    16913 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/mirroring/tests/test_parsers.py
--rw-r--r--   0 sasha      (501) staff       (20)     8973 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/mirroring/tests/test_scheduler.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.868754 irrd-4.2.8/irrd/rpki/
--rw-r--r--   0 sasha      (501) staff       (20)        0 2020-11-10 14:53:47.000000 irrd-4.2.8/irrd/rpki/__init__.py
--rw-r--r--   0 sasha      (501) staff       (20)     9467 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/rpki/importer.py
--rw-r--r--   0 sasha      (501) staff       (20)     5071 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/rpki/notifications.py
--rw-r--r--   0 sasha      (501) staff       (20)      115 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/rpki/status.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.872129 irrd-4.2.8/irrd/rpki/tests/
--rw-r--r--   0 sasha      (501) staff       (20)        0 2020-11-10 14:53:47.000000 irrd-4.2.8/irrd/rpki/tests/__init__.py
--rw-r--r--   0 sasha      (501) staff       (20)     9976 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/rpki/tests/test_importer.py
--rw-r--r--   0 sasha      (501) staff       (20)     6271 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/rpki/tests/test_notifications.py
--rw-r--r--   0 sasha      (501) staff       (20)    14142 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/rpki/tests/test_validators.py
--rw-r--r--   0 sasha      (501) staff       (20)     9248 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/rpki/validators.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.883612 irrd-4.2.8/irrd/rpsl/
--rw-r--r--   0 sasha      (501) staff       (20)        0 2020-10-12 17:35:22.000000 irrd-4.2.8/irrd/rpsl/__init__.py
--rw-r--r--   0 sasha      (501) staff       (20)      122 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/rpsl/config.py
--rw-r--r--   0 sasha      (501) staff       (20)    24126 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/rpsl/fields.py
--rw-r--r--   0 sasha      (501) staff       (20)    22222 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/rpsl/parser.py
--rw-r--r--   0 sasha      (501) staff       (20)     1614 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/rpsl/parser_state.py
--rw-r--r--   0 sasha      (501) staff       (20)    27633 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/rpsl/rpsl_objects.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.886755 irrd-4.2.8/irrd/rpsl/tests/
--rw-r--r--   0 sasha      (501) staff       (20)        0 2018-05-07 12:40:56.000000 irrd-4.2.8/irrd/rpsl/tests/__init__.py
--rw-r--r--   0 sasha      (501) staff       (20)    19630 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/rpsl/tests/test_fields.py
--rw-r--r--   0 sasha      (501) staff       (20)    24886 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/rpsl/tests/test_rpsl_objects.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.889971 irrd-4.2.8/irrd/scopefilter/
--rw-r--r--   0 sasha      (501) staff       (20)        0 2020-11-10 14:53:47.000000 irrd-4.2.8/irrd/scopefilter/__init__.py
--rw-r--r--   0 sasha      (501) staff       (20)      152 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/scopefilter/status.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.891651 irrd-4.2.8/irrd/scopefilter/tests/
--rw-r--r--   0 sasha      (501) staff       (20)        0 2020-11-10 14:53:47.000000 irrd-4.2.8/irrd/scopefilter/tests/__init__.py
--rw-r--r--   0 sasha      (501) staff       (20)     9113 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/scopefilter/tests/test_scopefilter.py
--rw-r--r--   0 sasha      (501) staff       (20)     6354 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/scopefilter/validators.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.904297 irrd-4.2.8/irrd/scripts/
--rw-r--r--   0 sasha      (501) staff       (20)        0 2018-05-11 14:51:11.000000 irrd-4.2.8/irrd/scripts/__init__.py
--rwxr-xr-x   0 sasha      (501) staff       (20)     1243 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/scripts/database_downgrade.py
--rwxr-xr-x   0 sasha      (501) staff       (20)     1228 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/scripts/database_upgrade.py
--rwxr-xr-x   0 sasha      (501) staff       (20)     2406 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/scripts/load_database.py
--rwxr-xr-x   0 sasha      (501) staff       (20)     1859 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/scripts/load_pgp_keys.py
--rwxr-xr-x   0 sasha      (501) staff       (20)     1472 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/scripts/load_test.py
--rwxr-xr-x   0 sasha      (501) staff       (20)     1236 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/scripts/mirror_force_reload.py
--rwxr-xr-x   0 sasha      (501) staff       (20)     8954 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/scripts/query_qa_comparison.py
--rwxr-xr-x   0 sasha      (501) staff       (20)     4499 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/scripts/rpsl_read.py
--rwxr-xr-x   0 sasha      (501) staff       (20)     2169 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/scripts/set_last_modified_auth.py
--rwxr-xr-x   0 sasha      (501) staff       (20)     1281 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/scripts/submit_changes.py
--rwxr-xr-x   0 sasha      (501) staff       (20)     1464 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/scripts/submit_email.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.912979 irrd-4.2.8/irrd/scripts/tests/
--rw-r--r--   0 sasha      (501) staff       (20)        0 2018-05-11 14:43:40.000000 irrd-4.2.8/irrd/scripts/tests/__init__.py
--rw-r--r--   0 sasha      (501) staff       (20)     2427 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/scripts/tests/test_load_database.py
--rw-r--r--   0 sasha      (501) staff       (20)     1227 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/scripts/tests/test_load_pgp_keys.py
--rw-r--r--   0 sasha      (501) staff       (20)      519 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/scripts/tests/test_mirror_force_reload.py
--rw-r--r--   0 sasha      (501) staff       (20)     3244 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/scripts/tests/test_rpsl_read.py
--rw-r--r--   0 sasha      (501) staff       (20)     1566 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/scripts/tests/test_set_last_modified_auth.py
--rw-r--r--   0 sasha      (501) staff       (20)      769 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/scripts/tests/test_submit_email.py
--rw-r--r--   0 sasha      (501) staff       (20)      575 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/scripts/tests/test_submit_update.py
--rw-r--r--   0 sasha      (501) staff       (20)     2299 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/scripts/tests/test_update_database.py
--rw-r--r--   0 sasha      (501) staff       (20)     2176 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/scripts/update_database.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.917094 irrd-4.2.8/irrd/server/
--rw-r--r--   0 sasha      (501) staff       (20)        0 2018-07-06 10:44:00.000000 irrd-4.2.8/irrd/server/__init__.py
--rw-r--r--   0 sasha      (501) staff       (20)     1490 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/server/access_check.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.921975 irrd-4.2.8/irrd/server/graphql/
--rw-r--r--   0 sasha      (501) staff       (20)       67 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/server/graphql/__init__.py
--rw-r--r--   0 sasha      (501) staff       (20)     1887 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/server/graphql/extensions.py
--rw-r--r--   0 sasha      (501) staff       (20)    13936 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/server/graphql/resolvers.py
--rw-r--r--   0 sasha      (501) staff       (20)     3430 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/server/graphql/schema_builder.py
--rw-r--r--   0 sasha      (501) staff       (20)    12876 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/server/graphql/schema_generator.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.925315 irrd-4.2.8/irrd/server/graphql/tests/
--rw-r--r--   0 sasha      (501) staff       (20)        0 2022-06-22 14:54:59.000000 irrd-4.2.8/irrd/server/graphql/tests/__init__.py
--rw-r--r--   0 sasha      (501) staff       (20)     1463 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/server/graphql/tests/test_extensions.py
--rw-r--r--   0 sasha      (501) staff       (20)    15060 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/server/graphql/tests/test_resolvers.py
--rw-r--r--   0 sasha      (501) staff       (20)    12987 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/server/graphql/tests/test_schema_generator.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.929412 irrd-4.2.8/irrd/server/http/
--rw-r--r--   0 sasha      (501) staff       (20)        0 2018-09-25 09:23:35.000000 irrd-4.2.8/irrd/server/http/__init__.py
--rw-r--r--   0 sasha      (501) staff       (20)     2958 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/server/http/app.py
--rw-r--r--   0 sasha      (501) staff       (20)     3406 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/server/http/endpoints.py
--rw-r--r--   0 sasha      (501) staff       (20)     1359 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/server/http/server.py
--rw-r--r--   0 sasha      (501) staff       (20)     8129 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/server/http/status_generator.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.931964 irrd-4.2.8/irrd/server/http/tests/
--rw-r--r--   0 sasha      (501) staff       (20)        0 2018-09-25 09:23:35.000000 irrd-4.2.8/irrd/server/http/tests/__init__.py
--rw-r--r--   0 sasha      (501) staff       (20)     7696 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/server/http/tests/test_endpoints.py
--rw-r--r--   0 sasha      (501) staff       (20)     9830 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/server/http/tests/test_status_generator.py
--rw-r--r--   0 sasha      (501) staff       (20)    18566 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/server/query_resolver.py
--rw-r--r--   0 sasha      (501) staff       (20)     2019 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/server/test_access_check.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.939345 irrd-4.2.8/irrd/server/tests/
--rw-r--r--   0 sasha      (501) staff       (20)        0 2022-06-22 14:54:59.000000 irrd-4.2.8/irrd/server/tests/__init__.py
--rw-r--r--   0 sasha      (501) staff       (20)    32444 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/server/tests/test_query_resolver.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.945163 irrd-4.2.8/irrd/server/whois/
--rw-r--r--   0 sasha      (501) staff       (20)        0 2018-07-06 10:44:00.000000 irrd-4.2.8/irrd/server/whois/__init__.py
--rw-r--r--   0 sasha      (501) staff       (20)    23973 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/server/whois/query_parser.py
--rw-r--r--   0 sasha      (501) staff       (20)     3535 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/server/whois/query_response.py
--rw-r--r--   0 sasha      (501) staff       (20)     8709 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/server/whois/server.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.949994 irrd-4.2.8/irrd/server/whois/tests/
--rw-r--r--   0 sasha      (501) staff       (20)        0 2018-07-06 10:44:00.000000 irrd-4.2.8/irrd/server/whois/tests/__init__.py
--rw-r--r--   0 sasha      (501) staff       (20)    39550 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/server/whois/tests/test_query_parser.py
--rw-r--r--   0 sasha      (501) staff       (20)     5132 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/server/whois/tests/test_query_response.py
--rw-r--r--   0 sasha      (501) staff       (20)     6040 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/server/whois/tests/test_server.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.957742 irrd-4.2.8/irrd/storage/
--rw-r--r--   0 sasha      (501) staff       (20)     1496 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/storage/__init__.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.960479 irrd-4.2.8/irrd/storage/alembic/
--rw-r--r--   0 sasha      (501) staff       (20)        0 2018-08-22 10:47:06.000000 irrd-4.2.8/irrd/storage/alembic/__init__.py
--rw-r--r--   0 sasha      (501) staff       (20)     1765 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/storage/alembic/env.py
--rw-r--r--   0 sasha      (501) staff       (20)      494 2018-08-22 10:47:06.000000 irrd-4.2.8/irrd/storage/alembic/script.py.mako
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.972556 irrd-4.2.8/irrd/storage/alembic/versions/
--rw-r--r--   0 sasha      (501) staff       (20)      594 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/storage/alembic/versions/1743f98a456d_add_serial_newest_mirror.py
--rw-r--r--   0 sasha      (501) staff       (20)     1226 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/storage/alembic/versions/181670a62643_add_journal_entry_origin.py
--rw-r--r--   0 sasha      (501) staff       (20)    10369 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/storage/alembic/versions/28dc1cd85bdc_initial_db.py
--rw-r--r--   0 sasha      (501) staff       (20)     1401 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/storage/alembic/versions/39e4f15ed80c_add_bogon_status.py
--rw-r--r--   0 sasha      (501) staff       (20)     2251 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/storage/alembic/versions/4a514ead8fc2_bogon_to_scope_filter.py
--rw-r--r--   0 sasha      (501) staff       (20)     1145 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/storage/alembic/versions/64a3d6faf6d4_add_prefix_length_rpki_status_to_rpsl_objects.py
--rw-r--r--   0 sasha      (501) staff       (20)     1049 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/storage/alembic/versions/8744b4b906bb_fix_rpsl_unique_key.py
--rw-r--r--   0 sasha      (501) staff       (20)      550 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/storage/alembic/versions/893d0d5363b3_add_rpsl_prefix_idx.py
--rw-r--r--   0 sasha      (501) staff       (20)        0 2018-08-22 10:47:06.000000 irrd-4.2.8/irrd/storage/alembic/versions/__init__.py
--rw-r--r--   0 sasha      (501) staff       (20)     1083 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/storage/alembic/versions/a7766c144d61_add_synchronised_serial_to_database_.py
--rw-r--r--   0 sasha      (501) staff       (20)     1518 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/storage/alembic/versions/a8609af97aa3_set_prefix_length_in_existing_rpsl_.py
--rw-r--r--   0 sasha      (501) staff       (20)      510 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/storage/alembic/versions/b175c262448f_set_rpsl_prefix.py
--rw-r--r--   0 sasha      (501) staff       (20)     1713 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/storage/alembic/versions/e07863eac52f_add_roa_object_table.py
--rw-r--r--   0 sasha      (501) staff       (20)      511 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/storage/alembic/versions/f4c837d8258c_add_rpsl_prefix.py
--rw-r--r--   0 sasha      (501) staff       (20)    34740 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/storage/database_handler.py
--rw-r--r--   0 sasha      (501) staff       (20)     8347 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/storage/models.py
--rw-r--r--   0 sasha      (501) staff       (20)    15540 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/storage/preload.py
--rw-r--r--   0 sasha      (501) staff       (20)    20251 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/storage/queries.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.975870 irrd-4.2.8/irrd/storage/tests/
--rw-r--r--   0 sasha      (501) staff       (20)        0 2018-08-22 10:47:06.000000 irrd-4.2.8/irrd/storage/tests/__init__.py
--rw-r--r--   0 sasha      (501) staff       (20)    40358 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/storage/tests/test_database.py
--rw-r--r--   0 sasha      (501) staff       (20)     8690 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/storage/tests/test_preload.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.980142 irrd-4.2.8/irrd/updates/
--rw-r--r--   0 sasha      (501) staff       (20)        0 2018-07-31 14:50:32.000000 irrd-4.2.8/irrd/updates/__init__.py
--rw-r--r--   0 sasha      (501) staff       (20)     2831 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/updates/email.py
--rw-r--r--   0 sasha      (501) staff       (20)    13269 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/updates/handler.py
--rw-r--r--   0 sasha      (501) staff       (20)    19472 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/updates/parser.py
--rw-r--r--   0 sasha      (501) staff       (20)      654 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/updates/parser_state.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.983775 irrd-4.2.8/irrd/updates/tests/
--rw-r--r--   0 sasha      (501) staff       (20)        0 2018-07-31 14:50:32.000000 irrd-4.2.8/irrd/updates/tests/__init__.py
--rw-r--r--   0 sasha      (501) staff       (20)     6010 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/updates/tests/test_email.py
--rw-r--r--   0 sasha      (501) staff       (20)    42451 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/updates/tests/test_handler.py
--rw-r--r--   0 sasha      (501) staff       (20)    58198 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/updates/tests/test_parser.py
--rw-r--r--   0 sasha      (501) staff       (20)    16514 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/updates/validators.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.992354 irrd-4.2.8/irrd/utils/
--rw-r--r--   0 sasha      (501) staff       (20)        0 2023-01-30 15:26:42.000000 irrd-4.2.8/irrd/utils/__init__.py
--rw-r--r--   0 sasha      (501) staff       (20)     3290 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/utils/email.py
--rw-r--r--   0 sasha      (501) staff       (20)     3000 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/utils/pgp.py
--rw-r--r--   0 sasha      (501) staff       (20)     1612 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/utils/process_support.py
--rw-r--r--   0 sasha      (501) staff       (20)    40716 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/utils/rpsl_samples.py
--rw-r--r--   0 sasha      (501) staff       (20)      800 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/utils/test_utils.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:40.003262 irrd-4.2.8/irrd/utils/tests/
--rw-r--r--   0 sasha      (501) staff       (20)        0 2023-01-30 15:26:37.000000 irrd-4.2.8/irrd/utils/tests/__init__.py
--rw-r--r--   0 sasha      (501) staff       (20)    26217 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/utils/tests/test_email.py
--rw-r--r--   0 sasha      (501) staff       (20)     7059 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/utils/tests/test_pgp.py
--rw-r--r--   0 sasha      (501) staff       (20)     2084 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/utils/tests/test_text.py
--rw-r--r--   0 sasha      (501) staff       (20)     1931 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/utils/tests/test_validators.py
--rw-r--r--   0 sasha      (501) staff       (20)    10324 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/utils/tests/test_whois_client.py
--rw-r--r--   0 sasha      (501) staff       (20)     3521 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/utils/text.py
--rw-r--r--   0 sasha      (501) staff       (20)     2252 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/utils/validators.py
--rw-r--r--   0 sasha      (501) staff       (20)     4752 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/utils/whois_client.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:40.004092 irrd-4.2.8/irrd/vendor/
--rw-r--r--   0 sasha      (501) staff       (20)        0 2020-11-10 14:53:47.000000 irrd-4.2.8/irrd/vendor/__init__.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:40.005193 irrd-4.2.8/irrd/vendor/dotted/
--rw-r--r--   0 sasha      (501) staff       (20)        0 2022-06-22 14:54:59.000000 irrd-4.2.8/irrd/vendor/dotted/__init__.py
--rw-r--r--   0 sasha      (501) staff       (20)    12374 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/vendor/dotted/collection.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:40.006315 irrd-4.2.8/irrd/vendor/postgres_copy/
--rw-r--r--   0 sasha      (501) staff       (20)     6359 2023-04-19 11:28:23.000000 irrd-4.2.8/irrd/vendor/postgres_copy/__init__.py
-drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-04-19 11:33:39.838994 irrd-4.2.8/irrd.egg-info/
--rw-r--r--   0 sasha      (501) staff       (20)     2912 2023-04-19 11:33:38.000000 irrd-4.2.8/irrd.egg-info/PKG-INFO
--rw-r--r--   0 sasha      (501) staff       (20)     7206 2023-04-19 11:33:39.000000 irrd-4.2.8/irrd.egg-info/SOURCES.txt
--rw-r--r--   0 sasha      (501) staff       (20)        1 2023-04-19 11:33:38.000000 irrd-4.2.8/irrd.egg-info/dependency_links.txt
--rw-r--r--   0 sasha      (501) staff       (20)      519 2023-04-19 11:33:38.000000 irrd-4.2.8/irrd.egg-info/entry_points.txt
--rw-r--r--   0 sasha      (501) staff       (20)      625 2023-04-19 11:33:38.000000 irrd-4.2.8/irrd.egg-info/requires.txt
--rw-r--r--   0 sasha      (501) staff       (20)        5 2023-04-19 11:33:38.000000 irrd-4.2.8/irrd.egg-info/top_level.txt
--rw-r--r--   0 sasha      (501) staff       (20)     2213 2023-04-19 11:28:23.000000 irrd-4.2.8/requirements.txt
--rw-r--r--   0 sasha      (501) staff       (20)      782 2023-04-19 11:33:40.008641 irrd-4.2.8/setup.cfg
--rwxr-xr-x   0 sasha      (501) staff       (20)     3096 2023-04-19 11:28:23.000000 irrd-4.2.8/setup.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:24.001078 irrd-4.3.0/
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.816701 irrd-4.3.0/.circleci/
+-rw-r--r--   0 sasha      (501) staff       (20)    11428 2023-06-06 08:42:53.000000 irrd-4.3.0/.circleci/config.yml
+-rw-r--r--   0 sasha      (501) staff       (20)      918 2023-06-06 08:42:53.000000 irrd-4.3.0/.coveragerc
+-rw-r--r--   0 sasha      (501) staff       (20)      725 2023-04-19 20:02:15.000000 irrd-4.3.0/.git-blame-ignore-revs
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.796704 irrd-4.3.0/.github/
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.817886 irrd-4.3.0/.github/ISSUE_TEMPLATE/
+-rw-r--r--   0 sasha      (501) staff       (20)      560 2018-12-20 15:44:20.000000 irrd-4.3.0/.github/ISSUE_TEMPLATE/bug_report.md
+-rw-r--r--   0 sasha      (501) staff       (20)      592 2018-12-20 15:44:20.000000 irrd-4.3.0/.github/ISSUE_TEMPLATE/new-feature-requests.md
+-rw-r--r--   0 sasha      (501) staff       (20)      193 2022-06-22 14:54:59.000000 irrd-4.3.0/.gitignore
+-rw-r--r--   0 sasha      (501) staff       (20)      307 2023-06-06 08:42:53.000000 irrd-4.3.0/.mergify.yml
+-rw-r--r--   0 sasha      (501) staff       (20)      127 2018-05-07 13:20:05.000000 irrd-4.3.0/.pyup.yml
+-rw-r--r--   0 sasha      (501) staff       (20)     3184 2022-06-22 14:54:59.000000 irrd-4.3.0/CONTRIBUTING.md
+-rw-r--r--   0 sasha      (501) staff       (20)     3780 2023-06-06 08:42:53.000000 irrd-4.3.0/LICENSE
+-rw-r--r--   0 sasha      (501) staff       (20)     2913 2023-06-06 08:43:24.001470 irrd-4.3.0/PKG-INFO
+-rw-r--r--   0 sasha      (501) staff       (20)     2209 2022-06-22 14:54:59.000000 irrd-4.3.0/README.rst
+-rw-r--r--   0 sasha      (501) staff       (20)     2254 2022-06-22 14:54:59.000000 irrd-4.3.0/SECURITY.rst
+-rw-r--r--   0 sasha      (501) staff       (20)       78 2018-12-11 13:18:23.000000 irrd-4.3.0/alembic.ini
+-rw-r--r--   0 sasha      (501) staff       (20)     2195 2023-06-06 08:42:53.000000 irrd-4.3.0/conftest.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.822666 irrd-4.3.0/docs/
+-rw-r--r--   0 sasha      (501) staff       (20)      619 2021-06-28 13:39:26.000000 irrd-4.3.0/docs/Makefile
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.823803 irrd-4.3.0/docs/_static/
+-rwxr-xr-x   0 sasha      (501) staff       (20)    24630 2023-06-06 08:42:53.000000 irrd-4.3.0/docs/_static/irr_rpsl_submit.py
+-rw-r--r--   0 sasha      (501) staff       (20)    63722 2021-06-28 13:39:26.000000 irrd-4.3.0/docs/_static/logo.png
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.835296 irrd-4.3.0/docs/admins/
+-rw-r--r--   0 sasha      (501) staff       (20)    12688 2023-02-23 13:04:37.000000 irrd-4.3.0/docs/admins/availability-and-migration.rst
+-rw-r--r--   0 sasha      (501) staff       (20)    42858 2023-06-06 08:42:53.000000 irrd-4.3.0/docs/admins/configuration.rst
+-rw-r--r--   0 sasha      (501) staff       (20)    14384 2023-06-06 08:42:53.000000 irrd-4.3.0/docs/admins/deployment.rst
+-rw-r--r--   0 sasha      (501) staff       (20)     1619 2022-06-22 14:54:59.000000 irrd-4.3.0/docs/admins/faq.rst
+-rw-r--r--   0 sasha      (501) staff       (20)     5147 2022-06-22 14:54:59.000000 irrd-4.3.0/docs/admins/migrating-legacy-irrd.rst
+-rw-r--r--   0 sasha      (501) staff       (20)     7191 2023-04-19 11:37:44.000000 irrd-4.3.0/docs/admins/object-suppression.rst
+-rw-r--r--   0 sasha      (501) staff       (20)     5956 2023-04-19 11:37:44.000000 irrd-4.3.0/docs/admins/object-validation.rst
+-rw-r--r--   0 sasha      (501) staff       (20)     4248 2023-04-19 11:37:44.000000 irrd-4.3.0/docs/admins/route-object-preference.rst
+-rw-r--r--   0 sasha      (501) staff       (20)     5778 2023-04-19 11:37:44.000000 irrd-4.3.0/docs/admins/rpki.rst
+-rw-r--r--   0 sasha      (501) staff       (20)     1630 2023-04-19 11:37:44.000000 irrd-4.3.0/docs/admins/scopefilter.rst
+-rw-r--r--   0 sasha      (501) staff       (20)     3218 2022-06-22 14:54:59.000000 irrd-4.3.0/docs/admins/status_page.rst
+-rw-r--r--   0 sasha      (501) staff       (20)     6661 2023-06-06 08:42:53.000000 irrd-4.3.0/docs/admins/suspension.rst
+-rwxr-xr-x   0 sasha      (501) staff       (20)     5511 2023-06-06 08:42:53.000000 irrd-4.3.0/docs/conf.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.836631 irrd-4.3.0/docs/development/
+-rw-r--r--   0 sasha      (501) staff       (20)    19639 2022-06-22 14:54:59.000000 irrd-4.3.0/docs/development/architecture.rst
+-rw-r--r--   0 sasha      (501) staff       (20)     5452 2023-06-06 08:42:53.000000 irrd-4.3.0/docs/development/development-setup.rst
+-rw-r--r--   0 sasha      (501) staff       (20)     9604 2021-06-28 13:39:26.000000 irrd-4.3.0/docs/development/storage.rst
+-rw-r--r--   0 sasha      (501) staff       (20)     3353 2023-06-06 08:42:53.000000 irrd-4.3.0/docs/index.rst
+-rw-r--r--   0 sasha      (501) staff       (20)       49 2021-06-28 13:39:26.000000 irrd-4.3.0/docs/license.rst
+-rw-r--r--   0 sasha      (501) staff       (20)      780 2021-06-28 13:39:26.000000 irrd-4.3.0/docs/make.bat
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.861432 irrd-4.3.0/docs/releases/
+-rw-r--r--   0 sasha      (501) staff       (20)      618 2021-06-28 13:39:26.000000 irrd-4.3.0/docs/releases/4.0.1.rst
+-rw-r--r--   0 sasha      (501) staff       (20)      957 2021-06-28 13:39:26.000000 irrd-4.3.0/docs/releases/4.0.2.rst
+-rw-r--r--   0 sasha      (501) staff       (20)     1099 2021-06-28 13:39:26.000000 irrd-4.3.0/docs/releases/4.0.3.rst
+-rw-r--r--   0 sasha      (501) staff       (20)      710 2021-06-28 13:39:26.000000 irrd-4.3.0/docs/releases/4.0.4.rst
+-rw-r--r--   0 sasha      (501) staff       (20)      502 2021-06-28 13:39:26.000000 irrd-4.3.0/docs/releases/4.0.5.rst
+-rw-r--r--   0 sasha      (501) staff       (20)      420 2021-06-28 13:39:26.000000 irrd-4.3.0/docs/releases/4.0.6.rst
+-rw-r--r--   0 sasha      (501) staff       (20)      404 2021-06-28 13:39:26.000000 irrd-4.3.0/docs/releases/4.0.7.rst
+-rw-r--r--   0 sasha      (501) staff       (20)      353 2021-06-28 13:39:26.000000 irrd-4.3.0/docs/releases/4.0.8.rst
+-rw-r--r--   0 sasha      (501) staff       (20)     8980 2023-04-19 11:37:44.000000 irrd-4.3.0/docs/releases/4.1.0.rst
+-rw-r--r--   0 sasha      (501) staff       (20)      342 2021-06-28 13:39:26.000000 irrd-4.3.0/docs/releases/4.1.1.rst
+-rw-r--r--   0 sasha      (501) staff       (20)     1259 2021-06-28 13:39:26.000000 irrd-4.3.0/docs/releases/4.1.2.rst
+-rw-r--r--   0 sasha      (501) staff       (20)     1327 2021-06-28 13:39:26.000000 irrd-4.3.0/docs/releases/4.1.3.rst
+-rw-r--r--   0 sasha      (501) staff       (20)      564 2021-06-28 13:39:26.000000 irrd-4.3.0/docs/releases/4.1.4.rst
+-rw-r--r--   0 sasha      (501) staff       (20)     1429 2021-08-23 13:43:15.000000 irrd-4.3.0/docs/releases/4.1.5.rst
+-rw-r--r--   0 sasha      (501) staff       (20)      349 2021-08-30 18:30:20.000000 irrd-4.3.0/docs/releases/4.1.6.rst
+-rw-r--r--   0 sasha      (501) staff       (20)      755 2021-09-07 17:56:33.000000 irrd-4.3.0/docs/releases/4.1.7.rst
+-rw-r--r--   0 sasha      (501) staff       (20)      712 2022-03-31 09:45:31.000000 irrd-4.3.0/docs/releases/4.1.8.rst
+-rw-r--r--   0 sasha      (501) staff       (20)     8090 2022-06-22 14:54:59.000000 irrd-4.3.0/docs/releases/4.2.0.rst
+-rw-r--r--   0 sasha      (501) staff       (20)      763 2022-03-31 09:45:31.000000 irrd-4.3.0/docs/releases/4.2.1.rst
+-rw-r--r--   0 sasha      (501) staff       (20)     2914 2022-06-22 14:54:59.000000 irrd-4.3.0/docs/releases/4.2.2.rst
+-rw-r--r--   0 sasha      (501) staff       (20)     4384 2023-04-19 11:37:44.000000 irrd-4.3.0/docs/releases/4.2.3.rst
+-rw-r--r--   0 sasha      (501) staff       (20)      827 2022-06-22 14:54:59.000000 irrd-4.3.0/docs/releases/4.2.4.rst
+-rw-r--r--   0 sasha      (501) staff       (20)     1099 2022-06-24 08:51:21.000000 irrd-4.3.0/docs/releases/4.2.5.rst
+-rw-r--r--   0 sasha      (501) staff       (20)      609 2023-04-19 11:37:44.000000 irrd-4.3.0/docs/releases/4.2.6.rst
+-rw-r--r--   0 sasha      (501) staff       (20)     1015 2023-02-23 13:04:37.000000 irrd-4.3.0/docs/releases/4.2.7.rst
+-rw-r--r--   0 sasha      (501) staff       (20)      366 2023-04-19 20:02:15.000000 irrd-4.3.0/docs/releases/4.2.8.rst
+-rw-r--r--   0 sasha      (501) staff       (20)    11136 2023-06-05 18:52:29.000000 irrd-4.3.0/docs/releases/4.3.0.rst
+-rw-r--r--   0 sasha      (501) staff       (20)      156 2022-06-22 14:54:59.000000 irrd-4.3.0/docs/releases/index.rst
+-rw-r--r--   0 sasha      (501) staff       (20)       29 2022-06-22 14:54:59.000000 irrd-4.3.0/docs/security.rst
+-rw-r--r--   0 sasha      (501) staff       (20)      723 2023-06-06 08:42:53.000000 irrd-4.3.0/docs/spelling_wordlist.txt
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.862778 irrd-4.3.0/docs/users/
+-rw-r--r--   0 sasha      (501) staff       (20)    18263 2023-06-06 08:42:53.000000 irrd-4.3.0/docs/users/database-changes.rst
+-rw-r--r--   0 sasha      (501) staff       (20)    14439 2022-06-22 14:54:59.000000 irrd-4.3.0/docs/users/mirroring.rst
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.868090 irrd-4.3.0/docs/users/queries/
+-rw-r--r--   0 sasha      (501) staff       (20)     8059 2023-06-05 18:09:54.000000 irrd-4.3.0/docs/users/queries/event-stream.rst
+-rw-r--r--   0 sasha      (501) staff       (20)    29050 2023-04-19 11:37:44.000000 irrd-4.3.0/docs/users/queries/graphql.rst
+-rw-r--r--   0 sasha      (501) staff       (20)     4186 2022-06-22 14:54:59.000000 irrd-4.3.0/docs/users/queries/index.rst
+-rw-r--r--   0 sasha      (501) staff       (20)    14823 2023-04-19 11:37:44.000000 irrd-4.3.0/docs/users/queries/whois.rst
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.868768 irrd-4.3.0/irrd/
+-rw-r--r--   0 sasha      (501) staff       (20)       69 2023-06-06 08:43:15.000000 irrd-4.3.0/irrd/__init__.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.873929 irrd-4.3.0/irrd/conf/
+-rw-r--r--   0 sasha      (501) staff       (20)    20231 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/conf/__init__.py
+-rw-r--r--   0 sasha      (501) staff       (20)     2909 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/conf/default_config.yaml
+-rw-r--r--   0 sasha      (501) staff       (20)      188 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/conf/defaults.py
+-rw-r--r--   0 sasha      (501) staff       (20)     3064 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/conf/known_keys.py
+-rw-r--r--   0 sasha      (501) staff       (20)    17608 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/conf/test_conf.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.874648 irrd-4.3.0/irrd/daemon/
+-rw-r--r--   0 sasha      (501) staff       (20)        0 2020-11-10 14:53:47.000000 irrd-4.3.0/irrd/daemon/__init__.py
+-rwxr-xr-x   0 sasha      (501) staff       (20)     8433 2023-04-26 13:05:14.000000 irrd-4.3.0/irrd/daemon/main.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.876841 irrd-4.3.0/irrd/integration_tests/
+-rw-r--r--   0 sasha      (501) staff       (20)        0 2019-01-16 10:28:28.000000 irrd-4.3.0/irrd/integration_tests/__init__.py
+-rw-r--r--   0 sasha      (501) staff       (20)      302 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/integration_tests/constants.py
+-rw-r--r--   0 sasha      (501) staff       (20)     2709 2020-11-10 14:53:47.000000 irrd-4.3.0/irrd/integration_tests/mailserver.tac
+-rw-r--r--   0 sasha      (501) staff       (20)    47044 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/integration_tests/run.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.882273 irrd-4.3.0/irrd/mirroring/
+-rw-r--r--   0 sasha      (501) staff       (20)        0 2018-08-22 10:47:06.000000 irrd-4.3.0/irrd/mirroring/__init__.py
+-rw-r--r--   0 sasha      (501) staff       (20)     4092 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/mirroring/mirror_runners_export.py
+-rw-r--r--   0 sasha      (501) staff       (20)    18138 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/mirroring/mirror_runners_import.py
+-rw-r--r--   0 sasha      (501) staff       (20)     4057 2023-05-05 10:38:29.000000 irrd-4.3.0/irrd/mirroring/nrtm_generator.py
+-rw-r--r--   0 sasha      (501) staff       (20)     5068 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/mirroring/nrtm_operation.py
+-rw-r--r--   0 sasha      (501) staff       (20)    20006 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/mirroring/parsers.py
+-rw-r--r--   0 sasha      (501) staff       (20)     7022 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/mirroring/scheduler.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.888313 irrd-4.3.0/irrd/mirroring/tests/
+-rw-r--r--   0 sasha      (501) staff       (20)        0 2018-08-22 10:47:06.000000 irrd-4.3.0/irrd/mirroring/tests/__init__.py
+-rw-r--r--   0 sasha      (501) staff       (20)     1908 2021-09-07 16:54:18.000000 irrd-4.3.0/irrd/mirroring/tests/nrtm_samples.py
+-rw-r--r--   0 sasha      (501) staff       (20)     7549 2023-04-26 13:05:14.000000 irrd-4.3.0/irrd/mirroring/tests/test_mirror_runners_export.py
+-rw-r--r--   0 sasha      (501) staff       (20)    28333 2023-04-26 13:05:14.000000 irrd-4.3.0/irrd/mirroring/tests/test_mirror_runners_import.py
+-rw-r--r--   0 sasha      (501) staff       (20)     7862 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/mirroring/tests/test_nrtm_generator.py
+-rw-r--r--   0 sasha      (501) staff       (20)     8261 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/mirroring/tests/test_nrtm_operation.py
+-rw-r--r--   0 sasha      (501) staff       (20)    17178 2023-04-26 13:05:14.000000 irrd-4.3.0/irrd/mirroring/tests/test_parsers.py
+-rw-r--r--   0 sasha      (501) staff       (20)    10264 2023-04-26 13:05:14.000000 irrd-4.3.0/irrd/mirroring/tests/test_scheduler.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.890564 irrd-4.3.0/irrd/routepref/
+-rw-r--r--   0 sasha      (501) staff       (20)        0 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/routepref/__init__.py
+-rw-r--r--   0 sasha      (501) staff       (20)     7784 2023-04-19 20:02:15.000000 irrd-4.3.0/irrd/routepref/routepref.py
+-rw-r--r--   0 sasha      (501) staff       (20)      234 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/routepref/status.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.892114 irrd-4.3.0/irrd/routepref/tests/
+-rw-r--r--   0 sasha      (501) staff       (20)        0 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/routepref/tests/__init__.py
+-rw-r--r--   0 sasha      (501) staff       (20)     7177 2023-04-26 13:05:14.000000 irrd-4.3.0/irrd/routepref/tests/test_routepref.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.895683 irrd-4.3.0/irrd/rpki/
+-rw-r--r--   0 sasha      (501) staff       (20)        0 2020-11-10 14:53:47.000000 irrd-4.3.0/irrd/rpki/__init__.py
+-rw-r--r--   0 sasha      (501) staff       (20)     9590 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/rpki/importer.py
+-rw-r--r--   0 sasha      (501) staff       (20)     5234 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/rpki/notifications.py
+-rw-r--r--   0 sasha      (501) staff       (20)      245 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/rpki/status.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.898122 irrd-4.3.0/irrd/rpki/tests/
+-rw-r--r--   0 sasha      (501) staff       (20)        0 2020-11-10 14:53:47.000000 irrd-4.3.0/irrd/rpki/tests/__init__.py
+-rw-r--r--   0 sasha      (501) staff       (20)    11086 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/rpki/tests/test_importer.py
+-rw-r--r--   0 sasha      (501) staff       (20)     6864 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/rpki/tests/test_notifications.py
+-rw-r--r--   0 sasha      (501) staff       (20)    15519 2023-04-26 13:05:14.000000 irrd-4.3.0/irrd/rpki/tests/test_validators.py
+-rw-r--r--   0 sasha      (501) staff       (20)     9350 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/rpki/validators.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.902139 irrd-4.3.0/irrd/rpsl/
+-rw-r--r--   0 sasha      (501) staff       (20)        0 2020-10-12 17:35:22.000000 irrd-4.3.0/irrd/rpsl/__init__.py
+-rw-r--r--   0 sasha      (501) staff       (20)    25963 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/rpsl/fields.py
+-rw-r--r--   0 sasha      (501) staff       (20)    22916 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/rpsl/parser.py
+-rw-r--r--   0 sasha      (501) staff       (20)     1770 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/rpsl/parser_state.py
+-rw-r--r--   0 sasha      (501) staff       (20)     1160 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/rpsl/passwords.py
+-rw-r--r--   0 sasha      (501) staff       (20)    32711 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/rpsl/rpsl_objects.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.904205 irrd-4.3.0/irrd/rpsl/tests/
+-rw-r--r--   0 sasha      (501) staff       (20)        0 2018-05-07 12:40:56.000000 irrd-4.3.0/irrd/rpsl/tests/__init__.py
+-rw-r--r--   0 sasha      (501) staff       (20)    20595 2023-04-26 13:05:14.000000 irrd-4.3.0/irrd/rpsl/tests/test_fields.py
+-rw-r--r--   0 sasha      (501) staff       (20)    25321 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/rpsl/tests/test_rpsl_objects.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.906020 irrd-4.3.0/irrd/scopefilter/
+-rw-r--r--   0 sasha      (501) staff       (20)        0 2020-11-10 14:53:47.000000 irrd-4.3.0/irrd/scopefilter/__init__.py
+-rw-r--r--   0 sasha      (501) staff       (20)      275 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/scopefilter/status.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.907266 irrd-4.3.0/irrd/scopefilter/tests/
+-rw-r--r--   0 sasha      (501) staff       (20)        0 2020-11-10 14:53:47.000000 irrd-4.3.0/irrd/scopefilter/tests/__init__.py
+-rw-r--r--   0 sasha      (501) staff       (20)     9504 2023-04-26 13:05:14.000000 irrd-4.3.0/irrd/scopefilter/tests/test_scopefilter.py
+-rw-r--r--   0 sasha      (501) staff       (20)     6344 2023-04-26 13:05:14.000000 irrd-4.3.0/irrd/scopefilter/validators.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.919569 irrd-4.3.0/irrd/scripts/
+-rw-r--r--   0 sasha      (501) staff       (20)        0 2018-05-11 14:51:11.000000 irrd-4.3.0/irrd/scripts/__init__.py
+-rwxr-xr-x   0 sasha      (501) staff       (20)     1281 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/scripts/database_downgrade.py
+-rwxr-xr-x   0 sasha      (501) staff       (20)     1266 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/scripts/database_upgrade.py
+-rwxr-xr-x   0 sasha      (501) staff       (20)     3182 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/scripts/expire_journal.py
+-rwxr-xr-x   0 sasha      (501) staff       (20)    24630 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/scripts/irr_rpsl_submit.py
+-rwxr-xr-x   0 sasha      (501) staff       (20)     2445 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/scripts/load_database.py
+-rwxr-xr-x   0 sasha      (501) staff       (20)     1850 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/scripts/load_pgp_keys.py
+-rwxr-xr-x   0 sasha      (501) staff       (20)     1415 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/scripts/load_test.py
+-rwxr-xr-x   0 sasha      (501) staff       (20)     1226 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/scripts/mirror_force_reload.py
+-rwxr-xr-x   0 sasha      (501) staff       (20)     8930 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/scripts/query_qa_comparison.py
+-rwxr-xr-x   0 sasha      (501) staff       (20)     4494 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/scripts/rpsl_read.py
+-rwxr-xr-x   0 sasha      (501) staff       (20)     2229 2023-04-26 13:05:15.000000 irrd-4.3.0/irrd/scripts/set_last_modified_auth.py
+-rwxr-xr-x   0 sasha      (501) staff       (20)     1295 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/scripts/submit_changes.py
+-rwxr-xr-x   0 sasha      (501) staff       (20)     1500 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/scripts/submit_email.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.927323 irrd-4.3.0/irrd/scripts/tests/
+-rw-r--r--   0 sasha      (501) staff       (20)        0 2018-05-11 14:43:40.000000 irrd-4.3.0/irrd/scripts/tests/__init__.py
+-rw-r--r--   0 sasha      (501) staff       (20)     3363 2023-04-26 13:05:15.000000 irrd-4.3.0/irrd/scripts/tests/test_expire_journal.py
+-rwxr-xr-x   0 sasha      (501) staff       (20)    31962 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/scripts/tests/test_irr_rpsl_submit.py
+-rw-r--r--   0 sasha      (501) staff       (20)     2466 2023-04-26 13:05:15.000000 irrd-4.3.0/irrd/scripts/tests/test_load_database.py
+-rw-r--r--   0 sasha      (501) staff       (20)     1229 2023-04-26 13:05:15.000000 irrd-4.3.0/irrd/scripts/tests/test_load_pgp_keys.py
+-rw-r--r--   0 sasha      (501) staff       (20)      520 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/scripts/tests/test_mirror_force_reload.py
+-rw-r--r--   0 sasha      (501) staff       (20)     3244 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/scripts/tests/test_rpsl_read.py
+-rw-r--r--   0 sasha      (501) staff       (20)     1579 2023-04-26 13:05:15.000000 irrd-4.3.0/irrd/scripts/tests/test_set_last_modified_auth.py
+-rw-r--r--   0 sasha      (501) staff       (20)      769 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/scripts/tests/test_submit_email.py
+-rw-r--r--   0 sasha      (501) staff       (20)      575 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/scripts/tests/test_submit_update.py
+-rw-r--r--   0 sasha      (501) staff       (20)     2320 2023-04-26 13:05:15.000000 irrd-4.3.0/irrd/scripts/tests/test_update_database.py
+-rw-r--r--   0 sasha      (501) staff       (20)     2199 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/scripts/update_database.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.930394 irrd-4.3.0/irrd/server/
+-rw-r--r--   0 sasha      (501) staff       (20)        0 2018-07-06 10:44:00.000000 irrd-4.3.0/irrd/server/__init__.py
+-rw-r--r--   0 sasha      (501) staff       (20)     1720 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/server/access_check.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.934118 irrd-4.3.0/irrd/server/graphql/
+-rw-r--r--   0 sasha      (501) staff       (20)       67 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/server/graphql/__init__.py
+-rw-r--r--   0 sasha      (501) staff       (20)     1890 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/server/graphql/extensions.py
+-rw-r--r--   0 sasha      (501) staff       (20)    14211 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/server/graphql/resolvers.py
+-rw-r--r--   0 sasha      (501) staff       (20)     3341 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/server/graphql/schema_builder.py
+-rw-r--r--   0 sasha      (501) staff       (20)    13327 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/server/graphql/schema_generator.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.937762 irrd-4.3.0/irrd/server/graphql/tests/
+-rw-r--r--   0 sasha      (501) staff       (20)        0 2022-06-22 14:54:59.000000 irrd-4.3.0/irrd/server/graphql/tests/__init__.py
+-rw-r--r--   0 sasha      (501) staff       (20)     1489 2023-04-26 13:05:15.000000 irrd-4.3.0/irrd/server/graphql/tests/test_extensions.py
+-rw-r--r--   0 sasha      (501) staff       (20)    16266 2023-04-26 13:05:15.000000 irrd-4.3.0/irrd/server/graphql/tests/test_resolvers.py
+-rw-r--r--   0 sasha      (501) staff       (20)    13250 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/server/graphql/tests/test_schema_generator.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.941678 irrd-4.3.0/irrd/server/http/
+-rw-r--r--   0 sasha      (501) staff       (20)        0 2018-09-25 09:23:35.000000 irrd-4.3.0/irrd/server/http/__init__.py
+-rw-r--r--   0 sasha      (501) staff       (20)     3499 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/server/http/app.py
+-rw-r--r--   0 sasha      (501) staff       (20)     4441 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/server/http/endpoints.py
+-rw-r--r--   0 sasha      (501) staff       (20)    12822 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/server/http/event_stream.py
+-rw-r--r--   0 sasha      (501) staff       (20)     1417 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/server/http/server.py
+-rw-r--r--   0 sasha      (501) staff       (20)     8507 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/server/http/status_generator.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.944463 irrd-4.3.0/irrd/server/http/tests/
+-rw-r--r--   0 sasha      (501) staff       (20)        0 2018-09-25 09:23:35.000000 irrd-4.3.0/irrd/server/http/tests/__init__.py
+-rw-r--r--   0 sasha      (501) staff       (20)     9546 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/server/http/tests/test_endpoints.py
+-rw-r--r--   0 sasha      (501) staff       (20)    11043 2023-04-26 13:05:15.000000 irrd-4.3.0/irrd/server/http/tests/test_event_stream.py
+-rw-r--r--   0 sasha      (501) staff       (20)    10454 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/server/http/tests/test_status_generator.py
+-rw-r--r--   0 sasha      (501) staff       (20)    19363 2023-04-26 13:05:15.000000 irrd-4.3.0/irrd/server/query_resolver.py
+-rw-r--r--   0 sasha      (501) staff       (20)     2157 2023-04-26 13:05:15.000000 irrd-4.3.0/irrd/server/test_access_check.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.945707 irrd-4.3.0/irrd/server/tests/
+-rw-r--r--   0 sasha      (501) staff       (20)        0 2022-06-22 14:54:59.000000 irrd-4.3.0/irrd/server/tests/__init__.py
+-rw-r--r--   0 sasha      (501) staff       (20)    34216 2023-04-26 13:05:15.000000 irrd-4.3.0/irrd/server/tests/test_query_resolver.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.949339 irrd-4.3.0/irrd/server/whois/
+-rw-r--r--   0 sasha      (501) staff       (20)        0 2018-07-06 10:44:00.000000 irrd-4.3.0/irrd/server/whois/__init__.py
+-rw-r--r--   0 sasha      (501) staff       (20)    24883 2023-04-26 13:05:15.000000 irrd-4.3.0/irrd/server/whois/query_parser.py
+-rw-r--r--   0 sasha      (501) staff       (20)     3540 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/server/whois/query_response.py
+-rw-r--r--   0 sasha      (501) staff       (20)     8816 2023-04-26 13:05:15.000000 irrd-4.3.0/irrd/server/whois/server.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.952762 irrd-4.3.0/irrd/server/whois/tests/
+-rw-r--r--   0 sasha      (501) staff       (20)        0 2018-07-06 10:44:00.000000 irrd-4.3.0/irrd/server/whois/tests/__init__.py
+-rw-r--r--   0 sasha      (501) staff       (20)    40937 2023-04-26 13:05:15.000000 irrd-4.3.0/irrd/server/whois/tests/test_query_parser.py
+-rw-r--r--   0 sasha      (501) staff       (20)     4637 2023-04-26 13:05:15.000000 irrd-4.3.0/irrd/server/whois/tests/test_query_response.py
+-rw-r--r--   0 sasha      (501) staff       (20)     6088 2023-04-26 13:05:15.000000 irrd-4.3.0/irrd/server/whois/tests/test_server.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.957965 irrd-4.3.0/irrd/storage/
+-rw-r--r--   0 sasha      (501) staff       (20)     1478 2023-04-26 13:05:15.000000 irrd-4.3.0/irrd/storage/__init__.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.960668 irrd-4.3.0/irrd/storage/alembic/
+-rw-r--r--   0 sasha      (501) staff       (20)        0 2018-08-22 10:47:06.000000 irrd-4.3.0/irrd/storage/alembic/__init__.py
+-rw-r--r--   0 sasha      (501) staff       (20)     1747 2023-04-26 13:05:15.000000 irrd-4.3.0/irrd/storage/alembic/env.py
+-rw-r--r--   0 sasha      (501) staff       (20)      494 2018-08-22 10:47:06.000000 irrd-4.3.0/irrd/storage/alembic/script.py.mako
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.975262 irrd-4.3.0/irrd/storage/alembic/versions/
+-rw-r--r--   0 sasha      (501) staff       (20)     2763 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/storage/alembic/versions/0548f1aa4f10_add_rpsl_objects_suspended.py
+-rw-r--r--   0 sasha      (501) staff       (20)      593 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/storage/alembic/versions/1743f98a456d_add_serial_newest_mirror.py
+-rw-r--r--   0 sasha      (501) staff       (20)     1606 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/storage/alembic/versions/181670a62643_add_journal_entry_origin.py
+-rw-r--r--   0 sasha      (501) staff       (20)    10096 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/storage/alembic/versions/28dc1cd85bdc_initial_db.py
+-rw-r--r--   0 sasha      (501) staff       (20)     1401 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/storage/alembic/versions/39e4f15ed80c_add_bogon_status.py
+-rw-r--r--   0 sasha      (501) staff       (20)     2347 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/storage/alembic/versions/4a514ead8fc2_bogon_to_scope_filter.py
+-rw-r--r--   0 sasha      (501) staff       (20)     1227 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/storage/alembic/versions/64a3d6faf6d4_add_prefix_length_rpki_status_to_rpsl_objects.py
+-rw-r--r--   0 sasha      (501) staff       (20)     1048 2023-04-26 13:05:15.000000 irrd-4.3.0/irrd/storage/alembic/versions/8744b4b906bb_fix_rpsl_unique_key.py
+-rw-r--r--   0 sasha      (501) staff       (20)      597 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/storage/alembic/versions/893d0d5363b3_add_rpsl_prefix_idx.py
+-rw-r--r--   0 sasha      (501) staff       (20)     2132 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/storage/alembic/versions/8b8357acd333_add_global_serial.py
+-rw-r--r--   0 sasha      (501) staff       (20)        0 2018-08-22 10:47:06.000000 irrd-4.3.0/irrd/storage/alembic/versions/__init__.py
+-rw-r--r--   0 sasha      (501) staff       (20)     1131 2023-04-26 13:05:15.000000 irrd-4.3.0/irrd/storage/alembic/versions/a7766c144d61_add_synchronised_serial_to_database_.py
+-rw-r--r--   0 sasha      (501) staff       (20)     1589 2023-04-26 13:05:15.000000 irrd-4.3.0/irrd/storage/alembic/versions/a8609af97aa3_set_prefix_length_in_existing_rpsl_.py
+-rw-r--r--   0 sasha      (501) staff       (20)      534 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/storage/alembic/versions/b175c262448f_set_rpsl_prefix.py
+-rw-r--r--   0 sasha      (501) staff       (20)     1640 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/storage/alembic/versions/e07863eac52f_add_roa_object_table.py
+-rw-r--r--   0 sasha      (501) staff       (20)      511 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/storage/alembic/versions/f4c837d8258c_add_rpsl_prefix.py
+-rw-r--r--   0 sasha      (501) staff       (20)     1662 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/storage/alembic/versions/fd4473bc1a10_add_route_preference_status.py
+-rw-r--r--   0 sasha      (501) staff       (20)    50385 2023-04-26 13:05:15.000000 irrd-4.3.0/irrd/storage/database_handler.py
+-rw-r--r--   0 sasha      (501) staff       (20)     2222 2023-04-26 13:05:15.000000 irrd-4.3.0/irrd/storage/event_stream.py
+-rw-r--r--   0 sasha      (501) staff       (20)    10570 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/storage/models.py
+-rw-r--r--   0 sasha      (501) staff       (20)    16189 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/storage/preload.py
+-rw-r--r--   0 sasha      (501) staff       (20)    22770 2023-04-26 13:05:15.000000 irrd-4.3.0/irrd/storage/queries.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.977699 irrd-4.3.0/irrd/storage/tests/
+-rw-r--r--   0 sasha      (501) staff       (20)        0 2018-08-22 10:47:06.000000 irrd-4.3.0/irrd/storage/tests/__init__.py
+-rw-r--r--   0 sasha      (501) staff       (20)    50736 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/storage/tests/test_database.py
+-rw-r--r--   0 sasha      (501) staff       (20)     1202 2023-04-26 13:05:15.000000 irrd-4.3.0/irrd/storage/tests/test_event_stream.py
+-rw-r--r--   0 sasha      (501) staff       (20)     9089 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/storage/tests/test_preload.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.982267 irrd-4.3.0/irrd/updates/
+-rw-r--r--   0 sasha      (501) staff       (20)        0 2018-07-31 14:50:32.000000 irrd-4.3.0/irrd/updates/__init__.py
+-rw-r--r--   0 sasha      (501) staff       (20)     3147 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/updates/email.py
+-rw-r--r--   0 sasha      (501) staff       (20)    15356 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/updates/handler.py
+-rw-r--r--   0 sasha      (501) staff       (20)    28096 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/updates/parser.py
+-rw-r--r--   0 sasha      (501) staff       (20)     1347 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/updates/parser_state.py
+-rw-r--r--   0 sasha      (501) staff       (20)     7991 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/updates/suspension.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.986997 irrd-4.3.0/irrd/updates/tests/
+-rw-r--r--   0 sasha      (501) staff       (20)        0 2018-07-31 14:50:32.000000 irrd-4.3.0/irrd/updates/tests/__init__.py
+-rw-r--r--   0 sasha      (501) staff       (20)     6047 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/updates/tests/test_email.py
+-rw-r--r--   0 sasha      (501) staff       (20)    44799 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/updates/tests/test_handler.py
+-rw-r--r--   0 sasha      (501) staff       (20)    70533 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/updates/tests/test_parser.py
+-rw-r--r--   0 sasha      (501) staff       (20)    10959 2023-04-26 13:05:15.000000 irrd-4.3.0/irrd/updates/tests/test_suspension.py
+-rw-r--r--   0 sasha      (501) staff       (20)    28731 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/updates/tests/test_validators.py
+-rw-r--r--   0 sasha      (501) staff       (20)    20220 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/updates/validators.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.993631 irrd-4.3.0/irrd/utils/
+-rw-r--r--   0 sasha      (501) staff       (20)        0 2023-01-30 15:26:42.000000 irrd-4.3.0/irrd/utils/__init__.py
+-rw-r--r--   0 sasha      (501) staff       (20)     3289 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/utils/email.py
+-rw-r--r--   0 sasha      (501) staff       (20)      338 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/utils/misc.py
+-rw-r--r--   0 sasha      (501) staff       (20)     3125 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/utils/pgp.py
+-rw-r--r--   0 sasha      (501) staff       (20)     1628 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/utils/process_support.py
+-rw-r--r--   0 sasha      (501) staff       (20)    40936 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/utils/rpsl_samples.py
+-rw-r--r--   0 sasha      (501) staff       (20)     4956 2023-04-26 13:05:15.000000 irrd-4.3.0/irrd/utils/test_utils.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.998095 irrd-4.3.0/irrd/utils/tests/
+-rw-r--r--   0 sasha      (501) staff       (20)        0 2023-01-30 15:26:37.000000 irrd-4.3.0/irrd/utils/tests/__init__.py
+-rw-r--r--   0 sasha      (501) staff       (20)    26563 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/utils/tests/test_email.py
+-rw-r--r--   0 sasha      (501) staff       (20)      172 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/utils/tests/test_misc.py
+-rw-r--r--   0 sasha      (501) staff       (20)     7278 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/utils/tests/test_pgp.py
+-rw-r--r--   0 sasha      (501) staff       (20)     2350 2023-04-26 13:05:15.000000 irrd-4.3.0/irrd/utils/tests/test_text.py
+-rw-r--r--   0 sasha      (501) staff       (20)     2264 2023-04-26 13:05:15.000000 irrd-4.3.0/irrd/utils/tests/test_validators.py
+-rw-r--r--   0 sasha      (501) staff       (20)    10238 2023-04-26 13:05:15.000000 irrd-4.3.0/irrd/utils/tests/test_whois_client.py
+-rw-r--r--   0 sasha      (501) staff       (20)     3537 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/utils/text.py
+-rw-r--r--   0 sasha      (501) staff       (20)     2708 2023-06-06 08:42:53.000000 irrd-4.3.0/irrd/utils/validators.py
+-rw-r--r--   0 sasha      (501) staff       (20)     4823 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/utils/whois_client.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.998705 irrd-4.3.0/irrd/vendor/
+-rw-r--r--   0 sasha      (501) staff       (20)        0 2020-11-10 14:53:47.000000 irrd-4.3.0/irrd/vendor/__init__.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.999487 irrd-4.3.0/irrd/vendor/dotted/
+-rw-r--r--   0 sasha      (501) staff       (20)        0 2022-06-22 14:54:59.000000 irrd-4.3.0/irrd/vendor/dotted/__init__.py
+-rw-r--r--   0 sasha      (501) staff       (20)    11736 2023-04-19 11:37:44.000000 irrd-4.3.0/irrd/vendor/dotted/collection.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:24.000329 irrd-4.3.0/irrd/vendor/postgres_copy/
+-rw-r--r--   0 sasha      (501) staff       (20)     6209 2023-04-26 13:05:15.000000 irrd-4.3.0/irrd/vendor/postgres_copy/__init__.py
+drwxr-xr-x   0 sasha      (501) staff       (20)        0 2023-06-06 08:43:23.871551 irrd-4.3.0/irrd.egg-info/
+-rw-r--r--   0 sasha      (501) staff       (20)     2913 2023-06-06 08:43:23.000000 irrd-4.3.0/irrd.egg-info/PKG-INFO
+-rw-r--r--   0 sasha      (501) staff       (20)     8272 2023-06-06 08:43:23.000000 irrd-4.3.0/irrd.egg-info/SOURCES.txt
+-rw-r--r--   0 sasha      (501) staff       (20)        1 2023-06-06 08:43:23.000000 irrd-4.3.0/irrd.egg-info/dependency_links.txt
+-rw-r--r--   0 sasha      (501) staff       (20)      626 2023-06-06 08:43:23.000000 irrd-4.3.0/irrd.egg-info/entry_points.txt
+-rw-r--r--   0 sasha      (501) staff       (20)      693 2023-06-06 08:43:23.000000 irrd-4.3.0/irrd.egg-info/requires.txt
+-rw-r--r--   0 sasha      (501) staff       (20)        5 2023-06-06 08:43:23.000000 irrd-4.3.0/irrd.egg-info/top_level.txt
+-rw-r--r--   0 sasha      (501) staff       (20)     2407 2023-06-06 08:43:07.000000 irrd-4.3.0/requirements.txt
+-rw-r--r--   0 sasha      (501) staff       (20)      840 2023-06-06 08:43:24.002570 irrd-4.3.0/setup.cfg
+-rwxr-xr-x   0 sasha      (501) staff       (20)     3335 2023-06-06 08:42:53.000000 irrd-4.3.0/setup.py
```

### Comparing `irrd-4.2.8/.coveragerc` & `irrd-4.3.0/.coveragerc`

 * *Files 9% similar despite different names*

```diff
@@ -1,23 +1,24 @@
 [flake8]
-ignore=E501,W503,E226,E252,W504
+ignore=E501,W503,E226,E252,W504,E203,E231
 
 [run]
 omit = setup.py
 
 [report]
 exclude_lines =
     pragma: no cover
     def __repr__
     if self.debug:
     if settings.DEBUG
     raise AssertionError
     raise NotImplementedError
     if 0:
     if __name__ == '__main__':
+    if __name__ == "__main__":
 
 omit =
     # Impractical for unit tests, but covered in integration tests
     irrd/daemon/main.py
     irrd/server/http/app.py
     irrd/server/graphql/schema_builder.py
     irrd/server/http/server.py
@@ -28,14 +29,15 @@
     irrd/scripts/database_downgrade.py
     irrd/scripts/load_test.py
     irrd/integration_tests/*
     irrd/vendor/*
 
 [tool:pytest]
 log_level=DEBUG
+asyncio_mode=auto
 
 [mypy]
 ignore_missing_imports = True
 install_types = True
 non_interactive = True
 exclude = irrd/storage/alembic/*
```

### Comparing `irrd-4.2.8/.github/ISSUE_TEMPLATE/bug_report.md` & `irrd-4.3.0/.github/ISSUE_TEMPLATE/bug_report.md`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/.github/ISSUE_TEMPLATE/new-feature-requests.md` & `irrd-4.3.0/.github/ISSUE_TEMPLATE/new-feature-requests.md`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/CONTRIBUTING.md` & `irrd-4.3.0/CONTRIBUTING.md`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/LICENSE` & `irrd-4.3.0/LICENSE`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/PKG-INFO` & `irrd-4.3.0/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,23 +1,23 @@
 Metadata-Version: 2.1
 Name: irrd
-Version: 4.2.8
+Version: 4.3.0
 Summary: Internet Routing Registry daemon (IRRd)
 Home-page: https://github.com/irrdnet/irrd
 Author: Reliably Coded for NTT Ltd. and others
 Author-email: irrd@reliablycoded.nl
 Classifier: License :: OSI Approved :: BSD License
 Classifier: Programming Language :: Python :: 3
-Classifier: Programming Language :: Python :: 3.6
 Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
+Classifier: Programming Language :: Python :: 3.11
 Classifier: Operating System :: OS Independent
-Requires-Python: >=3.6
+Requires-Python: >=3.7
 Description-Content-Type: text/x-rst
 License-File: LICENSE
 
 Internet Routing Registry Daemon (IRRd) Version 4
 =================================================
 
 .. image:: https://circleci.com/gh/irrdnet/irrd.svg?style=svg
```

### Comparing `irrd-4.2.8/README.rst` & `irrd-4.3.0/README.rst`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/SECURITY.rst` & `irrd-4.3.0/SECURITY.rst`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/conftest.py` & `irrd-4.3.0/conftest.py`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/docs/Makefile` & `irrd-4.3.0/docs/Makefile`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/docs/_static/logo.png` & `irrd-4.3.0/docs/_static/logo.png`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/docs/admins/availability-and-migration.rst` & `irrd-4.3.0/docs/admins/availability-and-migration.rst`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/docs/admins/configuration.rst` & `irrd-4.3.0/docs/admins/configuration.rst`

 * *Files 12% similar despite different names*

```diff
@@ -13,24 +13,31 @@
 this documentation and IRRd error messages. For example, when this
 documentation mentions a setting like ``rpki.roa_source``, this refers to
 a key ``roa_source`` under a key ``rpki``.
 
 .. contents::
    :backlinks: none
    :local:
+   :depth: 2
 
 Example configuration file
 --------------------------
 
 .. highlight:: yaml
     :linenothreshold: 5
 
-This sample shows most configuration options. Note that this is an example
-of what a configuration looks like, and many of these settings are optional,
-or the value shown in this example is their default::
+This sample shows most configuration options
+
+.. note:: 
+  Note that this is an example of what a configuration file can look like.
+  Many of these settings are optional, or the value shown in this example is
+  their default. This is intended as an example of syntax, not as a template
+  for your own configuration.
+
+::
 
     irrd:
         database_url: 'postgresql:///irrd'
         redis_url: 'unix:///usr/local/var/run/redis.sock'
         piddir: /var/run/
         user: irrd
         group: irrd
@@ -52,14 +59,16 @@
                 interface: '::0'
                 max_connections: 50
                 port: 8043
 
         auth:
             gnupg_keyring: /home/irrd/gnupg-keyring/
             override_password: {hash}
+            password_hashers:
+                md5-pw: legacy
 
         email:
             footer: 'email footer'
             from: example@example.com
             smtp: localhost
             notification_header: |
                 This is to notify you of changes in the {sources_str} database
@@ -230,14 +239,20 @@
   |br| **Default**: not defined, all access permitted for whois
   |br| **Change takes effect**: after SIGHUP.
 * ``server.http.status_access_list``: a reference to an access list in the
   configuration, where only IPs in the access list are permitted access to the
   :doc:`HTTP status page </admins/status_page>`. If not defined, all access is denied.
   |br| **Default**: not defined, all access denied for HTTP status page
   |br| **Change takes effect**: after SIGHUP.
+* ``server.http.event_stream_access_list``: a reference to an access list in the
+  configuration, where only IPs in the access list are permitted access to the
+  :doc:`event stream </users/queries/event-stream>` initial download and WebSocket
+  stream. If not defined, all access is denied.
+  |br| **Default**: not defined, all access denied for event stream.
+  |br| **Change takes effect**: after SIGHUP.
 * ``server.whois.max_connections``: the maximum number of simultaneous whois
   connections permitted. Note that each permitted connection will result in
   one IRRd whois worker to be started, each of which use about 200 MB memory.
   For example, if you set this to 50, you need about 10 GB of memory just for
   IRRd's whois server.
   (and additional memory for other components and PostgreSQL).
   |br| **Default**: ``10``.
@@ -290,36 +305,121 @@
   |br| `You may receive this message because you are listed in`
   |br| `the notify attribute on the changed object(s), because`
   |br| `you are listed in the mnt-nfy or upd-to attribute on a maintainer`
   |br| `of the object(s), or the upd-to attribute on the maintainer of a`
   |br| `parent of newly created object(s).`
 
 
-Authentication
-~~~~~~~~~~~~~~
+Authentication and validation
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 * ``auth.override_password``: a salted MD5 hash of the override password,
   which can be used to override any
   authorisation requirements for authoritative databases.
   |br| **Default**: not defined, no override password will be accepted.
   |br| **Change takes effect**: upon the next update attempt.
-* ``auth.authenticate_related_mntners``: whether to check for
-  :ref:`related object maintainers <auth-related-mntners>` when processing
-  updates.
-  |br| **Default**: true, check enabled
-  |br| **Change takes effect**: upon the next update attempt.
 * ``auth.gnupg_keyring``: the full path to the gnupg keyring.
   |br| **Default**: not defined, but required.
   |br| **Change takes effect**: after full IRRd restart.
+* ``auth.password_hashers``: which password hashers to allow in mntner objects.
+  This is a dictionary with the hashers (``crypt-pw``, ``md5-pw``, ``bcrypt-pw``) as
+  possible keys, and ``enabled``, ``legacy``, or ``disabled`` as possible values.
+  Enabled means the hash is enabled for all cases. Disabled
+  means IRRd treats the hash as if it is unknown, rejecting authentication and
+  rejecting ``auth`` attributes using this hasher in new or updated authoritative
+  `mntner` objects. Legacy is in between: authentication with existing hashes is
+  permitted, ``auth`` attributes in new or modified authoritative objects
+  are not.
+  |br| **Default**: ``enabled`` for ``md5-pw`` and ``bcrypt-pw``,
+  ``legacy`` for ``crypt-pw``
+  |br| **Change takes effect**: upon the next update attempt.
+* ``auth.authenticate_parents_route_creation``: whether to check for
+  :ref:`related object maintainers <auth-related-mntners-route>` when users create
+  new `route(6)` objects.
+  |br| **Default**: true, check enabled
+  |br| **Change takes effect**: upon the next update attempt.
 
 .. danger::
 
     IRRd loads keys into the gnupg keyring when `key-cert` objects are
     imported. Their presence in the keyring is then used to validate requested
     changes. Therefore, the keyring referred to by ``auth.gnupg_keyring`` can
     not be simply reset, or PGP authentications may fail.
+    However, you can use the ``irrd_load_pgp_keys`` command to refill the keyring
+    in ``auth.gnupg_keyring``.
+
+.. _conf-auth-set-creation:
+
+auth.set_creation
+"""""""""""""""""
+The ``auth.set_creation`` setting configures the requirements when creating new
+RPSL set objects. These are `as-set`, `filter-set`, `peering-set`, `route-set`
+and `rtr-set`. It is highly customisable, but therefore also more complex
+than most other settings.
+
+There are two underlying settings:
+
+* ``prefix_required`` configures whether an ASN prefix is required in the name
+  of a set object. When enabled ``AS-EXAMPLE`` is invalid, while
+  ``AS65537:AS-EXAMPLE`` or ``AS65537:AS-EXAMPLE:AS-CUSTOMERS``
+  are valid.
+* ``autnum_authentication`` controls whether the user also needs to pass
+  authentication for the `aut-num` corresponding to the AS number used as the set
+  name prefix. For example, if the set name is ``AS65537:AS-EXAMPLE:AS-CUSTOMERS``,
+  this setting may require the creation to also pass authentication for the
+  `aut-num` AS65537.
+  The options are ``disabled``, ``opportunistic`` or ``required``.
+  When disabled, this check is skipped. For opportunistic, the check is used, but
+  passes if the aut-num does not exist. For required, the check is used and fails
+  if the aut-num does not exist.
+  
+Note that even when ``autnum_authentication`` is set to ``required``,
+if at the same time ``prefix_required`` is set to false, a set can be created
+without a prefix or with one, per ``prefix_required``.
+But if it has a prefix, there *must* be a corresponding
+aut-num object for which authentication *must* pass, per ``autnum_authentication``.
+
+You can configure one common for all set classes under the key ``COMMON``,
+and/or specific settings for specific classes using the class name as key.
+An example::
+
+    irrd:
+      auth:
+          set_creation:
+              COMMON:
+                  prefix_required: true
+                  autnum_authentication: required
+              as-set:
+                  prefix_required: true
+                  autnum_authentication: opportunistic
+              rtr-set:
+                  prefix_required: false
+                  autnum_authentication: disabled
+
+This example means:
+
+* New `as-set` objects must include an ASN prefix in their name
+  and the user must pass authentication for the corresponding `aut-num` object,
+  if it exists. If the `aut-num` does not exist, the check passes.
+* New ``rtr-set`` objects are not required to include an ASN prefix in their
+  name, but this is permitted. The user never has to pass authentication for
+  the corresponding `aut-num` object, regardless of whether it exists.
+* All other new set objects must include an ASN prefix in their name, an `aut-num`
+  corresponding that AS number must exist, and the user must pass authentication
+  for that `aut-num` object.
+
+All checks are only applied when users create new set objects in authoritative
+databases. Authoritative updates to existing objects, deletions, or objects from
+mirrors are never affected. When looking for corresponding `aut-num` objects,
+IRRd only looks in the same IRR source.
+
+**Default**: ``prefix_required`` is enabled, ``autnum_authentication``
+set to ``opportunistic`` for all sets. Note that settings under the
+``COMMON`` key override these IRRd defaults, and settings under set-specific
+keys in turn override settings under the ``COMMON`` key.
+|br| **Change takes effect**: upon the next update attempt.
 
 
 Access lists
 ~~~~~~~~~~~~
 * ``access_lists.{list_name}``: a list of permitted IPv4 and/or IPv6 addresses
   and/or prefixes, which will be
   permitted access for any service that refers to access list ``{list_name}``.
@@ -429,14 +529,29 @@
   For details, see the
   :doc:`scope filter documentation </admins/scopefilter>`.
   May contain plain AS number, or a range, e.g. ``64496-64511``.
   |br| **Default**: none, ASN scope filter validation not enabled.
   |br| **Change takes effect**: after SIGHUP. Updating the status of
   existing objects may take 10-15 minutes.
 
+Route object preference
+~~~~~~~~~~~~~~~~~~~~~~~
+* ``route_object_preference.update_timer``: the time in seconds between
+  full updates of the
+  :doc:`route object preference </admins/route-object-preference>` status
+  for all objects in the database. Note that the status is already updated
+  on changes to objects as they are processed - this periodic process sets
+  the initial state and resolver small inconsistencies. This setting
+  has no effect unless at least one source has
+  ``sources.{name}.route_object_preference`` set.
+  |br| **Default**: 3600 (1 hour)
+  |br| **Change takes effect**: after SIGHUP.
+
+In addition to this, the route object preference of each source can be set
+in ``sources.{name}.route_object_preference``, documented below.
 
 Sources
 ~~~~~~~
 * ``sources_default``: a list of sources that are enabled by default, or when a
   user selects all sources with ``-a``. The order of this list defines the
   search priority as well. It is not required to include all known sources in
   the default selection. If ``rpki.roa_source`` is defined, this may also
@@ -561,15 +676,25 @@
   RPKI status.
   |br| **Default**: false, RPKI validation enabled.
   |br| **Change takes effect**: after SIGHUP, upon next full ROA import.
 * ``sources.{name}.scopefilter_excluded``: disable scope filter validation for
   this source. If set to ``true``, all objects will be considered in scope
   for their scope filter status.
   |br| **Default**: false, scope filter validation enabled.
-  |br| **Change takes effect**: after SIGHUP, within a few minutes
+  |br| **Change takes effect**: after SIGHUP, within a few minutes.
+* ``sources.{name}.route_object_preference``: the
+  :doc:`route object preference </admins/route-object-preference>` for
+  this source. Higher number is a higher preference.
+  |br| **Default**: unset, not considered for route object preference.
+  |br| **Change takes effect**: after SIGHUP, after next periodic update.
+* ``sources.{name}.suspension_enabled``: a boolean for whether this source
+  allows :doc:`suspension and reactivation of objects </admins/suspension>`.
+  Can only be enabled if `authoritative` is enabled.
+  |br| **Default**: ``false``.
+  |br| **Change takes effect**: after SIGHUP, for all subsequent changes.
 
 
 For more detail on mirroring other sources, and providing mirroring services
 to others, see the :doc:`mirroring documentation </users/mirroring>`.
 
 .. caution::
 
@@ -753,26 +878,14 @@
   recommended when the IRRd instance never processes `inetnum` objects.
   It enables :ref:`high performance prefix queries <performance_prefix_queries>`
   for all queries. However, if this is enabled and your IRRd instance does
   store `inetnum` objects, they may be missing from responses to queries.
   Therefore, only enable this when you do not process any `inetnum` objects.
   |br| **Default**: ``false``, i.e. `inetnum` search is enabled.
   |br| **Change takes effect**: after SIGHUP, for all subsequent queries.
-* ``compatibility.irrd42_migration_in_progress``: this setting is used
-  when doing a minimum downtime upgrade from IRRd 4.1.x to IRRd 4.2.x.
-  See the :doc:`4.2.0 release notes </releases/4.2.0>` for details.
-  |br| **Default**: ``false``, operating normally.
-  |br| **Change takes effect**: after SIGHUP, for all subsequent queries.
-* ``compatibility.permit_non_hierarchical_as_set_name``: by default,
-  `as-set` objects created in authoritative databases are required to have a
-  hierarchical name, like ``AS65540:AS-CUSTOMERS``. For example,
-  ``AS-CUSTOMERS`` would not be allowed. If this setting is set to ``true``,
-  this name requirement does not apply, and ``AS-CUSTOMERS`` is permitted.
-  |br| **Default**: ``false``, hierarchical name required.
-  |br| **Change takes effect**: after SIGHUP, for all subsequent updates.
 * ``compatibility.ipv4_only_route_set_members``: if set to ``true``, ``!i``
   queries will not return IPv6 prefixes. This option can be used for limited
   compatibility with IRRd version 2. Enabling this setting may have a
   performance impact on very large responses.
   |br| **Default**: ``false``, IPv6 members included.
   |br| **Change takes effect**: after SIGHUP, for all subsequent queries.
```

### Comparing `irrd-4.2.8/docs/admins/deployment.rst` & `irrd-4.3.0/docs/admins/deployment.rst`

 * *Files 13% similar despite different names*

```diff
@@ -16,33 +16,38 @@
    :local:
 
 Requirements
 ------------
 IRRd requires:
 
 * Linux, OpenBSD or MacOS. Other platforms are untested, but may work.
-* PyPy or CPython 3.6 through 3.9 with `pip` and `virtualenv` installed.
+* PyPy or CPython 3.7 through 3.11 with `pip` and `virtualenv` installed.
   PyPy is slightly recommended over CPython (the "default" Python interpreter),
   due to improved performance, in the order of 10% for some queries,
   and even higher for some GraphQL queries. However, CPython remains fully
   supported, and CPython may be the better option for you if it is easier to
   install on your deployment platform.
-* A recent version of PostgreSQL. Versions 9.6 and 10.5 have been
-  extensively tested.
+* A recent version of PostgreSQL. Versions 9.6, 11.16, 13.7, 15.0 are all
+  tested before release. 11 or higher is strongly recommended, due to faster
+  database migrations during upgrades.
+* Redis 5 or newer.
 * At least 32GB RAM
 * At least 4 CPU cores
 * At least 150GB of disk space (SSD recommended)
 
 Depending on your needs from IRRd, you may be able to run with
 fewer resources. This depends on how much data you intend to load,
 and your query load.
 
 A number of other Python packages are required. However, those are
 automatically installed in the installation process.
 
+You may need to install other packages on your OS to build IRRd's
+dependencies. That includes developer packages for Python, Redis
+and PostgreSQL. You will also need a Rust compiler.
 
 PostgreSQL configuration
 ------------------------
 IRRd requires a PostgreSQL database to store IRR and status data.
 Here's an example of how to correctly create the database and assign
 the necessary permissions, to be run as a superuser::
 
@@ -69,42 +74,44 @@
   sorting and merging.
 * ``shared_buffers`` should be set to around 1/8th - 1/4th of the system
   memory to benefit from caching, with a max of a few GB.
 * ``max_connections`` may need to be increased from 100. Generally, there
   will be one open connection for:
   * Each permitted whois connection
   * Each HTTP worker
-  * Each running mirror import and export process
-  * Each RPKI update process
+  * Each running mirror import or export process
+  * Each RPKI or scope filter update process
   * Each run of ``irrd_load_database``
   * Each run of ``irrd_submit_email``
+  * Each open WebSocket connection for the :doc:`event stream </users/queries/event-stream>`
+
 * ``log_min_duration_statement`` can be useful to set to ``0`` initially,
   to log all SQL queries to aid in debugging any issues.
   Note that initial imports of data produce significant logs if all queries
   are logged - importing 1GB of data can result in 2-3GB of logs.
 
 The transaction isolation level should be set to "Read committed". This is
 the default in PostgreSQL.
 
-The database will be in the order of three times as large as the size of
-the RPSL text imported.
+The initial database will be in the order of three times as large as the
+size of the RPSL text imported.
 
 .. important::
 
     The PostgreSQL database is the only source of IRRd's data.
     This means you need to run regular backups of the database.
     It is also possible to restore data from recent exports,
     but changes made since the most recent export will be lost.
 
 .. _deployment-redis-configuration:
 
 Redis configuration
 -------------------
 Redis is required for communication and persistence between IRRd's processes.
-IRRd has been tested on Redis 3 and 4.
+IRRd releases are tested on Redis 5, 6 and 7.
 Beyond a default Redis installation, it is recommended to:
 
 * Increase ``maxmemory`` to 1GB (no limit is also fine). This is a hard
   requirement - IRRd will exceed the default maximum memory otherwise.
 * Disable snapshotting, by removing all ``save`` lines from the
   Redis configuration. IRRd always reloads the existing data upon startup
   of restart of either IRRd or Redis, and therefore Redis persistence
@@ -254,43 +261,52 @@
 (plain HTTP to begin, HTTPS to follow)::
 
     http {
         include       mime.types;
         default_type  application/octet-stream;
 
         gzip on;
-        gzip_types application/json text/plain;
+        gzip_types application/json text/plain application/jsonl+json;
 
+        map $http_upgrade $connection_upgrade {
+            default upgrade;
+            ''      close;
+        }
         server {
             server_name  [your hostname];
             listen       80;
             listen       [::]:80;
 
             location / {
                 proxy_set_header Host $http_host;
                 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
                 proxy_set_header X-Forwarded-Proto $scheme;
+                proxy_set_header Upgrade $http_upgrade;
+                proxy_set_header Connection $connection_upgrade;
                 proxy_read_timeout 900;
                 proxy_connect_timeout 900;
                 proxy_send_timeout 900;
+                proxy_redirect off;
                 proxy_buffering off;
                 proxy_pass http://127.0.0.1:8000;
                 add_header Server $upstream_http_server;
             }
         }
     }
 
 Then, update this configuration to use HTTPS by running
 ``certbot --nginx``, which is available on most platforms,
 to generate the right certificates from LetsEncrypt and update the
 configuration to enable HTTPS, including redirects from plain HTTP.
 
 You can also use other services or your own configuration. You will likely
 need to increase some timeouts for slower queries. Enabling GZIP compression
-for ``text/plain`` and ``application/json`` responses is recommended.
+for ``text/plain``, ``application/json`` and
+``application/jsonl+json`` responses is recommended, for other responses
+compression should be disabled.
 If your service runs on a different host, set
 ``server.http.forwarded_allow_ips`` to let IRRd trust the
 ``X-Forwarded-For`` header.
 
 .. warning::
     While running the HTTP services over plain HTTP is possible, using
     HTTPS is strongly recommended, particularly so that clients can verify
@@ -336,16 +352,28 @@
     PIDFile=/home/irrd/irrd.pid  # must match piddir config in the settings
     ExecStart=/home/irrd/irrd-venv/bin/irrd --foreground
     Restart=on-failure
     ExecReload=/bin/kill -HUP $MAINPID
 
     [Install]
     WantedBy=multi-user.target
+    WantedBy=redis-server.service
+    WantedBy=postgresql@11-main.service
 
-You may need to update the PostgreSQL version if you are not using PosgreSQL 11.
+If you are not using PostgreSQL 11, you need to amend the service name
+``postgresql@11-main.service`` in both the ``Requires=`` and ``WantedBy=``
+directive.
+
+Please note that the combination of ``Requires=`` and ``WantedBy=`` in
+this unit file creates an indirect dependency between the service units
+of Redis and PostgreSQL, if ``irrd.service`` is enabled.
+This means that if you start either PostgreSQL or Redis, all three
+services are started, which might be somewhat surprising.
+This behaviour is needed for IRRd to be (re)started after (unattended)
+upgrades of PostgreSQL or Redis.
 
 Then, IRRd can be started under systemd with::
 
     systemctl daemon-reload
     systemctl enable irrd
     systemctl start irrd
```

### Comparing `irrd-4.2.8/docs/admins/faq.rst` & `irrd-4.3.0/docs/admins/faq.rst`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/docs/admins/migrating-legacy-irrd.rst` & `irrd-4.3.0/docs/admins/migrating-legacy-irrd.rst`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/docs/admins/object-validation.rst` & `irrd-4.3.0/docs/admins/object-validation.rst`

 * *Files 12% similar despite different names*

```diff
@@ -3,33 +3,33 @@
 =========================
 
 In general, IRRd follows RFC 2622, 2650, 2726, 2725, 4012 and 2769.
 However, most current IRR databases violate these RFCs in some
 ways, meaning some flexibility is needed.
 
 In addition to the validations as described below, IRRd supports
-an :doc:`RPKI-aware mode </admins/rpki>` where objects are also
-validated against ROAs.
+:doc:`object suppression </admins/object-suppression>` where objects are also
+filtered or validated against ROAs, a scope filter, or other route objects.
 
 
 General requirements
 --------------------
 The ``irrd.rpsl`` module deals with parsing and validation in general.
 The general requirements for validation are:
 
 * Any objects submitted to IRRd directly (i.e. not from mirrors)
   should always be entirely valid. If they are not, the end user
   can fix their object and continue from there.
-* The NTTCOM database has some legacy objects which require more
+* Many databases have some legacy objects which require more
   leniency with an initial import, but we aim to be as restrictive
   as reasonably possible. It should not be possible to update invalid
-  objects without correcting their issues.
+  authoritative objects without correcting their issues.
 * Mirrors contain wildly variant objects, so IRRd performs the minimal
   level of validation needed to correctly index and query them.
-* Under no condition may IRRd provide responses to any query, which
+* IRRd must never provide responses to any query, which
   are missing certain objects because indexed data could not be extracted
   from them, without logging errors about failing to import these objects.
 * If objects are received from mirrors that can not be accepted, e.g.
   a route object with an invalid prefix, the object will be ignored and
   IRRd will record errors in the logs.
 
 
@@ -96,29 +96,30 @@
 
 rpki-ov-state
 ^^^^^^^^^^^^^
 The ``rpki-ov-state`` attribute, which is used to indicate the
 :doc:`RPKI validation status </admins/rpki>`, is always discarded from all
 incoming objects. Where relevant, it is added to the output of queries.
 This applies to authoritative and non-authoritative sources.
+This attribute is not visible over NRTM and in exports.
 
 key-cert objects
 ^^^^^^^^^^^^^^^^
 In `key-cert` objects, the ``fingerpr`` and ``owner`` attributes are
 updated to values extracted from the PGP key. The ``method`` attribute is
 always set to PGP. This applies to objects from authoritative sources and
 sources for which ``strict_import_keycert_objects`` is set.
 
 .. _last-modified:
 
 last-modified
 ^^^^^^^^^^^^^
 For authoritative objects, the ``last-modified`` attribute is set when
 the object is created or updated. Any existing ``last-modified`` values are
-discarded. This timestamp is not updated for changes in RPKI validation
+discarded. This timestamp is not updated for changes in object suppression
 status. This attribute is visible over NRTM and in exports.
 
 By default, this attribute is only added when an object is changed or
 created. If you have upgraded to IRRd 4.1, you can use the
 ``irrd_set_last_modified_auth`` command to set it to the current time on
 all existing authoritative objects.
```

### Comparing `irrd-4.2.8/docs/admins/rpki.rst` & `irrd-4.3.0/docs/releases/4.1.0.rst`

 * *Files 25% similar despite different names*

```diff
@@ -1,186 +1,177 @@
-================
-RPKI integration
-================
-
-IRRd can operate in RPKI-aware mode, where it imports ROA objects and
-uses those to validate `route(6)` objects. IRRd also generates pseudo-IRR
-objects that represent ROA data.
-
-.. contents::
-   :backlinks: none
-
-Enabling RPKI-aware mode
-------------------------
-You can enable RPKI-aware mode by setting ``rpki.roa_source``
-to a URL of a ROA export in JSON format. RPKI-aware mode is **enabled**
-by default. To disable RPKI-aware mode, set this to ``null``.
-
-As soon as this is enabled and you (re)start IRRd or send a SIGHUP,
-IRRd will import the ROAs and mark any invalid existing `route(6)` as
-such in the database.
-
-Pseudo-IRR objects
-------------------
-IRRd creates a pseudo-IRR object for each ROA. These can be queried like
-any other IRR object, and are included in the output of queries like
-``!g``. Their source is set to ``RPKI``, and therefore it is invalid
-to add a regular IRR source with that name, when RPKI-aware mode
-is enabled. The ``RPKI`` source can be enabled or disabled like any
-other source with ``!s`` and ``-s``, and can be included in the
-``sources_default`` setting to be included in responses by default.
-
-Query responses
----------------
-By default, `route(6)` objects that conflict with a ROA are not included
-in any query response. This is determined using
-`RFC6811 origin validation <https://tools.ietf.org/html/rfc6811>`_ and
-applies to all query types.
-
-Query responses for the text of `route(6)` objects include a
-``rpki-ov-state`` attribute, showing the current status.
-This attribute is discarded from any objects submitted to IRRd,
-and omitted for pseudo-IRR objects.
-
-To aid in debugging, it is possible to include invalid objects in the
-response. The RPKI filter can be disabled for a connection with the
-``!fno-rpki-filter`` command. The filter is
-disabled only for ``!r`` queries and all RIPE style queries.
-
-Where validation takes place
-----------------------------
-* Every ``rpki.roa_import_timer``, IRRd re-imports the ROA file, and then
-  updates the validation status of all `route(6)` objects in the IRR database.
-  This ensures that the status is correct when ROAs are added or removed.
-* For each imported object from NRTM, periodic full imports, or manual data
-  loading, IRRd sets the validation status using the current known ROAs, both
-  on creations or changes.
-* IRRd checks creation or modification of objects in authoritative databases
-  against the ROAs, and rejects the objects when they are RPKI invalid.
-* Deletions of RPKI invalid object are permitted, both for authoritative
-  database and when receiving deletions over NRTM.
-* IRRd will always set objects from sources with
-  ``sources.{name}.rpki_excluded`` set to status not_found,
-  i.e. they are never regarded as RPKI invalid objects at any time.
-* Database exports and NRTM streams will not include RPKI invalid objects.
-* If the validation state changes, e.g. due to a new ROA, an NRTM ADD
-  or DEL is created in the journal.
-
-An example of validation in the context of mirroring: your IRRd
-mirrors source DEMO the authoritative source, you keep a local journal, and
-a third party mirrors DEMO from you. When the authoritative source for
-DEMO sends an NRTM ADD for an RPKI invalid route, that update is not
-recorded in your IRRd's local journal. The third party that mirrors from
-you will not see this ADD over NRTM.
-
-If a ROA is added the next day that results in the route being RPKI valid
-or not_found, an ADD is recorded in the local journal, and the third party
-can pick up the change from an NRTM query to your IRRd. If that ROA is
-deleted again, causing the route to return to RPKI invalid, a DEL is
-recorded in your local journal.
-
-Therefore, both the local state of your IRRd, and anyone mirroring from
-your IRRd, will be up to date with the RPKI status.
-This does not apply to excluded sources, whose objects are never seen
-as RPKI invalid.
-
-.. _rpki-notifications:
-
-Notifications
--------------
-If a route(6) object in an authoritative source is newly marked RPKI invalid,
-a notification may be sent to all contacts. Contacts are determined as any email
-address, of any tech-c and admin-c, on any mnt-by on the route object,
-combined with any mnt-nfy of any of those maintainers.
-Emails are aggregated, so a single address will receive a single email with
-all objects listed for which it is a contact.
-
-This behaviour is enabled or disabled with the ``rpki.notify_invalid_enabled``
-setting. If you have any authoritative sources configured, you must explicitly
-set this to ``true`` or ``false`` in your configuration.
-
-"Newly" invalid means that an object was previously valid or not_found, but
-a ROA update has changed the status to invalid. At the time this happens,
-the email is sent. If the status returns to valid or not_found, no email
-is sent. If it then returns to invalid, a new email is sent.
-
-When first enabling RPKI-aware mode, a large number of objects may be marked
-as newly invalid, which can cause a large amount of notifications.
-
-Notifications are never sent for objects from non-authoritative sources.
+============================
+Release notes for IRRd 4.1.0
+============================
+
+IRRd 4.1.0 adds several major new features, including
+:doc:`RPKI-aware mode </admins/rpki>`,
+a new daemon architecture with full multiprocessing, synthetic NRTM,
+and a scope filter.
+
+The changes between 4.0.x and 4.1.0 are major. Please read these release
+notes carefully before upgrading.
+Upgrading to IRRd 4.1.0 requires several changes to the deployment setup.
+Downgrading back to IRRd 4.0.x, if needed, also requires steps before
+downgrading the installed IRRd package.
+
+.. contents:: :backlinks: none
+
+New features
+------------
+
+RPKI-aware mode
+~~~~~~~~~~~~~~~
+:doc:`RPKI-aware mode </admins/rpki>` is now available, where IRRd
+imports RPKI ROAs and can filter or reject RPSL objects that are
+in conflict with ROAs. Pseudo-IRR objects are generated for all ROAs.
+When RPKI-aware mode is enabled, ``RPKI`` becomes an invalid as a regular
+IRR source name, as it is reserved for pseudo-IRR objects from ROAs.
+RPKI-aware mode also affects mirroring.
+
+IRRd 4.1.0 includes several database migrations to support RPKI-aware mode,
+whether enabled or disabled, and facilitate performance improvements needed
+for RPKI-aware mode. Running these migrations is required to run IRRd 4.1.0,
+even if RPKI-aware mode is never enabled.
 
 .. danger::
-    Care is required with the ``rpki.notify_invalid_enabled`` setting in testing
-    setups with live data, as it may send bulk emails to real resource contacts,
-    unless ``email.recipient_override`` is also set.
-
-First import with RPKI-aware mode
----------------------------------
-When you first enable RPKI-aware mode, the import and validation process
-will take considerably longer than on subsequent runs. On the first run,
-a large number of objects in the database need to be updated, whereas this
-number is much smaller on subsequent runs.
-Depending on ``rpki.notify_invalid_enabled``, many emails may be sent out
-as well. While the import and validation is running, processing other
-database changes may be delayed.
-
-The first full import after changing ``sources.{name}.rpki_excluded``
-may also be slower, for the same reason.
-
-.. note::
-    The first RPKI-aware import may also generate a significant amount
-    of local journal entries, which are used to generate NRTM responses
-    for anyone mirroring any source from your IRRd. Depending on the
-    sources, there may be up to 200.000 NRTM updates. It may be faster
-    to have mirrors reload their copy, as NRTM was not designed
-    for this volume.
+    The impact of running IRRd in RPKI-aware mode can be dramatic, and it is
+    strongly recommended to read the
+    :doc:`RPKI integration </admins/rpki>` documentation very carefully
+    before enabling it, to understand all consequences.
+    **By default, RPKI-aware mode is enabled**.
+    RPKI-aware mode can be disabled entirely, or certain sources can be
+    excluded from RPKI validation.
+
+New daemon architecture
+~~~~~~~~~~~~~~~~~~~~~~~
+IRRd has a new daemon architecture, where all whois queries, HTTP requests,
+mirror processes and the preloader run in their own process. This improves
+performance significantly, especially where many processes are running
+on servers with many cores. Previously, the entire IRRd process was limited
+to one CPU core.
+
+To communicate between processes, IRRd now requires a running Redis instance.
+The commands to start IRRd and several IRRd scripts have also changed.
+The ``--uid`` parameter is no longer supported.
+
+Synthetic NRTM
+~~~~~~~~~~~~~~
+The ``irrd_load_database`` command allowed loading RPSL data from local files,
+often used to load data generated by scripts or other systems. Data imported
+this way could not be mirrored over NRTM.
+
+In IRRd 4.1, the ``irrd_update_database`` command has been added. This
+supports periodically updating a source to the state in a particular file,
+and automatically generates journal entries for any differences, allowing
+NRTM mirroring. See the :doc:`mirroring documentation </users/mirroring>`
+for further details.
+
+Other changes
+~~~~~~~~~~~~~
+* A :doc:`scopefilter </admins/scopefilter>` has been added. This allows you
+  to filter RPSL objects matching certain prefixes and AS numbers from your
+  IRRd instance. By default, the scope filter is disabled.
+* The default for ``server.whois.max_connections`` has been reduced from 50
+  to 10. In 4.1, IRRd whois workers use considerably more memory, about 200 MB
+  each, and one worker is started for each permitted connection. Therefore,
+  at the default 10 connections, the whois processes use about 2 GB of memory,
+  at 50 connections, this is about 10 GB.
+* The ``last-modified`` attribute is set every time an object is created or
+  updated in an authoritative source. You can apply this to all existing
+  authoritative objects with the
+  :ref:`new irrd_set_last_modified_auth command <last-modified>`.
+* Serial handling in IRRd has changed for NRTM mirrors. If you mirror a
+  source over NRTM, and keep a local journal, IRRd used to keep these serials
+  identical. The NRTM ADD from the original source would be stored in the local
+  journal under the same serial number, unless it was ignored by the object
+  class filter.
+  In 4.1, if you enable RPKI-aware mode or the scope filter for a source, or
+  it has been enabled at any point since the last full reload, IRRd keeps its
+  own serial number range in the local journal for that source. This may be out
+  of sync with the NRTM source. Different IRRd instances mirroring from the
+  same NRTM source may have different serial numbers for the same change.
+  If neither RPKI-aware mode nor the scope filter is enabled, and hasn't been
+  since the last full reload, IRRd synchronises the local serials with the
+  NRTM source, the default behaviour in 4.0.
+  See the :ref`NRTM serial handling documentation <mirroring-nrtm-serials>`
+  for further details.
+* When users create `route(6)` objects in authoritative databases, IRRd
+  also verifies authorisation from
+  :ref:`related object maintainers <auth-related-mntners-route>`. This behaviour
+  is enabled by default, but can be disabled with the
+  ``auth.authenticate_related_mntners`` setting.
+* The ``!j`` command has changed, and now is exclusively used to check
+  mirroring status. It returns what the most recent serial processed from a
+  mirror is. For more extensive status information, like the local serials
+  in the journal,
+  :doc:`use the new !J command </users/queries/whois>`.
+* IRRd starts a maximum of three mirror processes at the same time,
+  to reduce peak loads. A further three, if needed, are started 15 seconds
+  later, regardless of whether the previous ones have finished.
+* HTTP(s) downloads are now supported for the ``sources.{name}.import_source``
+  and ``sources.{name}.import_serial_source`` settings.
+* A number of new configuration options were added, and some are required.
+  See the :doc:`configuration documentation </admins/configuration>` for more
+  information on these options.
+* RIPE style query responses now always end with two empty lines,
+  `consistent with the RIPE database`_.
+* A custom flexible logging config can now be set with the
+  ``log.logging_config_path``.
+* A timeout was added for FTP connections.
+* Memory usage during large RPSL imports has been reduced.
+* A bug was fixed where some invalid objects could cause parser exceptions.
 
-Temporary inconsistencies
--------------------------
-There are three situations that can cause temporary RPKI inconsistencies.
 
-First, when you enable RPKI-aware mode and **at the same time** add a new source,
-the objects for the new source may not have the correct RPKI status
-initially. This happens because in the new source import process, no ROAs
-are visible, and to the periodic ROA update, the objects in the new source
-are not visible yet. This situation automatically resolves itself upon
-the next periodic ROA update, but may cause objects that should be marked
-RPKI-invalid to be included in responses in the mean time.
-This issue only occurs when RPKI-aware mode is enabled for the first time,
-and at the same time a new source is added. At other times, the RPKI
-status of new sources will be correct.
-
-Second, when someone adds a ROA and a `route` object in a mirrored source,
-the ROA may not be imported by the time the `route` object is received
-over NRTM. The object may initially be marked as RPKI not_found, or, depending
-on the ROA change, as invalid. This will be resolved at the next ROA import.
-
-Third, when someone attempts to create a `route` object in an authoritative
-source and has just created or modified a ROA, the ROA may not have been
-imported yet. This can cause the object to be initially marked as RPKI
-not_found, or if the `route` is RPKI invalid without the ROA change,
-rejected for being invalid. This will be resolved at the next ROA import,
-allowing the user to create the `route`.
-When a user attempts to create any `route` that is RPKI invalid, the error
-messages includes a note of the configured ROA import time.
-
-.. _rpki-slurm:
-
-SLURM support
--------------
-IRRd supports `RFC8416`_ SLURM files to filter or amend the ROAs imported
-from ``rpki.roa_source``.
-The path to the SLURM file is set in ``rpki.slurm_source``. This supports
-HTTP(s), FTP or local file URLs, in ``file://<path>`` format.
-
-The ``prefixAssertions`` entries in the SLURM file are processed as if they
-were ROAs from ``rpki.roa_source``. This includes being used in RPKI
-validation and creating pseudo-IRR objects. Their trust anchor is set to
-"SLURM".
-
-The ``prefixFilters`` entries are used to filter the ROAs from
-``rpki.roa_source``. ROAs that match a filter are discarded. They are not
-considered in RPKI validation, and no pseudo-IRR objects are created.
+Steps required to upgrade
+-------------------------
+The following steps are required to upgrade to IRRd 4.1.0, regardless of
+whether RPKI-aware mode is enabled or not.
 
-The ``bgpsecFilters`` and ``bgpsecAssertions`` entries are ignored.
+* Disable all cron and e-mail triggered tasks. There should be no calls
+  to any IRRd scripts during the upgrade process.
+* Upgrade the IRRd package from within the virtualenv with
+  ``pip install -U irrd``
+* Install a Redis instance as documented in the
+  :ref:`deployment guide <deployment-redis-configuration>` and configure
+  the ``redis_url`` setting.
+* Note that unix sockets are strongly recommended over TCP sockets for both
+  PostgreSQL and Redis, for improved performance. The effect of this is more
+  significant with the new multi-process daemon architecture.
+* Set ``piddir`` to a directory where IRRd can write its PID file, ``irrd.pid``.
+* Run the database migrations, using the same command used to
+  :ref:`create the tables initially in deployment <deployment-database-upgrade>`.
+  **Important note**: some of the migrations change large amounts of data,
+  and may take up to 15-45 minutes to run in total. While the migrations are
+  running, IRRd should be shut down and any cron / e-mail triggered tasks
+  must be disabled. There must be no calls to ``irrd_submit_email`` or
+  ``irrd_load_database``.
+* Update any startup scripts or systemd for IRRd to call the new daemon process,
+  with the new command line arguments, and use ``setcap`` to allow IRRd to bind
+  to privileged ports: see the
+  :ref:`updated deployment guide <deployment-starting-irrd>`.
+* Remove the ``--irrd_pidfile`` parameter from calls to ``irrd_submit_email`` and
+  ``irrd_load_database``.
+* Ensure that RPKI-aware mode is configured as desired. By default it is
+  **enabled**.
+* Start IRRd and re-enable the cron / e-mail triggered tasks.
+* If you would like to set ``last-modified`` for existing authoritative
+  objects, use the
+  :ref:`new irrd_set_last_modified_auth command <last-modified>`.
+
+
+Downgrading from 4.1 to 4.0.x
+-----------------------------
+If you are running IRRd 4.1, and would like to downgrade back to 4.0.x,
+the database schema needs to be modified. You can either restore an older
+copy of your database, start with a fresh database, or use the database
+migrations.
+
+If you want to use the database migrations, run this command **before**
+downgrading your local package installation to 4.0.x::
+
+    /home/irrd/irrd-venv/bin/irrd_database_downgrade --version 28dc1cd85bdc
+
+If you would like to re-upgrade to 4.1 later on, you will need to run
+``irrd_database_upgrade`` again, as noted in the steps above.
+The downgrade migration typically takes a few seconds.
 
-.. _RFC8416: https://tools.ietf.org/html/rfc8416
+.. _consistent with the RIPE database: https://www.ripe.net/manage-ips-and-asns/db/support/documentation/ripe-database-query-reference-manual#2-0-querying-the-ripe-database
```

### Comparing `irrd-4.2.8/docs/admins/status_page.rst` & `irrd-4.3.0/docs/admins/status_page.rst`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/docs/conf.py` & `irrd-4.3.0/docs/conf.py`

 * *Files 2% similar despite different names*

```diff
@@ -44,15 +44,15 @@
 source_suffix = '.rst'
 
 # The master toctree document.
 master_doc = 'index'
 
 # General information about the project.
 project = u'IRRd'
-copyright = u'2018-2020, NTT Ltd.'
+copyright = u'2018-2023, NTT Ltd. and Reliably Coded B.V.'
 author = u"IRRd developers"
 
 # The version info for the project you're documenting, acts as replacement
 # for |version| and |release|, also used in various other places throughout
 # the built documents.
 #
 # The short X.Y version.
```

### Comparing `irrd-4.2.8/docs/development/architecture.rst` & `irrd-4.3.0/docs/development/architecture.rst`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/docs/development/development-setup.rst` & `irrd-4.3.0/docs/development/development-setup.rst`

 * *Files 2% similar despite different names*

```diff
@@ -85,14 +85,15 @@
 In addition to the tests, this project uses `mypy` for type checking and `flake8`
 for style checking. To run these, run::
 
     mypy irrd
     flake8
 
 If all is well, neither command should provide output.
+The versions of these tools may only work on newer CPython versions.
 
 Exclusions from checks
 ----------------------
 
 Code can be excluded from code coverage, and can be excluded from checks by
 `mypy` and `flake8`. This should be done in rare cases, where the quality of
 the code would suffer otherwise, and for tests where the risks are small and
```

### Comparing `irrd-4.2.8/docs/development/storage.rst` & `irrd-4.3.0/docs/development/storage.rst`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/docs/index.rst` & `irrd-4.3.0/docs/index.rst`

 * *Files 4% similar despite different names*

```diff
@@ -49,30 +49,40 @@
    :caption: For administrators
    :maxdepth: 1
 
    admins/deployment
    admins/configuration
    admins/availability-and-migration
    admins/migrating-legacy-irrd
-   admins/object-validation
    admins/status_page
+   admins/faq
+
+
+.. toctree::
+   :caption: Validation, suppression and suspension
+   :maxdepth: 1
+
+   admins/object-validation
+   admins/object-suppression
    admins/rpki
    admins/scopefilter
-   admins/faq
+   admins/route-object-preference
+   admins/suspension
 
 Running queries
 ---------------
 
 .. toctree::
    :caption: Running queries
    :maxdepth: 1
 
    users/queries/index
    users/queries/graphql
    users/queries/whois
+   users/queries/event-stream
 
 For end users
 -------------
 
 This documentation is mainly for end users, who are performing queries on IRRd
 instances, or trying to add objects to an instance, or running mirrors of
 an IRRd instance.
```

### Comparing `irrd-4.2.8/docs/make.bat` & `irrd-4.3.0/docs/make.bat`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/docs/releases/4.0.1.rst` & `irrd-4.3.0/docs/releases/4.0.1.rst`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/docs/releases/4.0.2.rst` & `irrd-4.3.0/docs/releases/4.0.2.rst`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/docs/releases/4.0.3.rst` & `irrd-4.3.0/docs/releases/4.0.3.rst`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/docs/releases/4.0.4.rst` & `irrd-4.3.0/docs/releases/4.0.4.rst`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/docs/releases/4.1.0.rst` & `irrd-4.3.0/docs/users/queries/event-stream.rst`

 * *Files 24% similar despite different names*

```diff
@@ -1,177 +1,214 @@
-============================
-Release notes for IRRd 4.1.0
-============================
-
-IRRd 4.1.0 adds several major new features, including
-:doc:`RPKI-aware mode </admins/rpki>`,
-a new daemon architecture with full multiprocessing, synthetic NRTM,
-and a scope filter.
-
-The changes between 4.0.x and 4.1.0 are major. Please read these release
-notes carefully before upgrading.
-Upgrading to IRRd 4.1.0 requires several changes to the deployment setup.
-Downgrading back to IRRd 4.0.x, if needed, also requires steps before
-downgrading the installed IRRd package.
+============
+Event Stream
+============
+
+IRRd offers an event stream over WebSocket with push messages for all changes
+to IRR objects. In the future, this may contain other events in IRRd.
+There is also an HTTP request to retrieve an initial copy of the IRR data
+for clients who want to maintain their own state.
+
+This may seem similar to :doc:`mirroring </users/mirroring>`, but there
+are several major differences:
+
+* Mirroring is completely separated by IRR source, where the event stream has
+  one stream per IRRd instance.
+* Mirroring is intended for large numbers of clients, where the event stream
+  does not scale as well in the current design.
+* The mirroring protocols are complex to implement, and are typically only
+  used between IRRd instances. The event stream uses WebSocket, JSON and
+  JSONL, making it easier to integrate with other systems.
+* The event stream does not require polling and is therefore easier to
+  implement with lower latency.
+
+
+Retrieval of all initial data is required for any clients that want to
+maintain their own local state of the IRR database. This is not needed
+for clients that are only interested in following changes.
+
+Access is controlled by the new ``server.http.event_stream_access_list``
+setting, and by default all access is denied.
+
+
+Initial retrieval over HTTPS
+----------------------------
+To retrieve all current IRR data, make a GET request to ``/v1/event-stream/initial/``
+on the IRRd instance. There are two optional GET parameters to filter the
+returned data:
+
+* ``sources``: a comma-separated list of source names
+* ``object_classes``: a comma-separated list of RPSL object classes
+
+Without parameters, all objects are returned.
+
+The return format is JSONL.
+The first line contains metadata about the response, e.g.::
+
+    {
+        "data_type": "irrd_event_stream_initial_download",
+        "sources_filter": [
+            "EXAMPLE"
+        ],
+        "object_classes_filter": [
+            "route6"
+        ],
+        "max_serial_global": 424242,
+        "last_change_timestamp": "2022-11-02T13:26:17.777893+00:00",
+        "generated_at": "2022-11-02T13:29:50.008351",
+        "generated_on": "host.example.net"
+    }
+
+The fields are:
+
+* ``data_type``: a fixed key to indicate the data type.
+* ``sources_filter`` and ``object_classes_filter``: the filters as set in
+  the GET parameters of the request.
+* ``max_serial_global``: the journal serial number of the most recent
+  change that was included in this data. Note that this is not an NRTM
+  serial - this serial is global for the entire journal of this IRRd instance.
+  May be ``null`` if there have been no changes.
+* ``last_change_timestamp``: the timestamp of the most recent change
+  that was included in this data. May be ``null`` if there have been
+  no changes.
+* ``generated_at`` and ``generated_on``: the timestamp at which the data
+  was generated and the hostname of the generator.
+
+The next lines contain IRR objects, e.g.::
+
+    {
+        "pk": "2001:db8::\/48AS65530",
+        "object_class": "route6",
+        "object_text": "route6: ...\norigin: ...\n",
+        "source": "RIPE",
+        "updated": "2022-05-24 22:18:22.085485+00",
+        "parsed_data": {...}
+    }
+
+Note that the request may take up to 10
+minutes, depending on the size of your database, and that it may take
+a few minutes for data to start streaming.
+
+If there is no data in the IRR database at all, ``max_serial_global``
+and ``last_change_timestamp`` will be ``null``, and no IRR records
+will be returned. If there are no (visible) objects for the selected
+sources, no IRR records will be returned, but these fields in the
+header will be filled.
 
-.. contents:: :backlinks: none
+.. danger::
+    This endpoint is not designed for frequent requests at this time.
 
-New features
-------------
 
-RPKI-aware mode
-~~~~~~~~~~~~~~~
-:doc:`RPKI-aware mode </admins/rpki>` is now available, where IRRd
-imports RPKI ROAs and can filter or reject RPSL objects that are
-in conflict with ROAs. Pseudo-IRR objects are generated for all ROAs.
-When RPKI-aware mode is enabled, ``RPKI`` becomes an invalid as a regular
-IRR source name, as it is reserved for pseudo-IRR objects from ROAs.
-RPKI-aware mode also affects mirroring.
-
-IRRd 4.1.0 includes several database migrations to support RPKI-aware mode,
-whether enabled or disabled, and facilitate performance improvements needed
-for RPKI-aware mode. Running these migrations is required to run IRRd 4.1.0,
-even if RPKI-aware mode is never enabled.
+WebSocket stream for changes
+----------------------------
 
-.. danger::
-    The impact of running IRRd in RPKI-aware mode can be dramatic, and it is
-    strongly recommended to read the
-    :doc:`RPKI integration </admins/rpki>` documentation very carefully
-    before enabling it, to understand all consequences.
-    **By default, RPKI-aware mode is enabled**.
-    RPKI-aware mode can be disabled entirely, or certain sources can be
-    excluded from RPKI validation.
-
-New daemon architecture
-~~~~~~~~~~~~~~~~~~~~~~~
-IRRd has a new daemon architecture, where all whois queries, HTTP requests,
-mirror processes and the preloader run in their own process. This improves
-performance significantly, especially where many processes are running
-on servers with many cores. Previously, the entire IRRd process was limited
-to one CPU core.
-
-To communicate between processes, IRRd now requires a running Redis instance.
-The commands to start IRRd and several IRRd scripts have also changed.
-The ``--uid`` parameter is no longer supported.
-
-Synthetic NRTM
-~~~~~~~~~~~~~~
-The ``irrd_load_database`` command allowed loading RPSL data from local files,
-often used to load data generated by scripts or other systems. Data imported
-this way could not be mirrored over NRTM.
-
-In IRRd 4.1, the ``irrd_update_database`` command has been added. This
-supports periodically updating a source to the state in a particular file,
-and automatically generates journal entries for any differences, allowing
-NRTM mirroring. See the :doc:`mirroring documentation </users/mirroring>`
-for further details.
-
-Other changes
-~~~~~~~~~~~~~
-* A :doc:`scopefilter </admins/scopefilter>` has been added. This allows you
-  to filter RPSL objects matching certain prefixes and AS numbers from your
-  IRRd instance. By default, the scope filter is disabled.
-* The default for ``server.whois.max_connections`` has been reduced from 50
-  to 10. In 4.1, IRRd whois workers use considerably more memory, about 200 MB
-  each, and one worker is started for each permitted connection. Therefore,
-  at the default 10 connections, the whois processes use about 2 GB of memory,
-  at 50 connections, this is about 10 GB.
-* The ``last-modified`` attribute is set every time an object is created or
-  updated in an authoritative source. You can apply this to all existing
-  authoritative objects with the
-  :ref:`new irrd_set_last_modified_auth command <last-modified>`.
-* Serial handling in IRRd has changed for NRTM mirrors. If you mirror a
-  source over NRTM, and keep a local journal, IRRd used to keep these serials
-  identical. The NRTM ADD from the original source would be stored in the local
-  journal under the same serial number, unless it was ignored by the object
-  class filter.
-  In 4.1, if you enable RPKI-aware mode or the scope filter for a source, or
-  it has been enabled at any point since the last full reload, IRRd keeps its
-  own serial number range in the local journal for that source. This may be out
-  of sync with the NRTM source. Different IRRd instances mirroring from the
-  same NRTM source may have different serial numbers for the same change.
-  If neither RPKI-aware mode nor the scope filter is enabled, and hasn't been
-  since the last full reload, IRRd synchronises the local serials with the
-  NRTM source, the default behaviour in 4.0.
-  See the :ref`NRTM serial handling documentation <mirroring-nrtm-serials>`
-  for further details.
-* When users create `route(6)` objects in authoritative databases, IRRd
-  also verifies authorisation from
-  :ref:`related object maintainers <auth-related-mntners>`. This behaviour
-  is enabled by default, but can be disabled with the
-  ``auth.authenticate_related_mntners`` setting.
-* The ``!j`` command has changed, and now is exclusively used to check
-  mirroring status. It returns what the most recent serial processed from a
-  mirror is. For more extensive status information, like the local serials
-  in the journal,
-  :doc:`use the new !J command </users/queries/whois>`.
-* IRRd starts a maximum of three mirror processes at the same time,
-  to reduce peak loads. A further three, if needed, are started 15 seconds
-  later, regardless of whether the previous ones have finished.
-* HTTP(s) downloads are now supported for the ``sources.{name}.import_source``
-  and ``sources.{name}.import_serial_source`` settings.
-* A number of new configuration options were added, and some are required.
-  See the :doc:`configuration documentation </admins/configuration>` for more
-  information on these options.
-* RIPE style query responses now always end with two empty lines,
-  `consistent with the RIPE database`_.
-* A custom flexible logging config can now be set with the
-  ``log.logging_config_path``.
-* A timeout was added for FTP connections.
-* Memory usage during large RPSL imports has been reduced.
-* A bug was fixed where some invalid objects could cause parser exceptions.
-
-
-Steps required to upgrade
--------------------------
-The following steps are required to upgrade to IRRd 4.1.0, regardless of
-whether RPKI-aware mode is enabled or not.
-
-* Disable all cron and e-mail triggered tasks. There should be no calls
-  to any IRRd scripts during the upgrade process.
-* Upgrade the IRRd package from within the virtualenv with
-  ``pip install -U irrd``
-* Install a Redis instance as documented in the
-  :ref:`deployment guide <deployment-redis-configuration>` and configure
-  the ``redis_url`` setting.
-* Note that unix sockets are strongly recommended over TCP sockets for both
-  PostgreSQL and Redis, for improved performance. The effect of this is more
-  significant with the new multi-process daemon architecture.
-* Set ``piddir`` to a directory where IRRd can write its PID file, ``irrd.pid``.
-* Run the database migrations, using the same command used to
-  :ref:`create the tables initially in deployment <deployment-database-upgrade>`.
-  **Important note**: some of the migrations change large amounts of data,
-  and may take up to 15-45 minutes to run in total. While the migrations are
-  running, IRRd should be shut down and any cron / e-mail triggered tasks
-  must be disabled. There must be no calls to ``irrd_submit_email`` or
-  ``irrd_load_database``.
-* Update any startup scripts or systemd for IRRd to call the new daemon process,
-  with the new command line arguments, and use ``setcap`` to allow IRRd to bind
-  to privileged ports: see the
-  :ref:`updated deployment guide <deployment-starting-irrd>`.
-* Remove the ``--irrd_pidfile`` parameter from calls to ``irrd_submit_email`` and
-  ``irrd_load_database``.
-* Ensure that RPKI-aware mode is configured as desired. By default it is
-  **enabled**.
-* Start IRRd and re-enable the cron / e-mail triggered tasks.
-* If you would like to set ``last-modified`` for existing authoritative
-  objects, use the
-  :ref:`new irrd_set_last_modified_auth command <last-modified>`.
-
-
-Downgrading from 4.1 to 4.0.x
------------------------------
-If you are running IRRd 4.1, and would like to downgrade back to 4.0.x,
-the database schema needs to be modified. You can either restore an older
-copy of your database, start with a fresh database, or use the database
-migrations.
-
-If you want to use the database migrations, run this command **before**
-downgrading your local package installation to 4.0.x::
-
-    /home/irrd/irrd-venv/bin/irrd_database_downgrade --version 28dc1cd85bdc
-
-If you would like to re-upgrade to 4.1 later on, you will need to run
-``irrd_database_upgrade`` again, as noted in the steps above.
-The downgrade migration typically takes a few seconds.
+The WebSocket stream is available on ``/v1/event-stream/`` and works as follows:
 
-.. _consistent with the RIPE database: https://www.ripe.net/manage-ips-and-asns/db/support/documentation/ripe-database-query-reference-manual#2-0-querying-the-ripe-database
+* The server sends a ``stream_status`` message with info on the status
+  of the local database.
+* The client sends a subscribe message.
+* The server streams messages of type ``rpsl_journal`` or ``event``.
+
+Stream status
+^^^^^^^^^^^^^
+The ``stream_status`` message includes these keys:
+
+* ``streamed_sources``: a list of sources for which this instance can
+  provide streaming data.
+* ``last_reload_times``: the timestamp per source of the last full reload.
+  See the section below for the relevance of this.
+
+Example::
+
+    {
+        "message_type": "stream_status",
+        "streamed_sources": ["EXAMPLE"],
+        "last_reload_times": {
+            "EXAMPLE": "2022-05-24T18:54:15.569301+00:00"
+        }
+    }
+
+Subscription
+^^^^^^^^^^^^
+To receive updates, the client must send a ``subscribe`` message, with
+``after_global_serial`` set to the journal-wide serial last seen by the client.
+The client will receive any journal entries after this serial.
+If the ``after_global_serial`` field is omitted, any changes newer
+than the subscription time are sent.
+Example::
+
+    {
+        "message_type": "subscribe",
+        "after_global_serial": 424242
+    }
+
+The ``after_global_serial`` value would typically be the
+``max_serial_global`` value from an initial file
+or the ``serial_global`` value from the most recently processed
+RPSL journal message.
+
+IRRd does not reply to a valid subscription message.
+
+RPSL journal
+^^^^^^^^^^^^
+The ``rpsl_journal`` message from IRRd contains an update to the RPSL journal.
+The message contains a key ``event_data`` which in turn contains:
+
+* ``operation``: the type of change, either ``add_or_update`` or ``delete``.
+* ``origin``: the reason for the update. Can include ``mirror`` for NRTM,
+  ``auth_change`` for authoritative submissions, ``rpki_status`` for a change
+  in RPKI validity.
+* ``timestamp``: the timestamp of the change.
+* ``serial_global``: the journal-wide serial of this change, i.e. the same
+  type of serial referred by ``max_serial_global`` in initial files
+  and ``after_global_serial`` in subscribe messages.
+* ``serial_nrtm``: the NRTM serial of this change, in the context of a single
+  IRR source.
+* ``pk``, ``object_class``, ``object_text``, ``source``, ``parsed_data``:
+  the RPSL primary key, object class full text, IRR source, and parsed
+  attribute values of the object. For ``add_or_update``, this is always the
+  new version of the object.
+
+Event
+^^^^^
+The ``event`` message contains other push events in IRRd.
+The message contains a key ``event_data`` which in turn contains:
+
+* ``source``: the IRR source to which this event applies.
+* ``operation``: the operation, either ``journal_extended`` or ``full_reload``.
+  When the journal is extended, this is followed by RPSL journal messages.
+  For full reload, see below.
+
+
+Full reloads
+------------
+The event stream is based on the internal IRRd journal. This journal
+includes all changes to IRR objects, when enabled, and therefore,
+taking an initial file and following updates will correctly reflect
+the current state of the database.
+
+However, this is not the case in "full reloads": when all records
+for a source are deleted from an IRRd instance, and IRRd performs a
+fresh reload of all objects. Operators typically due this for sources
+they are mirroring, when their mirror has run out of sync too far.
+
+If such a reload happens while you are following the event stream,
+you may miss changes to the database. To recover, you must delete
+your local data for this source, load the initial data, and then
+resume following the stream from that point.
+
+There are two ways for you to notice that this has happened:
+
+* The ``last_reload_times`` for a source in the ``stream_status``
+  message is more recent than your last full import from an
+  initial file.
+* You receive a ``event_rpsl`` message where the ``operation``
+  is ``full_reload``.
+
+
+Filtering
+---------
+Password hashes from `mntner` objects are removed in all output.
+
+:doc:`Suppressed objects </admins/object-suppression>` are omitted
+in the initial retrieval. Objects that change suppression status
+are included in the WebSocket stream as an add/delete, with the
+``origin`` indicating this reason.
```

### Comparing `irrd-4.2.8/docs/releases/4.1.2.rst` & `irrd-4.3.0/docs/releases/4.1.2.rst`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/docs/releases/4.1.3.rst` & `irrd-4.3.0/docs/releases/4.1.3.rst`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/docs/releases/4.1.4.rst` & `irrd-4.3.0/docs/releases/4.1.4.rst`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/docs/releases/4.1.5.rst` & `irrd-4.3.0/docs/releases/4.1.5.rst`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/docs/releases/4.1.7.rst` & `irrd-4.3.0/docs/releases/4.1.7.rst`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/docs/releases/4.1.8.rst` & `irrd-4.3.0/docs/releases/4.1.8.rst`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/docs/releases/4.2.0.rst` & `irrd-4.3.0/docs/releases/4.2.0.rst`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/docs/releases/4.2.1.rst` & `irrd-4.3.0/docs/releases/4.2.1.rst`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/docs/releases/4.2.2.rst` & `irrd-4.3.0/docs/releases/4.2.2.rst`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/docs/releases/4.2.3.rst` & `irrd-4.3.0/docs/releases/4.2.3.rst`

 * *Files 5% similar despite different names*

```diff
@@ -11,16 +11,14 @@
 perform a brute-force search for the clear-text passphrase, and use these
 to make unauthorised changes to affected IRR objects.
 
 This issue only affected instances that process password hashes, which means it
 is limited to IRRd instances that serve authoritative databases. IRRd instances
 operating solely as mirrors of other IRR databases are not affected.
 
-This issue was assigned CVE-2022-24798 and GHSA-cqxx-66wh-8pjw.
-
 Details
 -------
 In the IRR, authentication for object modification can be done through
 passwords. Hashes are stored in `mntner` objects for this purpose. IRRd
 should filter these hashes from the output before returning results,
 and replace them with dummy values.
```

### Comparing `irrd-4.2.8/docs/releases/4.2.4.rst` & `irrd-4.3.0/docs/releases/4.2.4.rst`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/docs/releases/4.2.5.rst` & `irrd-4.3.0/docs/releases/4.2.5.rst`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/docs/releases/4.2.6.rst` & `irrd-4.3.0/docs/releases/4.2.6.rst`

 * *Files 2% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 ============================
 Release notes for IRRd 4.2.6
 ============================
 
 IRRd 4.2.6 was released on November 18th, 2022, and fixes two issues:
 
 * IRRD whois connections could `stay stuck in FIN_WAIT2`_ state,
-  which may also have lead to `workers to stop responding`_ to
+  which may also have led to `workers to stop responding`_ to
   queries in some cases. This was addressed by adding a missing
   timeout to read calls on these connections.
 * A few small changes were made to improve unclear log messages.
 
 .. _stay stuck in FIN_WAIT2: https://github.com/irrdnet/irrd/issues/607
 .. _workers to stop responding: https://github.com/irrdnet/irrd/issues/693
```

### Comparing `irrd-4.2.8/docs/releases/4.2.7.rst` & `irrd-4.3.0/docs/releases/4.2.7.rst`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/docs/spelling_wordlist.txt` & `irrd-4.3.0/docs/spelling_wordlist.txt`

 * *Files 21% similar despite different names*

```diff
@@ -1,9 +1,11 @@
 ASes
+aut
 CPython
+customisable
 Escribano
 IPtables
 IPv
 IRR
 Mypy
 Pre
 RPSL
@@ -17,22 +19,27 @@
 balancers
 boolean
 bugfix
 conf
 config
 cron
 daemonised
+desynchronise
 dev
 enums
 gpg
 gzipped
+hasher
+hashers
 hostname
 http
 incrementing
+irr
 irrd
+integrations
 journaling
 keepalive
 keychain
 logfile
 loglevel
 logrotate
 mnt
@@ -42,27 +49,29 @@
 multirow
 nd
 nfy
 nginx
 noreply
 nrtm
 ntt
+num
 ov
 pidfile
 postgres
 pre
 preload
 preloaded
 preloader
 preloading
 preloads
 pubsub
 py
 rc
 reStructuredText
+reactivations
 resolvers
 rpki
 rpki-ov-state
 rpsl
 rr
 schemas
 setcap
```

### Comparing `irrd-4.2.8/docs/users/database-changes.rst` & `irrd-4.3.0/docs/users/database-changes.rst`

 * *Files 13% similar despite different names*

```diff
@@ -7,21 +7,21 @@
 checks are performed.
 Additionally, notifications may be sent on attempted or successful changes.
 
 .. highlight:: yaml
 
 Submission format
 -----------------
-There are two ways to submit changes:
+There are two ways to submit changes directly to IRRd:
 
-* By sending an e-mail with the RPSL objects. This method supports MD5-PW,
-  CRYPT-PW and PGPKEY authentication. You will receive a reply by e-mail
-  with the result.
-* Over HTTPS, through a REST API. This method supports MD5-PW and CRYPT-PW
-  authentication. You receive the results in the HTTP response.
+* By sending an e-mail with the RPSL objects. This method supports BCRYPT-PW,
+  MD5-PW, CRYPT-PW and PGPKEY authentication. You will receive a reply by
+  e-mail with the result.
+* Over HTTPS, through a REST API. This method supports BCRYPT-PW, MD5-PW and
+  CRYPT-PW authentication. You receive the results in the HTTP response.
 
 All objects submitted are validated for the presence, count and syntax,
 though the syntax validation is limited for some attributes.
 Values like prefixes are also rewritten into a standard format. If this
 results in changes compared to the original submitted text, an info message
 is added in the response.
 
@@ -76,14 +76,16 @@
                     }
                 ]
             }
         ],
         'passwords': ['password1', 'password2']
     }
 
+.. _database-changes-http-api-response:
+
 There are two possible responses:
 
 * If there is a syntax error in your JSON object, you will receive
   a ``text/plain`` response with status code 400. The response will
   tell you what the issue is with your JSON.
 * If the request was syntactically valid, you always receive a
   ``text/json`` response with status code 200, and the details of
@@ -134,15 +136,14 @@
     }
 
 The order of the ``objects`` in the response matches the order
 of ``objects`` in your request.
 
 Submitting over e-mail
 ^^^^^^^^^^^^^^^^^^^^^^
-
 The e-mail destination is configured by the IRRd administrator.
 Both ``text/plain`` e-mails as well as MIME multipart messages with
 a ``text/plain`` part are accepted.
 
 The message content should be the object texts, each separated by an empty
 line. If no objects exist with the same primary key, an object creation
 is attempted. If an object does exist, an update is attempted.
@@ -168,14 +169,47 @@
 You may submit multiple passwords, and each password will be considered
 for each authentication check.
 
 For PGP authentication, sign your message with a PGP/MIME signature
 or inline PGP. You can combine PGP signatures and passwords, and each method
 will be considered for each authentication check.
 
+.. _database-changes-irr-rpsl-submit:
+
+Submission through irr_rpsl_submit
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+You can also use the ``irr_rpsl_submit`` command to submit changes to IRRd.
+It is similar to the submit tool from IRRD v3, but calls the HTTP API under
+the hood. So unlike IRRD v3's version, it does not perform any validation
+itself - it is mostly a wrapper around the HTTP API.
+
+The main purpose of this script is to provide (limited) compatibility
+with existing integrations that called irr_rpsl_submit directly to submit
+to older IRRd versions. The IRRD v4 version does need different arguments.
+
+The command reads database objects from stdin in the same format as used
+in emails and prints a report to stdout.
+You must provide a URL to the IRRd HTTP API, and may enable
+debug logging or pass extra metadata.
+
+.. warning::
+   The input should not include any email headers. It is not recommended
+   to use this script to handle incoming email changes - see the
+   `deployment guide </admins/deployment>`_ for the ``irrd_submit_email``
+   instead.
+
+This command is included in the IRRd distribution, but is also
+`usable as a separate Python script for Python 3.7 or newer <../../_static/irr_rpsl_submit.py>`_.
+This script does not have
+any dependencies on IRRd or other Python libraries to make deployment
+on other hosts easier. You do not need a virtualenv, IRRd config file or
+SQL database on hosts that only run ``irr_rpsl_submit``. You can run this
+script on its own with any supported Python interpreter.
+
+
 Override password
 -----------------
 An IRRd administrator can configure an override password.
 This bypasses all authentication requirements.
 Even with the override password, changes can only be made to objects in
 authoritative databases, and will need to pass checks for syntax and
 referential integrity like any other change.
@@ -210,29 +244,29 @@
 
 Working with auth hash masking
 ------------------------------
 When querying for a `mntner` object, any lines with password hashes are
 masked for security reasons. For example::
 
     mntner: EXAMPLE-MNT
-    auth: CRYPT-PW DummyValue  # Filtered for security
+    auth: BCRYPT-PW DummyValue  # Filtered for security
     auth: MD5-PW DummyValue  # Filtered for security
     auth: PGPKEY-12345678
 
 When you submit a new `mntner` object, it must include at least one valid
 `auth` value, which can not be a dummy value.
 
 When you submit changes to an existing `mntner` object, there are two options:
 
 * Submit without any dummy values in `auth` values. If otherwise valid, the
   `auth` lines submitted will now be the only valid authentication methods.
 * Submit with exclusively dummy values (and optionally, PGP keys) and provide
   a single password in the entire submission. In this case, all password
   authentication hashes are deleted from the object, except for a single
-  MD5-PW that matches the password used to authenticate the change.
+  BCRYPT-PW that matches the password used to authenticate the change.
 
 Any other scenario, like submitting a mix of dummy and real hashes, or
 submitting dummy hashes along with multiple ``password`` attributes in
 the message, is considered an error.
 
 
 Referential integrity
@@ -265,28 +299,55 @@
 
 You can only make changes to objects in authoritative databases.
 
 When you create a new `mntner`, a submission must pass authorisation for
 one of the auth methods of the new mntner. You can submit other objects
 that depend on the new `mntner` in the same submission.
 
-.. _auth-related-mntners:
+.. _auth-related-mntners-route:
 
+Related maintainers in route objects
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 When you create new `route(6)` objects, authentication also needs to pass
 for the parent object. IRRd searches for the parent object in the following
-order, only considering the first match:
+order, only considering the first match, only looking in the same IRR source:
 
 * An `inet(6)num` that is an exact match to the new `route(6)`.
 * The smallest `inet(6)num` that is a less specific of the new `route(6)`.
 * The smallest `route(6)` that is a less specific of the new `route(6)`.
 
 If no objects match, there is no parent object, and there are no extra
 authentication requirements.
 This behaviour can be disabled by setting
-``auth.authenticate_related_mntners`` to false.
+``auth.authenticate_parents_route_creation`` to false.
+These requirements do not apply when you change or delete existing objects.
+
+.. _auth-related-mntners-set:
+
+Related maintainers in set objects
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+When you create new set objects, you may need to pass authentication for the
+parent `aut-num` object.
+RPSL set objects are `as-set`, `filter-set`, `peering-set`, `route-set` and
+`rtr-set`.
+
+The details of this behaviour and the strictness of the checks are
+:ref:`configured by the IRR operator <conf-auth-set-creation>`. This may
+include a requirement to:
+
+* Include an ASN prefix in the name of your set, e.g. ``AS65537:AS-EXAMPLE``
+  being valid, but ``AS-EXAMPLE`` being invalid.
+* Pass authentication for the corresponding `aut-num`, e.g. AS65537 in the
+  example, skipping this check if the `aut-num` does not exist.
+* Pass authentication for the corresponding `aut-num`, e.g. AS65537 in the
+  example, failing this check if the `aut-num` does not exist.
+
+These requirements do not apply when you change or delete existing objects.
+When looking for corresponding `aut-num` objects,
+IRRd only looks in the same IRR source.
 
 Object templates
 ----------------
 
 You can use the ``-t`` query to get the object template for a particular
 object class. This includes which attributes are permitted, which are
 mandatory, look-up keys, primary keys and references to other objects.
@@ -319,30 +380,36 @@
 * The primary key is the `route` combined with the `origin`. Only
   one object with the same values for the primary key and source can exist.
   Any change submitted with the same primary key, will be considered an
   attempt to update the current object.
 * The `member-of` attribute is a look-up key, meaning it can be used with
   ``-i`` queries.
 * The `member-of` attribute references to the `route-set` class. It is a
-  weak references, meaning the referred `route-set` does not have to exist,
+  weak reference, meaning the referred `route-set` does not have to exist,
   but is required to meet the syntax of a `route-set` name. The attribute
   is also optional, so it can be left out entirely.
 * The `admin-c` and `tech-c` attributes reference a `role` or `person`.
   This means they may refer to either object class, but must be a
-  reference to a valid, existing `person` or `role. This `person` or
+  reference to a valid, existing `person` or `role`. This `person` or
   `role` can be created as part of the same submission.
 
 
 Notifications
 -------------
 IRRd will always reply to a submission with a report on the requested
 changes. Depending on the request and its result, additional notifications
 may be sent. The overview below details all notifications that may be
 sent.
 
+IRRd collects some metadata for each request, which is included in
+notifications to maintainers and written to the server logs. For emails,
+this includes the from, date, subject and message ID headers
+For HTTP requests (including ``irr_rpsl_submit``) this includes the source IP,
+user agent and x-irrd-metadata header content.
+
 
 Authentication and notification overview
 ----------------------------------------
 
 .. list-table::
    :header-rows: 1
    :widths: 20 20 60
```

### Comparing `irrd-4.2.8/docs/users/mirroring.rst` & `irrd-4.3.0/docs/users/mirroring.rst`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/docs/users/queries/graphql.rst` & `irrd-4.3.0/docs/users/queries/graphql.rst`

 * *Files 2% similar despite different names*

```diff
@@ -462,14 +462,15 @@
         ipLessSpecific: IP
         ipLessSpecificOneLevel: IP
         ipMoreSpecific: IP
         ipAny: IP
         asn: [ASN!]
         rpkiStatus: [RPKIStatus!]
         scopeFilterStatus: [ScopeFilterStatus!]
+        routePreferenceStatus: [RoutePreferenceStatus!]
         textSearch: String
         recordLimit: Int
         sqlTrace: Boolean
       ): [RPSLObject!]
       ...
     }
 
@@ -490,14 +491,18 @@
   statuses in the database. If omitted, the default is to filter on
   not_found and valid objects. Valid values are defined in the ``RPKIStatus``
   enum in the schema.
 * ``scopeFilterStatus``: filter on objects that have one of these scope filter
   statuses in the database. If omitted, the default is to filter on
   in_scope objects. Valid values are defined in the ``ScopeFilterStatus``
   enum in the schema.
+* ``routePreferenceStatus``: filter on objects that have one of these
+  route preference statuses in the database.
+  If omitted, the default is to filter on visible objects. Valid values are
+  defined in the ``RoutePreferenceStatus`` enum in the schema.
 * ``recordLimit``: limits the query to return this many results. Related
   object query results (explained in detail later) do not count towards
   this limit.
 
 Most arguments expect an array, and this is interpreted as an OR query.
 The separate arguments are joined as an AND query.
 For example, this query::
@@ -858,16 +863,16 @@
 IRRd has a few custom types for specific purposes:
 
 * The ``ASN`` scalar. This is presented and validated as an integer, but
   GraphQL's built-in ``Int`` type is 32-bit signed, and therefore not
   sufficient.
 * The ``IP`` scalar. This is presented as a string. When used in query
   arguments, the value is validated to be a valid IP address or prefix.
-* The enums ``RPKIStatus`` and ``ScopeFilterStatus`` for querying and
-  returning these statuses on RPSL objects.
+* The enums ``RPKIStatus``, ``ScopeFilterStatus``, and ``RoutePreferenceStatus``
+  for querying and returning these statuses on RPSL objects.
 
 Tips
 ----
 * The fields you can query in ``rpslObjects`` only include fields IRRd knows
   about. For example, ``sponsoring-org`` is a RIPE-specific field, and not
   processed by IRRd. Therefore, you can't query it as a field. It will be
   included in the ``objectText`` field.
```

### Comparing `irrd-4.2.8/docs/users/queries/index.rst` & `irrd-4.3.0/docs/users/queries/index.rst`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/docs/users/queries/whois.rst` & `irrd-4.3.0/docs/users/queries/whois.rst`

 * *Files 2% similar despite different names*

```diff
@@ -94,14 +94,18 @@
 
   * ``authoritative``: true if this source is authoritative in this IRRd
     instance, i.e. whether local changes are allowed. False if the source
     is mirrored from elsewhere.
   * ``object_class_filter``: may be a list of object classes that are
     ignored by this IRRd instance, when mirroring from a remote source.
   * ``rpki_rov_filter``: whether RPKI validation is enabled for this source.
+  * ``scopefilter_enabled``: whether the scope filter is enabled on this instance,
+    and is also enabled for this source.
+  * ``route_preference``: the route order preference setting for this source,
+    if any is set.
   * ``local_journal_kept``: whether this IRRd instance keeps a local journal
     of the changes in this source, allowing it to be mirrored over NRTM.
   * ``serial_oldest_journal`` / ``serial_newest_journal``: the oldest and
     newest serials in the local journal on this IRRd instance for this source.
     IRRd does not guarantee that all changes in this range are available over
     NRTM. This serial range is entirely independent of that used by the
     mirror source, if any.
@@ -141,20 +145,18 @@
     including exact matches, and return them
   * ``M``, e.g. ``!r192.0.2.0/24,M``, to find one level more specific objects,
     excluding exact matches, and return them
 * ``!s<sources>`` restricts all responses to a specified list of sources,
   comma-separated, e.g. ``!sRIPE,NTTCOM``. In addition, ``!s-lc`` returns the
   sources currently selected. This persists across queries.
 * ``!v`` returns the current version of IRRd
-* ``!fno-rpki-filter`` disables filtering RPKI invalid routes. If
-  :doc:`RPKI-aware mode is enabled </admins/rpki>`, `route(6)` objects that
-  are RPKI invalid are not included in the output of any query by default.
-  After using ``!fno-rpki-filter``, this filter is disabled for the remainder of
-  the connection. Disabling the filter only applies to ``!r`` queries and
-  all RIPE style queries. This is only intended as a debugging aid.
+* ``!fno-rpki-filter``, ``!fno-scope-filter``, and ``!fno-route-preference-filter``
+  disables the filtering of :doc:`suppressed objects </admins/object-suppression>`
+  for the remainder of the connection. Disabling the filter only applies to ``!r``
+  queries and all RIPE style queries. This is only intended as a debugging aid.
 * ``!fno-scope-filter`` disables filtering out-of-scope objects. If
   the scope filter is enabled, objects that are
   :doc:`out of scope </admins/scopefilter>` are not included in the output of any query by default.
   After using ``!fno-scope-filter``, this filter is disabled for the remainder of
   the connection. Disabling the filter only applies to ``!r`` queries and
   all RIPE style queries. This is only intended as a debugging aid.
```

### Comparing `irrd-4.2.8/irrd/conf/__init__.py` & `irrd-4.3.0/irrd/conf/__init__.py`

 * *Files 17% similar despite different names*

```diff
@@ -9,135 +9,55 @@
 from typing import Any, List, Optional
 
 import yaml
 from IPy import IP
 
 from irrd.vendor.dotted.collection import DottedDict
 
-CONFIG_PATH_DEFAULT = '/etc/irrd.yaml'
+CONFIG_PATH_DEFAULT = "/etc/irrd.yaml"
 
 logger = logging.getLogger(__name__)
-PASSWORD_HASH_DUMMY_VALUE = 'DummyValue'
-SOURCE_NAME_RE = re.compile('^[A-Z][A-Z0-9-]*[A-Z0-9]$')
-RPKI_IRR_PSEUDO_SOURCE = 'RPKI'
+PASSWORD_HASH_DUMMY_VALUE = "DummyValue"
+SOURCE_NAME_RE = re.compile("^[A-Z][A-Z0-9-]*[A-Z0-9]$")
+RPKI_IRR_PSEUDO_SOURCE = "RPKI"
+ROUTEPREF_IMPORT_TIME = 3600
+AUTH_SET_CREATION_COMMON_KEY = "COMMON"
 SOCKET_DEFAULT_TIMEOUT = 30
 
-# Note that sources are checked separately,
-# and 'access_lists' is always permitted
-KNOWN_CONFIG_KEYS = DottedDict({
-    'database_url': {},
-    'database_readonly': {},
-    'redis_url': {},
-    'piddir': {},
-    'user': {},
-    'group': {},
-    'server': {
-        'http': {
-            'interface': {},
-            'port': {},
-            'status_access_list': {},
-            'workers': {},
-            'forwarded_allowed_ips': {},
-        },
-        'whois': {
-            'interface': {},
-            'port': {},
-            'access_list': {},
-            'max_connections': {},
-        },
-    },
-    'email': {
-        'from': {},
-        'footer': {},
-        'smtp': {},
-        'recipient_override': {},
-        'notification_header': {},
-    },
-    'auth': {
-        'override_password': {},
-        'authenticate_related_mntners': {},
-        'gnupg_keyring': {},
-    },
-    'rpki': {
-        'roa_source': {},
-        'roa_import_timer': {},
-        'slurm_source': {},
-        'pseudo_irr_remarks': {},
-        'notify_invalid_enabled': {},
-        'notify_invalid_subject': {},
-        'notify_invalid_header': {},
-    },
-    'scopefilter': {
-        'prefixes': {},
-        'asns': {},
-    },
-    'log': {
-        'logfile_path': {},
-        'level': {},
-        'logging_config_path': {},
-    },
-    'sources_default': {},
-    'compatibility': {
-        'inetnum_search_disabled': {},
-        'irrd42_migration_in_progress': {},
-        'permit_non_hierarchical_as_set_name': {},
-        'ipv4_only_route_set_members': {},
-    }
-})
-
-KNOWN_SOURCES_KEYS = {
-    'authoritative',
-    'keep_journal',
-    'nrtm_host',
-    'nrtm_port',
-    'import_source',
-    'import_serial_source',
-    'import_timer',
-    'object_class_filter',
-    'export_destination',
-    'export_destination_unfiltered',
-    'export_timer',
-    'nrtm_access_list',
-    'nrtm_access_list_unfiltered',
-    'nrtm_query_serial_range_limit',
-    'strict_import_keycert_objects',
-    'rpki_excluded',
-    'scopefilter_excluded',
-}
 
 LOGGING = {
-    'version': 1,
-    'disable_existing_loggers': False,
-    'formatters': {
-        'verbose': {
-            'format': '%(asctime)s irrd[%(process)d]: [%(name)s#%(levelname)s] %(message)s'
-        },
+    "version": 1,
+    "disable_existing_loggers": False,
+    "formatters": {
+        "verbose": {"format": "%(asctime)s irrd[%(process)d]: [%(name)s#%(levelname)s] %(message)s"},
     },
-    'handlers': {
-        'console': {
-            'class': 'logging.StreamHandler',
-            'formatter': 'verbose'
-        },
+    "handlers": {
+        "console": {"class": "logging.StreamHandler", "formatter": "verbose"},
     },
-    'loggers': {
+    "loggers": {
         # Tune down some very loud and not very useful loggers from libraries.
-        'passlib.registry': {
-            'level': 'INFO',
+        "passlib.registry": {
+            "level": "INFO",
+        },
+        "gnupg": {
+            "level": "INFO",
         },
-        'gnupg': {
-            'level': 'INFO',
+        # Must be specified explicitly to disable tracing middleware,
+        # which adds substantial overhead
+        "uvicorn.error": {
+            "level": "INFO",
         },
-        'sqlalchemy': {
-            'level': 'WARNING',
+        "sqlalchemy": {
+            "level": "WARNING",
         },
-        '': {
-            'handlers': ['console'],
-            'level': 'INFO',
+        "": {
+            "handlers": ["console"],
+            "level": "INFO",
         },
-    }
+    },
 }
 
 
 logging.config.dictConfig(LOGGING)
 logging.Formatter.converter = time.gmtime
 
 
@@ -152,78 +72,84 @@
 
 
 class Configuration:
     """
     The Configuration class stores the current IRRD configuration,
     checks the validity of the settings, and offers graceful reloads.
     """
+
     user_config_staging: DottedDict
     user_config_live: DottedDict
 
-    def __init__(self, user_config_path: Optional[str]=None, commit=True):
+    def __init__(self, user_config_path: Optional[str] = None, commit=True):
         """
         Load the default config and load and check the user provided config.
         If a logfile was specified, direct logs there.
         """
+        from .known_keys import KNOWN_CONFIG_KEYS, KNOWN_SOURCES_KEYS
+
+        self.known_config_keys = KNOWN_CONFIG_KEYS
+        self.known_sources_keys = KNOWN_SOURCES_KEYS
         self.user_config_path = user_config_path if user_config_path else CONFIG_PATH_DEFAULT
-        default_config_path = str(Path(__file__).resolve().parents[0] / 'default_config.yaml')
-        default_config_yaml = yaml.safe_load(open(default_config_path))
-        self.default_config = DottedDict(default_config_yaml['irrd'])
+        default_config_path = str(Path(__file__).resolve().parents[0] / "default_config.yaml")
+        with open(default_config_path) as default_config:
+            default_config_yaml = yaml.safe_load(default_config)
+        self.default_config = DottedDict(default_config_yaml["irrd"])
         self.logging_config = LOGGING
 
         errors = self._staging_reload_check(log_success=False)
         if errors:
-            raise ConfigurationError(f'Errors found in configuration, unable to start: {errors}')
+            raise ConfigurationError(f"Errors found in configuration, unable to start: {errors}")
 
         if commit:
             self._commit_staging()
 
-            logging_config_path = self.get_setting_live('log.logging_config_path')
-            logfile_path = self.get_setting_live('log.logfile_path')
+            logging_config_path = self.get_setting_live("log.logging_config_path")
+            logfile_path = self.get_setting_live("log.logfile_path")
             if logging_config_path:
                 spec = importlib.util.spec_from_file_location("logging_config", logging_config_path)
                 config_module = importlib.util.module_from_spec(spec)  # type: ignore
                 spec.loader.exec_module(config_module)  # type: ignore
                 self.logging_config = config_module.LOGGING  # type: ignore
                 logging.config.dictConfig(self.logging_config)
             elif logfile_path:
-                LOGGING['handlers']['file'] = {   # type:ignore
-                    'class': 'logging.handlers.WatchedFileHandler',
-                    'filename': logfile_path,
-                    'formatter': 'verbose',
+                LOGGING["handlers"]["file"] = {  # type:ignore
+                    "class": "logging.handlers.WatchedFileHandler",
+                    "filename": logfile_path,
+                    "formatter": "verbose",
                 }
                 # noinspection PyTypeChecker
-                LOGGING['loggers']['']['handlers'] = ['file']   # type:ignore
+                LOGGING["loggers"][""]["handlers"] = ["file"]  # type:ignore
                 logging.config.dictConfig(LOGGING)
 
             # Re-commit to apply loglevel
             self._commit_staging()
 
-    def get_setting_live(self, setting_name: str, default: Any=None) -> Any:
+    def get_setting_live(self, setting_name: str, default: Optional[Any] = None) -> Any:
         """
         Get a setting from the live config.
         In order, this will look in:
         - A env variable, uppercase and dots replaced by underscores, e.g.
           IRRD_SERVER_WHOIS_INTERFACE
         - The testing_overrides DottedDict
         - The live user config.
         - The default config.
 
         If it is not found in any, the value of the default paramater
         is returned, which is None by default.
         """
-        if setting_name.startswith('sources'):
-            components = setting_name.split('.')
-            if len(components) == 3 and components[2] not in KNOWN_SOURCES_KEYS:
-                raise ValueError(f'Unknown setting {setting_name}')
-        elif not setting_name.startswith('access_lists'):
-            if KNOWN_CONFIG_KEYS.get(setting_name) is None:
-                raise ValueError(f'Unknown setting {setting_name}')
+        if setting_name.startswith("sources"):
+            components = setting_name.split(".")
+            if len(components) == 3 and components[2] not in self.known_sources_keys:
+                raise ValueError(f"Unknown setting {setting_name}")
+        elif not setting_name.startswith("access_lists"):
+            if self.known_config_keys.get(setting_name) is None:
+                raise ValueError(f"Unknown setting {setting_name}")
 
-        env_key = 'IRRD_' + setting_name.upper().replace('.', '_')
+        env_key = "IRRD_" + setting_name.upper().replace(".", "_")
         if env_key in os.environ:
             return os.environ[env_key]
         if testing_overrides:
             try:
                 return testing_overrides[setting_name]
             except KeyError:
                 pass
@@ -234,212 +160,290 @@
 
     def reload(self) -> bool:
         """
         Reload the configuration, if it passes the checks.
         """
         errors = self._staging_reload_check()
         if errors:
-            logger.error(f'Errors found in configuration, continuing with current settings: {errors}')
+            logger.error(f"Errors found in configuration, continuing with current settings: {errors}")
             return False
 
         self._commit_staging()
         return True
 
     def _commit_staging(self) -> None:
         """
         Activate the current staging config as the live config.
         """
         self.user_config_live = self.user_config_staging
-        logging.getLogger('').setLevel(self.get_setting_live('log.level', default='INFO'))
-        if hasattr(sys, '_called_from_test'):
-            logging.getLogger('').setLevel('DEBUG')
+        logging.getLogger("").setLevel(self.get_setting_live("log.level", default="INFO"))
+        if hasattr(sys, "_called_from_test"):
+            logging.getLogger("").setLevel("DEBUG")
 
     def _staging_reload_check(self, log_success=True) -> List[str]:
         """
         Reload the staging configuration, and run the config checks on it.
         Returns a list of errors if any were found, or an empty list of the
         staging config is valid.
         """
         # While in testing, Configuration does not demand a valid config file
         # This simplifies test setup, as most tests do not need it.
         # If a non-default path is set during testing, it is still checked.
-        if hasattr(sys, '_called_from_test') and self.user_config_path == CONFIG_PATH_DEFAULT:
+        if hasattr(sys, "_called_from_test") and self.user_config_path == CONFIG_PATH_DEFAULT:
             self.user_config_staging = DottedDict({})
             return []
 
         try:
             with open(self.user_config_path) as fh:
                 user_config_yaml = yaml.safe_load(fh)
         except OSError as oe:
-            return [f'Error opening config file {self.user_config_path}: {oe}']
+            return [f"Error opening config file {self.user_config_path}: {oe}"]
         except yaml.YAMLError as ye:
-            return [f'Error parsing YAML file: {ye}']
+            return [f"Error parsing YAML file: {ye}"]
 
-        if not isinstance(user_config_yaml, dict) or 'irrd' not in user_config_yaml:
+        if not isinstance(user_config_yaml, dict) or "irrd" not in user_config_yaml:
             return [f'Could not find root item "irrd" in config file {self.user_config_path}']
-        self.user_config_staging = DottedDict(user_config_yaml['irrd'])
+        self.user_config_staging = DottedDict(user_config_yaml["irrd"])
 
         errors = self._check_staging_config()
         if not errors and log_success:
-            logger.info(f'Configuration successfully (re)loaded from {self.user_config_path} in PID {os.getpid()}')
+            logger.info(
+                f"Configuration successfully (re)loaded from {self.user_config_path} in PID {os.getpid()}"
+            )
         return errors
 
     def _check_staging_config(self) -> List[str]:
         """
         Validate the current staging configuration.
         Returns a list of any errors, or an empty list for a valid config.
         """
         errors = []
         config = self.user_config_staging
 
-        for key, value in config.items():
-            if key in ['sources', 'access_lists']:
-                continue
-            known = KNOWN_CONFIG_KEYS.get(key)
-            if known is None:
-                errors.append(f'Unknown setting key: {key}')
-            if hasattr(value, 'items'):
+        def _validate_subconfig(key, value):
+            if isinstance(value, (DottedDict, dict)):
                 for key2, value2 in value.items():
-                    subkey = key + '.' + key2
-                    known_sub = KNOWN_CONFIG_KEYS.get(subkey)
+                    subkey = key + "." + key2
+                    known_sub = self.known_config_keys.get(subkey)
+
                     if known_sub is None:
-                        errors.append(f'Unknown setting key: {subkey}')
+                        errors.append(f"Unknown setting key: {subkey}")
+                    _validate_subconfig(subkey, value2)
 
-        if not self._check_is_str(config, 'database_url'):
-            errors.append('Setting database_url is required.')
+        for key, value in config.items():
+            if key in ["sources", "access_lists"]:
+                continue
+            if self.known_config_keys.get(key) is None:
+                errors.append(f"Unknown setting key: {key}")
+            _validate_subconfig(key, value)
 
-        if not self._check_is_str(config, 'redis_url'):
-            errors.append('Setting redis_url is required.')
+        if not self._check_is_str(config, "database_url"):
+            errors.append("Setting database_url is required.")
 
-        if not self._check_is_str(config, 'piddir') or not os.path.isdir(config['piddir']):
-            errors.append('Setting piddir is required and must point to an existing directory.')
+        if not self._check_is_str(config, "redis_url"):
+            errors.append("Setting redis_url is required.")
+
+        if not self._check_is_str(config, "piddir") or not os.path.isdir(config["piddir"]):
+            errors.append("Setting piddir is required and must point to an existing directory.")
+
+        if not str(config.get("route_object_preference.update_timer", "0")).isnumeric():
+            errors.append("Setting route_object_preference.update_timer must be a number.")
 
         expected_access_lists = {
-            config.get('server.whois.access_list'),
-            config.get('server.http.status_access_list'),
+            config.get("server.whois.access_list"),
+            config.get("server.http.status_access_list"),
         }
 
-        if not self._check_is_str(config, 'email.from') or '@' not in config.get('email.from', ''):
-            errors.append('Setting email.from is required and must be an email address.')
-        if not self._check_is_str(config, 'email.smtp'):
-            errors.append('Setting email.smtp is required.')
-        if not self._check_is_str(config, 'email.recipient_override', required=False) \
-                or '@' not in config.get('email.recipient_override', '@'):
-            errors.append('Setting email.recipient_override must be an email address if set.')
-
-        string_not_required = ['email.footer', 'server.whois.access_list',
-                               'server.http.status_access_list', 'rpki.notify_invalid_subject',
-                               'rpki.notify_invalid_header', 'rpki.slurm_source', 'user', 'group']
+        if not self._check_is_str(config, "email.from") or "@" not in config.get("email.from", ""):
+            errors.append("Setting email.from is required and must be an email address.")
+        if not self._check_is_str(config, "email.smtp"):
+            errors.append("Setting email.smtp is required.")
+        if not self._check_is_str(
+            config, "email.recipient_override", required=False
+        ) or "@" not in config.get("email.recipient_override", "@"):
+            errors.append("Setting email.recipient_override must be an email address if set.")
+
+        string_not_required = [
+            "email.footer",
+            "server.whois.access_list",
+            "server.http.status_access_list",
+            "rpki.notify_invalid_subject",
+            "rpki.notify_invalid_header",
+            "rpki.slurm_source",
+            "user",
+            "group",
+        ]
         for setting in string_not_required:
             if not self._check_is_str(config, setting, required=False):
-                errors.append(f'Setting {setting} must be a string, if defined.')
+                errors.append(f"Setting {setting} must be a string, if defined.")
+
+        if bool(config.get("user")) != bool(config.get("group")):
+            errors.append("Settings user and group must both be defined, or neither.")
 
-        if bool(config.get('user')) != bool(config.get('group')):
-            errors.append('Settings user and group must both be defined, or neither.')
+        if not self._check_is_str(config, "auth.gnupg_keyring"):
+            errors.append("Setting auth.gnupg_keyring is required.")
 
-        if not self._check_is_str(config, 'auth.gnupg_keyring'):
-            errors.append('Setting auth.gnupg_keyring is required.')
+        from irrd.updates.parser_state import RPSLSetAutnumAuthenticationMode
+
+        valid_auth = [mode.value for mode in RPSLSetAutnumAuthenticationMode]
+        for set_name, params in config.get("auth.set_creation", {}).items():
+            if not isinstance(params.get("prefix_required", False), bool):
+                errors.append(f"Setting auth.set_creation.{set_name}.prefix_required must be a bool")
+            if (
+                params.get("autnum_authentication")
+                and params["autnum_authentication"].lower() not in valid_auth
+            ):
+                errors.append(
+                    f"Setting auth.set_creation.{set_name}.autnum_authentication must be one of"
+                    f" {valid_auth} if set"
+                )
+
+        from irrd.rpsl.passwords import PasswordHasherAvailability
+
+        valid_hasher_availability = [avl.value for avl in PasswordHasherAvailability]
+        for hasher_name, setting in config.get("auth.password_hashers", {}).items():
+            if setting.lower() not in valid_hasher_availability:
+                errors.append(
+                    f"Setting auth.password_hashers.{hasher_name} must be one of {valid_hasher_availability}"
+                )
 
-        for name, access_list in config.get('access_lists', {}).items():
+        for name, access_list in config.get("access_lists", {}).items():
             for item in access_list:
                 try:
                     IP(item)
                 except ValueError as ve:
-                    errors.append(f'Invalid item in access list {name}: {ve}.')
+                    errors.append(f"Invalid item in access list {name}: {ve}.")
 
-        for prefix in config.get('scopefilter.prefixes', []):
+        for prefix in config.get("scopefilter.prefixes", []):
             try:
                 IP(prefix)
             except ValueError as ve:
-                errors.append(f'Invalid item in prefix scopefilter: {prefix}: {ve}.')
+                errors.append(f"Invalid item in prefix scopefilter: {prefix}: {ve}.")
 
-        for asn in config.get('scopefilter.asns', []):
+        for asn in config.get("scopefilter.asns", []):
             try:
-                if '-' in str(asn):
-                    first, last = asn.split('-')
+                if "-" in str(asn):
+                    first, last = asn.split("-")
                     int(first)
                     int(last)
                 else:
                     int(asn)
             except ValueError:
-                errors.append(f'Invalid item in asn scopefilter: {asn}.')
+                errors.append(f"Invalid item in asn scopefilter: {asn}.")
 
-        known_sources = set(config.get('sources', {}).keys())
+        known_sources = set(config.get("sources", {}).keys())
 
         has_authoritative_sources = False
-        for name, details in config.get('sources', {}).items():
-            unknown_keys = set(details.keys()) - KNOWN_SOURCES_KEYS
+        for name, details in config.get("sources", {}).items():
+            unknown_keys = set(details.keys()) - self.known_sources_keys
             if unknown_keys:
                 errors.append(f'Unknown key(s) under source {name}: {", ".join(unknown_keys)}')
-            if details.get('authoritative'):
+            if details.get("authoritative"):
                 has_authoritative_sources = True
-            if config.get('rpki.roa_source') and name == RPKI_IRR_PSEUDO_SOURCE:
-                errors.append(f'Setting sources contains reserved source name: {RPKI_IRR_PSEUDO_SOURCE}')
+            if config.get("rpki.roa_source") and name == RPKI_IRR_PSEUDO_SOURCE:
+                errors.append(f"Setting sources contains reserved source name: {RPKI_IRR_PSEUDO_SOURCE}")
             if not SOURCE_NAME_RE.match(name):
-                errors.append(f'Invalid source name: {name}')
+                errors.append(f"Invalid source name: {name}")
 
-            nrtm_mirror = details.get('nrtm_host') and details.get('import_serial_source')
-            if details.get('keep_journal') and not (nrtm_mirror or details.get('authoritative')):
-                errors.append(f'Setting keep_journal for source {name} can not be enabled unless either authoritative '
-                              f'is enabled, or all three of nrtm_host, nrtm_port and import_serial_source.')
-            if details.get('nrtm_host') and not details.get('import_serial_source'):
-                errors.append(f'Setting nrtm_host for source {name} can not be enabled without setting '
-                              f'import_serial_source.')
-
-            if details.get('authoritative') and (details.get('nrtm_host') or details.get('import_source')):
-                errors.append(f'Setting authoritative for source {name} can not be enabled when either '
-                              f'nrtm_host or import_source are set.')
-
-            if config.get('database_readonly') and (details.get('authoritative') or details.get('nrtm_host') or details.get('import_source')):
-                errors.append(f'Source {name} can not have authoritative, import_source or nrtm_host set '
-                              f'when database_readonly is enabled.')
-
-            if not str(details.get('nrtm_port', '43')).isnumeric():
-                errors.append(f'Setting nrtm_port for source {name} must be a number.')
-            if not str(details.get('import_timer', '0')).isnumeric():
-                errors.append(f'Setting import_timer for source {name} must be a number.')
-            if not str(details.get('export_timer', '0')).isnumeric():
-                errors.append(f'Setting export_timer for source {name} must be a number.')
-            if not str(details.get('nrtm_query_serial_range_limit', '0')).isnumeric():
-                errors.append(f'Setting nrtm_query_serial_range_limit for source {name} must be a number.')
-
-            if details.get('nrtm_access_list'):
-                expected_access_lists.add(details.get('nrtm_access_list'))
-            if details.get('nrtm_access_list_unfiltered'):
-                expected_access_lists.add(details.get('nrtm_access_list_unfiltered'))
+            if details.get("suspension_enabled") and not details.get("authoritative"):
+                errors.append(
+                    f"Setting suspension_enabled for source {name} can not be enabled without enabling "
+                    "authoritative."
+                )
+
+            nrtm_mirror = details.get("nrtm_host") and details.get("import_serial_source")
+            if details.get("keep_journal") and not (nrtm_mirror or details.get("authoritative")):
+                errors.append(
+                    f"Setting keep_journal for source {name} can not be enabled unless either authoritative "
+                    "is enabled, or all three of nrtm_host, nrtm_port and import_serial_source."
+                )
+            if details.get("nrtm_host") and not details.get("import_serial_source"):
+                errors.append(
+                    f"Setting nrtm_host for source {name} can not be enabled without setting "
+                    "import_serial_source."
+                )
+
+            if details.get("authoritative") and (details.get("nrtm_host") or details.get("import_source")):
+                errors.append(
+                    f"Setting authoritative for source {name} can not be enabled when either "
+                    "nrtm_host or import_source are set."
+                )
+
+            if config.get("database_readonly") and (
+                details.get("authoritative") or details.get("nrtm_host") or details.get("import_source")
+            ):
+                errors.append(
+                    f"Source {name} can not have authoritative, import_source or nrtm_host set "
+                    "when database_readonly is enabled."
+                )
+
+            number_fields = [
+                "nrtm_port",
+                "import_timer",
+                "export_timer",
+                "route_object_preference",
+                "nrtm_query_serial_range_limit",
+            ]
+            for field_name in number_fields:
+                if not str(details.get(field_name, 0)).isnumeric():
+                    errors.append(f"Setting {field_name} for source {name} must be a number.")
+
+            if details.get("nrtm_access_list"):
+                expected_access_lists.add(details.get("nrtm_access_list"))
+            if details.get("nrtm_access_list_unfiltered"):
+                expected_access_lists.add(details.get("nrtm_access_list_unfiltered"))
 
-        if config.get('rpki.roa_source', 'https://rpki.gin.ntt.net/api/export.json'):
+        if config.get("rpki.roa_source", "https://rpki.gin.ntt.net/api/export.json"):
             known_sources.add(RPKI_IRR_PSEUDO_SOURCE)
-            if has_authoritative_sources and config.get('rpki.notify_invalid_enabled') is None:
-                errors.append('RPKI-aware mode is enabled and authoritative sources are configured, '
-                              'but rpki.notify_invalid_enabled is not set. Set to true or false.'
-                              'DANGER: care is required with this setting in testing setups with '
-                              'live data, as it may send bulk emails to real resource contacts '
-                              'unless email.recipient_override is also set. '
-                              'Read documentation carefully.')
+            if has_authoritative_sources and config.get("rpki.notify_invalid_enabled") is None:
+                errors.append(
+                    "RPKI-aware mode is enabled and authoritative sources are configured, "
+                    "but rpki.notify_invalid_enabled is not set. Set to true or false."
+                    "DANGER: care is required with this setting in testing setups with "
+                    "live data, as it may send bulk emails to real resource contacts "
+                    "unless email.recipient_override is also set. "
+                    "Read documentation carefully."
+                )
 
-        unknown_default_sources = set(config.get('sources_default', [])).difference(known_sources)
+        unknown_default_sources = set(config.get("sources_default", [])).difference(known_sources)
         if unknown_default_sources:
-            errors.append(f'Setting sources_default contains unknown sources: {", ".join(unknown_default_sources)}')
-
-        if not str(config.get('rpki.roa_import_timer', '0')).isnumeric():
-            errors.append('Setting rpki.roa_import_timer must be set to a number.')
-
-        if config.get('log.level') and not config.get('log.level') in ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL']:
-            errors.append(f'Invalid log.level: {config.get("log.level")}. '
-                          f'Valid settings for log.level are `DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`.')
-        if config.get('log.logging_config_path') and (config.get('log.logfile_path') or config.get('log.level')):
-            errors.append('Setting log.logging_config_path can not be combined with'
-                          'log.logfile_path or log.level')
-
-        access_lists = set(config.get('access_lists', {}).keys())
-        unresolved_access_lists = [x for x in expected_access_lists.difference(access_lists) if x and isinstance(x, str)]
+            errors.append(
+                f'Setting sources_default contains unknown sources: {", ".join(unknown_default_sources)}'
+            )
+
+        if not str(config.get("rpki.roa_import_timer", "0")).isnumeric():
+            errors.append("Setting rpki.roa_import_timer must be set to a number.")
+
+        if config.get("log.level") and not config.get("log.level") in [
+            "DEBUG",
+            "INFO",
+            "WARNING",
+            "ERROR",
+            "CRITICAL",
+        ]:
+            errors.append(
+                f'Invalid log.level: {config.get("log.level")}. '
+                "Valid settings for log.level are `DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`."
+            )
+        if config.get("log.logging_config_path") and (
+            config.get("log.logfile_path") or config.get("log.level")
+        ):
+            errors.append(
+                "Setting log.logging_config_path can not be combined withlog.logfile_path or log.level"
+            )
+
+        access_lists = set(config.get("access_lists", {}).keys())
+        unresolved_access_lists = [
+            x for x in expected_access_lists.difference(access_lists) if x and isinstance(x, str)
+        ]
         unresolved_access_lists.sort()
         if unresolved_access_lists:
-            errors.append(f'Access lists {", ".join(unresolved_access_lists)} referenced in settings, but not defined.')
+            errors.append(
+                f'Access lists {", ".join(unresolved_access_lists)} referenced in settings, but not defined.'
+            )
 
         return errors
 
     def _check_is_str(self, config, key, required=True):
         if required:
             return config.get(key) and isinstance(config.get(key), str)
         return config.get(key) is None or isinstance(config.get(key), str)
@@ -470,21 +474,21 @@
     Returns whether the configuration is initialised,
     i.e. whether get_setting() can be called.
     """
     configuration = get_configuration()
     return configuration is not None
 
 
-def get_setting(setting_name: str, default: Any=None) -> Any:
+def get_setting(setting_name: str, default: Optional[Any] = None) -> Any:
     """
     Convenience wrapper to get the value of a setting.
     """
     configuration = get_configuration()
     if not configuration:  # pragma: no cover
-        raise Exception('get_setting() called before configuration was initialised')
+        raise Exception("get_setting() called before configuration was initialised")
     return configuration.get_setting_live(setting_name, default)
 
 
 def sighup_handler(signum, frame) -> None:
     """
     Reload the settings when a SIGHUP is received.
     Note that not all processes re-read their settings on every run,
```

### Comparing `irrd-4.2.8/irrd/conf/default_config.yaml` & `irrd-4.3.0/irrd/conf/default_config.yaml`

 * *Files 14% similar despite different names*

```diff
@@ -1,11 +1,13 @@
 # Defaults for source settings are set in constants
 # in conf/defaults.py.
 irrd:
     database_url: null
+    route_object_preference:
+        update_timer: 3600
     rpki:
         roa_source: https://rpki.gin.ntt.net/api/export.json
         roa_import_timer: 3600
         pseudo_irr_remarks: |
             This AS{asn} route object represents routing data retrieved
             from the RPKI. This route object is the result of an automated
             RPKI-to-IRR conversion process performed by IRRd.
@@ -40,15 +42,24 @@
             forwarded_allowed_ips: 127.0.0.1
         whois:
             interface: '::0'
             port: 43
             max_connections: 10
     auth:
         gnupg_keyring: null
-        authenticate_related_mntners: true
+        authenticate_parents_route_creation: true
+        set_creation:
+            COMMON:
+                prefix_required: true
+                autnum_authentication: opportunistic
+        password_hashers:
+            crypt-pw: legacy
+            md5-pw: enabled
+            bcrypt-pw: enabled
+            
     email:
         footer: ''
         notification_header: |
             This is to notify you of changes in the {sources_str} database
             or object authorisation failures.
 
             You may receive this message because you are listed in
```

### Comparing `irrd-4.2.8/irrd/daemon/main.py` & `irrd-4.3.0/irrd/daemon/main.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,191 +1,241 @@
 #!/usr/bin/env python
 # flake8: noqa: E402
 import argparse
+import grp
 import logging
+import multiprocessing
 import os
 import pwd
 import signal
 import sys
 import time
 from pathlib import Path
-from typing import Tuple, Optional
+from typing import Optional, Tuple
 
-import daemon
-import grp
 import psutil
-from daemon.daemon import change_process_owner
 from pid import PidFile, PidFileError
 
+import daemon
+from daemon.daemon import change_process_owner
+
 logger = logging.getLogger(__name__)
 sys.path.append(str(Path(__file__).resolve().parents[2]))
 
-from irrd.utils.process_support import ExceptionLoggingProcess, set_traceback_handler
-from irrd.storage.preload import PreloadStoreManager
-from irrd.server.whois.server import start_whois_server
-from irrd.server.http.server import run_http_server
+from irrd import ENV_MAIN_PROCESS_PID, __version__
+from irrd.conf import CONFIG_PATH_DEFAULT, config_init, get_configuration, get_setting
 from irrd.mirroring.scheduler import MirrorScheduler
-from irrd.conf import config_init, CONFIG_PATH_DEFAULT, get_setting, get_configuration
-from irrd import __version__, ENV_MAIN_PROCESS_PID
-
+from irrd.server.http.server import run_http_server
+from irrd.server.whois.server import start_whois_server
+from irrd.storage.preload import PreloadStoreManager
+from irrd.utils.process_support import ExceptionLoggingProcess, set_traceback_handler
 
 # This file does not have a unit test, but is instead tested through
 # the integration tests. Writing a unit test would be too complex.
 
+
 def main():
     description = """IRRd main process"""
     parser = argparse.ArgumentParser(description=description)
-    parser.add_argument('--config', dest='config_file_path', type=str,
-                        help=f'use a different IRRd config file (default: {CONFIG_PATH_DEFAULT})')
-    parser.add_argument('--foreground', dest='foreground', action='store_true',
-                        help=f"run IRRd in the foreground, don't detach")
+    parser.add_argument(
+        "--config",
+        dest="config_file_path",
+        type=str,
+        help=f"use a different IRRd config file (default: {CONFIG_PATH_DEFAULT})",
+    )
+    parser.add_argument(
+        "--foreground",
+        dest="foreground",
+        action="store_true",
+        help=f"run IRRd in the foreground, don't detach",
+    )
     args = parser.parse_args()
 
-    mirror_frequency = int(os.environ.get('IRRD_SCHEDULER_TIMER_OVERRIDE', 15))
+    mirror_frequency = int(os.environ.get("IRRD_SCHEDULER_TIMER_OVERRIDE", 15))
 
     daemon_kwargs = {
-        'umask': 0o022,
+        "umask": 0o022,
     }
     if args.foreground:
-        daemon_kwargs['detach_process'] = False
-        daemon_kwargs['stdout'] = sys.stdout
-        daemon_kwargs['stderr'] = sys.stderr
+        daemon_kwargs["detach_process"] = False
+        daemon_kwargs["stdout"] = sys.stdout
+        daemon_kwargs["stderr"] = sys.stderr
+
+    # Since Python 3.8, the default method is spawn for MacOS,
+    # which creates several issues. For consistency, we force to fork.
+    multiprocessing.set_start_method("fork")
 
     # config_init with commit may only be called within DaemonContext,
     # but this call here causes fast failure for most misconfigurations
     config_init(args.config_file_path, commit=False)
 
-    if not any([
-        get_configuration().user_config_staging.get('log.logfile_path'),
-        get_configuration().user_config_staging.get('log.logging_config_path'),
-        args.foreground,
-    ]):
-        logging.critical('Unable to start: when not running in the foreground, you must set '
-                         'either log.logfile_path or log.logging_config_path in the settings')
+    staged_logfile_path = get_configuration().user_config_staging.get("log.logfile_path")
+    staged_logging_config_path = get_configuration().user_config_staging.get("log.logging_config_path")
+    if not any(
+        [
+            staged_logfile_path,
+            staged_logging_config_path,
+            args.foreground,
+        ]
+    ):
+        logging.critical(
+            "Unable to start: when not running in the foreground, you must set "
+            "either log.logfile_path or log.logging_config_path in the settings"
+        )
         return
 
+    uid, gid = get_configured_owner(from_staging=True)
+    if uid and gid:
+        os.setegid(gid)
+        os.seteuid(uid)
+        if staged_logfile_path and not os.access(staged_logfile_path, os.W_OK, effective_ids=True):
+            logging.critical(
+                f"Unable to start: logfile {staged_logfile_path} not writable by UID {uid} / GID {gid}"
+            )
+            return
+
     with daemon.DaemonContext(**daemon_kwargs):
         config_init(args.config_file_path)
 
         uid, gid = get_configured_owner()
         # Running as root is permitted on CI
-        if not os.environ.get('CI') and not uid and os.geteuid() == 0:
-            logging.critical('Unable to start: user and group must be defined in settings '
-                             'when starting IRRd as root')
+        if not os.environ.get("CI") and not uid and os.geteuid() == 0:
+            logging.critical(
+                "Unable to start: user and group must be defined in settings when starting IRRd as root"
+            )
             return
 
-        piddir = get_setting('piddir')
-        logger.info('IRRd attempting to secure PID')
+        piddir = get_setting("piddir")
+        logger.info("IRRd attempting to secure PID")
         try:
-            with PidFile(pidname='irrd', piddir=piddir):
-                logger.info(f'IRRd {__version__} starting, PID {os.getpid()}, PID file in {piddir}')
-                run_irrd(mirror_frequency=mirror_frequency,
-                         config_file_path=args.config_file_path if args.config_file_path else CONFIG_PATH_DEFAULT,
-                         uid=uid,
-                         gid=gid,
-                         )
+            with PidFile(pidname="irrd", piddir=piddir):
+                logger.info(f"IRRd {__version__} starting, PID {os.getpid()}, PID file in {piddir}")
+                run_irrd(
+                    mirror_frequency=mirror_frequency,
+                    config_file_path=args.config_file_path if args.config_file_path else CONFIG_PATH_DEFAULT,
+                    uid=uid,
+                    gid=gid,
+                )
         except PidFileError as pfe:
-            logger.error(f'Failed to start IRRd, unable to lock PID file irrd.pid in {piddir}: {pfe}')
+            logger.error(f"Failed to start IRRd, unable to lock PID file irrd.pid in {piddir}: {pfe}")
         except Exception as e:
-            logger.error(f'Error occurred in main process, terminating. Error follows:')
+            logger.error(f"Error occurred in main process, terminating. Error follows:")
             logger.exception(e)
             os.kill(os.getpid(), signal.SIGTERM)
 
 
 def run_irrd(mirror_frequency: int, config_file_path: str, uid: Optional[int], gid: Optional[int]):
     terminated = False
+
+    if sys.platform == "darwin":
+        logger.warning("Running on Mac OS, disabling proxy usage by setting no_proxy=*")
+        os.environ["no_proxy"] = " * "
+
     os.environ[ENV_MAIN_PROCESS_PID] = str(os.getpid())
     set_traceback_handler()
 
     whois_process = ExceptionLoggingProcess(
-        target=start_whois_server,
-        name='irrd-whois-server-listener',
-        kwargs={'uid': uid, 'gid': gid}
+        target=start_whois_server, name="irrd-whois-server-listener", kwargs={"uid": uid, "gid": gid}
     )
     whois_process.start()
     if uid and gid:
         change_process_owner(uid=uid, gid=gid, initgroups=True)
 
     mirror_scheduler = MirrorScheduler()
 
     preload_manager = None
-    if not get_setting(f'database_readonly'):
-        preload_manager = PreloadStoreManager(name='irrd-preload-store-manager')
+    if not get_setting(f"database_readonly"):
+        preload_manager = PreloadStoreManager(name="irrd-preload-store-manager")
         preload_manager.start()
 
-    uvicorn_process = ExceptionLoggingProcess(target=run_http_server, name='irrd-http-server-listener', args=(config_file_path, ))
+    uvicorn_process = ExceptionLoggingProcess(
+        target=run_http_server, name="irrd-http-server-listener", args=(config_file_path,)
+    )
     uvicorn_process.start()
 
     def sighup_handler(signum, frame):
         # On SIGHUP, check if the configuration is valid and reload in
         # this process, and if it is valid, signal SIGHUP to all
         # child processes.
         if get_configuration().reload():
             parent = psutil.Process(os.getpid())
             children = parent.children(recursive=True)
             for process in children:
                 process.send_signal(signal.SIGHUP)
             if children:
-                logging.info('Main process received SIGHUP with valid config, sent SIGHUP to '
-                             f'child processes {[c.pid for c in children]}')
+                logging.info(
+                    "Main process received SIGHUP with valid config, sent SIGHUP to "
+                    f"child processes {[c.pid for c in children]}"
+                )
+
     signal.signal(signal.SIGHUP, sighup_handler)
 
     def sigterm_handler(signum, frame):
         mirror_scheduler.terminate_children()
         parent = psutil.Process(os.getpid())
         children = parent.children(recursive=True)
         for process in children:
             try:
                 process.send_signal(signal.SIGTERM)
             except Exception:
                 # If we can't SIGTERM some of the processes,
                 # do the best we can.
                 pass
         if children:
-            logging.info('Main process received SIGTERM, sent SIGTERM to '
-                         f'child processes {[c.pid for c in children]}')
+            logging.info(
+                f"Main process received SIGTERM, sent SIGTERM to child processes {[c.pid for c in children]}"
+            )
 
         nonlocal terminated
         terminated = True
+
     signal.signal(signal.SIGTERM, sigterm_handler)
 
     sleeps = mirror_frequency
     while not terminated:
         # This loops every second to prevent long blocking on SIGTERM.
         mirror_scheduler.update_process_state()
         if sleeps >= mirror_frequency:
             mirror_scheduler.run()
             sleeps = 0
         time.sleep(1)
         sleeps += 1
 
-    logging.debug(f'Main process waiting for child processes to terminate')
+    logging.debug(f"Main process waiting for child processes to terminate")
     for child_process in whois_process, uvicorn_process, preload_manager:
         if child_process:
             child_process.join(timeout=3)
 
     parent = psutil.Process(os.getpid())
     children = parent.children(recursive=True)
     for process in children:
         try:
             process.send_signal(signal.SIGKILL)
         except Exception:
             pass
     if children:
-        logging.info('Some processes left alive after SIGTERM, send SIGKILL to '
-                     f'child processes {[c.pid for c in children]}')
+        logging.info(
+            "Some processes left alive after SIGTERM, send SIGKILL to "
+            f"child processes {[c.pid for c in children]}"
+        )
 
-    logging.info(f'Main process exiting')
+    logging.info(f"Main process exiting")
 
 
-def get_configured_owner() -> Tuple[Optional[int], Optional[int]]:
+def get_configured_owner(from_staging=False) -> Tuple[Optional[int], Optional[int]]:
     uid = gid = None
-    user = get_setting('user')
-    group = get_setting('group')
+    if not from_staging:
+        user = get_setting("user")
+        group = get_setting("group")
+    else:
+        config = get_configuration()
+        assert config
+        user = config.user_config_staging.get("user")
+        group = config.user_config_staging.get("group")
     if user and group:
         uid = pwd.getpwnam(user).pw_uid
         gid = grp.getgrnam(group).gr_gid
     return uid, gid
 
 
-if __name__ == '__main__':  # pragma: no cover
+if __name__ == "__main__":  # pragma: no cover
     main()
```

### Comparing `irrd-4.2.8/irrd/integration_tests/mailserver.tac` & `irrd-4.3.0/irrd/integration_tests/mailserver.tac`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/irrd/integration_tests/run.py` & `irrd-4.3.0/irrd/integration_tests/run.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,38 +1,58 @@
 # flake8: noqa: W293
-import sys
-import time
-import unittest
-
-import ujson
-
 import base64
 import email
 import os
-import requests
 import signal
 import socket
-import sqlalchemy as sa
 import subprocess
+import sys
 import textwrap
-import yaml
-from alembic import command, config
+import time
+import unittest
 from pathlib import Path
 
+import requests
+import sqlalchemy as sa
+import ujson
+import yaml
+from alembic import command, config
 from python_graphql_client import GraphqlClient
 
-from irrd.conf import config_init, PASSWORD_HASH_DUMMY_VALUE
-from irrd.utils.rpsl_samples import (SAMPLE_MNTNER, SAMPLE_PERSON, SAMPLE_KEY_CERT, SIGNED_PERSON_UPDATE_VALID,
-                                     SAMPLE_AS_SET, SAMPLE_AUT_NUM, SAMPLE_DOMAIN, SAMPLE_FILTER_SET, SAMPLE_INET_RTR,
-                                     SAMPLE_INET6NUM, SAMPLE_INETNUM, SAMPLE_PEERING_SET, SAMPLE_ROLE, SAMPLE_ROUTE,
-                                     SAMPLE_ROUTE_SET, SAMPLE_ROUTE6, SAMPLE_RTR_SET, SAMPLE_AS_BLOCK)
+from irrd.conf import PASSWORD_HASH_DUMMY_VALUE, config_init
+from irrd.utils.rpsl_samples import (
+    SAMPLE_AS_BLOCK,
+    SAMPLE_AS_SET,
+    SAMPLE_AUT_NUM,
+    SAMPLE_DOMAIN,
+    SAMPLE_FILTER_SET,
+    SAMPLE_INET6NUM,
+    SAMPLE_INET_RTR,
+    SAMPLE_INETNUM,
+    SAMPLE_KEY_CERT,
+    SAMPLE_MNTNER,
+    SAMPLE_PEERING_SET,
+    SAMPLE_PERSON,
+    SAMPLE_ROLE,
+    SAMPLE_ROUTE,
+    SAMPLE_ROUTE6,
+    SAMPLE_ROUTE_SET,
+    SAMPLE_RTR_SET,
+    SIGNED_PERSON_UPDATE_VALID,
+)
 from irrd.utils.whois_client import whois_query, whois_query_irrd
-from .constants import (EMAIL_SMTP_PORT, EMAIL_DISCARD_MSGS_COMMAND, EMAIL_RETURN_MSGS_COMMAND, EMAIL_SEPARATOR,
-                        EMAIL_END)
+
 from ..storage import translate_url
+from .constants import (
+    EMAIL_DISCARD_MSGS_COMMAND,
+    EMAIL_END,
+    EMAIL_RETURN_MSGS_COMMAND,
+    EMAIL_SEPARATOR,
+    EMAIL_SMTP_PORT,
+)
 
 IRRD_ROOT_PATH = str(Path(__file__).resolve().parents[2])
 sys.path.append(IRRD_ROOT_PATH)
 
 AS_SET_REFERRING_OTHER_SET = """as-set:         AS65537:AS-TESTREF
 descr:          description
 members:        AS65537:AS-SETTEST, AS65540
@@ -42,485 +62,541 @@
 notify:         notify@example.com
 mnt-by:         TEST-MNT
 changed:        changed@example.com 20190701 # comment
 source:         TEST
 remarks:        remark
 """
 
-SAMPLE_MNTNER_CLEAN = SAMPLE_MNTNER.replace('mnt-by:         OTHER1-MNT,OTHER2-MNT\n', '')
-LARGE_UPDATE = '\n\n'.join([
-    SAMPLE_AS_BLOCK,
-    SAMPLE_AS_SET,
-    SAMPLE_AUT_NUM,
-    SAMPLE_AUT_NUM.replace('aut-num:        as065537', 'aut-num: as65538'),
-    SAMPLE_AUT_NUM.replace('aut-num:        as065537', 'aut-num: as65539'),
-    SAMPLE_AUT_NUM.replace('aut-num:        as065537', 'aut-num: as65540'),
-    SAMPLE_DOMAIN,
-    SAMPLE_FILTER_SET,
-    SAMPLE_INET_RTR,
-    SAMPLE_INET6NUM,
-    SAMPLE_INETNUM,
-    SAMPLE_KEY_CERT,
-    SAMPLE_PEERING_SET,
-    SAMPLE_PERSON.replace('PERSON-TEST', 'DUMY2-TEST'),
-    SAMPLE_ROLE,
-    SAMPLE_ROUTE,
-    SAMPLE_ROUTE_SET,
-    SAMPLE_ROUTE6,
-    SAMPLE_RTR_SET,
-    AS_SET_REFERRING_OTHER_SET,
-])
+SAMPLE_MNTNER_CLEAN = SAMPLE_MNTNER.replace("mnt-by:         OTHER1-MNT,OTHER2-MNT\n", "")
+LARGE_UPDATE = "\n\n".join(
+    [
+        SAMPLE_AS_BLOCK,
+        SAMPLE_AS_SET,
+        SAMPLE_AUT_NUM,
+        SAMPLE_AUT_NUM.replace("aut-num:        as065537", "aut-num: as65538"),
+        SAMPLE_AUT_NUM.replace("aut-num:        as065537", "aut-num: as65539"),
+        SAMPLE_AUT_NUM.replace("aut-num:        as065537", "aut-num: as65540"),
+        SAMPLE_DOMAIN,
+        SAMPLE_FILTER_SET,
+        SAMPLE_INET_RTR,
+        SAMPLE_INET6NUM,
+        SAMPLE_INETNUM,
+        SAMPLE_KEY_CERT,
+        SAMPLE_PEERING_SET,
+        SAMPLE_PERSON.replace("PERSON-TEST", "DUMY2-TEST"),
+        SAMPLE_ROLE,
+        SAMPLE_ROUTE,
+        SAMPLE_ROUTE_SET,
+        SAMPLE_ROUTE6,
+        SAMPLE_RTR_SET,
+        AS_SET_REFERRING_OTHER_SET,
+    ]
+)
 
 
 class TestIntegration:
     """
     This integration test will start two instances of IRRd, one mirroring off the
     other, and an email server that captures all mail. It will then run a series
     of updates and queries, verify the contents of mails, the state of the
     databases, mirroring, utf-8 handling and run all basic types of queries.
 
     Note that this test will not be included in the default py.test discovery,
     this is intentional.
     """
+
     port_http1 = 6080
     port_whois1 = 6043
     port_http2 = 6081
     port_whois2 = 6044
 
     def test_irrd_integration(self, tmpdir):
         self.assertCountEqual = unittest.TestCase().assertCountEqual
         # IRRD_DATABASE_URL and IRRD_REDIS_URL override the yaml config, so should be removed
-        if 'IRRD_DATABASE_URL' in os.environ:
-            del os.environ['IRRD_DATABASE_URL']
-        if 'IRRD_REDIS_URL' in os.environ:
-            del os.environ['IRRD_REDIS_URL']
+        if "IRRD_DATABASE_URL" in os.environ:
+            del os.environ["IRRD_DATABASE_URL"]
+        if "IRRD_REDIS_URL" in os.environ:
+            del os.environ["IRRD_REDIS_URL"]
         # PYTHONPATH needs to contain the twisted plugin path to support the mailserver.
-        os.environ['PYTHONPATH'] = IRRD_ROOT_PATH
-        os.environ['IRRD_SCHEDULER_TIMER_OVERRIDE'] = '1'
+        os.environ["PYTHONPATH"] = IRRD_ROOT_PATH
+        os.environ["IRRD_SCHEDULER_TIMER_OVERRIDE"] = "1"
         self.tmpdir = tmpdir
 
         self._start_mailserver()
         self._start_irrds()
 
         # Attempt to load a mntner with valid auth, but broken references.
-        self._submit_update(self.config_path1, SAMPLE_MNTNER + '\n\noverride: override-password')
+        self._submit_update(self.config_path1, SAMPLE_MNTNER + "\n\noverride: override-password")
         messages = self._retrieve_mails()
         assert len(messages) == 1
         mail_text = self._extract_message_body(messages[0])
-        assert messages[0]['Subject'] == 'FAILED: my subject'
-        assert messages[0]['From'] == 'from@example.com'
-        assert messages[0]['To'] == 'Sasha <sasha@example.com>'
-        assert '\nCreate FAILED: [mntner] TEST-MNT\n' in mail_text
-        assert '\nERROR: Object PERSON-TEST referenced in field admin-c not found in database TEST - must reference one of role, person.\n' in mail_text
-        assert '\nERROR: Object OTHER1-MNT referenced in field mnt-by not found in database TEST - must reference mntner.\n' in mail_text
-        assert '\nERROR: Object OTHER2-MNT referenced in field mnt-by not found in database TEST - must reference mntner.\n' in mail_text
-        assert 'email footer' in mail_text
-        assert 'Generated by IRRd version ' in mail_text
+        assert messages[0]["Subject"] == "FAILED: my subject"
+        assert messages[0]["From"] == "from@example.com"
+        assert messages[0]["To"] == "Sasha <sasha@example.com>"
+        assert "\nCreate FAILED: [mntner] TEST-MNT\n" in mail_text
+        assert (
+            "\nERROR: Object PERSON-TEST referenced in field admin-c not found in database TEST - must"
+            " reference one of role, person.\n"
+            in mail_text
+        )
+        assert (
+            "\nERROR: Object OTHER1-MNT referenced in field mnt-by not found in database TEST - must"
+            " reference mntner.\n"
+            in mail_text
+        )
+        assert (
+            "\nERROR: Object OTHER2-MNT referenced in field mnt-by not found in database TEST - must"
+            " reference mntner.\n"
+            in mail_text
+        )
+        assert "email footer" in mail_text
+        assert "Generated by IRRd version " in mail_text
 
         # Load a regular valid mntner and person into the DB, and verify
         # the contents of the result.
-        self._submit_update(self.config_path1,
-                            SAMPLE_MNTNER_CLEAN + '\n\n' + SAMPLE_PERSON + '\n\noverride: override-password')
+        self._submit_update(
+            self.config_path1,
+            SAMPLE_MNTNER_CLEAN + "\n\n" + SAMPLE_PERSON + "\n\noverride: override-password",
+        )
         messages = self._retrieve_mails()
         assert len(messages) == 1
         mail_text = self._extract_message_body(messages[0])
-        assert messages[0]['Subject'] == 'SUCCESS: my subject'
-        assert messages[0]['From'] == 'from@example.com'
-        assert messages[0]['To'] == 'Sasha <sasha@example.com>'
-        assert '\nCreate succeeded: [mntner] TEST-MNT\n' in mail_text
-        assert '\nCreate succeeded: [person] PERSON-TEST\n' in mail_text
-        assert 'email footer' in mail_text
-        assert 'Generated by IRRd version ' in mail_text
+        assert messages[0]["Subject"] == "SUCCESS: my subject"
+        assert messages[0]["From"] == "from@example.com"
+        assert messages[0]["To"] == "Sasha <sasha@example.com>"
+        assert "\nCreate succeeded: [mntner] TEST-MNT\n" in mail_text
+        assert "\nCreate succeeded: [person] PERSON-TEST\n" in mail_text
+        assert "email footer" in mail_text
+        assert "Generated by IRRd version " in mail_text
 
         # Check whether the objects can be queried from irrd #1,
         # whether the hash is masked, and whether encoding is correct.
-        mntner_text = whois_query('127.0.0.1', self.port_whois1, 'TEST-MNT')
-        assert 'TEST-MNT' in mntner_text
+        mntner_text = whois_query("127.0.0.1", self.port_whois1, "TEST-MNT")
+        assert "TEST-MNT" in mntner_text
         assert PASSWORD_HASH_DUMMY_VALUE in mntner_text
-        assert 'unįcöde tæst 🌈🦄' in mntner_text
-        assert 'PERSON-TEST' in mntner_text
+        assert "unįcöde tæst 🌈🦄" in mntner_text
+        assert "PERSON-TEST" in mntner_text
 
         # After three seconds, a new export should have been generated by irrd #1,
         # loaded by irrd #2, and the objects should be available in irrd #2
         time.sleep(3)
-        mntner_text = whois_query('127.0.0.1', self.port_whois2, 'TEST-MNT')
-        assert 'TEST-MNT' in mntner_text
+        mntner_text = whois_query("127.0.0.1", self.port_whois2, "TEST-MNT")
+        assert "TEST-MNT" in mntner_text
         assert PASSWORD_HASH_DUMMY_VALUE in mntner_text
-        assert 'unįcöde tæst 🌈🦄' in mntner_text
-        assert 'PERSON-TEST' in mntner_text
+        assert "unįcöde tæst 🌈🦄" in mntner_text
+        assert "PERSON-TEST" in mntner_text
 
         # Load a key-cert. This should cause notifications to mnt-nfy (2x).
         # Change is authenticated by valid password.
-        self._submit_update(self.config_path1, SAMPLE_KEY_CERT + '\npassword: md5-password')
+        self._submit_update(self.config_path1, SAMPLE_KEY_CERT + "\npassword: md5-password")
         messages = self._retrieve_mails()
         assert len(messages) == 3
-        assert messages[0]['Subject'] == 'SUCCESS: my subject'
-        assert messages[0]['From'] == 'from@example.com'
-        assert messages[0]['To'] == 'Sasha <sasha@example.com>'
-        assert 'Create succeeded: [key-cert] PGPKEY-80F238C6' in self._extract_message_body(messages[0])
-
-        self._check_recipients_in_mails(messages[1:], [
-            'mnt-nfy@example.net', 'mnt-nfy2@example.net'
-        ])
-
-        self._check_text_in_mails(messages[1:], [
-            '\n> Message-ID: <1325754288.4989.6.camel@hostname>\n',
-            '\nCreate succeeded for object below: [key-cert] PGPKEY-80F238C6:\n',
-            'email footer',
-            'Generated by IRRd version ',
-        ])
+        assert messages[0]["Subject"] == "SUCCESS: my subject"
+        assert messages[0]["From"] == "from@example.com"
+        assert messages[0]["To"] == "Sasha <sasha@example.com>"
+        assert "Create succeeded: [key-cert] PGPKEY-80F238C6" in self._extract_message_body(messages[0])
+
+        self._check_recipients_in_mails(messages[1:], ["mnt-nfy@example.net", "mnt-nfy2@example.net"])
+
+        self._check_text_in_mails(
+            messages[1:],
+            [
+                "\n> Message-ID: <1325754288.4989.6.camel@hostname>\n",
+                "\nCreate succeeded for object below: [key-cert] PGPKEY-80F238C6:\n",
+                "email footer",
+                "Generated by IRRd version ",
+            ],
+        )
         for message in messages[1:]:
-            assert message['Subject'] == 'Notification of TEST database changes'
-            assert message['From'] == 'from@example.com'
+            assert message["Subject"] == "Notification of TEST database changes"
+            assert message["From"] == "from@example.com"
 
         # Use the new PGP key to make an update to PERSON-TEST. Should
         # again trigger mnt-nfy messages, and a mail to the notify address
         # of PERSON-TEST.
         self._submit_update(self.config_path1, SIGNED_PERSON_UPDATE_VALID)
         messages = self._retrieve_mails()
         assert len(messages) == 4
         mail_text = self._extract_message_body(messages[0])
-        assert messages[0]['Subject'] == 'SUCCESS: my subject'
-        assert messages[0]['From'] == 'from@example.com'
-        assert messages[0]['To'] == 'Sasha <sasha@example.com>'
-        assert '\nModify succeeded: [person] PERSON-TEST\n' in mail_text
-
-        self._check_recipients_in_mails(messages[1:], [
-            'mnt-nfy@example.net', 'mnt-nfy2@example.net', 'notify@example.com',
-        ])
-
-        self._check_text_in_mails(messages[1:], [
-            '\n> Message-ID: <1325754288.4989.6.camel@hostname>\n',
-            '\nModify succeeded for object below: [person] PERSON-TEST:\n',
-            '\n@@ -1,4 +1,4 @@\n',
-            '\nNew version of this object:\n',
-        ])
+        assert messages[0]["Subject"] == "SUCCESS: my subject"
+        assert messages[0]["From"] == "from@example.com"
+        assert messages[0]["To"] == "Sasha <sasha@example.com>"
+        assert "\nModify succeeded: [person] PERSON-TEST\n" in mail_text
+
+        self._check_recipients_in_mails(
+            messages[1:],
+            [
+                "mnt-nfy@example.net",
+                "mnt-nfy2@example.net",
+                "notify@example.com",
+            ],
+        )
+
+        self._check_text_in_mails(
+            messages[1:],
+            [
+                "\n> Message-ID: <1325754288.4989.6.camel@hostname>\n",
+                "\nModify succeeded for object below: [person] PERSON-TEST:\n",
+                "\n@@ -1,4 +1,4 @@\n",
+                "\nNew version of this object:\n",
+            ],
+        )
         for message in messages[1:]:
-            assert message['Subject'] == 'Notification of TEST database changes'
-            assert message['From'] == 'from@example.com'
+            assert message["Subject"] == "Notification of TEST database changes"
+            assert message["From"] == "from@example.com"
 
         # Check that the person is updated on irrd #1
-        person_text = whois_query('127.0.0.1', self.port_whois1, 'PERSON-TEST')
-        assert 'PERSON-TEST' in person_text
-        assert 'Test person changed by PGP signed update' in person_text
+        person_text = whois_query("127.0.0.1", self.port_whois1, "PERSON-TEST")
+        assert "PERSON-TEST" in person_text
+        assert "Test person changed by PGP signed update" in person_text
 
         # After 2s, NRTM from irrd #2 should have picked up the change.
         time.sleep(2)
-        person_text = whois_query('127.0.0.1', self.port_whois2, 'PERSON-TEST')
-        assert 'PERSON-TEST' in person_text
-        assert 'Test person changed by PGP signed update' in person_text
+        person_text = whois_query("127.0.0.1", self.port_whois2, "PERSON-TEST")
+        assert "PERSON-TEST" in person_text
+        assert "Test person changed by PGP signed update" in person_text
 
         # Submit an update back to the original person object, with an invalid
         # password and invalid override. Should trigger notification to upd-to.
-        self._submit_update(self.config_path1, SAMPLE_PERSON + '\npassword: invalid\noverride: invalid\n')
+        self._submit_update(self.config_path1, SAMPLE_PERSON + "\npassword: invalid\noverride: invalid\n")
         messages = self._retrieve_mails()
         assert len(messages) == 2
         mail_text = self._extract_message_body(messages[0])
-        assert messages[0]['Subject'] == 'FAILED: my subject'
-        assert messages[0]['From'] == 'from@example.com'
-        assert messages[0]['To'] == 'Sasha <sasha@example.com>'
-        assert '\nModify FAILED: [person] PERSON-TEST\n' in mail_text
-        assert '\nERROR: Authorisation for person PERSON-TEST failed: must by authenticated by one of: TEST-MNT\n' in mail_text
+        assert messages[0]["Subject"] == "FAILED: my subject"
+        assert messages[0]["From"] == "from@example.com"
+        assert messages[0]["To"] == "Sasha <sasha@example.com>"
+        assert "\nModify FAILED: [person] PERSON-TEST\n" in mail_text
+        assert (
+            "\nERROR: Authorisation for person PERSON-TEST failed: must be authenticated by one of:"
+            " TEST-MNT\n"
+            in mail_text
+        )
 
         mail_text = self._extract_message_body(messages[1])
-        assert messages[1]['Subject'] == 'Notification of TEST database changes'
-        assert messages[1]['From'] == 'from@example.com'
-        assert messages[1]['To'] == 'upd-to@example.net'
-        assert '\nModify FAILED AUTHORISATION for object below: [person] PERSON-TEST:\n' in mail_text
+        assert messages[1]["Subject"] == "Notification of TEST database changes"
+        assert messages[1]["From"] == "from@example.com"
+        assert messages[1]["To"] == "upd-to@example.net"
+        assert "\nModify FAILED AUTHORISATION for object below: [person] PERSON-TEST:\n" in mail_text
 
         # Object should not have changed by latest update.
-        person_text = whois_query('127.0.0.1', self.port_whois1, 'PERSON-TEST')
-        assert 'PERSON-TEST' in person_text
-        assert 'Test person changed by PGP signed update' in person_text
+        person_text = whois_query("127.0.0.1", self.port_whois1, "PERSON-TEST")
+        assert "PERSON-TEST" in person_text
+        assert "Test person changed by PGP signed update" in person_text
 
         # Submit a delete with a valid password for PERSON-TEST.
         # This should be rejected, because it creates a dangling reference.
         # No mail should be sent to upd-to.
-        self._submit_update(self.config_path1, SAMPLE_PERSON + 'password: md5-password\ndelete: delete\n')
+        self._submit_update(self.config_path1, SAMPLE_PERSON + "password: md5-password\ndelete: delete\n")
         messages = self._retrieve_mails()
         assert len(messages) == 1
         mail_text = self._extract_message_body(messages[0])
-        assert messages[0]['Subject'] == 'FAILED: my subject'
-        assert messages[0]['From'] == 'from@example.com'
-        assert messages[0]['To'] == 'Sasha <sasha@example.com>'
-        assert '\nDelete FAILED: [person] PERSON-TEST\n' in mail_text
-        assert '\nERROR: Object PERSON-TEST to be deleted, but still referenced by mntner TEST-MNT\n' in mail_text
-        assert '\nERROR: Object PERSON-TEST to be deleted, but still referenced by key-cert PGPKEY-80F238C6\n' in mail_text
+        assert messages[0]["Subject"] == "FAILED: my subject"
+        assert messages[0]["From"] == "from@example.com"
+        assert messages[0]["To"] == "Sasha <sasha@example.com>"
+        assert "\nDelete FAILED: [person] PERSON-TEST\n" in mail_text
+        assert (
+            "\nERROR: Object PERSON-TEST to be deleted, but still referenced by mntner TEST-MNT\n"
+            in mail_text
+        )
+        assert (
+            "\nERROR: Object PERSON-TEST to be deleted, but still referenced by key-cert PGPKEY-80F238C6\n"
+            in mail_text
+        )
 
         # Object should not have changed by latest update.
-        person_text = whois_query('127.0.0.1', self.port_whois1, 'PERSON-TEST')
-        assert 'PERSON-TEST' in person_text
-        assert 'Test person changed by PGP signed update' in person_text
+        person_text = whois_query("127.0.0.1", self.port_whois1, "PERSON-TEST")
+        assert "PERSON-TEST" in person_text
+        assert "Test person changed by PGP signed update" in person_text
 
         # Submit a valid delete for all our new objects.
-        self._submit_update(self.config_path1,
-                            f'{SAMPLE_PERSON}delete: delete\n\n{SAMPLE_KEY_CERT}delete: delete\n\n' +
-                            f'{SAMPLE_MNTNER_CLEAN}delete: delete\npassword: crypt-password\n')
+        self._submit_update(
+            self.config_path1,
+            f"{SAMPLE_PERSON}delete: delete\n\n{SAMPLE_KEY_CERT}delete: delete\n\n"
+            + f"{SAMPLE_MNTNER_CLEAN}delete: delete\npassword: crypt-password\n",
+        )
         messages = self._retrieve_mails()
         # Expected mails are status, mnt-nfy on mntner (2x), and notify on mntner
         # (notify on PERSON-TEST was removed in the PGP signed update)
         assert len(messages) == 4
         mail_text = self._extract_message_body(messages[0])
-        assert messages[0]['Subject'] == 'SUCCESS: my subject'
-        assert messages[0]['From'] == 'from@example.com'
-        assert messages[0]['To'] == 'Sasha <sasha@example.com>'
-        assert '\nDelete succeeded: [person] PERSON-TEST\n' in mail_text
-        assert '\nDelete succeeded: [mntner] TEST-MNT\n' in mail_text
-        assert '\nDelete succeeded: [key-cert] PGPKEY-80F238C6\n' in mail_text
-
-        self._check_recipients_in_mails(messages[1:], [
-            'mnt-nfy@example.net', 'mnt-nfy2@example.net', 'notify@example.net',
-        ])
-
-        mnt_nfy_msgs = [msg for msg in messages if msg['To'] in ['mnt-nfy@example.net', 'mnt-nfy2@example.net']]
-        self._check_text_in_mails(mnt_nfy_msgs, [
-            '\n> Message-ID: <1325754288.4989.6.camel@hostname>\n',
-            '\nDelete succeeded for object below: [person] PERSON-TEST:\n',
-            '\nDelete succeeded for object below: [mntner] TEST-MNT:\n',
-            '\nDelete succeeded for object below: [key-cert] PGPKEY-80F238C6:\n',
-            'unįcöde tæst 🌈🦄\n',
-            # The object submitted to be deleted has the original name,
-            # but when sending delete notifications, they should include the
-            # object as currently in the DB, not as submitted in the email.
-            'Test person changed by PGP signed update\n',
-        ])
+        assert messages[0]["Subject"] == "SUCCESS: my subject"
+        assert messages[0]["From"] == "from@example.com"
+        assert messages[0]["To"] == "Sasha <sasha@example.com>"
+        assert "\nDelete succeeded: [person] PERSON-TEST\n" in mail_text
+        assert "\nDelete succeeded: [mntner] TEST-MNT\n" in mail_text
+        assert "\nDelete succeeded: [key-cert] PGPKEY-80F238C6\n" in mail_text
+
+        self._check_recipients_in_mails(
+            messages[1:],
+            [
+                "mnt-nfy@example.net",
+                "mnt-nfy2@example.net",
+                "notify@example.net",
+            ],
+        )
+
+        mnt_nfy_msgs = [
+            msg for msg in messages if msg["To"] in ["mnt-nfy@example.net", "mnt-nfy2@example.net"]
+        ]
+        self._check_text_in_mails(
+            mnt_nfy_msgs,
+            [
+                "\n> Message-ID: <1325754288.4989.6.camel@hostname>\n",
+                "\nDelete succeeded for object below: [person] PERSON-TEST:\n",
+                "\nDelete succeeded for object below: [mntner] TEST-MNT:\n",
+                "\nDelete succeeded for object below: [key-cert] PGPKEY-80F238C6:\n",
+                "unįcöde tæst 🌈🦄\n",
+                # The object submitted to be deleted has the original name,
+                # but when sending delete notifications, they should include the
+                # object as currently in the DB, not as submitted in the email.
+                "Test person changed by PGP signed update\n",
+            ],
+        )
         for message in messages[1:]:
-            assert message['Subject'] == 'Notification of TEST database changes'
-            assert message['From'] == 'from@example.com'
+            assert message["Subject"] == "Notification of TEST database changes"
+            assert message["From"] == "from@example.com"
 
         # Notify attribute mails are only about the objects concerned.
-        notify_msg = [msg for msg in messages if msg['To'] == 'notify@example.net'][0]
+        notify_msg = [msg for msg in messages if msg["To"] == "notify@example.net"][0]
         mail_text = self._extract_message_body(notify_msg)
-        assert notify_msg['Subject'] == 'Notification of TEST database changes'
-        assert notify_msg['From'] == 'from@example.com'
-        assert '\n> Message-ID: <1325754288.4989.6.camel@hostname>\n' in mail_text
-        assert '\nDelete succeeded for object below: [person] PERSON-TEST:\n' not in mail_text
-        assert '\nDelete succeeded for object below: [mntner] TEST-MNT:\n' in mail_text
-        assert '\nDelete succeeded for object below: [key-cert] PGPKEY-80F238C6:\n' not in mail_text
+        assert notify_msg["Subject"] == "Notification of TEST database changes"
+        assert notify_msg["From"] == "from@example.com"
+        assert "\n> Message-ID: <1325754288.4989.6.camel@hostname>\n" in mail_text
+        assert "\nDelete succeeded for object below: [person] PERSON-TEST:\n" not in mail_text
+        assert "\nDelete succeeded for object below: [mntner] TEST-MNT:\n" in mail_text
+        assert "\nDelete succeeded for object below: [key-cert] PGPKEY-80F238C6:\n" not in mail_text
 
         # Object should be deleted
-        person_text = whois_query('127.0.0.1', self.port_whois1, 'PERSON-TEST')
-        assert 'No entries found for the selected source(s)' in person_text
-        assert 'PERSON-TEST' not in person_text
+        person_text = whois_query("127.0.0.1", self.port_whois1, "PERSON-TEST")
+        assert "No entries found for the selected source(s)" in person_text
+        assert "PERSON-TEST" not in person_text
 
         # Object should be deleted from irrd #2 as well through NRTM.
         time.sleep(2)
-        person_text = whois_query('127.0.0.1', self.port_whois2, 'PERSON-TEST')
-        assert 'No entries found for the selected source(s)' in person_text
-        assert 'PERSON-TEST' not in person_text
+        person_text = whois_query("127.0.0.1", self.port_whois2, "PERSON-TEST")
+        assert "No entries found for the selected source(s)" in person_text
+        assert "PERSON-TEST" not in person_text
 
         # Load the mntner and person again, using the override password
         # Note that the route/route6 objects are RPKI valid on IRRd #1,
         # and RPKI-invalid on IRRd #2
-        self._submit_update(self.config_path1,
-                            SAMPLE_MNTNER_CLEAN + '\n\n' + SAMPLE_PERSON + '\n\noverride: override-password')
+        self._submit_update(
+            self.config_path1,
+            SAMPLE_MNTNER_CLEAN + "\n\n" + SAMPLE_PERSON + "\n\noverride: override-password",
+        )
         messages = self._retrieve_mails()
         assert len(messages) == 1
         mail_text = self._extract_message_body(messages[0])
-        assert messages[0]['Subject'] == 'SUCCESS: my subject'
-        assert messages[0]['From'] == 'from@example.com'
-        assert messages[0]['To'] == 'Sasha <sasha@example.com>'
-        assert '\nCreate succeeded: [mntner] TEST-MNT\n' in mail_text
-        assert '\nCreate succeeded: [person] PERSON-TEST\n' in mail_text
-        assert 'email footer' in mail_text
-        assert 'Generated by IRRd version ' in mail_text
+        assert messages[0]["Subject"] == "SUCCESS: my subject"
+        assert messages[0]["From"] == "from@example.com"
+        assert messages[0]["To"] == "Sasha <sasha@example.com>"
+        assert "\nCreate succeeded: [mntner] TEST-MNT\n" in mail_text
+        assert "\nCreate succeeded: [person] PERSON-TEST\n" in mail_text
+        assert "email footer" in mail_text
+        assert "Generated by IRRd version " in mail_text
 
         # Load samples of all known objects, using the mntner password
-        self._submit_update(self.config_path1, LARGE_UPDATE + '\n\npassword: md5-password')
+        self._submit_update(self.config_path1, LARGE_UPDATE + "\n\npassword: md5-password")
         messages = self._retrieve_mails()
         assert len(messages) == 3
         mail_text = self._extract_message_body(messages[0])
-        assert messages[0]['Subject'] == 'SUCCESS: my subject'
-        assert messages[0]['From'] == 'from@example.com'
-        assert messages[0]['To'] == 'Sasha <sasha@example.com>'
-        assert '\nINFO: AS number as065537 was reformatted as AS65537\n' in mail_text
-        assert '\nCreate succeeded: [filter-set] FLTR-SETTEST\n' in mail_text
-        assert '\nINFO: Address range 192.0.2.0 - 192.0.02.255 was reformatted as 192.0.2.0 - 192.0.2.255\n' in mail_text
-        assert '\nINFO: Address prefix 192.0.02.0/24 was reformatted as 192.0.2.0/24\n' in mail_text
-        assert '\nINFO: Route set member 2001:0dB8::/48 was reformatted as 2001:db8::/48\n' in mail_text
+        assert messages[0]["Subject"] == "SUCCESS: my subject"
+        assert messages[0]["From"] == "from@example.com"
+        assert messages[0]["To"] == "Sasha <sasha@example.com>"
+        assert "\nINFO: AS number as065537 was reformatted as AS65537\n" in mail_text
+        assert "\nCreate succeeded: [filter-set] FLTR-SETTEST\n" in mail_text
+        assert (
+            "\nINFO: Address range 192.0.2.0 - 192.0.02.255 was reformatted as 192.0.2.0 - 192.0.2.255\n"
+            in mail_text
+        )
+        assert "\nINFO: Address prefix 192.0.02.0/24 was reformatted as 192.0.2.0/24\n" in mail_text
+        assert "\nINFO: Route set member 2001:0dB8::/48 was reformatted as 2001:db8::/48\n" in mail_text
 
         # Check whether the objects can be queried from irrd #1,
         # and whether the hash is masked.
-        mntner_text = whois_query('127.0.0.1', self.port_whois1, 'TEST-MNT')
-        assert 'TEST-MNT' in mntner_text
+        mntner_text = whois_query("127.0.0.1", self.port_whois1, "TEST-MNT")
+        assert "TEST-MNT" in mntner_text
         assert PASSWORD_HASH_DUMMY_VALUE in mntner_text
-        assert 'unįcöde tæst 🌈🦄' in mntner_text
-        assert 'PERSON-TEST' in mntner_text
+        assert "unįcöde tæst 🌈🦄" in mntner_text
+        assert "PERSON-TEST" in mntner_text
 
         # (This is the first instance of an object with unicode chars
         # appearing on the NRTM stream.)
         time.sleep(3)
-        mntner_text = whois_query('127.0.0.1', self.port_whois2, 'TEST-MNT')
-        assert 'TEST-MNT' in mntner_text
+        mntner_text = whois_query("127.0.0.1", self.port_whois2, "TEST-MNT")
+        assert "TEST-MNT" in mntner_text
         assert PASSWORD_HASH_DUMMY_VALUE in mntner_text
-        assert 'unįcöde tæst 🌈🦄' in mntner_text
-        assert 'PERSON-TEST' in mntner_text
+        assert "unįcöde tæst 🌈🦄" in mntner_text
+        assert "PERSON-TEST" in mntner_text
 
         # These queries have different responses on #1 than #2,
         # as all IPv4 routes are RPKI invalid on #2.
-        query_result = whois_query_irrd('127.0.0.1', self.port_whois1, '!gAS65537')
-        assert query_result == '192.0.2.0/24'
-        query_result = whois_query_irrd('127.0.0.1', self.port_whois1, '!gAS65547')
-        assert query_result == '192.0.2.0/32'  # Pseudo-IRR object from RPKI
-        query_result = whois_query_irrd('127.0.0.1', self.port_whois1, '!6AS65537')
-        assert query_result == '2001:db8::/48'
-        query_result = whois_query_irrd('127.0.0.1', self.port_whois1, '!iRS-TEST')
-        assert set(query_result.split(' ')) == {'192.0.2.0/24', '2001:db8::/48', 'RS-OTHER-SET'}
-        query_result = whois_query_irrd('127.0.0.1', self.port_whois1, '!aAS65537:AS-SETTEST')
-        assert set(query_result.split(' ')) == {'192.0.2.0/24', '2001:db8::/48'}
-        query_result = whois_query_irrd('127.0.0.1', self.port_whois1, '!aAS65537:AS-TESTREF')
-        assert set(query_result.split(' ')) == {'192.0.2.0/24', '2001:db8::/48'}
-        query_result = whois_query_irrd('127.0.0.1', self.port_whois1, '!a4AS65537:AS-TESTREF')
-        assert query_result == '192.0.2.0/24'
-        query_result = whois_query_irrd('127.0.0.1', self.port_whois1, '!a6AS65537:AS-TESTREF')
-        assert query_result == '2001:db8::/48'
-        query_result = whois_query_irrd('127.0.0.1', self.port_whois1, '!r192.0.2.0/24')
-        assert 'example route' in query_result
-        query_result = whois_query_irrd('127.0.0.1', self.port_whois1, '!r192.0.2.0/25,l')
-        assert 'example route' in query_result
-        query_result = whois_query_irrd('127.0.0.1', self.port_whois1, '!r192.0.2.0/24,L')
-        assert 'example route' in query_result
-        query_result = whois_query_irrd('127.0.0.1', self.port_whois1, '!r192.0.2.0/23,M')
-        assert 'example route' in query_result
-        query_result = whois_query_irrd('127.0.0.1', self.port_whois1, '!r192.0.2.0/24,M')
-        assert 'RPKI' in query_result  # Does not match the /24, does match the RPKI pseudo-IRR /32
-        query_result = whois_query_irrd('127.0.0.1', self.port_whois1, '!r192.0.2.0/24,o')
-        assert query_result == 'AS65537'
-        query_result = whois_query('127.0.0.1', self.port_whois1, '-x 192.0.02.0/24')
-        assert 'example route' in query_result
-        query_result = whois_query('127.0.0.1', self.port_whois1, '-l 192.0.02.0/25')
-        assert 'example route' in query_result
-        query_result = whois_query('127.0.0.1', self.port_whois1, '-L 192.0.02.0/24')
-        assert 'example route' in query_result
-        query_result = whois_query('127.0.0.1', self.port_whois1, '-M 192.0.02.0/23')
-        assert 'example route' in query_result
-        query_result = whois_query('127.0.0.1', self.port_whois1, '-i member-of RS-test')
-        assert 'example route' in query_result
+        query_result = whois_query_irrd("127.0.0.1", self.port_whois1, "!gAS65537")
+        assert query_result == "192.0.2.0/24"
+        query_result = whois_query_irrd("127.0.0.1", self.port_whois1, "!gAS65547")
+        assert query_result == "192.0.2.0/32"  # Pseudo-IRR object from RPKI
+        query_result = whois_query_irrd("127.0.0.1", self.port_whois1, "!6AS65537")
+        assert query_result == "2001:db8::/48"
+        query_result = whois_query_irrd("127.0.0.1", self.port_whois1, "!iRS-TEST")
+        assert set(query_result.split(" ")) == {"192.0.2.0/24", "2001:db8::/48", "RS-OTHER-SET"}
+        query_result = whois_query_irrd("127.0.0.1", self.port_whois1, "!aAS65537:AS-SETTEST")
+        assert set(query_result.split(" ")) == {"192.0.2.0/24", "2001:db8::/48"}
+        query_result = whois_query_irrd("127.0.0.1", self.port_whois1, "!aAS65537:AS-TESTREF")
+        assert set(query_result.split(" ")) == {"192.0.2.0/24", "2001:db8::/48"}
+        query_result = whois_query_irrd("127.0.0.1", self.port_whois1, "!a4AS65537:AS-TESTREF")
+        assert query_result == "192.0.2.0/24"
+        query_result = whois_query_irrd("127.0.0.1", self.port_whois1, "!a6AS65537:AS-TESTREF")
+        assert query_result == "2001:db8::/48"
+        query_result = whois_query_irrd("127.0.0.1", self.port_whois1, "!r192.0.2.0/24")
+        assert "example route" in query_result
+        query_result = whois_query_irrd("127.0.0.1", self.port_whois1, "!r192.0.2.0/25,l")
+        assert "example route" in query_result
+        query_result = whois_query_irrd("127.0.0.1", self.port_whois1, "!r192.0.2.0/24,L")
+        assert "example route" in query_result
+        query_result = whois_query_irrd("127.0.0.1", self.port_whois1, "!r192.0.2.0/23,M")
+        assert "example route" in query_result
+        query_result = whois_query_irrd("127.0.0.1", self.port_whois1, "!r192.0.2.0/24,M")
+        assert "RPKI" in query_result  # Does not match the /24, does match the RPKI pseudo-IRR /32
+        query_result = whois_query_irrd("127.0.0.1", self.port_whois1, "!r192.0.2.0/24,o")
+        assert query_result == "AS65537"
+        query_result = whois_query("127.0.0.1", self.port_whois1, "-x 192.0.02.0/24")
+        assert "example route" in query_result
+        query_result = whois_query("127.0.0.1", self.port_whois1, "-l 192.0.02.0/25")
+        assert "example route" in query_result
+        query_result = whois_query("127.0.0.1", self.port_whois1, "-L 192.0.02.0/24")
+        assert "example route" in query_result
+        query_result = whois_query("127.0.0.1", self.port_whois1, "-M 192.0.02.0/23")
+        assert "example route" in query_result
+        query_result = whois_query("127.0.0.1", self.port_whois1, "-i member-of RS-test")
+        assert "example route" in query_result
 
-        query_result = whois_query_irrd('127.0.0.1', self.port_whois2, '!gAS65537')
+        query_result = whois_query_irrd("127.0.0.1", self.port_whois2, "!gAS65537")
         assert not query_result
-        query_result = whois_query_irrd('127.0.0.1', self.port_whois2, '!6AS65537')
-        assert query_result == '2001:db8::/48'
-        query_result = whois_query_irrd('127.0.0.1', self.port_whois2, '!iRS-TEST')
-        assert query_result == '2001:db8::/48 RS-OTHER-SET'
-        query_result = whois_query_irrd('127.0.0.1', self.port_whois2, '!aAS65537:AS-SETTEST')
-        assert query_result == '2001:db8::/48'
-        query_result = whois_query_irrd('127.0.0.1', self.port_whois2, '!aAS65537:AS-TESTREF')
-        assert query_result == '2001:db8::/48'
-        query_result = whois_query('127.0.0.1', self.port_whois2, '-x 192.0.02.0/24')
-        assert 'example route' not in query_result
-        query_result = whois_query_irrd('127.0.0.1', self.port_whois2, '!r192.0.2.0/24,L')
-        assert 'RPKI' in query_result  # Pseudo-IRR object 0/0 from RPKI
+        query_result = whois_query_irrd("127.0.0.1", self.port_whois2, "!6AS65537")
+        assert query_result == "2001:db8::/48"
+        query_result = whois_query_irrd("127.0.0.1", self.port_whois2, "!iRS-TEST")
+        assert query_result == "2001:db8::/48 RS-OTHER-SET"
+        query_result = whois_query_irrd("127.0.0.1", self.port_whois2, "!aAS65537:AS-SETTEST")
+        assert query_result == "2001:db8::/48"
+        query_result = whois_query_irrd("127.0.0.1", self.port_whois2, "!aAS65537:AS-TESTREF")
+        assert query_result == "2001:db8::/48"
+        query_result = whois_query("127.0.0.1", self.port_whois2, "-x 192.0.02.0/24")
+        assert "example route" not in query_result
+        query_result = whois_query_irrd("127.0.0.1", self.port_whois2, "!r192.0.2.0/24,L")
+        assert "RPKI" in query_result  # Pseudo-IRR object 0/0 from RPKI
         # RPKI invalid object should not be in journal
-        query_result = whois_query('127.0.0.1', self.port_whois2, '-g TEST:3:1-LAST')
-        assert 'route:192.0.2.0/24' not in query_result.replace(' ', '')
+        query_result = whois_query("127.0.0.1", self.port_whois2, "-g TEST:3:1-LAST")
+        assert "route:192.0.2.0/24" not in query_result.replace(" ", "")
 
         # These queries should produce identical answers on both instances.
         for port in self.port_whois1, self.port_whois2:
-            query_result = whois_query_irrd('127.0.0.1', port, '!iAS65537:AS-SETTEST')
-            assert set(query_result.split(' ')) == {'AS65537', 'AS65538', 'AS65539', 'AS-OTHERSET'}
-            query_result = whois_query_irrd('127.0.0.1', port, '!iAS65537:AS-TESTREF')
-            assert set(query_result.split(' ')) == {'AS65537:AS-SETTEST', 'AS65540'}
-            query_result = whois_query_irrd('127.0.0.1', port, '!iAS65537:AS-TESTREF,1')
-            assert set(query_result.split(' ')) == {'AS65537', 'AS65538', 'AS65539', 'AS65540'}
-            query_result = whois_query_irrd('127.0.0.1', port, '!maut-num,as65537')
-            assert 'AS65537' in query_result
-            assert 'TEST-AS' in query_result
-            query_result = whois_query_irrd('127.0.0.1', port, '!oTEST-MNT')
-            assert 'AS65537' in query_result
-            assert 'TEST-AS' in query_result
-            assert 'AS65536 - AS65538' in query_result
-            assert 'rtrs-settest' in query_result
-            query_result = whois_query('127.0.0.1', port, '-T route6 -i member-of RS-TEST')
-            assert 'No entries found for the selected source(s)' in query_result
-            query_result = whois_query('127.0.0.1', port, 'dashcare')
-            assert 'ROLE-TEST' in query_result
+            query_result = whois_query_irrd("127.0.0.1", port, "!iAS65537:AS-SETTEST")
+            assert set(query_result.split(" ")) == {"AS65537", "AS65538", "AS65539", "AS-OTHERSET"}
+            query_result = whois_query_irrd("127.0.0.1", port, "!iAS65537:AS-TESTREF")
+            assert set(query_result.split(" ")) == {"AS65537:AS-SETTEST", "AS65540"}
+            query_result = whois_query_irrd("127.0.0.1", port, "!iAS65537:AS-TESTREF,1")
+            assert set(query_result.split(" ")) == {"AS65537", "AS65538", "AS65539", "AS65540"}
+            query_result = whois_query_irrd("127.0.0.1", port, "!maut-num,as65537")
+            assert "AS65537" in query_result
+            assert "TEST-AS" in query_result
+            query_result = whois_query_irrd("127.0.0.1", port, "!oTEST-MNT")
+            assert "AS65537" in query_result
+            assert "TEST-AS" in query_result
+            assert "AS65536 - AS65538" in query_result
+            assert "rtrs-settest" in query_result
+            query_result = whois_query("127.0.0.1", port, "-T route6 -i member-of RS-TEST")
+            assert "No entries found for the selected source(s)" in query_result
+            query_result = whois_query("127.0.0.1", port, "dashcare")
+            assert "ROLE-TEST" in query_result
 
         # Check the mirroring status
-        query_result = whois_query_irrd('127.0.0.1', self.port_whois1, '!J-*')
+        query_result = whois_query_irrd("127.0.0.1", self.port_whois1, "!J-*")
         result = ujson.loads(query_result)
-        assert result['TEST']['serial_newest_journal'] == 29
-        assert result['TEST']['serial_last_export'] == 29
-        assert result['TEST']['serial_newest_mirror'] is None
+        assert result["TEST"]["serial_newest_journal"] == 29
+        assert result["TEST"]["serial_last_export"] == 29
+        assert result["TEST"]["serial_newest_mirror"] is None
         # irrd #2 missed the first update from NRTM, as they were done at
         # the same time and loaded from the full export, and one RPKI-invalid object
         # was not recorded in the journal, so its local serial should
         # is lower by three
-        query_result = whois_query_irrd('127.0.0.1', self.port_whois2, '!J-*')
+        query_result = whois_query_irrd("127.0.0.1", self.port_whois2, "!J-*")
         result = ujson.loads(query_result)
-        assert result['TEST']['serial_newest_journal'] == 26
-        assert result['TEST']['serial_last_export'] == 26
-        assert result['TEST']['serial_newest_mirror'] == 29
+        assert result["TEST"]["serial_newest_journal"] == 26
+        assert result["TEST"]["serial_last_export"] == 26
+        assert result["TEST"]["serial_newest_mirror"] == 29
 
         # Make the v4 route in irrd2 valid
-        with open(self.roa_source2, 'w') as roa_file:
-            ujson.dump({'roas': [{'prefix': '198.51.100.0/24', 'asn': 'AS0', 'maxLength': '32', 'ta': 'TA'}]}, roa_file)
+        with open(self.roa_source2, "w") as roa_file:
+            ujson.dump(
+                {"roas": [{"prefix": "198.51.100.0/24", "asn": "AS0", "maxLength": "32", "ta": "TA"}]},
+                roa_file,
+            )
 
         time.sleep(3)
-        query_result = whois_query_irrd('127.0.0.1', self.port_whois2, '!gAS65537')
-        assert query_result == '192.0.2.0/24'
+        query_result = whois_query_irrd("127.0.0.1", self.port_whois2, "!gAS65537")
+        assert query_result == "192.0.2.0/24"
         # RPKI invalid object should now be added in the journal
-        query_result = whois_query('127.0.0.1', self.port_whois2, '-g TEST:3:27-27')
-        assert 'ADD 27' in query_result
-        assert '192.0.2.0/24' in query_result
-        query_result = whois_query_irrd('127.0.0.1', self.port_whois2, '!J-*')
+        query_result = whois_query("127.0.0.1", self.port_whois2, "-g TEST:3:27-27")
+        assert "ADD 27" in query_result
+        assert "192.0.2.0/24" in query_result
+        query_result = whois_query_irrd("127.0.0.1", self.port_whois2, "!J-*")
         result = ujson.loads(query_result)
-        assert result['TEST']['serial_newest_journal'] == 27
-        assert result['TEST']['serial_last_export'] == 27
+        assert result["TEST"]["serial_newest_journal"] == 27
+        assert result["TEST"]["serial_last_export"] == 27
         # This was a local journal update from RPKI status change,
         # so serial_newest_mirror did not update.
-        assert result['TEST']['serial_newest_mirror'] == 29
+        assert result["TEST"]["serial_newest_mirror"] == 29
 
         # Make the v4 route in irrd2 invalid again
-        with open(self.roa_source2, 'w') as roa_file:
-            ujson.dump({'roas': [{'prefix': '128/1', 'asn': 'AS0', 'maxLength': '32', 'ta': 'TA'}]}, roa_file)
+        with open(self.roa_source2, "w") as roa_file:
+            ujson.dump({"roas": [{"prefix": "128/1", "asn": "AS0", "maxLength": "32", "ta": "TA"}]}, roa_file)
 
         time.sleep(3)
-        query_result = whois_query_irrd('127.0.0.1', self.port_whois2, '!gAS65537')
+        query_result = whois_query_irrd("127.0.0.1", self.port_whois2, "!gAS65537")
         assert not query_result
         # RPKI invalid object should now be deleted in the journal
-        query_result = whois_query('127.0.0.1', self.port_whois2, '-g TEST:3:28-28')
-        assert 'DEL 28' in query_result
-        assert '192.0.2.0/24' in query_result
-        query_result = whois_query_irrd('127.0.0.1', self.port_whois2, '!J-*')
+        query_result = whois_query("127.0.0.1", self.port_whois2, "-g TEST:3:28-28")
+        assert "DEL 28" in query_result
+        assert "192.0.2.0/24" in query_result
+        query_result = whois_query_irrd("127.0.0.1", self.port_whois2, "!J-*")
         result = ujson.loads(query_result)
-        assert result['TEST']['serial_newest_journal'] == 28
-        assert result['TEST']['serial_last_export'] == 28
-        assert result['TEST']['serial_newest_mirror'] == 29
+        assert result["TEST"]["serial_newest_journal"] == 28
+        assert result["TEST"]["serial_last_export"] == 28
+        assert result["TEST"]["serial_newest_mirror"] == 29
 
         # Make the v4 route in irrd1 invalid, triggering a mail
-        with open(self.roa_source1, 'w') as roa_file:
-            ujson.dump({'roas': [{'prefix': '128/1', 'asn': 'AS0', 'maxLength': '32', 'ta': 'TA'}]}, roa_file)
+        with open(self.roa_source1, "w") as roa_file:
+            ujson.dump({"roas": [{"prefix": "128/1", "asn": "AS0", "maxLength": "32", "ta": "TA"}]}, roa_file)
 
         # irrd1 is authoritative for the now invalid v4 route, should have sent mail
         time.sleep(2)
         messages = self._retrieve_mails()
         assert len(messages) == 3
         mail_text = self._extract_message_body(messages[0])
-        assert messages[0]['Subject'] == 'route(6) objects in TEST marked RPKI invalid'
-        expected_recipients = {'email@example.com', 'mnt-nfy@example.net', 'mnt-nfy2@example.net'}
-        assert {m['To'] for m in messages} == expected_recipients
-        assert '192.0.2.0/24' in mail_text
+        assert messages[0]["Subject"] == "route(6) objects in TEST marked RPKI invalid"
+        expected_recipients = {"email@example.com", "mnt-nfy@example.net", "mnt-nfy2@example.net"}
+        assert {m["To"] for m in messages} == expected_recipients
+        assert "192.0.2.0/24" in mail_text
 
         self.check_http()
         self.check_graphql()
 
     def check_http(self):
-        status1 = requests.get(f'http://127.0.0.1:{self.port_http1}/v1/status/')
-        status2 = requests.get(f'http://127.0.0.1:{self.port_http2}/v1/status/')
+        status1 = requests.get(f"http://127.0.0.1:{self.port_http1}/v1/status/")
+        status2 = requests.get(f"http://127.0.0.1:{self.port_http2}/v1/status/")
         assert status1.status_code == 200
         assert status2.status_code == 200
-        assert 'IRRD version' in status1.text
-        assert 'IRRD version' in status2.text
-        assert 'TEST' in status1.text
-        assert 'TEST' in status2.text
-        assert 'RPKI' in status1.text
-        assert 'RPKI' in status2.text
-        assert 'Authoritative: Yes' in status1.text
-        assert 'Authoritative: Yes' not in status2.text
+        assert "IRRD version" in status1.text
+        assert "IRRD version" in status2.text
+        assert "TEST" in status1.text
+        assert "TEST" in status2.text
+        assert "RPKI" in status1.text
+        assert "RPKI" in status2.text
+        assert "Authoritative: Yes" in status1.text
+        assert "Authoritative: Yes" not in status2.text
 
     def check_graphql(self):
         client = GraphqlClient(endpoint=f"http://127.0.0.1:{self.port_http1}/graphql/")
         # Regular rpslObjects query including journal and several references
         query = """query {
           rpslObjects(rpslPk: "PERSON-TEST") {
             rpslPk
@@ -545,43 +621,48 @@
               operation
               origin
             }
           }
         }
         """
         result = client.execute(query=query)
-        assert result['data']['rpslObjects'] == [{
-            'rpslPk': 'PERSON-TEST',
-            'mntBy': ['TEST-MNT'],
-            'mntByObjs': [{'rpslPk': 'TEST-MNT', 'adminCObjs': [{'rpslPk': 'PERSON-TEST'}]}],
-            'journal': [
-                {'serialNrtm': 2, 'operation': 'add_or_update', 'origin': 'auth_change'},
-                {'serialNrtm': 4, 'operation': 'add_or_update', 'origin': 'auth_change'},
-                {'serialNrtm': 5, 'operation': 'delete', 'origin': 'auth_change'},
-                {'serialNrtm': 9, 'operation': 'add_or_update', 'origin': 'auth_change'}
-            ]
-        }]
+        assert result["data"]["rpslObjects"] == [
+            {
+                "rpslPk": "PERSON-TEST",
+                "mntBy": ["TEST-MNT"],
+                "mntByObjs": [{"rpslPk": "TEST-MNT", "adminCObjs": [{"rpslPk": "PERSON-TEST"}]}],
+                "journal": [
+                    {"serialNrtm": 2, "operation": "add_or_update", "origin": "auth_change"},
+                    {"serialNrtm": 4, "operation": "add_or_update", "origin": "auth_change"},
+                    {"serialNrtm": 5, "operation": "delete", "origin": "auth_change"},
+                    {"serialNrtm": 9, "operation": "add_or_update", "origin": "auth_change"},
+                ],
+            }
+        ]
 
         # Test memberOfObjs resolving and IP search
         query = """query {
           rpslObjects(ipLessSpecificOneLevel: "192.0.2.1" rpkiStatus:[invalid,valid,not_found]) {
             rpslPk
             ... on RPSLRoute {
                 memberOfObjs {
                   rpslPk
                 }
             }
           }
         }
         """
         result = client.execute(query=query)
-        self.assertCountEqual(result['data']['rpslObjects'], [
-            {'rpslPk': '192.0.2.0/24AS65537', 'memberOfObjs': [{'rpslPk': 'RS-TEST'}]},
-            {'rpslPk': '192.0.2.0 - 192.0.2.255'}
-        ])
+        self.assertCountEqual(
+            result["data"]["rpslObjects"],
+            [
+                {"rpslPk": "192.0.2.0/24AS65537", "memberOfObjs": [{"rpslPk": "RS-TEST"}]},
+                {"rpslPk": "192.0.2.0 - 192.0.2.255"},
+            ],
+        )
 
         # Test membersObjs and mbrsByRefObjs resolving
         query = """query {
           rpslObjects(rpslPk: ["AS65537:AS-TESTREF", "DOESNOTEXIST"]) {
             rpslPk
             ... on RPSLAsSet {
                 membersObjs {
@@ -591,284 +672,305 @@
                   rpslPk
                 }
             }
           }
         }
         """
         result = client.execute(query=query)
-        assert result['data']['rpslObjects'] == [{
-            'rpslPk': 'AS65537:AS-TESTREF',
-            'membersObjs': [{'rpslPk': 'AS65537:AS-SETTEST'}],
-            'mbrsByRefObjs': [{'rpslPk': 'TEST-MNT'}],
-        }]
+        assert result["data"]["rpslObjects"] == [
+            {
+                "rpslPk": "AS65537:AS-TESTREF",
+                "membersObjs": [{"rpslPk": "AS65537:AS-SETTEST"}],
+                "mbrsByRefObjs": [{"rpslPk": "TEST-MNT"}],
+            }
+        ]
 
         # Test databaseStatus query
         query = """query {
           databaseStatus {
             source
             authoritative
             serialOldestJournal
             serialNewestJournal
             serialNewestMirror
           }
         }
         """
         result = client.execute(query=query)
-        self.assertCountEqual(result['data']['databaseStatus'], [
-            {
-                'source': 'TEST',
-                'authoritative': True,
-                'serialOldestJournal': 1,
-                'serialNewestJournal': 30,
-                'serialNewestMirror': None
-            }, {
-                'source': 'RPKI',
-                'authoritative': False,
-                'serialOldestJournal': None,
-                'serialNewestJournal': None,
-                'serialNewestMirror': None
-            }
-        ])
+        self.assertCountEqual(
+            result["data"]["databaseStatus"],
+            [
+                {
+                    "source": "TEST",
+                    "authoritative": True,
+                    "serialOldestJournal": 1,
+                    "serialNewestJournal": 30,
+                    "serialNewestMirror": None,
+                },
+                {
+                    "source": "RPKI",
+                    "authoritative": False,
+                    "serialOldestJournal": None,
+                    "serialNewestJournal": None,
+                    "serialNewestMirror": None,
+                },
+            ],
+        )
 
         # Test asnPrefixes query
         query = """query {
           asnPrefixes(asns: [65537]) {
             asn
             prefixes
           }
         }
         """
         result = client.execute(query=query)
-        asnPrefixes = result['data']['asnPrefixes']
+        asnPrefixes = result["data"]["asnPrefixes"]
         assert len(asnPrefixes) == 1
-        assert asnPrefixes[0]['asn'] == 65537
-        assert set(asnPrefixes[0]['prefixes']) == {'2001:db8::/48'}
+        assert asnPrefixes[0]["asn"] == 65537
+        assert set(asnPrefixes[0]["prefixes"]) == {"2001:db8::/48"}
 
         # Test asSetPrefixes query
         query = """query {
           asSetPrefixes(setNames: ["AS65537:AS-TESTREF"]) {
             rpslPk
             prefixes
           }
         }
         """
         result = client.execute(query=query)
-        asSetPrefixes = result['data']['asSetPrefixes']
+        asSetPrefixes = result["data"]["asSetPrefixes"]
         assert len(asSetPrefixes) == 1
-        assert asSetPrefixes[0]['rpslPk'] == 'AS65537:AS-TESTREF'
-        assert set(asSetPrefixes[0]['prefixes']) == {'2001:db8::/48'}
+        assert asSetPrefixes[0]["rpslPk"] == "AS65537:AS-TESTREF"
+        assert set(asSetPrefixes[0]["prefixes"]) == {"2001:db8::/48"}
 
         # Test recursiveSetMembers query
         query = """query {
           recursiveSetMembers(setNames: ["AS65537:AS-TESTREF"]) {
             rpslPk
             rootSource
             members
           }
         }
         """
         result = client.execute(query=query)
-        recursiveSetMembers = result['data']['recursiveSetMembers']
+        recursiveSetMembers = result["data"]["recursiveSetMembers"]
         assert len(recursiveSetMembers) == 1
-        assert recursiveSetMembers[0]['rpslPk'] == 'AS65537:AS-TESTREF'
-        assert recursiveSetMembers[0]['rootSource'] == 'TEST'
-        assert set(recursiveSetMembers[0]['members']) == {
-            'AS65537', 'AS65538', 'AS65539', 'AS65540'
-        }
+        assert recursiveSetMembers[0]["rpslPk"] == "AS65537:AS-TESTREF"
+        assert recursiveSetMembers[0]["rootSource"] == "TEST"
+        assert set(recursiveSetMembers[0]["members"]) == {"AS65537", "AS65538", "AS65539", "AS65540"}
 
     def _start_mailserver(self):
         """
         Start the mailserver through twisted. This special SMTP server is
         configured as the SMTP server for both IRRd instances.
         It keeps mails in memory, and _retrieve_mails() can retrieve them
         using special SMTP commands.
         """
-        self.pidfile_mailserver = str(self.tmpdir) + '/mailserver.pid'
-        self.logfile_mailserver = str(self.tmpdir) + '/mailserver.log'
-        mailserver_path = IRRD_ROOT_PATH + '/irrd/integration_tests/mailserver.tac'
-        assert not subprocess.call(['twistd', f'--pidfile={self.pidfile_mailserver}',
-                                    f'--logfile={self.logfile_mailserver}', '-y', mailserver_path])
+        self.pidfile_mailserver = str(self.tmpdir) + "/mailserver.pid"
+        self.logfile_mailserver = str(self.tmpdir) + "/mailserver.log"
+        mailserver_path = IRRD_ROOT_PATH + "/irrd/integration_tests/mailserver.tac"
+        assert not subprocess.call(
+            [
+                "twistd",
+                f"--pidfile={self.pidfile_mailserver}",
+                f"--logfile={self.logfile_mailserver}",
+                "-y",
+                mailserver_path,
+            ]
+        )
 
     # noinspection PyTypeChecker
     def _start_irrds(self):
         """
         Configure and start two independent instances of IRRd.
         IRRd #1 has an authoritative database, IRRd #2 mirrors that database
         from #1.
         """
-        self.database_url1 = os.environ['IRRD_DATABASE_URL_INTEGRATION_1']
-        self.database_url2 = os.environ['IRRD_DATABASE_URL_INTEGRATION_2']
-        self.redis_url1 = os.environ['IRRD_REDIS_URL_INTEGRATION_1']
-        self.redis_url2 = os.environ['IRRD_REDIS_URL_INTEGRATION_2']
-
-        self.config_path1 = str(self.tmpdir) + '/irrd1_config.yaml'
-        self.config_path2 = str(self.tmpdir) + '/irrd2_config.yaml'
-        self.logfile1 = str(self.tmpdir) + '/irrd1.log'
-        self.logfile2 = str(self.tmpdir) + '/irrd2.log'
-        self.roa_source1 = str(self.tmpdir) + '/roa1.json'
-        self.roa_source2 = str(self.tmpdir) + '/roa2.json'
-        self.export_dir1 = str(self.tmpdir) + '/export1/'
-        self.export_dir2 = str(self.tmpdir) + '/export2/'
-        self.piddir1 = str(self.tmpdir) + '/piddir1/'
-        self.piddir2 = str(self.tmpdir) + '/piddir2/'
-        self.pidfile1 = self.piddir1 + 'irrd.pid'
-        self.pidfile2 = self.piddir2 + 'irrd.pid'
+        self.database_url1 = os.environ["IRRD_DATABASE_URL_INTEGRATION_1"]
+        self.database_url2 = os.environ["IRRD_DATABASE_URL_INTEGRATION_2"]
+        self.redis_url1 = os.environ["IRRD_REDIS_URL_INTEGRATION_1"]
+        self.redis_url2 = os.environ["IRRD_REDIS_URL_INTEGRATION_2"]
+
+        self.config_path1 = str(self.tmpdir) + "/irrd1_config.yaml"
+        self.config_path2 = str(self.tmpdir) + "/irrd2_config.yaml"
+        self.logfile1 = str(self.tmpdir) + "/irrd1.log"
+        self.logfile2 = str(self.tmpdir) + "/irrd2.log"
+        self.roa_source1 = str(self.tmpdir) + "/roa1.json"
+        self.roa_source2 = str(self.tmpdir) + "/roa2.json"
+        self.export_dir1 = str(self.tmpdir) + "/export1/"
+        self.export_dir2 = str(self.tmpdir) + "/export2/"
+        self.piddir1 = str(self.tmpdir) + "/piddir1/"
+        self.piddir2 = str(self.tmpdir) + "/piddir2/"
+        self.pidfile1 = self.piddir1 + "irrd.pid"
+        self.pidfile2 = self.piddir2 + "irrd.pid"
         os.mkdir(self.export_dir1)
         os.mkdir(self.export_dir2)
         os.mkdir(self.piddir1)
         os.mkdir(self.piddir2)
 
-        print(textwrap.dedent(f"""
+        print(
+            textwrap.dedent(
+                f"""
             Preparing to start IRRd for integration test.
             
             IRRd #1 running on HTTP port {self.port_http1}, whois port {self.port_whois1}
             Config in: {self.config_path1}
             Database URL: {self.database_url1}
             PID file: {self.pidfile1}
             Logfile: {self.logfile1}
 
             IRRd #2 running on HTTP port {self.port_http2}, whois port {self.port_whois2}
             Config in: {self.config_path2}
             Database URL: {self.database_url2}
             PID file: {self.pidfile2}
             Logfile: {self.logfile2}
-        """))
+        """
+            )
+        )
 
-        with open(self.roa_source1, 'w') as roa_file:
-            ujson.dump({'roas': [{'prefix': '192.0.2.0/32', 'asn': 'AS65547', 'maxLength': '32', 'ta': 'TA'}]}, roa_file)
-        with open(self.roa_source2, 'w') as roa_file:
-            ujson.dump({'roas': [{'prefix': '128/1', 'asn': 'AS0', 'maxLength': '1', 'ta': 'TA'}]}, roa_file)
+        with open(self.roa_source1, "w") as roa_file:
+            ujson.dump(
+                {"roas": [{"prefix": "192.0.2.0/32", "asn": "AS65547", "maxLength": "32", "ta": "TA"}]},
+                roa_file,
+            )
+        with open(self.roa_source2, "w") as roa_file:
+            ujson.dump({"roas": [{"prefix": "128/1", "asn": "AS0", "maxLength": "1", "ta": "TA"}]}, roa_file)
 
         base_config = {
-            'irrd': {
-                'access_lists': {
-                    'localhost': ['::/32', '127.0.0.1']
+            "irrd": {
+                "access_lists": {"localhost": ["::/32", "127.0.0.1"]},
+                "server": {
+                    "http": {"status_access_list": "localhost", "interface": "::1", "port": 8080},
+                    "whois": {"interface": "::1", "max_connections": 10, "port": 8043},
                 },
-
-                'server': {
-                    'http': {
-                        'status_access_list': 'localhost',
-                        'interface': '::1',
-                        'port': 8080
+                "rpki": {
+                    "roa_import_timer": 1,
+                    "notify_invalid_enabled": True,
+                },
+                "auth": {
+                    "gnupg_keyring": None,
+                    "override_password": "$1$J6KycItM$MbPaBU6iFSGFV299Rk7Di0",
+                    "set_creation": {
+                        "filter-set": {
+                            "prefix_required": False,
+                        },
+                        "peering-set": {
+                            "prefix_required": False,
+                        },
+                        "route-set": {
+                            "prefix_required": False,
+                        },
+                        "rtr-set": {
+                            "prefix_required": False,
+                        },
                     },
-                    'whois': {
-                        'interface': '::1',
-                        'max_connections': 10,
-                        'port': 8043
+                    "password_hashers": {
+                        "crypt-pw": "enabled",
                     },
                 },
-
-                'rpki':{
-                    'roa_import_timer': 1,
-                    'notify_invalid_enabled': True,
+                "email": {
+                    "footer": "email footer",
+                    "from": "from@example.com",
+                    "smtp": f"localhost:{EMAIL_SMTP_PORT}",
                 },
-
-                'auth': {
-                    'gnupg_keyring': None,
-                    'override_password': '$1$J6KycItM$MbPaBU6iFSGFV299Rk7Di0',
-                },
-
-                'email': {
-                    'footer': 'email footer',
-                    'from': 'from@example.com',
-                    'smtp': f'localhost:{EMAIL_SMTP_PORT}',
+                "log": {
+                    "logfile_path": None,
+                    "level": "DEBUG",
                 },
-
-                'log': {
-                    'logfile_path': None,
-                    'level': 'DEBUG',
-                },
-
-                'sources': {}
+                "sources": {},
             }
         }
 
         config1 = base_config.copy()
-        config1['irrd']['piddir'] = self.piddir1
-        config1['irrd']['database_url'] = self.database_url1
-        config1['irrd']['redis_url'] = self.redis_url1
-        config1['irrd']['server']['http']['interface'] = '127.0.0.1'  # #306
-        config1['irrd']['server']['http']['port'] = self.port_http1
-        config1['irrd']['server']['whois']['interface'] = '127.0.0.1'
-        config1['irrd']['server']['whois']['port'] = self.port_whois1
-        config1['irrd']['auth']['gnupg_keyring'] = str(self.tmpdir) + '/gnupg1'
-        config1['irrd']['log']['logfile_path'] = self.logfile1
-        config1['irrd']['rpki']['roa_source'] = 'file://' + self.roa_source1
-        config1['irrd']['sources']['TEST'] = {
-            'authoritative': True,
-            'keep_journal': True,
-            'export_destination': self.export_dir1,
-            'export_timer': '1',
-            'nrtm_access_list': 'localhost',
+        config1["irrd"]["piddir"] = self.piddir1
+        config1["irrd"]["database_url"] = self.database_url1
+        config1["irrd"]["redis_url"] = self.redis_url1
+        config1["irrd"]["server"]["http"]["interface"] = "127.0.0.1"  # #306
+        config1["irrd"]["server"]["http"]["port"] = self.port_http1
+        config1["irrd"]["server"]["whois"]["interface"] = "127.0.0.1"
+        config1["irrd"]["server"]["whois"]["port"] = self.port_whois1
+        config1["irrd"]["auth"]["gnupg_keyring"] = str(self.tmpdir) + "/gnupg1"
+        config1["irrd"]["log"]["logfile_path"] = self.logfile1
+        config1["irrd"]["rpki"]["roa_source"] = "file://" + self.roa_source1
+        config1["irrd"]["sources"]["TEST"] = {
+            "authoritative": True,
+            "keep_journal": True,
+            "export_destination": self.export_dir1,
+            "export_timer": "1",
+            "nrtm_access_list": "localhost",
         }
-        with open(self.config_path1, 'w') as yaml_file:
+        with open(self.config_path1, "w") as yaml_file:
             yaml.safe_dump(config1, yaml_file)
 
         config2 = base_config.copy()
-        config2['irrd']['piddir'] = self.piddir2
-        config2['irrd']['database_url'] = self.database_url2
-        config2['irrd']['redis_url'] = self.redis_url2
-        config2['irrd']['server']['http']['port'] = self.port_http2
-        config2['irrd']['server']['whois']['port'] = self.port_whois2
-        config2['irrd']['auth']['gnupg_keyring'] = str(self.tmpdir) + '/gnupg2'
-        config2['irrd']['log']['logfile_path'] = self.logfile2
-        config2['irrd']['rpki']['roa_source'] = 'file://' + self.roa_source2
-        config2['irrd']['sources']['TEST'] = {
-            'keep_journal': True,
-            'import_serial_source': f'file://{self.export_dir1}/TEST.CURRENTSERIAL',
-            'import_source': f'file://{self.export_dir1}/test.db.gz',
-            'export_destination': self.export_dir2,
-            'import_timer': '1',
-            'export_timer': '1',
-            'nrtm_host': '127.0.0.1',
-            'nrtm_port': str(self.port_whois1),
-            'nrtm_access_list': 'localhost',
+        config2["irrd"]["piddir"] = self.piddir2
+        config2["irrd"]["database_url"] = self.database_url2
+        config2["irrd"]["redis_url"] = self.redis_url2
+        config2["irrd"]["server"]["http"]["port"] = self.port_http2
+        config2["irrd"]["server"]["whois"]["port"] = self.port_whois2
+        config2["irrd"]["auth"]["gnupg_keyring"] = str(self.tmpdir) + "/gnupg2"
+        config2["irrd"]["log"]["logfile_path"] = self.logfile2
+        config2["irrd"]["rpki"]["roa_source"] = "file://" + self.roa_source2
+        config2["irrd"]["sources"]["TEST"] = {
+            "keep_journal": True,
+            "import_serial_source": f"file://{self.export_dir1}/TEST.CURRENTSERIAL",
+            "import_source": f"file://{self.export_dir1}/test.db.gz",
+            "export_destination": self.export_dir2,
+            "import_timer": "1",
+            "export_timer": "1",
+            "nrtm_host": "127.0.0.1",
+            "nrtm_port": str(self.port_whois1),
+            "nrtm_access_list": "localhost",
         }
-        with open(self.config_path2, 'w') as yaml_file:
+        with open(self.config_path2, "w") as yaml_file:
             yaml.safe_dump(config2, yaml_file)
 
         self._prepare_database()
 
-        assert not subprocess.call(['irrd/daemon/main.py', f'--config={self.config_path1}'])
-        assert not subprocess.call(['irrd/daemon/main.py', f'--config={self.config_path2}'])
+        assert not subprocess.call(["irrd/daemon/main.py", f"--config={self.config_path1}"])
+        assert not subprocess.call(["irrd/daemon/main.py", f"--config={self.config_path2}"])
 
     def _prepare_database(self):
         """
         Prepare the databases for IRRd #1 and #2. This includes running
         migrations to create tables, and *wiping existing content*.
         """
         config_init(self.config_path1)
         alembic_cfg = config.Config()
-        alembic_cfg.set_main_option('script_location', f'{IRRD_ROOT_PATH}/irrd/storage/alembic')
-        command.upgrade(alembic_cfg, 'head')
+        alembic_cfg.set_main_option("script_location", f"{IRRD_ROOT_PATH}/irrd/storage/alembic")
+        command.upgrade(alembic_cfg, "head")
 
         connection = sa.create_engine(translate_url(self.database_url1)).connect()
-        connection.execute('DELETE FROM rpsl_objects')
-        connection.execute('DELETE FROM rpsl_database_journal')
-        connection.execute('DELETE FROM database_status')
-        connection.execute('DELETE FROM roa_object')
+        connection.execute("DELETE FROM rpsl_objects")
+        connection.execute("DELETE FROM rpsl_database_journal")
+        connection.execute("DELETE FROM database_status")
+        connection.execute("DELETE FROM roa_object")
 
         config_init(self.config_path2)
         alembic_cfg = config.Config()
-        alembic_cfg.set_main_option('script_location', f'{IRRD_ROOT_PATH}/irrd/storage/alembic')
-        command.upgrade(alembic_cfg, 'head')
+        alembic_cfg.set_main_option("script_location", f"{IRRD_ROOT_PATH}/irrd/storage/alembic")
+        command.upgrade(alembic_cfg, "head")
 
         connection = sa.create_engine(translate_url(self.database_url2)).connect()
-        connection.execute('DELETE FROM rpsl_objects')
-        connection.execute('DELETE FROM rpsl_database_journal')
-        connection.execute('DELETE FROM database_status')
-        connection.execute('DELETE FROM roa_object')
+        connection.execute("DELETE FROM rpsl_objects")
+        connection.execute("DELETE FROM rpsl_database_journal")
+        connection.execute("DELETE FROM database_status")
+        connection.execute("DELETE FROM roa_object")
 
     def _submit_update(self, config_path, request):
         """
         Submit an update to an IRRd by calling the email submission process
         with a specific config path. Request is the raw RPSL update, possibly
         signed with inline PGP.
         """
-        email = textwrap.dedent("""
+        email = (
+            textwrap.dedent(
+                """
             From submitter@example.com@localhost  Thu Jan  5 10:04:48 2018
             Received: from [127.0.0.1] (localhost.localdomain [127.0.0.1])
               by hostname (Postfix) with ESMTPS id 740AD310597
               for <irrd@example.com>; Thu,  5 Jan 2018 10:04:48 +0100 (CET)
             Message-ID: <1325754288.4989.6.camel@hostname>
             Subject: my subject
             Subject: not my subject
@@ -876,90 +978,96 @@
             To: sasha@localhost
             Date: Thu, 05 Jan 2018 10:04:48 +0100
             X-Mailer: Python 3.7
             Content-Transfer-Encoding: base64
             Content-Type: text/plain; charset=utf-8
             Mime-Version: 1.0
 
-        """).lstrip().encode('utf-8')
-        email += base64.b64encode(request.encode('utf-8'))
+        """
+            )
+            .lstrip()
+            .encode("utf-8")
+        )
+        email += base64.b64encode(request.encode("utf-8"))
 
-        script = IRRD_ROOT_PATH + '/irrd/scripts/submit_email.py'
-        p = subprocess.Popen([script, f'--config={config_path}'],
-                             stdin=subprocess.PIPE)
+        script = IRRD_ROOT_PATH + "/irrd/scripts/submit_email.py"
+        p = subprocess.Popen([script, f"--config={config_path}"], stdin=subprocess.PIPE)
         p.communicate(email)
         p.wait()
 
     def _retrieve_mails(self):
         """
         Retrieve all mails kept in storage by the special integration test
         SMTP server. Returns a list of email.Message objects.
         Will only return new mails since the last call.
         """
         s = socket.socket()
         s.settimeout(5)
-        s.connect(('localhost', EMAIL_SMTP_PORT))
+        s.connect(("localhost", EMAIL_SMTP_PORT))
 
-        s.sendall(f'{EMAIL_RETURN_MSGS_COMMAND}\r\n'.encode('ascii'))
+        s.sendall(f"{EMAIL_RETURN_MSGS_COMMAND}\r\n".encode("ascii"))
 
-        buffer = b''
+        buffer = b""
         while EMAIL_END not in buffer:
             data = s.recv(1024 * 1024)
             buffer += data
-        buffer = buffer.split(b'\n', 1)[1]
+        buffer = buffer.split(b"\n", 1)[1]
         buffer = buffer.split(EMAIL_END, 1)[0]
 
-        s.sendall(f'{EMAIL_DISCARD_MSGS_COMMAND}\r\n'.encode('ascii'))
-        messages = [email.message_from_string(m.strip().decode('ascii')) for m in buffer.split(EMAIL_SEPARATOR.encode('ascii'))]
+        s.sendall(f"{EMAIL_DISCARD_MSGS_COMMAND}\r\n".encode("ascii"))
+        messages = [
+            email.message_from_string(m.strip().decode("ascii"))
+            for m in buffer.split(EMAIL_SEPARATOR.encode("ascii"))
+        ]
         return messages
 
     def _extract_message_body(self, message):
         """
         Convenience method to extract the main body from a non-multipart
         email.Message object.
         """
-        charset = message.get_content_charset(failobj='ascii')
-        return message.get_payload(decode=True).decode(charset, 'backslashreplace')  # type: ignore
+        charset = message.get_content_charset(failobj="ascii")
+        return message.get_payload(decode=True).decode(charset, "backslashreplace")  # type: ignore
 
     def _check_text_in_mails(self, messages, expected_texts):
         """
         Check a list of email.Message objects for each of a list of
         expected texts. I.e. every message should contain every text.
         """
         for expected_text in expected_texts:
             for message in messages:
                 message_text = self._extract_message_body(message)
-                assert expected_text in message_text, f'Missing text {expected_text} in mail:\n{message_text}'
+                assert expected_text in message_text, f"Missing text {expected_text} in mail:\n{message_text}"
 
     def _check_recipients_in_mails(self, messages, expected_recipients):
         """
         Check whether a list of email.Message objects match a list of
         expected email recipients, in any order.
 
         Order may very due to unordered data structures being used when
         generating some notifications.
         """
         assert len(messages) == len(expected_recipients)
         original_expected_recipients = set(expected_recipients)
         leftover_expected_recipients = original_expected_recipients.copy()
         for message in messages:
             for recipient in original_expected_recipients:
-                if message['To'] == recipient:
+                if message["To"] == recipient:
                     leftover_expected_recipients.remove(recipient)
         assert not leftover_expected_recipients
 
     def teardown_method(self, method):
         """
         This teardown method is always called after tests complete, whether
         or not they succeed. It is used to kill any leftover IRRd or SMTP
         server processes.
         """
-        print('\n')
+        print("\n")
         for pidfile in self.pidfile1, self.pidfile2, self.pidfile_mailserver:
             try:
                 with open(pidfile) as fh:
                     pid = int(fh.read())
-                    print(f'Terminating PID {pid} from {pidfile}')
+                    print(f"Terminating PID {pid} from {pidfile}")
                     os.kill(pid, signal.SIGTERM)
             except (FileNotFoundError, ProcessLookupError, ValueError) as exc:
-                print(f'Failed to kill: {pidfile}: {exc}')
+                print(f"Failed to kill: {pidfile}: {exc}")
                 pass
```

### Comparing `irrd-4.2.8/irrd/mirroring/mirror_runners_export.py` & `irrd-4.3.0/irrd/mirroring/mirror_runners_export.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,20 +1,20 @@
-import os
-
 import gzip
 import logging
+import os
 import shutil
 from pathlib import Path
 from tempfile import NamedTemporaryFile
 
 from irrd.conf import get_setting
+from irrd.routepref.status import RoutePreferenceStatus
 from irrd.rpki.status import RPKIStatus
 from irrd.scopefilter.status import ScopeFilterStatus
 from irrd.storage.database_handler import DatabaseHandler
-from irrd.storage.queries import RPSLDatabaseQuery, DatabaseStatusQuery
+from irrd.storage.queries import DatabaseStatusQuery, RPSLDatabaseQuery
 from irrd.utils.text import remove_auth_hashes as remove_auth_hashes_func
 
 EXPORT_PERMISSIONS = 0o644
 
 logger = logging.getLogger(__name__)
 
 
@@ -32,63 +32,73 @@
 
     def __init__(self, source: str) -> None:
         self.source = source
 
     def run(self) -> None:
         self.database_handler = DatabaseHandler()
         try:
-            export_destination = get_setting(f'sources.{self.source}.export_destination')
+            export_destination = get_setting(f"sources.{self.source}.export_destination")
             if export_destination:
-                logger.info(f'Starting a source export for {self.source} to {export_destination}')
+                logger.info(f"Starting a source export for {self.source} to {export_destination}")
                 self._export(export_destination)
 
-            export_destination_unfiltered = get_setting(f'sources.{self.source}.export_destination_unfiltered')
+            export_destination_unfiltered = get_setting(
+                f"sources.{self.source}.export_destination_unfiltered"
+            )
             if export_destination_unfiltered:
-                logger.info(f'Starting an unfiltered source export for {self.source} '
-                            f'to {export_destination_unfiltered}')
+                logger.info(
+                    f"Starting an unfiltered source export for {self.source} "
+                    f"to {export_destination_unfiltered}"
+                )
                 self._export(export_destination_unfiltered, remove_auth_hashes=False)
 
             self.database_handler.commit()
         except Exception as exc:
-            logger.error(f'An exception occurred while attempting to run an export '
-                         f'for {self.source}: {exc}', exc_info=exc)
+            logger.error(
+                f"An exception occurred while attempting to run an export for {self.source}: {exc}",
+                exc_info=exc,
+            )
         finally:
             self.database_handler.close()
 
     def _export(self, export_destination, remove_auth_hashes=True):
-        filename_export = Path(export_destination) / f'{self.source.lower()}.db.gz'
+        filename_export = Path(export_destination) / f"{self.source.lower()}.db.gz"
         export_tmpfile = NamedTemporaryFile(delete=False)
-        filename_serial = Path(export_destination) / f'{self.source.upper()}.CURRENTSERIAL'
+        filename_serial = Path(export_destination) / f"{self.source.upper()}.CURRENTSERIAL"
 
         query = DatabaseStatusQuery().source(self.source)
 
         try:
-            serial = next(self.database_handler.execute_query(query))['serial_newest_seen']
+            serial = next(self.database_handler.execute_query(query))["serial_newest_seen"]
         except StopIteration:
             serial = None
 
-        with gzip.open(export_tmpfile.name, 'wb') as fh:
+        with gzip.open(export_tmpfile.name, "wb") as fh:
             query = RPSLDatabaseQuery().sources([self.source])
             query = query.rpki_status([RPKIStatus.not_found, RPKIStatus.valid])
             query = query.scopefilter_status([ScopeFilterStatus.in_scope])
+            query = query.route_preference_status([RoutePreferenceStatus.visible])
             for obj in self.database_handler.execute_query(query):
-                object_text = obj['object_text']
+                object_text = obj["object_text"]
                 if remove_auth_hashes:
                     object_text = remove_auth_hashes_func(object_text)
-                object_bytes = object_text.encode('utf-8')
-                fh.write(object_bytes + b'\n')
-            fh.write(b'# EOF\n')
+                object_bytes = object_text.encode("utf-8")
+                fh.write(object_bytes + b"\n")
+            fh.write(b"# EOF\n")
 
         os.chmod(export_tmpfile.name, EXPORT_PERMISSIONS)
         if filename_export.exists():
             os.unlink(filename_export)
         if filename_serial.exists():
             os.unlink(filename_serial)
         shutil.move(export_tmpfile.name, filename_export)
 
         if serial is not None:
-            with open(filename_serial, 'w') as fh:
+            with open(filename_serial, "w") as fh:
                 fh.write(str(serial))
             os.chmod(filename_serial, EXPORT_PERMISSIONS)
 
         self.database_handler.record_serial_exported(self.source, serial)
-        logger.info(f'Export for {self.source} complete at serial {serial}, stored in {filename_export} / {filename_serial}')
+        logger.info(
+            f"Export for {self.source} complete at serial {serial}, stored in {filename_export} /"
+            f" {filename_serial}"
+        )
```

### Comparing `irrd-4.2.8/irrd/mirroring/mirror_runners_import.py` & `irrd-4.3.0/irrd/mirroring/mirror_runners_import.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,79 +1,103 @@
 import gzip
 import logging
 import os
 import shutil
 from io import BytesIO
 from tempfile import NamedTemporaryFile
-from typing import Optional, Tuple, Any, IO
+from typing import IO, Any, Optional, Tuple
 from urllib import request
-from urllib.parse import urlparse
 from urllib.error import URLError
+from urllib.parse import urlparse
 
 import requests
 
-from irrd.conf import get_setting, RPKI_IRR_PSEUDO_SOURCE
+from irrd.conf import RPKI_IRR_PSEUDO_SOURCE, get_setting
 from irrd.conf.defaults import DEFAULT_SOURCE_NRTM_PORT
+from irrd.routepref.routepref import update_route_preference_status
 from irrd.rpki.importer import ROADataImporter, ROAParserException
 from irrd.rpki.notifications import notify_rpki_invalid_owners
 from irrd.rpki.validators import BulkRouteROAValidator
 from irrd.scopefilter.validators import ScopeFilterValidator
 from irrd.storage.database_handler import DatabaseHandler
+from irrd.storage.event_stream import EventStreamPublisher
 from irrd.storage.queries import DatabaseStatusQuery
 from irrd.utils.whois_client import whois_query
+
 from .parsers import MirrorFileImportParser, NRTMStreamParser
 
 logger = logging.getLogger(__name__)
+DOWNLOAD_TIMEOUT = 10
 
 
 class RPSLMirrorImportUpdateRunner:
     """
     This RPSLMirrorImportUpdateRunner is the entry point for updating a single
     database mirror, depending on current state.
 
     If there is no current mirrored data, will call RPSLMirrorFullImportRunner
     to run a new import from full export files. Otherwise, will call
     NRTMImportUpdateStreamRunner to retrieve new updates from NRTM.
     """
+
     def __init__(self, source: str) -> None:
         self.source = source
         self.full_import_runner = RPSLMirrorFullImportRunner(source)
         self.update_stream_runner = NRTMImportUpdateStreamRunner(source)
 
     def run(self) -> None:
         self.database_handler = DatabaseHandler()
 
         try:
             serial_newest_mirror, force_reload = self._status()
-            nrtm_enabled = bool(get_setting(f'sources.{self.source}.nrtm_host'))
-            logger.debug(f'Most recent mirrored serial for {self.source}: {serial_newest_mirror}, '
-                         f'force_reload: {force_reload}, nrtm enabled: {nrtm_enabled}')
-            if force_reload or not serial_newest_mirror or not nrtm_enabled:
-                self.full_import_runner.run(database_handler=self.database_handler,
-                                            serial_newest_mirror=serial_newest_mirror, force_reload=force_reload)
+            nrtm_enabled = bool(get_setting(f"sources.{self.source}.nrtm_host"))
+            logger.debug(
+                f"Most recent mirrored serial for {self.source}: {serial_newest_mirror}, "
+                f"force_reload: {force_reload}, nrtm enabled: {nrtm_enabled}"
+            )
+            full_reload = force_reload or not serial_newest_mirror or not nrtm_enabled
+            if full_reload:
+                self.full_import_runner.run(
+                    database_handler=self.database_handler,
+                    serial_newest_mirror=serial_newest_mirror,
+                    force_reload=force_reload,
+                )
             else:
+                assert serial_newest_mirror
                 self.update_stream_runner.run(serial_newest_mirror, database_handler=self.database_handler)
 
             self.database_handler.commit()
+            if full_reload:
+                event_stream_publisher = EventStreamPublisher()
+                event_stream_publisher.publish_rpsl_full_reload(self.source)
+                event_stream_publisher.close()
+
         except OSError as ose:
             # I/O errors can occur and should not log a full traceback (#177)
-            logger.error(f'An error occurred while attempting a mirror update or initial import '
-                         f'for {self.source}: {ose}')
+            logger.error(
+                "An error occurred while attempting a mirror update or initial import "
+                f"for {self.source}: {ose}"
+            )
         except Exception as exc:
-            logger.error(f'An exception occurred while attempting a mirror update or initial import '
-                         f'for {self.source}: {exc}', exc_info=exc)
+            logger.error(
+                (
+                    "An exception occurred while attempting a mirror update or initial import "
+                    f"for {self.source}: {exc}"
+                ),
+                exc_info=exc,
+            )
         finally:
             self.database_handler.close()
 
     def _status(self) -> Tuple[Optional[int], Optional[bool]]:
         query = DatabaseStatusQuery().source(self.source)
         result = self.database_handler.execute_query(query)
         try:
             status = next(result)
-            return status['serial_newest_mirror'], status['force_reload']
+            return status["serial_newest_mirror"], status["force_reload"]
         except StopIteration:
             return None, None
 
 
 class FileImportRunnerBase:
     def _retrieve_file(self, url: str, return_contents=True) -> Tuple[str, bool]:
         """
@@ -88,20 +112,20 @@
         unlink the path later.
 
         If the URL ends in .gz, the file is gunzipped before being processed,
         but only for HTTP(s) and FTP downloads.
         """
         url_parsed = urlparse(url)
 
-        if url_parsed.scheme in ['ftp', 'http', 'https']:
+        if url_parsed.scheme in ["ftp", "http", "https"]:
             return self._retrieve_file_download(url, url_parsed, return_contents)
-        if url_parsed.scheme == 'file':
+        if url_parsed.scheme == "file":
             return self._retrieve_file_local(url_parsed.path, return_contents)
 
-        raise ValueError(f'Invalid URL: {url} - scheme {url_parsed.scheme} is not supported')
+        raise ValueError(f"Invalid URL: {url} - scheme {url_parsed.scheme} is not supported")
 
     def _retrieve_file_download(self, url, url_parsed, return_contents=False) -> Tuple[str, bool]:
         """
         Retrieve a file from HTTP(s) or FTP
 
         If return_contents is False, the file is read, stripped and then the
         contents are returned. If return_contents is True, the data is written
@@ -114,58 +138,58 @@
         destination: IO[Any]
         if return_contents:
             destination = BytesIO()
         else:
             destination = NamedTemporaryFile(delete=False)
         self._download_file(destination, url, url_parsed)
         if return_contents:
-            value = destination.getvalue().decode('utf-8').strip()  # type: ignore
-            logger.info(f'Downloaded {url}, contained {value}')
+            value = destination.getvalue().decode("utf-8").strip()  # type: ignore
+            logger.info(f"Downloaded {url}, contained {value}")
             return value, False
         else:
-            if url.endswith('.gz'):
+            if url.endswith(".gz"):
                 zipped_file = destination
                 zipped_file.close()
                 destination = NamedTemporaryFile(delete=False)
-                logger.debug(f'Downloaded file is expected to be gzipped, gunzipping from {zipped_file.name}')
-                with gzip.open(zipped_file.name, 'rb') as f_in:
+                logger.debug(f"Downloaded file is expected to be gzipped, gunzipping from {zipped_file.name}")
+                with gzip.open(zipped_file.name, "rb") as f_in:
                     shutil.copyfileobj(f_in, destination)
                 os.unlink(zipped_file.name)
 
             destination.close()
 
-            logger.info(f'Downloaded (and gunzipped if applicable) {url} to {destination.name}')
+            logger.info(f"Downloaded (and gunzipped if applicable) {url} to {destination.name}")
             return destination.name, True
 
     def _download_file(self, destination: IO[Any], url: str, url_parsed):
         """
         Download a file from HTTP(s) or FTP.
         The file contents are written to the destination parameter,
         which can be a BytesIO() or a regular file.
         """
-        if url_parsed.scheme == 'ftp':
+        if url_parsed.scheme == "ftp":
             try:
-                r = request.urlopen(url)
+                r = request.urlopen(url, timeout=DOWNLOAD_TIMEOUT)
                 shutil.copyfileobj(r, destination)
             except URLError as error:
-                raise IOError(f'Failed to download {url}: {str(error)}')
-        elif url_parsed.scheme in ['http', 'https']:
-            r = requests.get(url, stream=True, timeout=10)
+                raise OSError(f"Failed to download {url}: {str(error)}")
+        elif url_parsed.scheme in ["http", "https"]:
+            r = requests.get(url, stream=True, timeout=DOWNLOAD_TIMEOUT)
             if r.status_code == 200:
                 for chunk in r.iter_content(10240):
                     destination.write(chunk)
             else:
-                raise IOError(f'Failed to download {url}: {r.status_code}: {str(r.content)}')
+                raise OSError(f"Failed to download {url}: {r.status_code}: {str(r.content)}")
 
     def _retrieve_file_local(self, path, return_contents=False) -> Tuple[str, bool]:
         if not return_contents:
-            if path.endswith('.gz'):
+            if path.endswith(".gz"):
                 destination = NamedTemporaryFile(delete=False)
-                logger.debug(f'Local file is expected to be gzipped, gunzipping from {path}')
-                with gzip.open(path, 'rb') as f_in:
+                logger.debug(f"Local file is expected to be gzipped, gunzipping from {path}")
+                with gzip.open(path, "rb") as f_in:
                     shutil.copyfileobj(f_in, destination)
                 destination.close()
                 return destination.name, True
             else:
                 return path, False
         with open(path) as fh:
             value = fh.read().strip()
@@ -177,62 +201,88 @@
     This runner performs a full import from database exports for a single
     mirrored source. URLs for full export file(s), and the URL for the serial
     they match, are provided in configuration.
 
     Files are downloaded, gunzipped if needed, and then sent through the
     MirrorFileImportParser.
     """
+
     def __init__(self, source: str) -> None:
         self.source = source
 
-    def run(self, database_handler: DatabaseHandler, serial_newest_mirror: Optional[int]=None, force_reload=False):
-        import_sources = get_setting(f'sources.{self.source}.import_source')
+    def run(
+        self,
+        database_handler: DatabaseHandler,
+        serial_newest_mirror: Optional[int] = None,
+        force_reload=False,
+    ):
+        import_sources = get_setting(f"sources.{self.source}.import_source")
         if isinstance(import_sources, str):
             import_sources = [import_sources]
-        import_serial_source = get_setting(f'sources.{self.source}.import_serial_source')
+        import_serial_source = get_setting(f"sources.{self.source}.import_serial_source")
 
         if not import_sources:
-            logger.info(f'Skipping full RPSL import for {self.source}, import_source not set.')
+            logger.info(f"Skipping full RPSL import for {self.source}, import_source not set.")
             return
 
-        logger.info(f'Running full RPSL import of {self.source} from {import_sources}, serial from {import_serial_source}')
+        logger.info(
+            f"Running full RPSL import of {self.source} from {import_sources}, serial from"
+            f" {import_serial_source}"
+        )
 
         import_serial = None
         if import_serial_source:
             import_serial = int(self._retrieve_file(import_serial_source, return_contents=True)[0])
 
-            if not force_reload and serial_newest_mirror is not None and import_serial <= serial_newest_mirror:
-                logger.info(f'Current newest serial seen from mirror for {self.source} is '
-                            f'{serial_newest_mirror}, import_serial is {import_serial}, cancelling import.')
+            if (
+                not force_reload
+                and serial_newest_mirror is not None
+                and import_serial <= serial_newest_mirror
+            ):
+                logger.info(
+                    f"Current newest serial seen from mirror for {self.source} is "
+                    f"{serial_newest_mirror}, import_serial is {import_serial}, cancelling import."
+                )
                 return
 
         database_handler.delete_all_rpsl_objects_with_journal(self.source)
-        import_data = [self._retrieve_file(import_source, return_contents=False) for import_source in import_sources]
+        import_data = [
+            self._retrieve_file(import_source, return_contents=False) for import_source in import_sources
+        ]
 
         roa_validator = None
-        if get_setting('rpki.roa_source'):
+        if get_setting("rpki.roa_source"):
             roa_validator = BulkRouteROAValidator(database_handler)
 
         database_handler.disable_journaling()
         for import_filename, to_delete in import_data:
-            p = MirrorFileImportParser(source=self.source, filename=import_filename, serial=None,
-                                       database_handler=database_handler, roa_validator=roa_validator)
-            p.run_import()
-            if to_delete:
-                os.unlink(import_filename)
+            p = MirrorFileImportParser(
+                source=self.source,
+                filename=import_filename,
+                serial=None,
+                database_handler=database_handler,
+                roa_validator=roa_validator,
+            )
+            try:
+                p.run_import()
+            finally:
+                if to_delete:
+                    os.unlink(import_filename)
+
         if import_serial:
             database_handler.record_serial_newest_mirror(self.source, import_serial)
 
 
 class ROAImportRunner(FileImportRunnerBase):
     """
     This runner performs a full import of ROA objects.
     The URL file for the ROA export in JSON format is provided
     in the configuration.
     """
+
     # API consistency with other importers, source is actually ignored
     def __init__(self, source=None):
         pass
 
     def run(self):
         self.database_handler = DatabaseHandler()
 
@@ -250,33 +300,35 @@
             self.database_handler.update_rpki_status(
                 rpsl_objs_now_valid=objs_now_valid,
                 rpsl_objs_now_invalid=objs_now_invalid,
                 rpsl_objs_now_not_found=objs_now_not_found,
             )
             self.database_handler.commit()
             notified = notify_rpki_invalid_owners(self.database_handler, objs_now_invalid)
-            logger.info(f'RPKI status updated for all routes, {len(objs_now_valid)} newly valid, '
-                        f'{len(objs_now_invalid)} newly invalid, '
-                        f'{len(objs_now_not_found)} newly not_found routes, '
-                        f'{notified} emails sent to contacts of newly invalid authoritative objects')
+            logger.info(
+                f"RPKI status updated for all routes, {len(objs_now_valid)} newly valid, "
+                f"{len(objs_now_invalid)} newly invalid, "
+                f"{len(objs_now_not_found)} newly not_found routes, "
+                f"{notified} emails sent to contacts of newly invalid authoritative objects"
+            )
 
         except OSError as ose:
             # I/O errors can occur and should not log a full traceback (#177)
-            logger.error(f'An error occurred while attempting a ROA import: {ose}')
+            logger.error(f"An error occurred while attempting a ROA import: {ose}")
         except ROAParserException as rpe:
-            logger.error(f'An exception occurred while attempting a ROA import: {rpe}')
+            logger.error(f"An exception occurred while attempting a ROA import: {rpe}")
         except Exception as exc:
-            logger.error(f'An exception occurred while attempting a ROA import: {exc}', exc_info=exc)
+            logger.error(f"An exception occurred while attempting a ROA import: {exc}", exc_info=exc)
         finally:
             self.database_handler.close()
 
     def _import_roas(self):
-        roa_source = get_setting('rpki.roa_source')
-        slurm_source = get_setting('rpki.slurm_source')
-        logger.info(f'Running full ROA import from: {roa_source}, SLURM {slurm_source}')
+        roa_source = get_setting("rpki.roa_source")
+        slurm_source = get_setting("rpki.slurm_source")
+        logger.info(f"Running full ROA import from: {roa_source}, SLURM {slurm_source}")
 
         self.database_handler.delete_all_roa_objects()
         self.database_handler.delete_all_rpsl_objects_with_journal(
             RPKI_IRR_PSEUDO_SOURCE,
             journal_guaranteed_empty=True,
         )
 
@@ -285,24 +337,28 @@
             slurm_data, _ = self._retrieve_file(slurm_source, return_contents=True)
 
         roa_filename, roa_to_delete = self._retrieve_file(roa_source, return_contents=False)
         with open(roa_filename) as fh:
             roa_importer = ROADataImporter(fh.read(), slurm_data, self.database_handler)
         if roa_to_delete:
             os.unlink(roa_filename)
-        logger.info(f'ROA import from {roa_source}, SLURM {slurm_source}, imported {len(roa_importer.roa_objs)} ROAs, running validator')
+        logger.info(
+            f"ROA import from {roa_source}, SLURM {slurm_source}, imported {len(roa_importer.roa_objs)} ROAs,"
+            " running validator"
+        )
         return roa_importer.roa_objs
 
 
 class ScopeFilterUpdateRunner:
     """
     Update the scope filter status for all objects.
     This runner does not actually import anything, the scope filter
     is in the configuration.
     """
+
     # API consistency with other importers, source is actually ignored
     def __init__(self, source=None):
         pass
 
     def run(self):
         self.database_handler = DatabaseHandler()
 
@@ -312,51 +368,85 @@
             rpsl_objs_now_in_scope, rpsl_objs_now_out_scope_as, rpsl_objs_now_out_scope_prefix = status
             self.database_handler.update_scopefilter_status(
                 rpsl_objs_now_in_scope=rpsl_objs_now_in_scope,
                 rpsl_objs_now_out_scope_as=rpsl_objs_now_out_scope_as,
                 rpsl_objs_now_out_scope_prefix=rpsl_objs_now_out_scope_prefix,
             )
             self.database_handler.commit()
-            logger.info(f'Scopefilter status updated for all routes, '
-                        f'{len(rpsl_objs_now_in_scope)} newly in scope, '
-                        f'{len(rpsl_objs_now_out_scope_as)} newly out of scope AS, '
-                        f'{len(rpsl_objs_now_out_scope_prefix)} newly out of scope prefix')
+            logger.info(
+                "Scopefilter status updated for all routes, "
+                f"{len(rpsl_objs_now_in_scope)} newly in scope, "
+                f"{len(rpsl_objs_now_out_scope_as)} newly out of scope AS, "
+                f"{len(rpsl_objs_now_out_scope_prefix)} newly out of scope prefix"
+            )
 
         except Exception as exc:
-            logger.error(f'An exception occurred while attempting a scopefilter status update: {exc}', exc_info=exc)
+            logger.error(
+                f"An exception occurred while attempting a scopefilter status update: {exc}", exc_info=exc
+            )
         finally:
             self.database_handler.close()
 
 
+class RoutePreferenceUpdateRunner:
+    """
+    Update the route preference filter status for all objects.
+    This runner does not actually import anything external, all
+    data is already in our database.
+    """
+
+    # API consistency with other importers, source is actually ignored
+    def __init__(self, source=None):
+        pass
+
+    def run(self):
+        database_handler = DatabaseHandler()
+
+        try:
+            update_route_preference_status(database_handler)
+            database_handler.commit()
+            logger.info("route preference update commit complete")
+        except Exception as exc:
+            logger.error(
+                f"An exception occurred while attempting a route preference status update: {exc}",
+                exc_info=exc,
+            )
+        finally:
+            database_handler.close()
+
+
 class NRTMImportUpdateStreamRunner:
     """
     This runner attempts to pull updates from an NRTM stream for a specific
     mirrored database.
     """
+
     def __init__(self, source: str) -> None:
         self.source = source
 
     def run(self, serial_newest_mirror: int, database_handler: DatabaseHandler):
         serial_start = serial_newest_mirror + 1
-        nrtm_host = get_setting(f'sources.{self.source}.nrtm_host')
-        nrtm_port = int(get_setting(f'sources.{self.source}.nrtm_port', DEFAULT_SOURCE_NRTM_PORT))
+        nrtm_host = get_setting(f"sources.{self.source}.nrtm_host")
+        nrtm_port = int(get_setting(f"sources.{self.source}.nrtm_port", DEFAULT_SOURCE_NRTM_PORT))
         if not nrtm_host:
-            logger.debug(f'Skipping NRTM updates for {self.source}, nrtm_host not set.')
+            logger.debug(f"Skipping NRTM updates for {self.source}, nrtm_host not set.")
             return
 
         end_markings = [
-            f'\n%END {self.source}\n',
-            f'\n% END {self.source}\n',
-            '\n%ERROR',
-            '\n% ERROR',
-            '\n% Warning: there are no newer updates available',
-            '\n% Warning (1): there are no newer updates available',
+            f"\n%END {self.source}\n",
+            f"\n% END {self.source}\n",
+            "\n%ERROR",
+            "\n% ERROR",
+            "\n% Warning: there are no newer updates available",
+            "\n% Warning (1): there are no newer updates available",
         ]
 
-        logger.info(f'Retrieving NRTM updates for {self.source} from serial {serial_start} on {nrtm_host}:{nrtm_port}')
-        query = f'-g {self.source}:3:{serial_start}-LAST'
+        logger.info(
+            f"Retrieving NRTM updates for {self.source} from serial {serial_start} on {nrtm_host}:{nrtm_port}"
+        )
+        query = f"-g {self.source}:3:{serial_start}-LAST"
         response = whois_query(nrtm_host, nrtm_port, query, end_markings)
-        logger.debug(f'Received NRTM response for {self.source}: {response.strip()}')
+        logger.debug(f"Received NRTM response for {self.source}: {response.strip()}")
 
         stream_parser = NRTMStreamParser(self.source, response, database_handler)
         for operation in stream_parser.operations:
             operation.save(database_handler)
```

### Comparing `irrd-4.2.8/irrd/mirroring/nrtm_generator.py` & `irrd-4.3.0/irrd/mirroring/nrtm_generator.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,71 +1,88 @@
 from typing import Optional
 
 from irrd.conf import get_setting
 from irrd.storage.database_handler import DatabaseHandler
-from irrd.storage.queries import RPSLDatabaseJournalQuery, DatabaseStatusQuery
+from irrd.storage.queries import DatabaseStatusQuery, RPSLDatabaseJournalQuery
 from irrd.utils.text import remove_auth_hashes as remove_auth_hashes_func
 
 
 class NRTMGeneratorException(Exception):  # noqa: N818
     pass
 
 
 class NRTMGenerator:
-    def generate(self, source: str, version: str,
-                 serial_start_requested: int, serial_end_requested: Optional[int],
-                 database_handler: DatabaseHandler, remove_auth_hashes=True) -> str:
+    def generate(
+        self,
+        source: str,
+        version: str,
+        serial_start_requested: int,
+        serial_end_requested: Optional[int],
+        database_handler: DatabaseHandler,
+        remove_auth_hashes=True,
+    ) -> str:
         """
         Generate an NRTM response for a particular source, serial range and
         NRTM version. Raises NRTMGeneratorException for various error conditions.
 
         For queries where the user requested NRTM updates up to LAST,
         serial_end_requested is None.
         """
-        if not get_setting(f'sources.{source}.keep_journal'):
-            raise NRTMGeneratorException('No journal kept for this source, unable to serve NRTM queries')
+        if not get_setting(f"sources.{source}.keep_journal"):
+            raise NRTMGeneratorException("No journal kept for this source, unable to serve NRTM queries")
 
         q = DatabaseStatusQuery().source(source)
         try:
             status = next(database_handler.execute_query(q))
         except StopIteration:
-            raise NRTMGeneratorException('There are no journal entries for this source.')
+            raise NRTMGeneratorException("There are no journal entries for this source.")
 
         if serial_end_requested and serial_end_requested < serial_start_requested:
-            raise NRTMGeneratorException(f'Start of the serial range ({serial_start_requested}) must be lower or '
-                                         f'equal to end of the serial range ({serial_end_requested})')
+            raise NRTMGeneratorException(
+                f"Start of the serial range ({serial_start_requested}) must be lower or "
+                f"equal to end of the serial range ({serial_end_requested})"
+            )
 
-        serial_start_available = status['serial_oldest_journal']
-        serial_end_available = status['serial_newest_journal']
+        serial_start_available = status["serial_oldest_journal"]
+        serial_end_available = status["serial_newest_journal"]
 
         if serial_start_available is None or serial_end_available is None:
-            return '% Warning: there are no updates available'
+            return "% Warning: there are no updates available"
 
         if serial_start_requested < serial_start_available:
-            raise NRTMGeneratorException(f'Serials {serial_start_requested} - {serial_start_available} do not exist')
+            raise NRTMGeneratorException(
+                f"Serials {serial_start_requested} - {serial_start_available} do not exist"
+            )
 
         if serial_end_requested is not None and serial_end_requested > serial_end_available:
-            raise NRTMGeneratorException(f'Serials {serial_end_available} - {serial_end_requested} do not exist')
+            raise NRTMGeneratorException(
+                f"Serials {serial_end_available} - {serial_end_requested} do not exist"
+            )
 
         if serial_end_requested is None:
             if serial_start_requested == serial_end_available + 1:
                 # A specific message is triggered when starting from a serial
                 # that is the current plus one, until LAST
-                return '% Warning: there are no newer updates available'
+                return "% Warning: there are no newer updates available"
             elif serial_start_requested > serial_end_available:
                 raise NRTMGeneratorException(
-                    f'Serials {serial_end_available} - {serial_start_requested} do not exist')
+                    f"Serials {serial_end_available} - {serial_start_requested} do not exist"
+                )
 
         serial_end_display = serial_end_available if serial_end_requested is None else serial_end_requested
 
-        range_limit = get_setting(f'sources.{source}.nrtm_query_serial_range_limit')
+        range_limit = get_setting(f"sources.{source}.nrtm_query_serial_range_limit")
         if range_limit and int(range_limit) < (serial_end_display - serial_start_requested):
-            raise NRTMGeneratorException(f'Serial range requested exceeds maximum range of {range_limit}')
+            raise NRTMGeneratorException(f"Serial range requested exceeds maximum range of {range_limit}")
 
-        q = RPSLDatabaseJournalQuery().sources([source]).serial_range(serial_start_requested, serial_end_requested)
+        q = (
+            RPSLDatabaseJournalQuery()
+            .sources([source])
+            .serial_nrtm_range(serial_start_requested, serial_end_requested)
+        )
 
         output = [f"%START Version: {version} {source} {serial_start_requested}-{serial_end_display}\n"]
 
         for operation in database_handler.execute_query(q):
             operation_str = operation["operation"].value
             if version == "3":
                 operation_str += " " + str(operation["serial_nrtm"])
```

### Comparing `irrd-4.2.8/irrd/mirroring/nrtm_operation.py` & `irrd-4.3.0/irrd/mirroring/nrtm_operation.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 import logging
-from typing import Optional, List
+from typing import List, Optional
 
 from irrd.rpki.validators import SingleRouteROAValidator
 from irrd.rpsl.parser import UnknownRPSLObjectClassException
-from irrd.rpsl.rpsl_objects import rpsl_object_from_text, RPSLKeyCert
+from irrd.rpsl.rpsl_objects import RPSLKeyCert, rpsl_object_from_text
 from irrd.scopefilter.validators import ScopeFilterValidator
 from irrd.storage.database_handler import DatabaseHandler
 from irrd.storage.models import DatabaseOperation, JournalEntryOrigin
 
 logger = logging.getLogger(__name__)
 
 
@@ -16,17 +16,25 @@
     NRTMOperation represents a single NRTM operation, i.e. an ADD/DEL
     with a serial number and source, from an NRTM stream.
 
     Note that the operation may contain an incomplete object, without a
     source attribute, but with other PK attribute(s) present.
     For deletion operations, this is permitted.
     """
-    def __init__(self, source: str, operation: DatabaseOperation, serial: int, object_text: str,
-                 strict_validation_key_cert: bool, rpki_aware: bool = False,
-                 object_class_filter: Optional[List[str]] = None) -> None:
+
+    def __init__(
+        self,
+        source: str,
+        operation: DatabaseOperation,
+        serial: int,
+        object_text: str,
+        strict_validation_key_cert: bool,
+        rpki_aware: bool = False,
+        object_class_filter: Optional[List[str]] = None,
+    ) -> None:
         self.source = source
         self.operation = operation
         self.serial = serial
         self.object_text = object_text
         self.strict_validation_key_cert = strict_validation_key_cert
         self.rpki_aware = rpki_aware
         self.object_class_filter = object_class_filter
@@ -35,57 +43,65 @@
         default_source = self.source if self.operation == DatabaseOperation.delete else None
         try:
             object_text = self.object_text.strip()
             # If an object turns out to be a key-cert, and strict_import_keycert_objects
             # is set, parse it again with strict validation to load it in the GPG keychain.
             obj = rpsl_object_from_text(object_text, strict_validation=False, default_source=default_source)
             if self.strict_validation_key_cert and obj.__class__ == RPSLKeyCert:
-                obj = rpsl_object_from_text(object_text, strict_validation=True, default_source=default_source)
+                obj = rpsl_object_from_text(
+                    object_text, strict_validation=True, default_source=default_source
+                )
 
         except UnknownRPSLObjectClassException as exc:
             # Unknown object classes are only logged if they have not been filtered out.
             if not self.object_class_filter or exc.rpsl_object_class.lower() in self.object_class_filter:
-                logger.info(f'Ignoring NRTM operation {str(self)}: {exc}')
+                logger.info(f"Ignoring NRTM operation {str(self)}: {exc}")
             return False
 
         if self.object_class_filter and obj.rpsl_object_class.lower() not in self.object_class_filter:
             return False
 
         if obj.messages.errors():
-            errors = '; '.join(obj.messages.errors())
-            logger.critical(f'Parsing errors occurred while processing NRTM operation {str(self)}. '
-                            f'This operation is ignored, causing potential data inconsistencies. '
-                            f'A new operation for this update, without errors, '
-                            f'will still be processed and cause the inconsistency to be resolved. '
-                            f'Parser error messages: {errors}; original object text follows:\n{self.object_text}')
-            database_handler.record_mirror_error(self.source, f'Parsing errors: {obj.messages.errors()}, '
-                                                              f'original object text follows:\n{self.object_text}')
+            errors = "; ".join(obj.messages.errors())
+            logger.critical(
+                f"Parsing errors occurred while processing NRTM operation {str(self)}. "
+                "This operation is ignored, causing potential data inconsistencies. "
+                "A new operation for this update, without errors, "
+                "will still be processed and cause the inconsistency to be resolved. "
+                f"Parser error messages: {errors}; original object text follows:\n{self.object_text}"
+            )
+            database_handler.record_mirror_error(
+                self.source,
+                f"Parsing errors: {obj.messages.errors()}, original object text follows:\n{self.object_text}",
+            )
             return False
 
-        if 'source' in obj.parsed_data and obj.parsed_data['source'].upper() != self.source:
-            msg = (f'Incorrect source in NRTM object: stream has source {self.source}, found object with '
-                   f'source {obj.source()} in operation {self.serial}/{self.operation.value}/{obj.pk()}. '
-                   f'This operation is ignored, causing potential data inconsistencies.')
+        if "source" in obj.parsed_data and obj.parsed_data["source"].upper() != self.source:
+            msg = (
+                f"Incorrect source in NRTM object: stream has source {self.source}, found object with "
+                f"source {obj.source()} in operation {self.serial}/{self.operation.value}/{obj.pk()}. "
+                "This operation is ignored, causing potential data inconsistencies."
+            )
             database_handler.record_mirror_error(self.source, msg)
             logger.critical(msg)
             return False
 
         if self.operation == DatabaseOperation.add_or_update:
-            if self.rpki_aware and obj.rpki_relevant and obj.prefix and obj.asn_first:
+            if self.rpki_aware and obj.is_route and obj.prefix and obj.asn_first:
                 roa_validator = SingleRouteROAValidator(database_handler)
                 obj.rpki_status = roa_validator.validate_route(obj.prefix, obj.asn_first, obj.source())
             scope_validator = ScopeFilterValidator()
             obj.scopefilter_status, _ = scope_validator.validate_rpsl_object(obj)
-            database_handler.upsert_rpsl_object(obj, JournalEntryOrigin.mirror,
-                                                source_serial=self.serial)
+            database_handler.upsert_rpsl_object(obj, JournalEntryOrigin.mirror, source_serial=self.serial)
         elif self.operation == DatabaseOperation.delete:
-            database_handler.delete_rpsl_object(rpsl_object=obj, origin=JournalEntryOrigin.mirror,
-                                                source_serial=self.serial)
-
-        log = f'Completed NRTM operation {str(self)}/{obj.rpsl_object_class}/{obj.pk()}'
-        if self.rpki_aware and obj.rpki_relevant:
-            log += f', RPKI status {obj.rpki_status.value}'
+            database_handler.delete_rpsl_object(
+                rpsl_object=obj, origin=JournalEntryOrigin.mirror, source_serial=self.serial
+            )
+
+        log = f"Completed NRTM operation {str(self)}/{obj.rpsl_object_class}/{obj.pk()}"
+        if self.rpki_aware and obj.is_route:
+            log += f", RPKI status {obj.rpki_status.value}"
         logger.info(log)
         return True
 
     def __repr__(self):
-        return f'{self.source}/{self.serial}/{self.operation.value}'
+        return f"{self.source}/{self.serial}/{self.operation.value}"
```

### Comparing `irrd-4.2.8/irrd/mirroring/parsers.py` & `irrd-4.3.0/irrd/mirroring/parsers.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,63 +1,78 @@
 import logging
 import re
-from typing import List, Set, Optional
+from typing import List, Optional, Set
 
 from irrd.conf import get_setting
 from irrd.rpki.validators import BulkRouteROAValidator
-from irrd.rpsl.parser import UnknownRPSLObjectClassException, RPSLObject
-from irrd.rpsl.rpsl_objects import rpsl_object_from_text, RPSLKeyCert
+from irrd.rpsl.parser import RPSLObject, UnknownRPSLObjectClassException
+from irrd.rpsl.rpsl_objects import RPSLKeyCert, rpsl_object_from_text
 from irrd.scopefilter.validators import ScopeFilterValidator
 from irrd.storage.database_handler import DatabaseHandler
 from irrd.storage.models import DatabaseOperation, JournalEntryOrigin
-from irrd.utils.text import split_paragraphs_rpsl, remove_last_modified
-from .nrtm_operation import NRTMOperation
+from irrd.utils.text import remove_last_modified, split_paragraphs_rpsl
+
 from ..storage.queries import RPSLDatabaseQuery
+from .nrtm_operation import NRTMOperation
 
 logger = logging.getLogger(__name__)
-nrtm_start_line_re = re.compile(r'^% *START *Version: *(?P<version>\d+) +(?P<source>[\w-]+) +(?P<first_serial>\d+)-(?P<last_serial>\d+)( FILTERED)?\n$', flags=re.MULTILINE)
+nrtm_start_line_re = re.compile(
+    (
+        r"^% *START *Version: *(?P<version>\d+) +(?P<source>[\w-]+)"
+        r" +(?P<first_serial>\d+)-(?P<last_serial>\d+)( FILTERED)?\n$"
+    ),
+    flags=re.MULTILINE,
+)
 
 
 class RPSLImportError(Exception):
     def __init__(self, message: str) -> None:
         self.message = message
 
 
 class MirrorParser:
     def __init__(self):
-        object_class_filter = get_setting(f'sources.{self.source}.object_class_filter')
+        object_class_filter = get_setting(f"sources.{self.source}.object_class_filter")
         if object_class_filter:
             if isinstance(object_class_filter, str):
                 object_class_filter = [object_class_filter]
             self.object_class_filter = [c.strip().lower() for c in object_class_filter]
         else:
             self.object_class_filter = None
 
-        self.strict_validation_key_cert = get_setting(f'sources.{self.source}.strict_import_keycert_objects', False)
+        self.strict_validation_key_cert = get_setting(
+            f"sources.{self.source}.strict_import_keycert_objects", False
+        )
 
 
 class MirrorFileImportParserBase(MirrorParser):
     """
     This parser handles imports of files for mirror databases.
     Note that this parser can be called multiple times for a single
     full import, as some databases use split files.
 
     If direct_error_return is set, run_import() immediately returns
     upon an encountering an error message. It will return an error
     string.
     """
+
     obj_parsed = 0  # Total objects found
     obj_errors = 0  # Objects with errors
     obj_ignored_class = 0  # Objects ignored due to object_class_filter setting
     obj_unknown = 0  # Objects with unknown classes
     unknown_object_classes: Set[str] = set()  # Set of encountered unknown classes
 
-    def __init__(self, source: str, filename: str,
-                 database_handler: DatabaseHandler, direct_error_return: bool=False,
-                 roa_validator: Optional[BulkRouteROAValidator] = None) -> None:
+    def __init__(
+        self,
+        source: str,
+        filename: str,
+        database_handler: DatabaseHandler,
+        direct_error_return: bool = False,
+        roa_validator: Optional[BulkRouteROAValidator] = None,
+    ) -> None:
         self.source = source
         self.filename = filename
         self.database_handler = database_handler
         self.direct_error_return = direct_error_return
         self.roa_validator = roa_validator
         self.obj_parsed = 0  # Total objects found
         self.obj_errors = 0  # Objects with errors
@@ -79,56 +94,60 @@
             # If an object turns out to be a key-cert, and strict_import_keycert_objects
             # is set, parse it again with strict validation to load it in the GPG keychain.
             obj = rpsl_object_from_text(rpsl_text.strip(), strict_validation=False)
             if self.strict_validation_key_cert and obj.__class__ == RPSLKeyCert:
                 obj = rpsl_object_from_text(rpsl_text.strip(), strict_validation=True)
 
             if obj.messages.errors():
-                log_msg = f'Parsing errors: {obj.messages.errors()}, original object text follows:\n{rpsl_text}'
+                log_msg = (
+                    f"Parsing errors: {obj.messages.errors()}, original object text follows:\n{rpsl_text}"
+                )
                 if self.direct_error_return:
                     raise RPSLImportError(log_msg)
                 self.database_handler.record_mirror_error(self.source, log_msg)
-                logger.critical(f'Parsing errors occurred while importing from file for {self.source}. '
-                                f'This object is ignored, causing potential data inconsistencies. A new operation for '
-                                f'this update, without errors, will still be processed and cause the inconsistency to '
-                                f'be resolved. Parser error messages: {obj.messages.errors()}; '
-                                f'original object text follows:\n{rpsl_text}')
+                logger.critical(
+                    f"Parsing errors occurred while importing from file for {self.source}. "
+                    "This object is ignored, causing potential data inconsistencies. A new operation for "
+                    "this update, without errors, will still be processed and cause the inconsistency to "
+                    f"be resolved. Parser error messages: {obj.messages.errors()}; "
+                    f"original object text follows:\n{rpsl_text}"
+                )
                 self.obj_errors += 1
                 return None
 
             if obj.source() != self.source:
-                msg = f'Invalid source {obj.source()} for object {obj.pk()}, expected {self.source}'
+                msg = f"Invalid source {obj.source()} for object {obj.pk()}, expected {self.source}"
                 if self.direct_error_return:
                     raise RPSLImportError(msg)
-                logger.critical(msg + '. This object is ignored, causing potential data inconsistencies.')
+                logger.critical(msg + ". This object is ignored, causing potential data inconsistencies.")
                 self.database_handler.record_mirror_error(self.source, msg)
                 self.obj_errors += 1
                 return None
 
             if self.object_class_filter and obj.rpsl_object_class.lower() not in self.object_class_filter:
                 self.obj_ignored_class += 1
                 return None
 
-            if self.roa_validator and obj.rpki_relevant and obj.prefix_length and obj.asn_first:
+            if self.roa_validator and obj.is_route and obj.prefix_length and obj.asn_first:
                 obj.rpki_status = self.roa_validator.validate_route(
                     str(obj.ip_first), obj.prefix_length, obj.asn_first, obj.source()
                 )
 
             obj.scopefilter_status, _ = self.scopefilter_validator.validate_rpsl_object(obj)
 
             return obj
 
         except UnknownRPSLObjectClassException as e:
             # Ignore legacy IRRd artifacts
             # https://github.com/irrdnet/irrd4/issues/232
-            if e.rpsl_object_class.startswith('*xx'):
+            if e.rpsl_object_class.startswith("*xx"):
                 self.obj_parsed -= 1  # This object does not exist to us
                 return None
             if self.direct_error_return:
-                raise RPSLImportError(f'Unknown object class: {e.rpsl_object_class}')
+                raise RPSLImportError(f"Unknown object class: {e.rpsl_object_class}")
             self.obj_unknown += 1
             self.unknown_object_classes.add(e.rpsl_object_class)
         return None
 
 
 class MirrorFileImportParser(MirrorFileImportParserBase):
     """
@@ -136,54 +155,58 @@
     Note that this parser can be called multiple times for a single
     full import, as some databases use split files.
 
     If direct_error_return is set, run_import() immediately returns
     upon an encountering an error message. It will return an error
     string.
     """
-    def __init__(self, serial: Optional[int]=None, *args, **kwargs):
+
+    def __init__(self, serial: Optional[int] = None, *args, **kwargs):
         super().__init__(*args, **kwargs)
         self.serial = serial
-        logger.debug(f'Starting file import of {self.source} from {self.filename}')
+        logger.debug(f"Starting file import of {self.source} from {self.filename}")
 
     def run_import(self) -> Optional[str]:
         """
         Run the actual import. If direct_error_return is set, returns an error
         string on encountering the first error. Otherwise, returns None.
         """
-        f = open(self.filename, encoding='utf-8', errors='backslashreplace')
+        f = open(self.filename, encoding="utf-8", errors="backslashreplace")
         for paragraph in split_paragraphs_rpsl(f):
             try:
                 rpsl_obj = self._parse_object(paragraph)
             except RPSLImportError as e:
                 if self.direct_error_return:
                     return e.message
             else:
                 if rpsl_obj:
-                    self.database_handler.upsert_rpsl_object(
-                        rpsl_obj, origin=JournalEntryOrigin.mirror)
+                    self.database_handler.upsert_rpsl_object(rpsl_obj, origin=JournalEntryOrigin.mirror)
 
         self.log_report()
         f.close()
         if self.serial:
             self.database_handler.record_serial_seen(self.source, self.serial)
 
         return None
 
     def log_report(self) -> None:
         obj_successful = self.obj_parsed - self.obj_unknown - self.obj_errors - self.obj_ignored_class
-        logger.info(f'File import for {self.source}: {self.obj_parsed} objects read, '
-                    f'{obj_successful} objects inserted, '
-                    f'ignored {self.obj_errors} due to errors, '
-                    f'ignored {self.obj_ignored_class} due to object_class_filter, '
-                    f'source {self.filename}')
+        logger.info(
+            f"File import for {self.source}: {self.obj_parsed} objects read, "
+            f"{obj_successful} objects inserted, "
+            f"ignored {self.obj_errors} due to errors, "
+            f"ignored {self.obj_ignored_class} due to object_class_filter, "
+            f"source {self.filename}"
+        )
         if self.obj_unknown:
-            unknown_formatted = ', '.join(self.unknown_object_classes)
-            logger.warning(f'Ignored {self.obj_unknown} objects found in file import for {self.source} due to unknown '
-                           f'object classes: {unknown_formatted}')
+            unknown_formatted = ", ".join(self.unknown_object_classes)
+            logger.warning(
+                f"Ignored {self.obj_unknown} objects found in file import for {self.source} due to unknown "
+                f"object classes: {unknown_formatted}"
+            )
 
 
 class MirrorUpdateFileImportParser(MirrorFileImportParserBase):
     """
     This parser handles files for mirror databases, and processes them
     as the new state of that mirror, including journal entries if needed.
     Note that this parser *can not* be called multiple times for a single
@@ -191,101 +214,108 @@
     a particular file, that is treated as the full current state of that
     particular source.
 
     If direct_error_return is set, run_import() immediately returns
     upon an encountering an error message. It will return an error
     string.
     """
+
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
-        logger.debug(f'Starting update import for {self.source} from {self.filename}')
+        logger.debug(f"Starting update import for {self.source} from {self.filename}")
         self.obj_new = 0  # New objects
         self.obj_modified = 0  # Modified objects
         self.obj_retained = 0  # Retained and possibly modified objects
         self.obj_deleted = 0  # Deleted objects
 
     def run_import(self) -> Optional[str]:
         """
         Run the actual import. If direct_error_return is set, returns an error
         string on encountering the first error. Otherwise, returns None.
         """
         objs_from_file = []
-        f = open(self.filename, encoding='utf-8', errors='backslashreplace')
+        f = open(self.filename, encoding="utf-8", errors="backslashreplace")
         for paragraph in split_paragraphs_rpsl(f):
             try:
                 rpsl_obj = self._parse_object(paragraph)
             except RPSLImportError as e:
                 if self.direct_error_return:
                     return e.message
             else:
                 if rpsl_obj:
                     objs_from_file.append(rpsl_obj)
         f.close()
 
-        query = RPSLDatabaseQuery(ordered_by_sources=False, enable_ordering=False,
-                                  column_names=['rpsl_pk', 'object_class']).sources([self.source])
+        query = RPSLDatabaseQuery(
+            ordered_by_sources=False, enable_ordering=False, column_names=["rpsl_pk", "object_class"]
+        ).sources([self.source])
         current_pks = {
-            (row['rpsl_pk'], row['object_class'])
-            for row in self.database_handler.execute_query(query)
+            (row["rpsl_pk"], row["object_class"]) for row in self.database_handler.execute_query(query)
         }
 
-        file_objs_by_pk = {
-            (obj.pk(), obj.rpsl_object_class): obj
-            for obj in objs_from_file
-        }
+        file_objs_by_pk = {(obj.pk(), obj.rpsl_object_class): obj for obj in objs_from_file}
         file_pks = set(file_objs_by_pk.keys())
         new_pks = file_pks - current_pks
         deleted_pks = current_pks - file_pks
         retained_pks = file_pks.intersection(current_pks)
 
         self.obj_new = len(new_pks)
         self.obj_deleted = len(deleted_pks)
         self.obj_retained = len(retained_pks)
 
         for (rpsl_pk, object_class), file_obj in filter(lambda i: i[0] in new_pks, file_objs_by_pk.items()):
             self.database_handler.upsert_rpsl_object(file_obj, JournalEntryOrigin.synthetic_nrtm)
 
-        for (rpsl_pk, object_class) in deleted_pks:
+        for rpsl_pk, object_class in deleted_pks:
             self.database_handler.delete_rpsl_object(
-                rpsl_pk=rpsl_pk, source=self.source, object_class=object_class,
+                rpsl_pk=rpsl_pk,
+                source=self.source,
+                object_class=object_class,
                 origin=JournalEntryOrigin.synthetic_nrtm,
             )
 
         # This query does not filter on retained_pks. The expectation is that most
         # objects are retained, and therefore it is much faster to query the entire source.
-        query = RPSLDatabaseQuery(ordered_by_sources=False, enable_ordering=False,
-                                  column_names=['rpsl_pk', 'object_class', 'object_text'])
+        query = RPSLDatabaseQuery(
+            ordered_by_sources=False,
+            enable_ordering=False,
+            column_names=["rpsl_pk", "object_class", "object_text"],
+        )
         query = query.sources([self.source])
         for row in self.database_handler.execute_query(query):
             try:
-                file_obj = file_objs_by_pk[(row['rpsl_pk'], row['object_class'])]
+                file_obj = file_objs_by_pk[(row["rpsl_pk"], row["object_class"])]
             except KeyError:
                 continue
-            if file_obj.render_rpsl_text() != remove_last_modified(row['object_text']):
+            if file_obj.render_rpsl_text() != remove_last_modified(row["object_text"]):
                 self.database_handler.upsert_rpsl_object(file_obj, JournalEntryOrigin.synthetic_nrtm)
                 self.obj_modified += 1
 
         self.log_report()
         return None
 
     def log_report(self) -> None:
         obj_successful = self.obj_parsed - self.obj_unknown - self.obj_errors - self.obj_ignored_class
-        logger.info(f'File update for {self.source}: {self.obj_parsed} objects read, '
-                    f'{obj_successful} objects processed, '
-                    f'{self.obj_new} objects newly inserted, '
-                    f'{self.obj_deleted} objects newly deleted, '
-                    f'{self.obj_retained} objects retained, of which '
-                    f'{self.obj_modified} modified, '
-                    f'ignored {self.obj_errors} due to errors, '
-                    f'ignored {self.obj_ignored_class} due to object_class_filter, '
-                    f'source {self.filename}')
+        logger.info(
+            f"File update for {self.source}: {self.obj_parsed} objects read, "
+            f"{obj_successful} objects processed, "
+            f"{self.obj_new} objects newly inserted, "
+            f"{self.obj_deleted} objects newly deleted, "
+            f"{self.obj_retained} objects retained, of which "
+            f"{self.obj_modified} modified, "
+            f"ignored {self.obj_errors} due to errors, "
+            f"ignored {self.obj_ignored_class} due to object_class_filter, "
+            f"source {self.filename}"
+        )
         if self.obj_unknown:
-            unknown_formatted = ', '.join(self.unknown_object_classes)
-            logger.warning(f'Ignored {self.obj_unknown} objects found in file import for {self.source} due to unknown '
-                           f'object classes: {unknown_formatted}')
+            unknown_formatted = ", ".join(self.unknown_object_classes)
+            logger.warning(
+                f"Ignored {self.obj_unknown} objects found in file import for {self.source} due to unknown "
+                f"object classes: {unknown_formatted}"
+            )
 
 
 class NRTMStreamParser(MirrorParser):
     """
     The NRTM parser takes the data of an NRTM string, and splits it
     into individual operations, matched with their serial and
     whether they are an ADD/DEL operation.
@@ -294,121 +324,143 @@
     - first_serial: the first serial found in the data
     - last_serial: the last serial found
     - nrtm_source: the RPSL source recorded in the START header (must be equal to expected source)
     - operations: a list of NRTMOperation objects
 
     Raises a ValueError for invalid NRTM data.
     """
+
     first_serial = -1
     last_serial = -1
     nrtm_source: Optional[str] = None
     _current_op_serial = -1
 
     def __init__(self, source: str, nrtm_data: str, database_handler: DatabaseHandler) -> None:
         self.source = source
         self.database_handler = database_handler
-        self.rpki_aware = bool(get_setting('rpki.roa_source'))
+        self.rpki_aware = bool(get_setting("rpki.roa_source"))
         super().__init__()
         self.operations: List[NRTMOperation] = []
         self._split_stream(nrtm_data)
 
     def _split_stream(self, data: str) -> None:
         """Split a stream into individual operations."""
         paragraphs = split_paragraphs_rpsl(data, strip_comments=False)
-        last_comment_seen = ''
+        last_comment_seen = ""
 
         for paragraph in paragraphs:
             if self._handle_possible_start_line(paragraph):
                 continue
-            elif paragraph.startswith('%') or paragraph.startswith('#'):
+            elif paragraph.startswith("%") or paragraph.startswith("#"):
                 last_comment_seen = paragraph
-            elif paragraph.startswith('ADD') or paragraph.startswith('DEL'):
+            elif paragraph.startswith("ADD") or paragraph.startswith("DEL"):
                 self._handle_operation(paragraph, paragraphs)
 
-        if self.nrtm_source and last_comment_seen.upper().strip() != f'%END {self.source}':
-            msg = f'NRTM stream error for {self.source}: last comment paragraph expected to be ' \
-                  f'"%END {self.source}", but is actually "{last_comment_seen.upper().strip()}" - ' \
-                  'could be caused by TCP disconnection during NRTM query or mirror server ' \
-                  'returning an error or an otherwise incomplete or invalid response'
+        if self.nrtm_source and last_comment_seen.upper().strip() != f"%END {self.source}":
+            msg = (
+                f"NRTM stream error for {self.source}: last comment paragraph expected to be "
+                f'"%END {self.source}", but is actually "{last_comment_seen.upper().strip()}" - '
+                "could be caused by TCP disconnection during NRTM query or mirror server "
+                "returning an error or an otherwise incomplete or invalid response"
+            )
             logger.error(msg)
             self.database_handler.record_mirror_error(self.source, msg)
             raise ValueError(msg)
 
-        if self._current_op_serial > self.last_serial and self.version != '3':
-            msg = f'NRTM stream error for {self.source}: expected operations up to and including serial ' \
-                  f'{self.last_serial}, last operation was {self._current_op_serial}'
+        if self._current_op_serial > self.last_serial and self.version != "3":
+            msg = (
+                f"NRTM stream error for {self.source}: expected operations up to and including serial "
+                f"{self.last_serial}, last operation was {self._current_op_serial}"
+            )
             logger.error(msg)
             self.database_handler.record_mirror_error(self.source, msg)
             raise ValueError(msg)
 
         if self.last_serial > 0:
             self.database_handler.record_serial_newest_mirror(self.source, self.last_serial)
 
     def _handle_possible_start_line(self, line: str) -> bool:
         """Check whether a line is an NRTM START line, and if so, handle it."""
         start_line_match = nrtm_start_line_re.match(line)
         if not start_line_match:
             return False
 
         if self.nrtm_source:  # nrtm_source can only be defined if this is a second START line
-            msg = f'Encountered second START line in NRTM stream, first was {self.source} ' \
-                  f'{self.first_serial}-{self.last_serial}, new line is: {line}'
+            msg = (
+                f"Encountered second START line in NRTM stream, first was {self.source} "
+                f"{self.first_serial}-{self.last_serial}, new line is: {line}"
+            )
             self.database_handler.record_mirror_error(self.source, msg)
             logger.error(msg)
             raise ValueError(msg)
 
-        self.version = start_line_match.group('version')
-        self.nrtm_source = start_line_match.group('source').upper()
-        self.first_serial = int(start_line_match.group('first_serial'))
-        self.last_serial = int(start_line_match.group('last_serial'))
+        self.version = start_line_match.group("version")
+        self.nrtm_source = start_line_match.group("source").upper()
+        self.first_serial = int(start_line_match.group("first_serial"))
+        self.last_serial = int(start_line_match.group("last_serial"))
 
         if self.source != self.nrtm_source:
-            msg = f'Invalid NRTM source in START line: expected {self.source} but found ' \
-                  f'{self.nrtm_source} in line: {line}'
+            msg = (
+                f"Invalid NRTM source in START line: expected {self.source} but found "
+                f"{self.nrtm_source} in line: {line}"
+            )
             self.database_handler.record_mirror_error(self.source, msg)
             logger.error(msg)
             raise ValueError(msg)
 
-        if self.version not in ['1', '3']:
-            msg = f'Invalid NRTM version {self.version} in START line: {line}'
+        if self.version not in ["1", "3"]:
+            msg = f"Invalid NRTM version {self.version} in START line: {line}"
             self.database_handler.record_mirror_error(self.source, msg)
             logger.error(msg)
             raise ValueError(msg)
 
-        logger.debug(f'Found valid start line for {self.source}, range {self.first_serial}-{self.last_serial}')
+        logger.debug(
+            f"Found valid start line for {self.source}, range {self.first_serial}-{self.last_serial}"
+        )
 
         return True
 
     def _handle_operation(self, current_paragraph: str, paragraphs) -> None:
         """Handle a single ADD/DEL operation."""
         if not self.nrtm_source:
-            msg = f'Encountered operation before valid NRTM START line, paragraph encountered: {current_paragraph}'
+            msg = (
+                "Encountered operation before valid NRTM START line, paragraph encountered:"
+                f" {current_paragraph}"
+            )
             self.database_handler.record_mirror_error(self.source, msg)
             logger.error(msg)
             raise ValueError(msg)
 
         if self._current_op_serial == -1:
             self._current_op_serial = self.first_serial
         else:
             self._current_op_serial += 1
 
-        if ' ' in current_paragraph:
-            operation_str, line_serial_str = current_paragraph.split(' ')
+        if " " in current_paragraph:
+            operation_str, line_serial_str = current_paragraph.split(" ")
             line_serial = int(line_serial_str)
             # Gaps are allowed, but the line serial can never be lower, as that
             # means operations are served in the wrong order.
             if line_serial < self._current_op_serial:
-                msg = f'Invalid NRTM serial for {self.source}: ADD/DEL has serial {line_serial}, ' \
-                      f'expected at least {self._current_op_serial}'
+                msg = (
+                    f"Invalid NRTM serial for {self.source}: ADD/DEL has serial {line_serial}, "
+                    f"expected at least {self._current_op_serial}"
+                )
                 logger.error(msg)
                 self.database_handler.record_mirror_error(self.source, msg)
                 raise ValueError(msg)
             self._current_op_serial = line_serial
         else:
             operation_str = current_paragraph.strip()
 
         operation = DatabaseOperation(operation_str)
         object_text = next(paragraphs)
-        nrtm_operation = NRTMOperation(self.source, operation, self._current_op_serial,
-                                       object_text, self.strict_validation_key_cert, self.rpki_aware,
-                                       self.object_class_filter)
+        nrtm_operation = NRTMOperation(
+            self.source,
+            operation,
+            self._current_op_serial,
+            object_text,
+            self.strict_validation_key_cert,
+            self.rpki_aware,
+            self.object_class_filter,
+        )
         self.operations.append(nrtm_operation)
```

### Comparing `irrd-4.2.8/irrd/mirroring/scheduler.py` & `irrd-4.3.0/irrd/mirroring/scheduler.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,23 +1,27 @@
-import time
-from collections import defaultdict
-
 import gc
 import logging
 import multiprocessing
-
 import signal
-from setproctitle import setproctitle
+import time
+from collections import defaultdict
 from typing import Dict
 
-from irrd.conf import get_setting, RPKI_IRR_PSEUDO_SOURCE
-from irrd.conf.defaults import DEFAULT_SOURCE_IMPORT_TIMER, DEFAULT_SOURCE_EXPORT_TIMER
+from setproctitle import setproctitle
+
+from irrd.conf import RPKI_IRR_PSEUDO_SOURCE, get_setting
+from irrd.conf.defaults import DEFAULT_SOURCE_EXPORT_TIMER, DEFAULT_SOURCE_IMPORT_TIMER
+
 from .mirror_runners_export import SourceExportRunner
-from .mirror_runners_import import RPSLMirrorImportUpdateRunner, ROAImportRunner, \
-    ScopeFilterUpdateRunner
+from .mirror_runners_import import (
+    ROAImportRunner,
+    RoutePreferenceUpdateRunner,
+    RPSLMirrorImportUpdateRunner,
+    ScopeFilterUpdateRunner,
+)
 
 logger = logging.getLogger(__name__)
 
 MAX_SIMULTANEOUS_RUNS = 3
 
 
 class ScheduledTaskProcess(multiprocessing.Process):
@@ -26,130 +30,147 @@
         super().__init__(*args, **kwargs)
 
     def close(self):  # pragma: no cover
         """
         close() is not available in Python 3.6,
         use our own implementation if needed.
         """
-        if hasattr(super, 'close'):
+        if hasattr(super, "close"):
             return super().close()
         if self._popen is not None:
             if self._popen.poll() is None:
-                raise ValueError("Cannot close a process while it is still running. "
-                                 "You should first call join() or terminate().")
+                raise ValueError(
+                    "Cannot close a process while it is still running. "
+                    "You should first call join() or terminate()."
+                )
             self._popen = None
             del self._sentinel
         self._closed = True
 
     def run(self):
         # Disable the special sigterm_handler defined in main()
         # (signal handlers are inherited)
         signal.signal(signal.SIGTERM, signal.SIG_DFL)
 
-        setproctitle(f'irrd-{self.name}')
+        setproctitle(f"irrd-{self.name}")
         self.runner.run()
 
 
 class MirrorScheduler:
     """
     Scheduler for mirroring processes.
 
     For each time run() is called, will start a process for each mirror database
     unless a process is still running for that database (which is likely to be
     the case in some full imports).
     """
+
     processes: Dict[str, ScheduledTaskProcess]
     last_started_time: Dict[str, int]
 
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
         self.processes = dict()
         self.last_started_time = defaultdict(lambda: 0)
         self.previous_scopefilter_prefixes = None
         self.previous_scopefilter_asns = None
         self.previous_scopefilter_excluded = None
 
     def run(self) -> None:
-        if get_setting('database_readonly'):
+        if get_setting("database_readonly"):
             return
 
-        if get_setting('rpki.roa_source'):
-            import_timer = int(get_setting('rpki.roa_import_timer'))
+        if get_setting("rpki.roa_source"):
+            import_timer = int(get_setting("rpki.roa_import_timer"))
             self.run_if_relevant(RPKI_IRR_PSEUDO_SOURCE, ROAImportRunner, import_timer)
 
+        if get_setting("sources") and any(
+            [
+                source_settings.get("route_object_preference")
+                for source_settings in get_setting("sources").values()
+            ]
+        ):
+            import_timer = int(get_setting("route_object_preference.update_timer"))
+            self.run_if_relevant("routepref", RoutePreferenceUpdateRunner, import_timer)
+
         if self._check_scopefilter_change():
-            self.run_if_relevant('scopefilter', ScopeFilterUpdateRunner, 0)
+            self.run_if_relevant("scopefilter", ScopeFilterUpdateRunner, 0)
 
         sources_started = 0
-        for source in get_setting('sources', {}).keys():
+        for source in get_setting("sources", {}).keys():
             if sources_started >= MAX_SIMULTANEOUS_RUNS:
                 break
             started_import = False
             started_export = False
 
-            is_mirror = get_setting(f'sources.{source}.import_source') or get_setting(f'sources.{source}.nrtm_host')
-            import_timer = int(get_setting(f'sources.{source}.import_timer', DEFAULT_SOURCE_IMPORT_TIMER))
+            is_mirror = get_setting(f"sources.{source}.import_source") or get_setting(
+                f"sources.{source}.nrtm_host"
+            )
+            import_timer = int(get_setting(f"sources.{source}.import_timer", DEFAULT_SOURCE_IMPORT_TIMER))
 
             if is_mirror:
                 started_import = self.run_if_relevant(source, RPSLMirrorImportUpdateRunner, import_timer)
 
-            runs_export = get_setting(f'sources.{source}.export_destination') or get_setting(f'sources.{source}.export_destination_unfiltered')
-            export_timer = int(get_setting(f'sources.{source}.export_timer', DEFAULT_SOURCE_EXPORT_TIMER))
+            runs_export = get_setting(f"sources.{source}.export_destination") or get_setting(
+                f"sources.{source}.export_destination_unfiltered"
+            )
+            export_timer = int(get_setting(f"sources.{source}.export_timer", DEFAULT_SOURCE_EXPORT_TIMER))
 
             if runs_export:
                 started_export = self.run_if_relevant(source, SourceExportRunner, export_timer)
 
             if started_import or started_export:
                 sources_started += 1
 
     def _check_scopefilter_change(self) -> bool:
         """
         Check whether the scope filter has changed since last call.
         Always returns True on the first call.
         """
-        if not get_setting('scopefilter'):
+        if not get_setting("scopefilter"):
             return False
 
-        current_prefixes = list(get_setting('scopefilter.prefixes', []))
-        current_asns = list(get_setting('scopefilter.asns', []))
+        current_prefixes = list(get_setting("scopefilter.prefixes", []))
+        current_asns = list(get_setting("scopefilter.asns", []))
         current_exclusions = {
             name
-            for name, settings in get_setting('sources', {}).items()
-            if settings.get('scopefilter_excluded')
+            for name, settings in get_setting("sources", {}).items()
+            if settings.get("scopefilter_excluded")
         }
 
-        if any([
-            self.previous_scopefilter_prefixes != current_prefixes,
-            self.previous_scopefilter_asns != current_asns,
-            self.previous_scopefilter_excluded != current_exclusions,
-        ]):
+        if any(
+            [
+                self.previous_scopefilter_prefixes != current_prefixes,
+                self.previous_scopefilter_asns != current_asns,
+                self.previous_scopefilter_excluded != current_exclusions,
+            ]
+        ):
             self.previous_scopefilter_prefixes = current_prefixes
             self.previous_scopefilter_asns = current_asns
             self.previous_scopefilter_excluded = current_exclusions
             return True
         return False
 
     def run_if_relevant(self, source: str, runner_class, timer: int) -> bool:
-        process_name = f'{runner_class.__name__}-{source}'
-
+        process_name = f"{runner_class.__name__}-{source}"
         current_time = time.time()
         has_expired = (self.last_started_time[process_name] + timer) < current_time
         if not has_expired or process_name in self.processes:
             return False
 
-        logger.debug(f'Started new process {process_name} for mirror import/export for {source}')
+        logger.debug(f"Started new process {process_name} for mirror import/export for {source}")
         initiator = runner_class(source=source)
         process = ScheduledTaskProcess(runner=initiator, name=process_name)
         self.processes[process_name] = process
         process.start()
         self.last_started_time[process_name] = int(current_time)
         return True
 
     def terminate_children(self) -> None:  # pragma: no cover
-        logger.info('MirrorScheduler terminating children')
+        logger.info("MirrorScheduler terminating children")
         for process in self.processes.values():
             try:
                 process.terminate()
                 process.join()
             except Exception:
                 pass
 
@@ -158,14 +179,15 @@
         gc_collect_needed = False
         for process_name, process in list(self.processes.items()):
             if process.is_alive():
                 continue
             try:
                 process.close()
             except Exception as e:  # pragma: no cover
-                logging.error(f'Failed to close {process_name} (pid {process.pid}), '
-                              f'possible resource leak: {e}')
+                logging.error(
+                    f"Failed to close {process_name} (pid {process.pid}), possible resource leak: {e}"
+                )
             del self.processes[process_name]
             gc_collect_needed = True
         if gc_collect_needed:
             # prevents FIFO pipe leak, see #578
             gc.collect()
```

### Comparing `irrd-4.2.8/irrd/mirroring/tests/nrtm_samples.py` & `irrd-4.3.0/irrd/mirroring/tests/nrtm_samples.py`

 * *Files identical despite different names*

### Comparing `irrd-4.2.8/irrd/mirroring/tests/test_mirror_runners_export.py` & `irrd-4.3.0/irrd/mirroring/tests/test_mirror_runners_export.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,181 +1,198 @@
 import gzip
 import os
 from itertools import cycle, repeat
 from pathlib import Path
 from unittest.mock import Mock
 
+from irrd.routepref.status import RoutePreferenceStatus
 from irrd.rpki.status import RPKIStatus
 from irrd.scopefilter.status import ScopeFilterStatus
 from irrd.utils.test_utils import flatten_mock_calls
-from ..mirror_runners_export import SourceExportRunner, EXPORT_PERMISSIONS
+
+from ..mirror_runners_export import EXPORT_PERMISSIONS, SourceExportRunner
 
 
 class TestSourceExportRunner:
     def test_export(self, tmpdir, config_override, monkeypatch, caplog):
-        config_override({
-            'sources': {
-                'TEST': {
-                    'export_destination': str(tmpdir),
+        config_override(
+            {
+                "sources": {
+                    "TEST": {
+                        "export_destination": str(tmpdir),
+                    }
                 }
             }
-        })
+        )
 
         mock_dh = Mock()
         mock_dq = Mock()
         mock_dsq = Mock()
 
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_export.DatabaseHandler', lambda: mock_dh)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_export.RPSLDatabaseQuery', lambda: mock_dq)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_export.DatabaseStatusQuery', lambda: mock_dsq)
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_export.DatabaseHandler", lambda: mock_dh)
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_export.RPSLDatabaseQuery", lambda: mock_dq)
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_export.DatabaseStatusQuery", lambda: mock_dsq)
 
-        responses = cycle([
-            repeat({'serial_newest_seen': '424242'}),
+        responses = cycle(
             [
-                # The CRYPT-PW hash must not appear in the output
-                {'object_text': 'object 1 🦄\nauth: CRYPT-PW foobar\n'},
-                {'object_text': 'object 2 🌈\n'},
-            ],
-        ])
+                repeat({"serial_newest_seen": "424242"}),
+                [
+                    # The CRYPT-PW hash must not appear in the output
+                    {"object_text": "object 1 🦄\nauth: CRYPT-PW foobar\n"},
+                    {"object_text": "object 2 🌈\n"},
+                ],
+            ]
+        )
         mock_dh.execute_query = lambda q: next(responses)
 
-        runner = SourceExportRunner('TEST')
+        runner = SourceExportRunner("TEST")
         runner.run()
         runner.run()
 
-        serial_filename = tmpdir + '/TEST.CURRENTSERIAL'
+        serial_filename = tmpdir + "/TEST.CURRENTSERIAL"
         assert oct(os.lstat(serial_filename).st_mode)[-3:] == oct(EXPORT_PERMISSIONS)[-3:]
         with open(serial_filename) as fh:
-            assert fh.read() == '424242'
+            assert fh.read() == "424242"
 
-        export_filename = tmpdir + '/test.db.gz'
+        export_filename = tmpdir + "/test.db.gz"
         assert oct(os.lstat(export_filename).st_mode)[-3:] == oct(EXPORT_PERMISSIONS)[-3:]
         with gzip.open(export_filename) as fh:
-            assert fh.read().decode('utf-8') == 'object 1 🦄\nauth: CRYPT-PW DummyValue  # Filtered for security\n\n' \
-                                                'object 2 🌈\n\n# EOF\n'
+            assert (
+                fh.read().decode("utf-8")
+                == "object 1 🦄\nauth: CRYPT-PW DummyValue  # Filtered for security\n\nobject 2 🌈\n\n# EOF\n"
+            )
 
         assert flatten_mock_calls(mock_dh) == [
-            ['record_serial_exported', ('TEST', '424242'), {}],
-            ['commit', (), {}],
-            ['close', (), {}],
-            ['record_serial_exported', ('TEST', '424242'), {}],
-            ['commit', (), {}],
-            ['close', (), {}]
+            ["record_serial_exported", ("TEST", "424242"), {}],
+            ["commit", (), {}],
+            ["close", (), {}],
+            ["record_serial_exported", ("TEST", "424242"), {}],
+            ["commit", (), {}],
+            ["close", (), {}],
         ]
         assert flatten_mock_calls(mock_dq) == [
-            ['sources', (['TEST'],), {}],
-            ['rpki_status', ([RPKIStatus.not_found, RPKIStatus.valid],), {}],
-            ['scopefilter_status', ([ScopeFilterStatus.in_scope],), {}],
-            ['sources', (['TEST'],), {}],
-            ['rpki_status', ([RPKIStatus.not_found, RPKIStatus.valid],), {}],
-            ['scopefilter_status', ([ScopeFilterStatus.in_scope],), {}],
+            ["sources", (["TEST"],), {}],
+            ["rpki_status", ([RPKIStatus.not_found, RPKIStatus.valid],), {}],
+            ["scopefilter_status", ([ScopeFilterStatus.in_scope],), {}],
+            ["route_preference_status", ([RoutePreferenceStatus.visible],), {}],
+            ["sources", (["TEST"],), {}],
+            ["rpki_status", ([RPKIStatus.not_found, RPKIStatus.valid],), {}],
+            ["scopefilter_status", ([ScopeFilterStatus.in_scope],), {}],
+            ["route_preference_status", ([RoutePreferenceStatus.visible],), {}],
         ]
-        assert 'Starting a source export for TEST' in caplog.text
-        assert 'Export for TEST complete' in caplog.text
+        assert "Starting a source export for TEST" in caplog.text
+        assert "Export for TEST complete" in caplog.text
 
     def test_export_unfiltered(self, tmpdir, config_override, monkeypatch, caplog):
-        config_override({
-            'sources': {
-                'TEST': {
-                    'export_destination_unfiltered': str(tmpdir),
+        config_override(
+            {
+                "sources": {
+                    "TEST": {
+                        "export_destination_unfiltered": str(tmpdir),
+                    }
                 }
             }
-        })
+        )
 
         mock_dh = Mock()
         mock_dq = Mock()
         mock_dsq = Mock()
 
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_export.DatabaseHandler', lambda: mock_dh)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_export.RPSLDatabaseQuery', lambda: mock_dq)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_export.DatabaseStatusQuery', lambda: mock_dsq)
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_export.DatabaseHandler", lambda: mock_dh)
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_export.RPSLDatabaseQuery", lambda: mock_dq)
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_export.DatabaseStatusQuery", lambda: mock_dsq)
 
-        responses = cycle([
-            repeat({'serial_newest_seen': '424242'}),
+        responses = cycle(
             [
-                # The CRYPT-PW hash should appear in the output
-                {'object_text': 'object 1 🦄\nauth: CRYPT-PW foobar\n'},
-                {'object_text': 'object 2 🌈\n'},
-            ],
-        ])
+                repeat({"serial_newest_seen": "424242"}),
+                [
+                    # The CRYPT-PW hash should appear in the output
+                    {"object_text": "object 1 🦄\nauth: CRYPT-PW foobar\n"},
+                    {"object_text": "object 2 🌈\n"},
+                ],
+            ]
+        )
         mock_dh.execute_query = lambda q: next(responses)
 
-        runner = SourceExportRunner('TEST')
+        runner = SourceExportRunner("TEST")
         runner.run()
 
-        serial_filename = tmpdir + '/TEST.CURRENTSERIAL'
+        serial_filename = tmpdir + "/TEST.CURRENTSERIAL"
         assert oct(os.lstat(serial_filename).st_mode)[-3:] == oct(EXPORT_PERMISSIONS)[-3:]
         with open(serial_filename) as fh:
-            assert fh.read() == '424242'
+            assert fh.read() == "424242"
 
-        export_filename = tmpdir + '/test.db.gz'
+        export_filename = tmpdir + "/test.db.gz"
         assert oct(os.lstat(export_filename).st_mode)[-3:] == oct(EXPORT_PERMISSIONS)[-3:]
         with gzip.open(export_filename) as fh:
-            assert fh.read().decode('utf-8') == 'object 1 🦄\nauth: CRYPT-PW foobar\n\n' \
-                                                'object 2 🌈\n\n# EOF\n'
+            assert fh.read().decode("utf-8") == "object 1 🦄\nauth: CRYPT-PW foobar\n\nobject 2 🌈\n\n# EOF\n"
 
     def test_failure(self, tmpdir, config_override, monkeypatch, caplog):
-        config_override({
-            'sources': {
-                'TEST': {
-                    'export_destination': str(tmpdir),
+        config_override(
+            {
+                "sources": {
+                    "TEST": {
+                        "export_destination": str(tmpdir),
+                    }
                 }
             }
-        })
+        )
 
         mock_dh = Mock()
         mock_dsq = Mock()
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_export.DatabaseHandler', lambda: mock_dh)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_export.DatabaseStatusQuery', lambda: mock_dsq)
-        mock_dh.execute_query = Mock(side_effect=ValueError('expected-test-error'))
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_export.DatabaseHandler", lambda: mock_dh)
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_export.DatabaseStatusQuery", lambda: mock_dsq)
+        mock_dh.execute_query = Mock(side_effect=ValueError("expected-test-error"))
 
-        runner = SourceExportRunner('TEST')
+        runner = SourceExportRunner("TEST")
         runner.run()
 
-        assert 'An exception occurred while attempting to run an export for TEST' in caplog.text
-        assert 'expected-test-error' in caplog.text
+        assert "An exception occurred while attempting to run an export for TEST" in caplog.text
+        assert "expected-test-error" in caplog.text
 
     def test_export_no_serial(self, tmpdir, config_override, monkeypatch, caplog):
-        config_override({
-            'sources': {
-                'TEST': {
-                    'export_destination': str(tmpdir),
+        config_override(
+            {
+                "sources": {
+                    "TEST": {
+                        "export_destination": str(tmpdir),
+                    }
                 }
             }
-        })
+        )
 
         mock_dh = Mock()
         mock_dq = Mock()
         mock_dsq = Mock()
 
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_export.DatabaseHandler',
-                            lambda: mock_dh)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_export.RPSLDatabaseQuery',
-                            lambda: mock_dq)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_export.DatabaseStatusQuery',
-                            lambda: mock_dsq)
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_export.DatabaseHandler", lambda: mock_dh)
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_export.RPSLDatabaseQuery", lambda: mock_dq)
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_export.DatabaseStatusQuery", lambda: mock_dsq)
 
-        responses = cycle([
-            iter([]),
+        responses = cycle(
             [
-                # The CRYPT-PW hash must not appear in the output
-                {'object_text': 'object 1 🦄\nauth: CRYPT-PW foobar\n'},
-                {'object_text': 'object 2 🌈\n'},
-            ],
-        ])
+                iter([]),
+                [
+                    # The CRYPT-PW hash must not appear in the output
+                    {"object_text": "object 1 🦄\nauth: CRYPT-PW foobar\n"},
+                    {"object_text": "object 2 🌈\n"},
+                ],
+            ]
+        )
         mock_dh.execute_query = lambda q: next(responses)
 
-        runner = SourceExportRunner('TEST')
+        runner = SourceExportRunner("TEST")
         runner.run()
         runner.run()
 
-        serial_filename = Path(tmpdir + '/TEST.CURRENTSERIAL')
+        serial_filename = Path(tmpdir + "/TEST.CURRENTSERIAL")
         assert not serial_filename.exists()
 
-        export_filename = tmpdir + '/test.db.gz'
+        export_filename = tmpdir + "/test.db.gz"
         with gzip.open(export_filename) as fh:
-            assert fh.read().decode(
-                'utf-8') == 'object 1 🦄\nauth: CRYPT-PW DummyValue  # Filtered for security\n\n' \
-                            'object 2 🌈\n\n# EOF\n'
+            assert (
+                fh.read().decode("utf-8")
+                == "object 1 🦄\nauth: CRYPT-PW DummyValue  # Filtered for security\n\nobject 2 🌈\n\n# EOF\n"
+            )
 
-        assert 'Starting a source export for TEST' in caplog.text
-        assert 'Export for TEST complete' in caplog.text
+        assert "Starting a source export for TEST" in caplog.text
+        assert "Export for TEST complete" in caplog.text
```

### Comparing `irrd-4.2.8/irrd/mirroring/tests/test_mirror_runners_import.py` & `irrd-4.3.0/irrd/mirroring/tests/test_mirror_runners_import.py`

 * *Files 24% similar despite different names*

```diff
@@ -2,587 +2,715 @@
 from io import BytesIO
 from typing import List
 from unittest.mock import Mock
 from urllib.error import URLError
 
 import pytest
 
+from irrd.routepref.routepref import update_route_preference_status
 from irrd.rpki.importer import ROAParserException
 from irrd.rpki.validators import BulkRouteROAValidator
+from irrd.scopefilter.validators import ScopeFilterValidator
 from irrd.storage.database_handler import DatabaseHandler
 from irrd.utils.test_utils import flatten_mock_calls
-from ..mirror_runners_import import RPSLMirrorImportUpdateRunner, RPSLMirrorFullImportRunner, \
-    NRTMImportUpdateStreamRunner, ROAImportRunner, ScopeFilterUpdateRunner
-from ...scopefilter.validators import ScopeFilterValidator
+
+from ..mirror_runners_import import (
+    NRTMImportUpdateStreamRunner,
+    ROAImportRunner,
+    RoutePreferenceUpdateRunner,
+    RPSLMirrorFullImportRunner,
+    RPSLMirrorImportUpdateRunner,
+    ScopeFilterUpdateRunner,
+)
 
 
 class TestRPSLMirrorImportUpdateRunner:
     def test_full_import_call(self, monkeypatch):
         mock_dh = Mock()
         mock_dq = Mock()
         mock_full_import_runner = Mock()
 
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.DatabaseHandler', lambda: mock_dh)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.DatabaseStatusQuery', lambda: mock_dq)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.RPSLMirrorFullImportRunner', lambda source: mock_full_import_runner)
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_import.DatabaseHandler", lambda: mock_dh)
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_import.DatabaseStatusQuery", lambda: mock_dq)
+        monkeypatch.setattr(
+            "irrd.mirroring.mirror_runners_import.RPSLMirrorFullImportRunner",
+            lambda source: mock_full_import_runner,
+        )
 
         mock_dh.execute_query = lambda q: iter([])
-        runner = RPSLMirrorImportUpdateRunner(source='TEST')
+        runner = RPSLMirrorImportUpdateRunner(source="TEST")
         runner.run()
 
-        assert flatten_mock_calls(mock_dq) == [['source', ('TEST',), {}]]
-        assert flatten_mock_calls(mock_dh) == [['commit', (), {}], ['close', (), {}]]
+        assert flatten_mock_calls(mock_dq) == [["source", ("TEST",), {}]]
+        assert flatten_mock_calls(mock_dh) == [["commit", (), {}], ["close", (), {}]]
 
         assert len(mock_full_import_runner.mock_calls) == 1
-        assert mock_full_import_runner.mock_calls[0][0] == 'run'
+        assert mock_full_import_runner.mock_calls[0][0] == "run"
 
     def test_force_reload(self, monkeypatch, config_override):
-        config_override({
-            'sources': {
-                'TEST': {
-                    'nrtm_host': '192.0.2.1',
+        config_override(
+            {
+                "sources": {
+                    "TEST": {
+                        "nrtm_host": "192.0.2.1",
+                    }
                 }
             }
-        })
+        )
         mock_dh = Mock()
         mock_dq = Mock()
         mock_full_import_runner = Mock()
 
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.DatabaseHandler', lambda: mock_dh)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.DatabaseStatusQuery', lambda: mock_dq)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.RPSLMirrorFullImportRunner', lambda source: mock_full_import_runner)
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_import.DatabaseHandler", lambda: mock_dh)
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_import.DatabaseStatusQuery", lambda: mock_dq)
+        monkeypatch.setattr(
+            "irrd.mirroring.mirror_runners_import.RPSLMirrorFullImportRunner",
+            lambda source: mock_full_import_runner,
+        )
 
-        mock_dh.execute_query = lambda q: iter([{'serial_newest_mirror': 424242, 'force_reload': True}])
-        runner = RPSLMirrorImportUpdateRunner(source='TEST')
+        mock_dh.execute_query = lambda q: iter([{"serial_newest_mirror": 424242, "force_reload": True}])
+        runner = RPSLMirrorImportUpdateRunner(source="TEST")
         runner.run()
 
-        assert flatten_mock_calls(mock_dq) == [['source', ('TEST',), {}]]
-        assert flatten_mock_calls(mock_dh) == [['commit', (), {}], ['close', (), {}]]
+        assert flatten_mock_calls(mock_dq) == [["source", ("TEST",), {}]]
+        assert flatten_mock_calls(mock_dh) == [["commit", (), {}], ["close", (), {}]]
 
         assert len(mock_full_import_runner.mock_calls) == 1
-        assert mock_full_import_runner.mock_calls[0][0] == 'run'
+        assert mock_full_import_runner.mock_calls[0][0] == "run"
 
     def test_update_stream_call(self, monkeypatch, config_override):
-        config_override({
-            'sources': {
-                'TEST': {
-                    'nrtm_host': '192.0.2.1',
+        config_override(
+            {
+                "sources": {
+                    "TEST": {
+                        "nrtm_host": "192.0.2.1",
+                    }
                 }
             }
-        })
+        )
         mock_dh = Mock()
         mock_dq = Mock()
         mock_stream_runner = Mock()
 
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.DatabaseHandler', lambda: mock_dh)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.DatabaseStatusQuery', lambda: mock_dq)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.NRTMImportUpdateStreamRunner', lambda source: mock_stream_runner)
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_import.DatabaseHandler", lambda: mock_dh)
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_import.DatabaseStatusQuery", lambda: mock_dq)
+        monkeypatch.setattr(
+            "irrd.mirroring.mirror_runners_import.NRTMImportUpdateStreamRunner",
+            lambda source: mock_stream_runner,
+        )
 
-        mock_dh.execute_query = lambda q: iter([{'serial_newest_mirror': 424242, 'force_reload': False}])
-        runner = RPSLMirrorImportUpdateRunner(source='TEST')
+        mock_dh.execute_query = lambda q: iter([{"serial_newest_mirror": 424242, "force_reload": False}])
+        runner = RPSLMirrorImportUpdateRunner(source="TEST")
         runner.run()
 
-        assert flatten_mock_calls(mock_dq) == [['source', ('TEST',), {}]]
-        assert flatten_mock_calls(mock_dh) == [['commit', (), {}], ['close', (), {}]]
+        assert flatten_mock_calls(mock_dq) == [["source", ("TEST",), {}]]
+        assert flatten_mock_calls(mock_dh) == [["commit", (), {}], ["close", (), {}]]
 
         assert len(mock_stream_runner.mock_calls) == 1
-        assert mock_stream_runner.mock_calls[0][0] == 'run'
+        assert mock_stream_runner.mock_calls[0][0] == "run"
         assert mock_stream_runner.mock_calls[0][1] == (424242,)
 
     def test_io_exception_handling(self, monkeypatch, caplog):
         mock_dh = Mock()
         mock_dq = Mock()
         mock_full_import_runner = Mock()
 
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.DatabaseHandler', lambda: mock_dh)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.DatabaseStatusQuery', lambda: mock_dq)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.RPSLMirrorFullImportRunner', lambda source: mock_full_import_runner)
-        mock_full_import_runner.run = Mock(side_effect=ConnectionResetError('test-error'))
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_import.DatabaseHandler", lambda: mock_dh)
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_import.DatabaseStatusQuery", lambda: mock_dq)
+        monkeypatch.setattr(
+            "irrd.mirroring.mirror_runners_import.RPSLMirrorFullImportRunner",
+            lambda source: mock_full_import_runner,
+        )
+        mock_full_import_runner.run = Mock(side_effect=ConnectionResetError("test-error"))
 
-        mock_dh.execute_query = lambda q: iter([{'serial_newest_mirror': 424242, 'force_reload': False}])
-        runner = RPSLMirrorImportUpdateRunner(source='TEST')
+        mock_dh.execute_query = lambda q: iter([{"serial_newest_mirror": 424242, "force_reload": False}])
+        runner = RPSLMirrorImportUpdateRunner(source="TEST")
         runner.run()
 
-        assert flatten_mock_calls(mock_dh) == [['close', (), {}]]
-        assert 'An error occurred while attempting a mirror update or initial import for TEST' in caplog.text
-        assert 'test-error' in caplog.text
-        assert 'Traceback' not in caplog.text
+        assert flatten_mock_calls(mock_dh) == [["close", (), {}]]
+        assert "An error occurred while attempting a mirror update or initial import for TEST" in caplog.text
+        assert "test-error" in caplog.text
+        assert "Traceback" not in caplog.text
 
     def test_unexpected_exception_handling(self, monkeypatch, caplog):
         mock_dh = Mock()
         mock_dq = Mock()
         mock_full_import_runner = Mock()
 
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.DatabaseHandler', lambda: mock_dh)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.DatabaseStatusQuery', lambda: mock_dq)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.RPSLMirrorFullImportRunner', lambda source: mock_full_import_runner)
-        mock_full_import_runner.run = Mock(side_effect=Exception('test-error'))
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_import.DatabaseHandler", lambda: mock_dh)
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_import.DatabaseStatusQuery", lambda: mock_dq)
+        monkeypatch.setattr(
+            "irrd.mirroring.mirror_runners_import.RPSLMirrorFullImportRunner",
+            lambda source: mock_full_import_runner,
+        )
+        mock_full_import_runner.run = Mock(side_effect=Exception("test-error"))
 
-        mock_dh.execute_query = lambda q: iter([{'serial_newest_mirror': 424242, 'force_reload': False}])
-        runner = RPSLMirrorImportUpdateRunner(source='TEST')
+        mock_dh.execute_query = lambda q: iter([{"serial_newest_mirror": 424242, "force_reload": False}])
+        runner = RPSLMirrorImportUpdateRunner(source="TEST")
         runner.run()
 
-        assert flatten_mock_calls(mock_dh) == [['close', (), {}]]
-        assert 'An exception occurred while attempting a mirror update or initial import for TEST' in caplog.text
-        assert 'test-error' in caplog.text
-        assert 'Traceback' in caplog.text
+        assert flatten_mock_calls(mock_dh) == [["close", (), {}]]
+        assert (
+            "An exception occurred while attempting a mirror update or initial import for TEST" in caplog.text
+        )
+        assert "test-error" in caplog.text
+        assert "Traceback" in caplog.text
 
 
 class TestRPSLMirrorFullImportRunner:
     def test_run_import_ftp(self, monkeypatch, config_override):
-        config_override({
-            'rpki': {'roa_source': 'https://example.com/roa.json'},
-            'sources': {
-                'TEST': {
-                    'import_source': ['ftp://host/source1.gz', 'ftp://host/source2'],
-                    'import_serial_source': 'ftp://host/serial',
-                }
+        config_override(
+            {
+                "rpki": {"roa_source": "https://example.com/roa.json"},
+                "sources": {
+                    "TEST": {
+                        "import_source": ["ftp://host/source1.gz", "ftp://host/source2"],
+                        "import_serial_source": "ftp://host/serial",
+                    }
+                },
             }
-        })
+        )
 
         mock_dh = Mock()
         request = Mock()
         MockMirrorFileImportParser.rpsl_data_calls = []
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.MirrorFileImportParser', MockMirrorFileImportParser)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.request', request)
+        monkeypatch.setattr(
+            "irrd.mirroring.mirror_runners_import.MirrorFileImportParser", MockMirrorFileImportParser
+        )
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_import.request", request)
 
         mock_bulk_validator_init = Mock()
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.BulkRouteROAValidator', mock_bulk_validator_init)
+        monkeypatch.setattr(
+            "irrd.mirroring.mirror_runners_import.BulkRouteROAValidator", mock_bulk_validator_init
+        )
 
         responses = {
             # gzipped data, contains 'source1'
-            'ftp://host/source1.gz': b64decode('H4sIAE4CfFsAAyvOLy1KTjUEAE5Fj0oHAAAA'),
-            'ftp://host/source2': b'source2',
-            'ftp://host/serial': b'424242',
+            "ftp://host/source1.gz": b64decode("H4sIAE4CfFsAAyvOLy1KTjUEAE5Fj0oHAAAA"),
+            "ftp://host/source2": b"source2",
+            "ftp://host/serial": b"424242",
         }
-        request.urlopen = lambda url: MockUrlopenResponse(responses[url])
-        RPSLMirrorFullImportRunner('TEST').run(mock_dh, serial_newest_mirror=424241)
+        request.urlopen = lambda url, timeout: MockUrlopenResponse(responses[url])
+        RPSLMirrorFullImportRunner("TEST").run(mock_dh, serial_newest_mirror=424241)
 
-        assert MockMirrorFileImportParser.rpsl_data_calls == ['source1', 'source2']
+        assert MockMirrorFileImportParser.rpsl_data_calls == ["source1", "source2"]
         assert flatten_mock_calls(mock_dh) == [
-            ['delete_all_rpsl_objects_with_journal', ('TEST',), {}],
-            ['disable_journaling', (), {}],
-            ['record_serial_newest_mirror', ('TEST', 424242), {}],
+            ["delete_all_rpsl_objects_with_journal", ("TEST",), {}],
+            ["disable_journaling", (), {}],
+            ["record_serial_newest_mirror", ("TEST", 424242), {}],
         ]
         assert mock_bulk_validator_init.mock_calls[0][1][0] == mock_dh
 
     def test_failed_import_ftp(self, monkeypatch, config_override):
-        config_override({
-            'rpki': {'roa_source': 'https://example.com/roa.json'},
-            'sources': {
-                'TEST': {
-                    'import_source': 'ftp://host/source1.gz',
-                }
+        config_override(
+            {
+                "rpki": {"roa_source": "https://example.com/roa.json"},
+                "sources": {
+                    "TEST": {
+                        "import_source": "ftp://host/source1.gz",
+                    }
+                },
             }
-        })
+        )
 
         mock_dh = Mock()
         request = Mock()
         MockMirrorFileImportParser.rpsl_data_calls = []
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.MirrorFileImportParser', MockMirrorFileImportParser)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.request', request)
+        monkeypatch.setattr(
+            "irrd.mirroring.mirror_runners_import.MirrorFileImportParser", MockMirrorFileImportParser
+        )
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_import.request", request)
 
         mock_bulk_validator_init = Mock()
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.BulkRouteROAValidator', mock_bulk_validator_init)
+        monkeypatch.setattr(
+            "irrd.mirroring.mirror_runners_import.BulkRouteROAValidator", mock_bulk_validator_init
+        )
 
-        request.urlopen = lambda url: MockUrlopenResponse(b'', fail=True)
+        request.urlopen = lambda url, timeout: MockUrlopenResponse(b"", fail=True)
         with pytest.raises(IOError):
-            RPSLMirrorFullImportRunner('TEST').run(mock_dh, serial_newest_mirror=424241)
+            RPSLMirrorFullImportRunner("TEST").run(mock_dh, serial_newest_mirror=424241)
 
     def test_run_import_local_file(self, monkeypatch, config_override, tmpdir):
-        tmp_import_source1 = tmpdir + '/source1.rpsl.gz'
-        with open(tmp_import_source1, 'wb') as fh:
+        tmp_import_source1 = tmpdir + "/source1.rpsl.gz"
+        with open(tmp_import_source1, "wb") as fh:
             # gzipped data, contains 'source1'
-            fh.write(b64decode('H4sIAE4CfFsAAyvOLy1KTjUEAE5Fj0oHAAAA'))
-        tmp_import_source2 = tmpdir + '/source2.rpsl'
-        with open(tmp_import_source2, 'w') as fh:
-            fh.write('source2')
-        tmp_import_serial = tmpdir + '/serial'
-        with open(tmp_import_serial, 'w') as fh:
-            fh.write('424242')
-
-        config_override({
-            'rpki': {'roa_source': None},
-            'sources': {
-                'TEST': {
-                    'import_source': ['file://' + str(tmp_import_source1), 'file://' + str(tmp_import_source2)],
-                    'import_serial_source': 'file://' + str(tmp_import_serial),
-                }
+            fh.write(b64decode("H4sIAE4CfFsAAyvOLy1KTjUEAE5Fj0oHAAAA"))
+        tmp_import_source2 = tmpdir + "/source2.rpsl"
+        with open(tmp_import_source2, "w") as fh:
+            fh.write("source2")
+        tmp_import_serial = tmpdir + "/serial"
+        with open(tmp_import_serial, "w") as fh:
+            fh.write("424242")
+
+        config_override(
+            {
+                "rpki": {"roa_source": None},
+                "sources": {
+                    "TEST": {
+                        "import_source": [
+                            "file://" + str(tmp_import_source1),
+                            "file://" + str(tmp_import_source2),
+                        ],
+                        "import_serial_source": "file://" + str(tmp_import_serial),
+                    }
+                },
             }
-        })
+        )
 
         mock_dh = Mock()
         MockMirrorFileImportParser.rpsl_data_calls = []
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.MirrorFileImportParser', MockMirrorFileImportParser)
+        monkeypatch.setattr(
+            "irrd.mirroring.mirror_runners_import.MirrorFileImportParser", MockMirrorFileImportParser
+        )
 
-        RPSLMirrorFullImportRunner('TEST').run(mock_dh)
+        RPSLMirrorFullImportRunner("TEST").run(mock_dh)
 
-        assert MockMirrorFileImportParser.rpsl_data_calls == ['source1', 'source2']
+        assert MockMirrorFileImportParser.rpsl_data_calls == ["source1", "source2"]
         assert flatten_mock_calls(mock_dh) == [
-            ['delete_all_rpsl_objects_with_journal', ('TEST',), {}],
-            ['disable_journaling', (), {}],
-            ['record_serial_newest_mirror', ('TEST', 424242), {}],
+            ["delete_all_rpsl_objects_with_journal", ("TEST",), {}],
+            ["disable_journaling", (), {}],
+            ["record_serial_newest_mirror", ("TEST", 424242), {}],
         ]
 
     def test_no_serial_ftp(self, monkeypatch, config_override):
-        config_override({
-            'rpki': {'roa_source': None},
-            'sources': {
-                'TEST': {
-                    'import_source': ['ftp://host/source1.gz', 'ftp://host/source2'],
-                }
+        config_override(
+            {
+                "rpki": {"roa_source": None},
+                "sources": {
+                    "TEST": {
+                        "import_source": ["ftp://host/source1.gz", "ftp://host/source2"],
+                    }
+                },
             }
-        })
+        )
 
         mock_dh = Mock()
         request = Mock()
         MockMirrorFileImportParser.rpsl_data_calls = []
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.MirrorFileImportParser', MockMirrorFileImportParser)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.request', request)
+        monkeypatch.setattr(
+            "irrd.mirroring.mirror_runners_import.MirrorFileImportParser", MockMirrorFileImportParser
+        )
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_import.request", request)
 
         responses = {
             # gzipped data, contains 'source1'
-            'ftp://host/source1.gz': b64decode('H4sIAE4CfFsAAyvOLy1KTjUEAE5Fj0oHAAAA'),
-            'ftp://host/source2': b'source2',
+            "ftp://host/source1.gz": b64decode("H4sIAE4CfFsAAyvOLy1KTjUEAE5Fj0oHAAAA"),
+            "ftp://host/source2": b"source2",
         }
-        request.urlopen = lambda url: MockUrlopenResponse(responses[url])
-        RPSLMirrorFullImportRunner('TEST').run(mock_dh, serial_newest_mirror=42)
+        request.urlopen = lambda url, timeout: MockUrlopenResponse(responses[url])
+        RPSLMirrorFullImportRunner("TEST").run(mock_dh, serial_newest_mirror=42)
 
-        assert MockMirrorFileImportParser.rpsl_data_calls == ['source1', 'source2']
+        assert MockMirrorFileImportParser.rpsl_data_calls == ["source1", "source2"]
         assert flatten_mock_calls(mock_dh) == [
-            ['delete_all_rpsl_objects_with_journal', ('TEST',), {}],
-            ['disable_journaling', (), {}],
+            ["delete_all_rpsl_objects_with_journal", ("TEST",), {}],
+            ["disable_journaling", (), {}],
         ]
 
     def test_import_cancelled_serial_too_old(self, monkeypatch, config_override, caplog):
-        config_override({
-            'sources': {
-                'TEST': {
-                    'import_source': ['ftp://host/source1.gz', 'ftp://host/source2'],
-                    'import_serial_source': 'ftp://host/serial',
+        config_override(
+            {
+                "sources": {
+                    "TEST": {
+                        "import_source": ["ftp://host/source1.gz", "ftp://host/source2"],
+                        "import_serial_source": "ftp://host/serial",
+                    }
                 }
             }
-        })
+        )
 
         mock_dh = Mock()
         request = Mock()
         MockMirrorFileImportParser.rpsl_data_calls = []
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.MirrorFileImportParser', MockMirrorFileImportParser)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.request', request)
+        monkeypatch.setattr(
+            "irrd.mirroring.mirror_runners_import.MirrorFileImportParser", MockMirrorFileImportParser
+        )
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_import.request", request)
 
         responses = {
             # gzipped data, contains 'source1'
-            'ftp://host/source1.gz': b64decode('H4sIAE4CfFsAAyvOLy1KTjUEAE5Fj0oHAAAA'),
-            'ftp://host/source2': b'source2',
-            'ftp://host/serial': b'424242',
+            "ftp://host/source1.gz": b64decode("H4sIAE4CfFsAAyvOLy1KTjUEAE5Fj0oHAAAA"),
+            "ftp://host/source2": b"source2",
+            "ftp://host/serial": b"424242",
         }
-        request.urlopen = lambda url: MockUrlopenResponse(responses[url])
-        RPSLMirrorFullImportRunner('TEST').run(mock_dh, serial_newest_mirror=424243)
+        request.urlopen = lambda url, timeout: MockUrlopenResponse(responses[url])
+        RPSLMirrorFullImportRunner("TEST").run(mock_dh, serial_newest_mirror=424243)
 
         assert not MockMirrorFileImportParser.rpsl_data_calls
         assert flatten_mock_calls(mock_dh) == []
-        assert 'Current newest serial seen for TEST is 424243, import_serial is 424242, cancelling import.'
+        assert "Current newest serial seen for TEST is 424243, import_serial is 424242, cancelling import."
 
     def test_import_force_reload_with_serial_too_old(self, monkeypatch, config_override):
-        config_override({
-            'rpki': {'roa_source': None},
-            'sources': {
-                'TEST': {
-                    'import_source': ['ftp://host/source1.gz', 'ftp://host/source2'],
-                    'import_serial_source': 'ftp://host/serial',
-                }
+        config_override(
+            {
+                "rpki": {"roa_source": None},
+                "sources": {
+                    "TEST": {
+                        "import_source": ["ftp://host/source1.gz", "ftp://host/source2"],
+                        "import_serial_source": "ftp://host/serial",
+                    }
+                },
             }
-        })
+        )
 
         mock_dh = Mock()
         request = Mock()
         MockMirrorFileImportParser.rpsl_data_calls = []
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.MirrorFileImportParser', MockMirrorFileImportParser)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.request', request)
+        monkeypatch.setattr(
+            "irrd.mirroring.mirror_runners_import.MirrorFileImportParser", MockMirrorFileImportParser
+        )
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_import.request", request)
 
         responses = {
             # gzipped data, contains 'source1'
-            'ftp://host/source1.gz': b64decode('H4sIAE4CfFsAAyvOLy1KTjUEAE5Fj0oHAAAA'),
-            'ftp://host/source2': b'source2',
-            'ftp://host/serial': b'424242',
+            "ftp://host/source1.gz": b64decode("H4sIAE4CfFsAAyvOLy1KTjUEAE5Fj0oHAAAA"),
+            "ftp://host/source2": b"source2",
+            "ftp://host/serial": b"424242",
         }
-        request.urlopen = lambda url: MockUrlopenResponse(responses[url])
-        RPSLMirrorFullImportRunner('TEST').run(mock_dh, serial_newest_mirror=424243, force_reload=True)
+        request.urlopen = lambda url, timeout: MockUrlopenResponse(responses[url])
+        RPSLMirrorFullImportRunner("TEST").run(mock_dh, serial_newest_mirror=424243, force_reload=True)
 
-        assert MockMirrorFileImportParser.rpsl_data_calls == ['source1', 'source2']
+        assert MockMirrorFileImportParser.rpsl_data_calls == ["source1", "source2"]
         assert flatten_mock_calls(mock_dh) == [
-            ['delete_all_rpsl_objects_with_journal', ('TEST',), {}],
-            ['disable_journaling', (), {}],
-            ['record_serial_newest_mirror', ('TEST', 424242), {}],
+            ["delete_all_rpsl_objects_with_journal", ("TEST",), {}],
+            ["disable_journaling", (), {}],
+            ["record_serial_newest_mirror", ("TEST", 424242), {}],
         ]
 
     def test_missing_source_settings_ftp(self, config_override):
-        config_override({
-            'sources': {
-                'TEST': {
-                    'import_serial_source': 'ftp://host/serial',
+        config_override(
+            {
+                "sources": {
+                    "TEST": {
+                        "import_serial_source": "ftp://host/serial",
+                    }
                 }
             }
-        })
+        )
 
         mock_dh = Mock()
-        RPSLMirrorFullImportRunner('TEST').run(mock_dh)
+        RPSLMirrorFullImportRunner("TEST").run(mock_dh)
         assert not flatten_mock_calls(mock_dh)
 
     def test_unsupported_protocol(self, config_override):
-        config_override({
-            'sources': {
-                'TEST': {
-                    'import_source': 'ftp://host/source1.gz',
-                    'import_serial_source': 'gopher://host/serial',
+        config_override(
+            {
+                "sources": {
+                    "TEST": {
+                        "import_source": "ftp://host/source1.gz",
+                        "import_serial_source": "gopher://host/serial",
+                    }
                 }
             }
-        })
+        )
 
         mock_dh = Mock()
         with pytest.raises(ValueError) as ve:
-            RPSLMirrorFullImportRunner('TEST').run(mock_dh)
-        assert 'scheme gopher is not supported' in str(ve.value)
+            RPSLMirrorFullImportRunner("TEST").run(mock_dh)
+        assert "scheme gopher is not supported" in str(ve.value)
 
 
 class MockUrlopenResponse(BytesIO):
-    def __init__(self, bytes: bytes, fail: bool=False):
+    def __init__(self, bytes: bytes, fail: bool = False):
         if fail:
-            raise URLError('error')
+            raise URLError("error")
         super().__init__(bytes)
 
 
 class MockMirrorFileImportParser:
     rpsl_data_calls: List[str] = []
 
-    def __init__(self, source, filename, serial, database_handler, direct_error_return=False, roa_validator=None):
+    def __init__(
+        self, source, filename, serial, database_handler, direct_error_return=False, roa_validator=None
+    ):
         self.filename = filename
-        assert source == 'TEST'
+        assert source == "TEST"
         assert serial is None
 
     def run_import(self):
-        with open(self.filename, 'r') as f:
+        with open(self.filename) as f:
             self.rpsl_data_calls.append(f.read())
 
 
 class TestROAImportRunner:
     # As the code for retrieving files from HTTP, FTP or local file
     # is shared between ROAImportRunner and RPSLMirrorFullImportRunner,
     # not all protocols are tested here.
     def test_run_import_http_file_success(self, monkeypatch, config_override, tmpdir, caplog):
-        slurm_path = str(tmpdir) + '/slurm.json'
-        config_override({
-            'rpki': {
-                'roa_source': 'https://host/roa.json',
-                'slurm_source': 'file://' + slurm_path
-            }
-        })
+        slurm_path = str(tmpdir) + "/slurm.json"
+        config_override(
+            {"rpki": {"roa_source": "https://host/roa.json", "slurm_source": "file://" + slurm_path}}
+        )
 
         class MockRequestsSuccess:
             status_code = 200
 
             def __init__(self, url, stream, timeout):
-                assert url == 'https://host/roa.json'
+                assert url == "https://host/roa.json"
                 assert stream
                 assert timeout
 
             def iter_content(self, size):
-                return iter([b'roa_', b'data'])
+                return iter([b"roa_", b"data"])
 
-        with open(slurm_path, 'wb') as fh:
-            fh.write(b'slurm_data')
+        with open(slurm_path, "wb") as fh:
+            fh.write(b"slurm_data")
 
         mock_dh = Mock(spec=DatabaseHandler)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.DatabaseHandler', lambda: mock_dh)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.ROADataImporter', MockROADataImporter)
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_import.DatabaseHandler", lambda: mock_dh)
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_import.ROADataImporter", MockROADataImporter)
         mock_bulk_validator = Mock(spec=BulkRouteROAValidator)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.BulkRouteROAValidator', lambda dh, roas: mock_bulk_validator)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.requests.get', MockRequestsSuccess)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.notify_rpki_invalid_owners', lambda dh, invalids: 1)
+        monkeypatch.setattr(
+            "irrd.mirroring.mirror_runners_import.BulkRouteROAValidator", lambda dh, roas: mock_bulk_validator
+        )
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_import.requests.get", MockRequestsSuccess)
+        monkeypatch.setattr(
+            "irrd.mirroring.mirror_runners_import.notify_rpki_invalid_owners", lambda dh, invalids: 1
+        )
 
         mock_bulk_validator.validate_all_routes = lambda: (
-            [{'rpsl_pk': 'pk_now_valid1'}, {'rpsl_pk': 'pk_now_valid2'}],
-            [{'rpsl_pk': 'pk_now_invalid1'}, {'rpsl_pk': 'pk_now_invalid2'}],
-            [{'rpsl_pk': 'pk_now_unknown1'}, {'rpsl_pk': 'pk_now_unknown2'}],
+            [{"rpsl_pk": "pk_now_valid1"}, {"rpsl_pk": "pk_now_valid2"}],
+            [{"rpsl_pk": "pk_now_invalid1"}, {"rpsl_pk": "pk_now_invalid2"}],
+            [{"rpsl_pk": "pk_now_unknown1"}, {"rpsl_pk": "pk_now_unknown2"}],
         )
         ROAImportRunner().run()
 
         assert flatten_mock_calls(mock_dh) == [
-            ['disable_journaling', (), {}],
-            ['delete_all_roa_objects', (), {}],
-            ['delete_all_rpsl_objects_with_journal', ('RPKI',), {'journal_guaranteed_empty': True}],
-            ['commit', (), {}],
-            ['enable_journaling', (), {}],
-            ['update_rpki_status', (), {
-                'rpsl_objs_now_valid': [{'rpsl_pk': 'pk_now_valid1'}, {'rpsl_pk': 'pk_now_valid2'}],
-                'rpsl_objs_now_invalid': [{'rpsl_pk': 'pk_now_invalid1'}, {'rpsl_pk': 'pk_now_invalid2'}],
-                'rpsl_objs_now_not_found': [{'rpsl_pk': 'pk_now_unknown1'}, {'rpsl_pk': 'pk_now_unknown2'}],
-            }],
-            ['commit', (), {}],
-            ['close', (), {}]
+            ["disable_journaling", (), {}],
+            ["delete_all_roa_objects", (), {}],
+            ["delete_all_rpsl_objects_with_journal", ("RPKI",), {"journal_guaranteed_empty": True}],
+            ["commit", (), {}],
+            ["enable_journaling", (), {}],
+            [
+                "update_rpki_status",
+                (),
+                {
+                    "rpsl_objs_now_valid": [{"rpsl_pk": "pk_now_valid1"}, {"rpsl_pk": "pk_now_valid2"}],
+                    "rpsl_objs_now_invalid": [{"rpsl_pk": "pk_now_invalid1"}, {"rpsl_pk": "pk_now_invalid2"}],
+                    "rpsl_objs_now_not_found": [
+                        {"rpsl_pk": "pk_now_unknown1"},
+                        {"rpsl_pk": "pk_now_unknown2"},
+                    ],
+                },
+            ],
+            ["commit", (), {}],
+            ["close", (), {}],
         ]
-        assert '2 newly valid, 2 newly invalid, 2 newly not_found routes, 1 emails sent to contacts of newly invalid authoritative objects' in caplog.text
+        assert (
+            "2 newly valid, 2 newly invalid, 2 newly not_found routes, 1 emails sent to contacts of newly"
+            " invalid authoritative objects"
+            in caplog.text
+        )
 
     def test_run_import_http_file_failed_download(self, monkeypatch, config_override, tmpdir, caplog):
-        config_override({
-            'rpki': {
-                'roa_source': 'https://host/roa.json',
+        config_override(
+            {
+                "rpki": {
+                    "roa_source": "https://host/roa.json",
+                }
             }
-        })
+        )
 
         class MockRequestsSuccess:
             status_code = 500
-            content = 'expected-test-error'
+            content = "expected-test-error"
 
             def __init__(self, url, stream, timeout):
-                assert url == 'https://host/roa.json'
+                assert url == "https://host/roa.json"
                 assert stream
                 assert timeout
 
         mock_dh = Mock(spec=DatabaseHandler)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.DatabaseHandler', lambda: mock_dh)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.requests.get', MockRequestsSuccess)
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_import.DatabaseHandler", lambda: mock_dh)
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_import.requests.get", MockRequestsSuccess)
 
         ROAImportRunner().run()
-        assert 'Failed to download https://host/roa.json: 500: expected-test-error' in caplog.text
+        assert "Failed to download https://host/roa.json: 500: expected-test-error" in caplog.text
 
     def test_exception_handling(self, monkeypatch, config_override, tmpdir, caplog):
-        tmp_roa_source = tmpdir + '/roa.json'
-        with open(tmp_roa_source, 'wb') as fh:
-            fh.write(b'roa_data')
-        config_override({
-            'rpki': {
-                'roa_source': 'file://' + str(tmp_roa_source),
+        tmp_roa_source = tmpdir + "/roa.json"
+        with open(tmp_roa_source, "wb") as fh:
+            fh.write(b"roa_data")
+        config_override(
+            {
+                "rpki": {
+                    "roa_source": "file://" + str(tmp_roa_source),
+                }
             }
-        })
+        )
 
         mock_dh = Mock(spec=DatabaseHandler)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.DatabaseHandler', lambda: mock_dh)
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_import.DatabaseHandler", lambda: mock_dh)
 
-        mock_importer = Mock(side_effect=ValueError('expected-test-error-1'))
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.ROADataImporter', mock_importer)
+        mock_importer = Mock(side_effect=ValueError("expected-test-error-1"))
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_import.ROADataImporter", mock_importer)
         ROAImportRunner().run()
 
-        mock_importer = Mock(side_effect=ROAParserException('expected-test-error-2'))
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.ROADataImporter', mock_importer)
+        mock_importer = Mock(side_effect=ROAParserException("expected-test-error-2"))
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_import.ROADataImporter", mock_importer)
         ROAImportRunner().run()
 
         assert flatten_mock_calls(mock_dh) == 2 * [
-            ['disable_journaling', (), {}],
-            ['delete_all_roa_objects', (), {}],
-            ['delete_all_rpsl_objects_with_journal', ('RPKI',), {'journal_guaranteed_empty': True}],
-            ['close', (), {}]
+            ["disable_journaling", (), {}],
+            ["delete_all_roa_objects", (), {}],
+            ["delete_all_rpsl_objects_with_journal", ("RPKI",), {"journal_guaranteed_empty": True}],
+            ["close", (), {}],
         ]
 
-        assert 'expected-test-error-1' in caplog.text
-        assert 'expected-test-error-2' in caplog.text
+        assert "expected-test-error-1" in caplog.text
+        assert "expected-test-error-2" in caplog.text
 
     def test_file_error_handling(self, monkeypatch, config_override, tmpdir, caplog):
-        tmp_roa_source = tmpdir + '/roa.json'
-        config_override({
-            'rpki': {
-                'roa_source': 'file://' + str(tmp_roa_source),
+        tmp_roa_source = tmpdir + "/roa.json"
+        config_override(
+            {
+                "rpki": {
+                    "roa_source": "file://" + str(tmp_roa_source),
+                }
             }
-        })
+        )
 
         mock_dh = Mock(spec=DatabaseHandler)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.DatabaseHandler', lambda: mock_dh)
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_import.DatabaseHandler", lambda: mock_dh)
         ROAImportRunner().run()
 
         assert flatten_mock_calls(mock_dh) == [
-            ['disable_journaling', (), {}],
-            ['delete_all_roa_objects', (), {}],
-            ['delete_all_rpsl_objects_with_journal', ('RPKI',), {'journal_guaranteed_empty': True}],
-            ['close', (), {}]
+            ["disable_journaling", (), {}],
+            ["delete_all_roa_objects", (), {}],
+            ["delete_all_rpsl_objects_with_journal", ("RPKI",), {"journal_guaranteed_empty": True}],
+            ["close", (), {}],
         ]
 
-        assert 'No such file or directory' in caplog.text
+        assert "No such file or directory" in caplog.text
 
 
 class MockROADataImporter:
     def __init__(self, rpki_text: str, slurm_text: str, database_handler: DatabaseHandler):
-        assert rpki_text == 'roa_data'
-        assert slurm_text == 'slurm_data'
-        self.roa_objs = ['roa1', 'roa2']
+        assert rpki_text == "roa_data"
+        assert slurm_text == "slurm_data"
+        self.roa_objs = ["roa1", "roa2"]
 
 
 class TestScopeFilterUpdateRunner:
     def test_run(self, monkeypatch, config_override, tmpdir, caplog):
         mock_dh = Mock(spec=DatabaseHandler)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.DatabaseHandler', lambda: mock_dh)
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_import.DatabaseHandler", lambda: mock_dh)
         mock_scopefilter = Mock(spec=ScopeFilterValidator)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.ScopeFilterValidator', lambda: mock_scopefilter)
+        monkeypatch.setattr(
+            "irrd.mirroring.mirror_runners_import.ScopeFilterValidator", lambda: mock_scopefilter
+        )
 
         mock_scopefilter.validate_all_rpsl_objects = lambda database_handler: (
-            [{'rpsl_pk': 'pk_now_in_scope1'}, {'rpsl_pk': 'pk_now_in_scope2'}],
-            [{'rpsl_pk': 'pk_now_out_scope_as1'}, {'rpsl_pk': 'pk_now_out_scope_as2'}],
-            [{'rpsl_pk': 'pk_now_out_scope_prefix1'}, {'rpsl_pk': 'pk_now_out_scope_prefix2'}],
+            [{"rpsl_pk": "pk_now_in_scope1"}, {"rpsl_pk": "pk_now_in_scope2"}],
+            [{"rpsl_pk": "pk_now_out_scope_as1"}, {"rpsl_pk": "pk_now_out_scope_as2"}],
+            [{"rpsl_pk": "pk_now_out_scope_prefix1"}, {"rpsl_pk": "pk_now_out_scope_prefix2"}],
         )
         ScopeFilterUpdateRunner().run()
 
         assert flatten_mock_calls(mock_dh) == [
-            ['update_scopefilter_status', (), {
-                'rpsl_objs_now_in_scope': [{'rpsl_pk': 'pk_now_in_scope1'}, {'rpsl_pk': 'pk_now_in_scope2'}],
-                'rpsl_objs_now_out_scope_as': [{'rpsl_pk': 'pk_now_out_scope_as1'}, {'rpsl_pk': 'pk_now_out_scope_as2'}],
-                'rpsl_objs_now_out_scope_prefix': [{'rpsl_pk': 'pk_now_out_scope_prefix1'}, {'rpsl_pk': 'pk_now_out_scope_prefix2'}],
-            }],
-            ['commit', (), {}],
-            ['close', (), {}]
+            [
+                "update_scopefilter_status",
+                (),
+                {
+                    "rpsl_objs_now_in_scope": [
+                        {"rpsl_pk": "pk_now_in_scope1"},
+                        {"rpsl_pk": "pk_now_in_scope2"},
+                    ],
+                    "rpsl_objs_now_out_scope_as": [
+                        {"rpsl_pk": "pk_now_out_scope_as1"},
+                        {"rpsl_pk": "pk_now_out_scope_as2"},
+                    ],
+                    "rpsl_objs_now_out_scope_prefix": [
+                        {"rpsl_pk": "pk_now_out_scope_prefix1"},
+                        {"rpsl_pk": "pk_now_out_scope_prefix2"},
+                    ],
+                },
+            ],
+            ["commit", (), {}],
+            ["close", (), {}],
         ]
-        assert '2 newly in scope, 2 newly out of scope AS, 2 newly out of scope prefix' in caplog.text
+        assert "2 newly in scope, 2 newly out of scope AS, 2 newly out of scope prefix" in caplog.text
 
     def test_exception_handling(self, monkeypatch, config_override, tmpdir, caplog):
         mock_dh = Mock(spec=DatabaseHandler)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.DatabaseHandler', lambda: mock_dh)
-        mock_scopefilter = Mock(side_effect=ValueError('expected-test-error'))
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.ScopeFilterValidator', mock_scopefilter)
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_import.DatabaseHandler", lambda: mock_dh)
+        mock_scopefilter = Mock(side_effect=ValueError("expected-test-error"))
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_import.ScopeFilterValidator", mock_scopefilter)
 
         ScopeFilterUpdateRunner().run()
 
-        assert flatten_mock_calls(mock_dh) == [
-            ['close', (), {}]
-        ]
-        assert 'expected-test-error' in caplog.text
+        assert flatten_mock_calls(mock_dh) == [["close", (), {}]]
+        assert "expected-test-error" in caplog.text
+
+
+class TestRoutePreferenceUpdateRunner:
+    def test_run(self, monkeypatch, config_override, tmpdir, caplog):
+        mock_dh = Mock(spec=DatabaseHandler)
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_import.DatabaseHandler", lambda: mock_dh)
+        mock_update_function = Mock(spec=update_route_preference_status)
+        monkeypatch.setattr(
+            "irrd.mirroring.mirror_runners_import.update_route_preference_status", mock_update_function
+        )
+
+        RoutePreferenceUpdateRunner().run()
+
+        assert flatten_mock_calls(mock_dh) == [["commit", (), {}], ["close", (), {}]]
+
+    def test_exception_handling(self, monkeypatch, config_override, tmpdir, caplog):
+        mock_dh = Mock(spec=DatabaseHandler)
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_import.DatabaseHandler", lambda: mock_dh)
+        mock_update_function = Mock(side_effect=ValueError("expected-test-error"))
+        monkeypatch.setattr(
+            "irrd.mirroring.mirror_runners_import.update_route_preference_status", mock_update_function
+        )
+
+        RoutePreferenceUpdateRunner().run()
+
+        assert flatten_mock_calls(mock_dh) == [["close", (), {}]]
+        assert "expected-test-error" in caplog.text
 
 
 class TestNRTMImportUpdateStreamRunner:
     def test_run_import(self, monkeypatch, config_override):
-        config_override({
-            'sources': {
-                'TEST': {
-                    'nrtm_host': '192.0.2.1',
-                    'nrtm_port': 43,
+        config_override(
+            {
+                "sources": {
+                    "TEST": {
+                        "nrtm_host": "192.0.2.1",
+                        "nrtm_port": 43,
+                    }
                 }
             }
-        })
+        )
 
         def mock_whois_query(host, port, query, end_markings) -> str:
-            assert host == '192.0.2.1'
+            assert host == "192.0.2.1"
             assert port == 43
-            assert query == '-g TEST:3:424243-LAST'
-            assert 'TEST' in end_markings[0]
-            return 'response'
+            assert query == "-g TEST:3:424243-LAST"
+            assert "TEST" in end_markings[0]
+            return "response"
 
         mock_dh = Mock()
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.NRTMStreamParser', MockNRTMStreamParser)
-        monkeypatch.setattr('irrd.mirroring.mirror_runners_import.whois_query', mock_whois_query)
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_import.NRTMStreamParser", MockNRTMStreamParser)
+        monkeypatch.setattr("irrd.mirroring.mirror_runners_import.whois_query", mock_whois_query)
 
-        NRTMImportUpdateStreamRunner('TEST').run(424242, mock_dh)
+        NRTMImportUpdateStreamRunner("TEST").run(424242, mock_dh)
 
     def test_missing_source_settings(self, monkeypatch, config_override):
-        config_override({
-            'sources': {
-                'TEST': {
-                    'nrtm_port': '4343',
+        config_override(
+            {
+                "sources": {
+                    "TEST": {
+                        "nrtm_port": "4343",
+                    }
                 }
             }
-        })
+        )
 
         mock_dh = Mock()
-        NRTMImportUpdateStreamRunner('TEST').run(424242, mock_dh)
+        NRTMImportUpdateStreamRunner("TEST").run(424242, mock_dh)
 
 
-class MockNRTMStreamParser(object):
+class MockNRTMStreamParser:
     def __init__(self, source, response, database_handler):
-        assert source == 'TEST'
-        assert response == 'response'
+        assert source == "TEST"
+        assert response == "response"
         self.operations = [Mock()]
```

### Comparing `irrd-4.2.8/irrd/mirroring/tests/test_nrtm_operation.py` & `irrd-4.3.0/irrd/mirroring/tests/test_nrtm_operation.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,199 +1,202 @@
 from unittest.mock import Mock
 
 from irrd.rpki.status import RPKIStatus
 from irrd.rpsl.rpsl_objects import rpsl_object_from_text
 from irrd.scopefilter.status import ScopeFilterStatus
 from irrd.scopefilter.validators import ScopeFilterValidator
 from irrd.storage.models import DatabaseOperation, JournalEntryOrigin
-from irrd.utils.rpsl_samples import (SAMPLE_MNTNER, SAMPLE_UNKNOWN_CLASS,
-                                     SAMPLE_MALFORMED_EMPTY_LINE, SAMPLE_KEY_CERT,
-                                     KEY_CERT_SIGNED_MESSAGE_VALID, SAMPLE_ROUTE)
+from irrd.utils.rpsl_samples import (
+    KEY_CERT_SIGNED_MESSAGE_VALID,
+    SAMPLE_KEY_CERT,
+    SAMPLE_MALFORMED_EMPTY_LINE,
+    SAMPLE_MNTNER,
+    SAMPLE_ROUTE,
+    SAMPLE_UNKNOWN_CLASS,
+)
+
 from ..nrtm_operation import NRTMOperation
 
 
 class TestNRTMOperation:
-
     def test_nrtm_add_valid_without_strict_import_keycert(self, monkeypatch, tmp_gpg_dir):
         mock_dh = Mock()
         mock_scopefilter = Mock(spec=ScopeFilterValidator)
-        monkeypatch.setattr('irrd.mirroring.nrtm_operation.ScopeFilterValidator',
-                            lambda: mock_scopefilter)
-        mock_scopefilter.validate_rpsl_object = lambda obj: (ScopeFilterStatus.in_scope, '')
+        monkeypatch.setattr("irrd.mirroring.nrtm_operation.ScopeFilterValidator", lambda: mock_scopefilter)
+        mock_scopefilter.validate_rpsl_object = lambda obj: (ScopeFilterStatus.in_scope, "")
 
         operation = NRTMOperation(
-            source='TEST',
+            source="TEST",
             operation=DatabaseOperation.add_or_update,
             serial=42424242,
             object_text=SAMPLE_KEY_CERT,
             strict_validation_key_cert=False,
-            object_class_filter=['route', 'route6', 'mntner', 'key-cert'],
+            object_class_filter=["route", "route6", "mntner", "key-cert"],
         )
         assert operation.save(database_handler=mock_dh)
 
         assert mock_dh.upsert_rpsl_object.call_count == 1
-        assert mock_dh.mock_calls[0][1][0].pk() == 'PGPKEY-80F238C6'
+        assert mock_dh.mock_calls[0][1][0].pk() == "PGPKEY-80F238C6"
         assert mock_dh.mock_calls[0][1][1] == JournalEntryOrigin.mirror
 
         # key-cert should not be imported in the keychain, therefore
         # verification should fail
         key_cert_obj = rpsl_object_from_text(SAMPLE_KEY_CERT, strict_validation=False)
         assert not key_cert_obj.verify(KEY_CERT_SIGNED_MESSAGE_VALID)
 
     def test_nrtm_add_valid_with_strict_import_keycert(self, monkeypatch, tmp_gpg_dir):
         mock_dh = Mock()
         mock_scopefilter = Mock(spec=ScopeFilterValidator)
-        monkeypatch.setattr('irrd.mirroring.nrtm_operation.ScopeFilterValidator',
-                            lambda: mock_scopefilter)
-        mock_scopefilter.validate_rpsl_object = lambda obj: (ScopeFilterStatus.in_scope, '')
+        monkeypatch.setattr("irrd.mirroring.nrtm_operation.ScopeFilterValidator", lambda: mock_scopefilter)
+        mock_scopefilter.validate_rpsl_object = lambda obj: (ScopeFilterStatus.in_scope, "")
 
         operation = NRTMOperation(
-            source='TEST',
+            source="TEST",
             operation=DatabaseOperation.add_or_update,
             serial=42424242,
             object_text=SAMPLE_KEY_CERT,
             strict_validation_key_cert=True,
-            object_class_filter=['route', 'route6', 'mntner', 'key-cert'],
+            object_class_filter=["route", "route6", "mntner", "key-cert"],
         )
         assert operation.save(database_handler=mock_dh)
 
         assert mock_dh.upsert_rpsl_object.call_count == 1
-        assert mock_dh.mock_calls[0][1][0].pk() == 'PGPKEY-80F238C6'
+        assert mock_dh.mock_calls[0][1][0].pk() == "PGPKEY-80F238C6"
         assert mock_dh.mock_calls[0][1][1] == JournalEntryOrigin.mirror
 
         # key-cert should be imported in the keychain, therefore
         # verification should succeed
         key_cert_obj = rpsl_object_from_text(SAMPLE_KEY_CERT, strict_validation=False)
         assert key_cert_obj.verify(KEY_CERT_SIGNED_MESSAGE_VALID)
 
     def test_nrtm_add_valid_rpki_scopefilter_aware(self, tmp_gpg_dir, monkeypatch):
         mock_dh = Mock()
         mock_route_validator = Mock()
-        monkeypatch.setattr('irrd.mirroring.nrtm_operation.SingleRouteROAValidator',
-                            lambda dh: mock_route_validator)
+        monkeypatch.setattr(
+            "irrd.mirroring.nrtm_operation.SingleRouteROAValidator", lambda dh: mock_route_validator
+        )
         mock_scopefilter = Mock(spec=ScopeFilterValidator)
-        monkeypatch.setattr('irrd.mirroring.nrtm_operation.ScopeFilterValidator',
-                            lambda: mock_scopefilter)
+        monkeypatch.setattr("irrd.mirroring.nrtm_operation.ScopeFilterValidator", lambda: mock_scopefilter)
 
         mock_route_validator.validate_route = lambda prefix, asn, source: RPKIStatus.invalid
-        mock_scopefilter.validate_rpsl_object = lambda obj: (ScopeFilterStatus.out_scope_prefix, '')
+        mock_scopefilter.validate_rpsl_object = lambda obj: (ScopeFilterStatus.out_scope_prefix, "")
         operation = NRTMOperation(
-            source='TEST',
+            source="TEST",
             operation=DatabaseOperation.add_or_update,
             serial=42424242,
             object_text=SAMPLE_ROUTE,
             strict_validation_key_cert=False,
             rpki_aware=True,
         )
         assert operation.save(database_handler=mock_dh)
 
         assert mock_dh.upsert_rpsl_object.call_count == 1
-        assert mock_dh.mock_calls[0][1][0].pk() == '192.0.2.0/24AS65537'
+        assert mock_dh.mock_calls[0][1][0].pk() == "192.0.2.0/24AS65537"
         assert mock_dh.mock_calls[0][1][0].rpki_status == RPKIStatus.invalid
         assert mock_dh.mock_calls[0][1][0].scopefilter_status == ScopeFilterStatus.out_scope_prefix
         assert mock_dh.mock_calls[0][1][1] == JournalEntryOrigin.mirror
 
     def test_nrtm_add_valid_ignored_object_class(self):
         mock_dh = Mock()
 
         operation = NRTMOperation(
-            source='TEST',
+            source="TEST",
             operation=DatabaseOperation.add_or_update,
             serial=42424242,
             object_text=SAMPLE_MNTNER,
             strict_validation_key_cert=False,
-            object_class_filter=['route', 'route6'],
+            object_class_filter=["route", "route6"],
         )
         assert not operation.save(database_handler=mock_dh)
         assert mock_dh.upsert_rpsl_object.call_count == 0
 
     def test_nrtm_delete_valid(self):
         mock_dh = Mock()
 
         operation = NRTMOperation(
-            source='TEST',
+            source="TEST",
             operation=DatabaseOperation.delete,
             serial=42424242,
             strict_validation_key_cert=False,
             object_text=SAMPLE_MNTNER,
         )
         assert operation.save(database_handler=mock_dh)
 
         assert mock_dh.delete_rpsl_object.call_count == 1
-        assert mock_dh.mock_calls[0][2]['rpsl_object'].pk() == 'TEST-MNT'
-        assert mock_dh.mock_calls[0][2]['origin'] == JournalEntryOrigin.mirror
+        assert mock_dh.mock_calls[0][2]["rpsl_object"].pk() == "TEST-MNT"
+        assert mock_dh.mock_calls[0][2]["origin"] == JournalEntryOrigin.mirror
 
     def test_nrtm_add_invalid_unknown_object_class(self):
         mock_dh = Mock()
 
         operation = NRTMOperation(
-            source='TEST',
+            source="TEST",
             operation=DatabaseOperation.add_or_update,
             serial=42424242,
             strict_validation_key_cert=False,
             object_text=SAMPLE_UNKNOWN_CLASS,
         )
         assert not operation.save(database_handler=mock_dh)
         assert mock_dh.upsert_rpsl_object.call_count == 0
 
     def test_nrtm_add_invalid_inconsistent_source(self):
         mock_dh = Mock()
 
         operation = NRTMOperation(
-            source='NOT-TEST',
+            source="NOT-TEST",
             operation=DatabaseOperation.add_or_update,
             serial=42424242,
             strict_validation_key_cert=False,
             object_text=SAMPLE_MNTNER,
         )
         assert not operation.save(database_handler=mock_dh)
         assert mock_dh.upsert_rpsl_object.call_count == 0
 
     def test_nrtm_add_invalid_rpsl_errors(self):
         mock_dh = Mock()
 
         operation = NRTMOperation(
-            source='TEST',
+            source="TEST",
             operation=DatabaseOperation.add_or_update,
             serial=42424242,
             strict_validation_key_cert=False,
             object_text=SAMPLE_MALFORMED_EMPTY_LINE,
         )
         assert not operation.save(database_handler=mock_dh)
         assert mock_dh.upsert_rpsl_object.call_count == 0
 
     def test_nrtm_delete_valid_incomplete_object(self):
         # In some rare cases, NRTM updates will arrive without
         # a source attribute. However, as the source of the NRTM
         # stream is known, we can guess this.
         # This is accepted for deletions only.
-        obj_text = 'route: 192.0.02.0/24\norigin: AS65537'
+        obj_text = "route: 192.0.02.0/24\norigin: AS65537"
         mock_dh = Mock()
 
         operation = NRTMOperation(
-            source='TEST',
+            source="TEST",
             operation=DatabaseOperation.delete,
             serial=42424242,
             object_text=obj_text,
             strict_validation_key_cert=False,
         )
         assert operation.save(database_handler=mock_dh)
 
         assert mock_dh.delete_rpsl_object.call_count == 1
-        assert mock_dh.mock_calls[0][2]['rpsl_object'].pk() == '192.0.2.0/24AS65537'
-        assert mock_dh.mock_calls[0][2]['rpsl_object'].source() == 'TEST'
-        assert mock_dh.mock_calls[0][2]['origin'] == JournalEntryOrigin.mirror
+        assert mock_dh.mock_calls[0][2]["rpsl_object"].pk() == "192.0.2.0/24AS65537"
+        assert mock_dh.mock_calls[0][2]["rpsl_object"].source() == "TEST"
+        assert mock_dh.mock_calls[0][2]["origin"] == JournalEntryOrigin.mirror
 
     def test_nrtm_add_invalid_incomplete_object(self):
         # Source-less objects are not accepted for add/update
-        obj_text = 'route: 192.0.02.0/24\norigin: AS65537'
+        obj_text = "route: 192.0.02.0/24\norigin: AS65537"
         mock_dh = Mock()
 
         operation = NRTMOperation(
-            source='TEST',
+            source="TEST",
             operation=DatabaseOperation.add_or_update,
             serial=42424242,
             object_text=obj_text,
             strict_validation_key_cert=False,
         )
         assert not operation.save(database_handler=mock_dh)
         assert not mock_dh.upsert_rpsl_object.call_count
```

### Comparing `irrd-4.2.8/irrd/mirroring/tests/test_parsers.py` & `irrd-4.3.0/irrd/mirroring/tests/test_parsers.py`

 * *Files 8% similar despite different names*

```diff
@@ -6,392 +6,425 @@
 from irrd.rpki.status import RPKIStatus
 from irrd.rpki.validators import BulkRouteROAValidator
 from irrd.rpsl.rpsl_objects import rpsl_object_from_text
 from irrd.scopefilter.status import ScopeFilterStatus
 from irrd.scopefilter.validators import ScopeFilterValidator
 from irrd.storage.models import DatabaseOperation, JournalEntryOrigin
 from irrd.utils.rpsl_samples import (
-    SAMPLE_ROUTE, SAMPLE_UNKNOWN_CLASS, SAMPLE_UNKNOWN_ATTRIBUTE, SAMPLE_MALFORMED_PK,
-    SAMPLE_ROUTE6, SAMPLE_KEY_CERT, KEY_CERT_SIGNED_MESSAGE_VALID, SAMPLE_LEGACY_IRRD_ARTIFACT,
-    SAMPLE_ROLE, SAMPLE_RTR_SET)
+    KEY_CERT_SIGNED_MESSAGE_VALID,
+    SAMPLE_KEY_CERT,
+    SAMPLE_LEGACY_IRRD_ARTIFACT,
+    SAMPLE_MALFORMED_PK,
+    SAMPLE_ROLE,
+    SAMPLE_ROUTE,
+    SAMPLE_ROUTE6,
+    SAMPLE_RTR_SET,
+    SAMPLE_UNKNOWN_ATTRIBUTE,
+    SAMPLE_UNKNOWN_CLASS,
+)
 from irrd.utils.test_utils import flatten_mock_calls
-from .nrtm_samples import (SAMPLE_NRTM_V3, SAMPLE_NRTM_V1, SAMPLE_NRTM_V1_TOO_MANY_ITEMS,
-                           SAMPLE_NRTM_INVALID_VERSION, SAMPLE_NRTM_V3_NO_END,
-                           SAMPLE_NRTM_V3_SERIAL_GAP, SAMPLE_NRTM_V3_INVALID_MULTIPLE_START_LINES,
-                           SAMPLE_NRTM_INVALID_NO_START_LINE, SAMPLE_NRTM_V3_SERIAL_OUT_OF_ORDER)
-from ..parsers import NRTMStreamParser, MirrorFileImportParser, MirrorUpdateFileImportParser
+
+from ..parsers import (
+    MirrorFileImportParser,
+    MirrorUpdateFileImportParser,
+    NRTMStreamParser,
+)
+from .nrtm_samples import (
+    SAMPLE_NRTM_INVALID_NO_START_LINE,
+    SAMPLE_NRTM_INVALID_VERSION,
+    SAMPLE_NRTM_V1,
+    SAMPLE_NRTM_V1_TOO_MANY_ITEMS,
+    SAMPLE_NRTM_V3,
+    SAMPLE_NRTM_V3_INVALID_MULTIPLE_START_LINES,
+    SAMPLE_NRTM_V3_NO_END,
+    SAMPLE_NRTM_V3_SERIAL_GAP,
+    SAMPLE_NRTM_V3_SERIAL_OUT_OF_ORDER,
+)
 
 
 @pytest.fixture
 def mock_scopefilter(monkeypatch):
     mock_scopefilter = Mock(spec=ScopeFilterValidator)
-    monkeypatch.setattr('irrd.mirroring.parsers.ScopeFilterValidator',
-                        lambda: mock_scopefilter)
-    mock_scopefilter.validate_rpsl_object = lambda obj: (ScopeFilterStatus.in_scope, '')
+    monkeypatch.setattr("irrd.mirroring.parsers.ScopeFilterValidator", lambda: mock_scopefilter)
+    mock_scopefilter.validate_rpsl_object = lambda obj: (ScopeFilterStatus.in_scope, "")
     return mock_scopefilter
 
 
 class TestMirrorFileImportParser:
     # This test also covers the common parts of MirrorFileImportParserBase
     def test_parse(self, mock_scopefilter, caplog, tmp_gpg_dir, config_override):
-        config_override({
-            'sources': {
-                'TEST': {
-                    'object_class_filter': ['route', 'key-cert'],
-                    'strict_import_keycert_objects': True,
+        config_override(
+            {
+                "sources": {
+                    "TEST": {
+                        "object_class_filter": ["route", "key-cert"],
+                        "strict_import_keycert_objects": True,
+                    }
                 }
             }
-        })
+        )
         mock_dh = Mock()
         mock_roa_validator = Mock(spec=BulkRouteROAValidator)
         mock_roa_validator.validate_route = lambda ip, length, asn, source: RPKIStatus.invalid
 
         test_data = [
             SAMPLE_UNKNOWN_ATTRIBUTE,  # valid, because mirror imports are non-strict
             SAMPLE_ROUTE6,  # Valid, excluded by object class filter
             SAMPLE_KEY_CERT,
-            SAMPLE_ROUTE.replace('TEST', 'BADSOURCE'),
+            SAMPLE_ROUTE.replace("TEST", "BADSOURCE"),
             SAMPLE_UNKNOWN_CLASS,
             SAMPLE_MALFORMED_PK,
             SAMPLE_LEGACY_IRRD_ARTIFACT,
         ]
-        test_input = '\n\n'.join(test_data)
+        test_input = "\n\n".join(test_data)
 
         with tempfile.NamedTemporaryFile() as fp:
-            fp.write(test_input.encode('utf-8'))
+            fp.write(test_input.encode("utf-8"))
             fp.seek(0)
             parser = MirrorFileImportParser(
-                source='TEST',
+                source="TEST",
                 filename=fp.name,
                 serial=424242,
                 database_handler=mock_dh,
                 roa_validator=mock_roa_validator,
             )
             parser.run_import()
         assert len(mock_dh.mock_calls) == 5
-        assert mock_dh.mock_calls[0][0] == 'upsert_rpsl_object'
-        assert mock_dh.mock_calls[0][1][0].pk() == '192.0.2.0/24AS65537'
+        assert mock_dh.mock_calls[0][0] == "upsert_rpsl_object"
+        assert mock_dh.mock_calls[0][1][0].pk() == "192.0.2.0/24AS65537"
         assert mock_dh.mock_calls[0][1][0].rpki_status == RPKIStatus.invalid
         assert mock_dh.mock_calls[0][1][0].scopefilter_status == ScopeFilterStatus.in_scope
-        assert mock_dh.mock_calls[1][0] == 'upsert_rpsl_object'
-        assert mock_dh.mock_calls[1][1][0].pk() == 'PGPKEY-80F238C6'
-        assert mock_dh.mock_calls[2][0] == 'record_mirror_error'
-        assert mock_dh.mock_calls[3][0] == 'record_mirror_error'
-        assert mock_dh.mock_calls[4][0] == 'record_serial_seen'
-        assert mock_dh.mock_calls[4][1][0] == 'TEST'
+        assert mock_dh.mock_calls[1][0] == "upsert_rpsl_object"
+        assert mock_dh.mock_calls[1][1][0].pk() == "PGPKEY-80F238C6"
+        assert mock_dh.mock_calls[2][0] == "record_mirror_error"
+        assert mock_dh.mock_calls[3][0] == "record_mirror_error"
+        assert mock_dh.mock_calls[4][0] == "record_serial_seen"
+        assert mock_dh.mock_calls[4][1][0] == "TEST"
         assert mock_dh.mock_calls[4][1][1] == 424242
 
-        assert 'Invalid source BADSOURCE for object' in caplog.text
-        assert 'Invalid address prefix' in caplog.text
-        assert 'File import for TEST: 6 objects read, 2 objects inserted, ignored 2 due to errors' in caplog.text
-        assert 'ignored 1 due to object_class_filter' in caplog.text
-        assert 'Ignored 1 objects found in file import for TEST due to unknown object classes' in caplog.text
+        assert "Invalid source BADSOURCE for object" in caplog.text
+        assert "Invalid address prefix" in caplog.text
+        assert (
+            "File import for TEST: 6 objects read, 2 objects inserted, ignored 2 due to errors" in caplog.text
+        )
+        assert "ignored 1 due to object_class_filter" in caplog.text
+        assert "Ignored 1 objects found in file import for TEST due to unknown object classes" in caplog.text
 
         key_cert_obj = rpsl_object_from_text(SAMPLE_KEY_CERT, strict_validation=False)
         assert key_cert_obj.verify(KEY_CERT_SIGNED_MESSAGE_VALID)
 
     def test_direct_error_return_invalid_source(self, mock_scopefilter, caplog, tmp_gpg_dir, config_override):
-        config_override({
-            'sources': {
-                'TEST': {},
+        config_override(
+            {
+                "sources": {
+                    "TEST": {},
+                }
             }
-        })
+        )
         mock_dh = Mock()
 
         test_data = [
             SAMPLE_UNKNOWN_ATTRIBUTE,  # valid, because mirror imports are non-strict
-            SAMPLE_ROUTE.replace('TEST', 'BADSOURCE'),
+            SAMPLE_ROUTE.replace("TEST", "BADSOURCE"),
         ]
-        test_input = '\n\n'.join(test_data)
+        test_input = "\n\n".join(test_data)
 
         with tempfile.NamedTemporaryFile() as fp:
-            fp.write(test_input.encode('utf-8'))
+            fp.write(test_input.encode("utf-8"))
             fp.seek(0)
             parser = MirrorFileImportParser(
-                source='TEST',
+                source="TEST",
                 filename=fp.name,
                 serial=424242,
                 database_handler=mock_dh,
                 direct_error_return=True,
             )
             error = parser.run_import()
-            assert error == 'Invalid source BADSOURCE for object 192.0.2.0/24AS65537, expected TEST'
+            assert error == "Invalid source BADSOURCE for object 192.0.2.0/24AS65537, expected TEST"
         assert len(mock_dh.mock_calls) == 1
-        assert mock_dh.mock_calls[0][0] == 'upsert_rpsl_object'
-        assert mock_dh.mock_calls[0][1][0].pk() == '192.0.2.0/24AS65537'
+        assert mock_dh.mock_calls[0][0] == "upsert_rpsl_object"
+        assert mock_dh.mock_calls[0][1][0].pk() == "192.0.2.0/24AS65537"
         assert mock_dh.mock_calls[0][1][0].rpki_status == RPKIStatus.not_found
 
-        assert 'Invalid source BADSOURCE for object' not in caplog.text
-        assert 'File import for TEST' not in caplog.text
+        assert "Invalid source BADSOURCE for object" not in caplog.text
+        assert "File import for TEST" not in caplog.text
 
     def test_direct_error_return_malformed_pk(self, mock_scopefilter, caplog, tmp_gpg_dir, config_override):
-        config_override({
-            'sources': {
-                'TEST': {},
+        config_override(
+            {
+                "sources": {
+                    "TEST": {},
+                }
             }
-        })
+        )
         mock_dh = Mock()
 
         with tempfile.NamedTemporaryFile() as fp:
-            fp.write(SAMPLE_MALFORMED_PK.encode('utf-8'))
+            fp.write(SAMPLE_MALFORMED_PK.encode("utf-8"))
             fp.seek(0)
             parser = MirrorFileImportParser(
-                source='TEST',
+                source="TEST",
                 filename=fp.name,
                 serial=424242,
                 database_handler=mock_dh,
                 direct_error_return=True,
             )
             error = parser.run_import()
-            assert 'Invalid address prefix: not-a-prefix' in error
+            assert "Invalid address prefix: not-a-prefix" in error
         assert not len(mock_dh.mock_calls)
 
-        assert 'Invalid address prefix: not-a-prefix' not in caplog.text
-        assert 'File import for TEST' not in caplog.text
+        assert "Invalid address prefix: not-a-prefix" not in caplog.text
+        assert "File import for TEST" not in caplog.text
 
     def test_direct_error_return_unknown_class(self, mock_scopefilter, caplog, tmp_gpg_dir, config_override):
-        config_override({
-            'sources': {
-                'TEST': {},
+        config_override(
+            {
+                "sources": {
+                    "TEST": {},
+                }
             }
-        })
+        )
         mock_dh = Mock()
 
         with tempfile.NamedTemporaryFile() as fp:
-            fp.write(SAMPLE_UNKNOWN_CLASS.encode('utf-8'))
+            fp.write(SAMPLE_UNKNOWN_CLASS.encode("utf-8"))
             fp.seek(0)
             parser = MirrorFileImportParser(
-                source='TEST',
+                source="TEST",
                 filename=fp.name,
                 serial=424242,
                 database_handler=mock_dh,
                 direct_error_return=True,
             )
             error = parser.run_import()
-            assert error == 'Unknown object class: foo-block'
+            assert error == "Unknown object class: foo-block"
         assert not len(mock_dh.mock_calls)
 
-        assert 'Unknown object class: foo-block' not in caplog.text
-        assert 'File import for TEST' not in caplog.text
+        assert "Unknown object class: foo-block" not in caplog.text
+        assert "File import for TEST" not in caplog.text
 
 
 class TestMirrorUpdateFileImportParser:
     def test_parse(self, mock_scopefilter, caplog, config_override):
-        config_override({
-            'sources': {
-                'TEST': {
-                    'object_class_filter': ['route', 'route6', 'key-cert', 'role'],
+        config_override(
+            {
+                "sources": {
+                    "TEST": {
+                        "object_class_filter": ["route", "route6", "key-cert", "role"],
+                    }
                 }
             }
-        })
+        )
         mock_dh = Mock()
 
         test_data = [
             SAMPLE_ROUTE,  # Valid retained
             SAMPLE_ROUTE6,  # Valid modified
             SAMPLE_ROLE,  # Valid new object
-            SAMPLE_ROUTE.replace('TEST', 'BADSOURCE'),
+            SAMPLE_ROUTE.replace("TEST", "BADSOURCE"),
             SAMPLE_UNKNOWN_CLASS,
             SAMPLE_MALFORMED_PK,
         ]
-        test_input = '\n\n'.join(test_data)
+        test_input = "\n\n".join(test_data)
 
-        route_with_last_modified = SAMPLE_ROUTE + 'last-modified:  2020-01-01T00:00:00Z\n'
+        route_with_last_modified = SAMPLE_ROUTE + "last-modified:  2020-01-01T00:00:00Z\n"
         mock_query_result = [
             {
                 # Retained object (with format cleaning)
                 # includes a last-modified which should be ignored in the comparison
-                'rpsl_pk': '192.0.2.0/24AS65537',
-                'object_class': 'route',
-                'object_text': rpsl_object_from_text(route_with_last_modified).render_rpsl_text(),
+                "rpsl_pk": "192.0.2.0/24AS65537",
+                "object_class": "route",
+                "object_text": rpsl_object_from_text(route_with_last_modified).render_rpsl_text(),
             },
             {
                 # Modified object
-                'rpsl_pk': '2001:DB8::/48AS65537',
-                'object_class': 'route6',
-                'object_text': SAMPLE_ROUTE6.replace('test-MNT', 'existing-mnt'),
+                "rpsl_pk": "2001:DB8::/48AS65537",
+                "object_class": "route6",
+                "object_text": SAMPLE_ROUTE6.replace("test-MNT", "existing-mnt"),
             },
             {
                 # Deleted object
-                'rpsl_pk': 'rtrs-settest',
-                'object_class': 'route-set',
-                'object_text': SAMPLE_RTR_SET,
+                "rpsl_pk": "rtrs-settest",
+                "object_class": "route-set",
+                "object_text": SAMPLE_RTR_SET,
             },
         ]
         mock_dh.execute_query = lambda query: mock_query_result
 
         with tempfile.NamedTemporaryFile() as fp:
-            fp.write(test_input.encode('utf-8'))
+            fp.write(test_input.encode("utf-8"))
             fp.seek(0)
             parser = MirrorUpdateFileImportParser(
-                source='TEST',
+                source="TEST",
                 filename=fp.name,
                 database_handler=mock_dh,
             )
             parser.run_import()
 
         assert len(mock_dh.mock_calls) == 5
-        assert mock_dh.mock_calls[0][0] == 'record_mirror_error'
-        assert mock_dh.mock_calls[1][0] == 'record_mirror_error'
-        assert mock_dh.mock_calls[2][0] == 'upsert_rpsl_object'
-        assert mock_dh.mock_calls[2][1][0].pk() == 'ROLE-TEST'
-        assert mock_dh.mock_calls[3][0] == 'delete_rpsl_object'
-        assert mock_dh.mock_calls[3][2]['source'] == 'TEST'
-        assert mock_dh.mock_calls[3][2]['rpsl_pk'] == 'rtrs-settest'
-        assert mock_dh.mock_calls[3][2]['object_class'] == 'route-set'
-        assert mock_dh.mock_calls[3][2]['origin'] == JournalEntryOrigin.synthetic_nrtm
-        assert mock_dh.mock_calls[4][0] == 'upsert_rpsl_object'
-        assert mock_dh.mock_calls[4][1][0].pk() == '2001:DB8::/48AS65537'
-
-        assert 'Invalid source BADSOURCE for object' in caplog.text
-        assert 'Invalid address prefix' in caplog.text
-        assert 'File update for TEST: 6 objects read, 3 objects processed, 1 objects newly inserted, 1 objects newly deleted, 2 objects retained, of which 1 modified' in caplog.text
-        assert 'ignored 0 due to object_class_filter' in caplog.text
-        assert 'Ignored 1 objects found in file import for TEST due to unknown object classes' in caplog.text
+        assert mock_dh.mock_calls[0][0] == "record_mirror_error"
+        assert mock_dh.mock_calls[1][0] == "record_mirror_error"
+        assert mock_dh.mock_calls[2][0] == "upsert_rpsl_object"
+        assert mock_dh.mock_calls[2][1][0].pk() == "ROLE-TEST"
+        assert mock_dh.mock_calls[3][0] == "delete_rpsl_object"
+        assert mock_dh.mock_calls[3][2]["source"] == "TEST"
+        assert mock_dh.mock_calls[3][2]["rpsl_pk"] == "rtrs-settest"
+        assert mock_dh.mock_calls[3][2]["object_class"] == "route-set"
+        assert mock_dh.mock_calls[3][2]["origin"] == JournalEntryOrigin.synthetic_nrtm
+        assert mock_dh.mock_calls[4][0] == "upsert_rpsl_object"
+        assert mock_dh.mock_calls[4][1][0].pk() == "2001:DB8::/48AS65537"
+
+        assert "Invalid source BADSOURCE for object" in caplog.text
+        assert "Invalid address prefix" in caplog.text
+        assert (
+            "File update for TEST: 6 objects read, 3 objects processed, 1 objects newly inserted, 1 objects"
+            " newly deleted, 2 objects retained, of which 1 modified"
+            in caplog.text
+        )
+        assert "ignored 0 due to object_class_filter" in caplog.text
+        assert "Ignored 1 objects found in file import for TEST due to unknown object classes" in caplog.text
 
     def test_direct_error_return(self, mock_scopefilter, config_override):
-        config_override({
-            'sources': {
-                'TEST': {}
-            }
-        })
+        config_override({"sources": {"TEST": {}}})
         mock_dh = Mock()
 
         test_data = [
             SAMPLE_UNKNOWN_CLASS,
             SAMPLE_MALFORMED_PK,
         ]
-        test_input = '\n\n'.join(test_data)
+        test_input = "\n\n".join(test_data)
 
         with tempfile.NamedTemporaryFile() as fp:
-            fp.write(test_input.encode('utf-8'))
+            fp.write(test_input.encode("utf-8"))
             fp.seek(0)
             parser = MirrorUpdateFileImportParser(
-                source='TEST',
+                source="TEST",
                 filename=fp.name,
                 database_handler=mock_dh,
                 direct_error_return=True,
             )
-            assert parser.run_import() == 'Unknown object class: foo-block'
+            assert parser.run_import() == "Unknown object class: foo-block"
 
         assert len(mock_dh.mock_calls) == 0
 
 
 class TestNRTMStreamParser:
     def test_test_parse_nrtm_v3_valid(self):
         mock_dh = Mock()
-        parser = NRTMStreamParser('TEST', SAMPLE_NRTM_V3, mock_dh)
+        parser = NRTMStreamParser("TEST", SAMPLE_NRTM_V3, mock_dh)
         self._assert_valid(parser)
-        assert flatten_mock_calls(mock_dh) == [['record_serial_newest_mirror', ('TEST', 11012701), {}]]
+        assert flatten_mock_calls(mock_dh) == [["record_serial_newest_mirror", ("TEST", 11012701), {}]]
 
     def test_test_parse_nrtm_v1_valid(self, config_override):
-        config_override({
-            'sources': {
-                'TEST': {
-                    'object_class_filter': 'person',
-                    'strict_import_keycert_objects': True,
+        config_override(
+            {
+                "sources": {
+                    "TEST": {
+                        "object_class_filter": "person",
+                        "strict_import_keycert_objects": True,
+                    }
                 }
             }
-        })
+        )
         mock_dh = Mock()
-        parser = NRTMStreamParser('TEST', SAMPLE_NRTM_V1, mock_dh)
+        parser = NRTMStreamParser("TEST", SAMPLE_NRTM_V1, mock_dh)
         self._assert_valid(parser)
-        assert flatten_mock_calls(mock_dh) == [['record_serial_newest_mirror', ('TEST', 11012701), {}]]
+        assert flatten_mock_calls(mock_dh) == [["record_serial_newest_mirror", ("TEST", 11012701), {}]]
 
     def test_test_parse_nrtm_v3_valid_serial_gap(self):
         mock_dh = Mock()
-        parser = NRTMStreamParser('TEST', SAMPLE_NRTM_V3_SERIAL_GAP, mock_dh)
+        parser = NRTMStreamParser("TEST", SAMPLE_NRTM_V3_SERIAL_GAP, mock_dh)
         self._assert_valid(parser)
-        assert flatten_mock_calls(mock_dh) == [['record_serial_newest_mirror', ('TEST', 11012703), {}]]
+        assert flatten_mock_calls(mock_dh) == [["record_serial_newest_mirror", ("TEST", 11012703), {}]]
 
     def test_test_parse_nrtm_v3_invalid_serial_out_of_order(self):
         mock_dh = Mock()
         with pytest.raises(ValueError) as ve:
-            NRTMStreamParser('TEST', SAMPLE_NRTM_V3_SERIAL_OUT_OF_ORDER, mock_dh)
+            NRTMStreamParser("TEST", SAMPLE_NRTM_V3_SERIAL_OUT_OF_ORDER, mock_dh)
 
-        error_msg = 'expected at least'
+        error_msg = "expected at least"
         assert error_msg in str(ve.value)
         assert len(mock_dh.mock_calls) == 1
-        assert mock_dh.mock_calls[0][0] == 'record_mirror_error'
-        assert mock_dh.mock_calls[0][1][0] == 'TEST'
+        assert mock_dh.mock_calls[0][0] == "record_mirror_error"
+        assert mock_dh.mock_calls[0][1][0] == "TEST"
         assert error_msg in mock_dh.mock_calls[0][1][1]
 
     def test_test_parse_nrtm_v3_invalid_unexpected_source(self):
         mock_dh = Mock()
         with pytest.raises(ValueError) as ve:
-            NRTMStreamParser('BADSOURCE', SAMPLE_NRTM_V3, mock_dh)
+            NRTMStreamParser("BADSOURCE", SAMPLE_NRTM_V3, mock_dh)
 
-        error_msg = 'Invalid NRTM source in START line: expected BADSOURCE but found TEST '
+        error_msg = "Invalid NRTM source in START line: expected BADSOURCE but found TEST "
         assert error_msg in str(ve.value)
         assert len(mock_dh.mock_calls) == 1
-        assert mock_dh.mock_calls[0][0] == 'record_mirror_error'
-        assert mock_dh.mock_calls[0][1][0] == 'BADSOURCE'
+        assert mock_dh.mock_calls[0][0] == "record_mirror_error"
+        assert mock_dh.mock_calls[0][1][0] == "BADSOURCE"
         assert error_msg in mock_dh.mock_calls[0][1][1]
 
     def test_test_parse_nrtm_v1_invalid_too_many_items(self):
         mock_dh = Mock()
         with pytest.raises(ValueError) as ve:
-            NRTMStreamParser('TEST', SAMPLE_NRTM_V1_TOO_MANY_ITEMS, mock_dh)
-        error_msg = 'expected operations up to and including'
+            NRTMStreamParser("TEST", SAMPLE_NRTM_V1_TOO_MANY_ITEMS, mock_dh)
+        error_msg = "expected operations up to and including"
         assert error_msg in str(ve.value)
 
         assert len(mock_dh.mock_calls) == 1
-        assert mock_dh.mock_calls[0][0] == 'record_mirror_error'
-        assert mock_dh.mock_calls[0][1][0] == 'TEST'
+        assert mock_dh.mock_calls[0][0] == "record_mirror_error"
+        assert mock_dh.mock_calls[0][1][0] == "TEST"
         assert error_msg in mock_dh.mock_calls[0][1][1]
 
     def test_test_parse_nrtm_invalid_invalid_version(self):
         mock_dh = Mock()
         with pytest.raises(ValueError) as ve:
-            NRTMStreamParser('TEST', SAMPLE_NRTM_INVALID_VERSION, mock_dh)
+            NRTMStreamParser("TEST", SAMPLE_NRTM_INVALID_VERSION, mock_dh)
 
-        error_msg = 'Invalid NRTM version 99 in START line'
+        error_msg = "Invalid NRTM version 99 in START line"
         assert error_msg in str(ve.value)
         assert len(mock_dh.mock_calls) == 1
-        assert mock_dh.mock_calls[0][0] == 'record_mirror_error'
-        assert mock_dh.mock_calls[0][1][0] == 'TEST'
+        assert mock_dh.mock_calls[0][0] == "record_mirror_error"
+        assert mock_dh.mock_calls[0][1][0] == "TEST"
         assert error_msg in mock_dh.mock_calls[0][1][1]
 
     def test_test_parse_nrtm_invalid_multiple_start_lines(self):
         mock_dh = Mock()
         with pytest.raises(ValueError) as ve:
-            NRTMStreamParser('TEST', SAMPLE_NRTM_V3_INVALID_MULTIPLE_START_LINES, mock_dh)
+            NRTMStreamParser("TEST", SAMPLE_NRTM_V3_INVALID_MULTIPLE_START_LINES, mock_dh)
 
-        error_msg = 'Encountered second START line'
+        error_msg = "Encountered second START line"
         assert error_msg in str(ve.value)
 
         assert len(mock_dh.mock_calls) == 1
-        assert mock_dh.mock_calls[0][0] == 'record_mirror_error'
-        assert mock_dh.mock_calls[0][1][0] == 'TEST'
+        assert mock_dh.mock_calls[0][0] == "record_mirror_error"
+        assert mock_dh.mock_calls[0][1][0] == "TEST"
         assert error_msg in mock_dh.mock_calls[0][1][1]
 
     def test_test_parse_nrtm_invalid_no_start_line(self):
         mock_dh = Mock()
         with pytest.raises(ValueError) as ve:
-            NRTMStreamParser('TEST', SAMPLE_NRTM_INVALID_NO_START_LINE, mock_dh)
+            NRTMStreamParser("TEST", SAMPLE_NRTM_INVALID_NO_START_LINE, mock_dh)
 
-        error_msg = 'Encountered operation before valid NRTM START line'
+        error_msg = "Encountered operation before valid NRTM START line"
         assert error_msg in str(ve.value)
         assert len(mock_dh.mock_calls) == 1
-        assert mock_dh.mock_calls[0][0] == 'record_mirror_error'
-        assert mock_dh.mock_calls[0][1][0] == 'TEST'
+        assert mock_dh.mock_calls[0][0] == "record_mirror_error"
+        assert mock_dh.mock_calls[0][1][0] == "TEST"
         assert error_msg in mock_dh.mock_calls[0][1][1]
 
     def test_test_parse_nrtm_no_end(self):
         mock_dh = Mock()
         with pytest.raises(ValueError) as ve:
-            NRTMStreamParser('TEST', SAMPLE_NRTM_V3_NO_END, mock_dh)
+            NRTMStreamParser("TEST", SAMPLE_NRTM_V3_NO_END, mock_dh)
 
-        error_msg = 'last comment paragraph expected to be'
+        error_msg = "last comment paragraph expected to be"
         assert error_msg in str(ve.value)
         assert len(mock_dh.mock_calls) == 1
-        assert mock_dh.mock_calls[0][0] == 'record_mirror_error'
-        assert mock_dh.mock_calls[0][1][0] == 'TEST'
+        assert mock_dh.mock_calls[0][0] == "record_mirror_error"
+        assert mock_dh.mock_calls[0][1][0] == "TEST"
         assert error_msg in mock_dh.mock_calls[0][1][1]
 
     def _assert_valid(self, parser: NRTMStreamParser):
         assert parser.operations[0].operation == DatabaseOperation.add_or_update
         assert parser.operations[0].serial == 11012700
-        assert parser.operations[0].object_text == 'person: NRTM test\naddress: NowhereLand\nsource: TEST\n'
+        assert parser.operations[0].object_text == "person: NRTM test\naddress: NowhereLand\nsource: TEST\n"
         assert parser.operations[1].operation == DatabaseOperation.delete
         assert parser.operations[1].serial == 11012701
-        assert parser.operations[1].object_text == 'inetnum: 192.0.2.0 - 192.0.2.255\nsource: TEST\n'
+        assert parser.operations[1].object_text == "inetnum: 192.0.2.0 - 192.0.2.255\nsource: TEST\n"
```

### Comparing `irrd-4.2.8/irrd/mirroring/tests/test_scheduler.py` & `irrd-4.3.0/irrd/mirroring/tests/test_scheduler.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,52 +1,55 @@
-import time
-
 import threading
+import time
 
-from ..scheduler import MirrorScheduler, ScheduledTaskProcess, MAX_SIMULTANEOUS_RUNS
+from ..scheduler import MAX_SIMULTANEOUS_RUNS, MirrorScheduler, ScheduledTaskProcess
 
 thread_run_count = 0
 
 
 class TestMirrorScheduler:
     def test_scheduler_database_readonly(self, monkeypatch, config_override):
-        monkeypatch.setattr('irrd.mirroring.scheduler.ScheduledTaskProcess', MockScheduledTaskProcess)
+        monkeypatch.setattr("irrd.mirroring.scheduler.ScheduledTaskProcess", MockScheduledTaskProcess)
         global thread_run_count
         thread_run_count = 0
 
-        config_override({
-            'database_readonly': True,
-            'sources': {
-                'TEST': {
-                    'import_source': 'url',
-                    'import_timer': 0,
-                }
+        config_override(
+            {
+                "database_readonly": True,
+                "sources": {
+                    "TEST": {
+                        "import_source": "url",
+                        "import_timer": 0,
+                    }
+                },
             }
-        })
+        )
 
-        monkeypatch.setattr('irrd.mirroring.scheduler.RPSLMirrorImportUpdateRunner', MockRunner)
+        monkeypatch.setattr("irrd.mirroring.scheduler.RPSLMirrorImportUpdateRunner", MockRunner)
         scheduler = MirrorScheduler()
         scheduler.run()
         assert thread_run_count == 0
 
     def test_scheduler_runs_rpsl_import(self, monkeypatch, config_override):
-        monkeypatch.setattr('irrd.mirroring.scheduler.ScheduledTaskProcess', MockScheduledTaskProcess)
+        monkeypatch.setattr("irrd.mirroring.scheduler.ScheduledTaskProcess", MockScheduledTaskProcess)
         global thread_run_count
         thread_run_count = 0
 
-        config_override({
-            'sources': {
-                'TEST': {
-                    'import_source': 'url',
-                    'import_timer': 0,
+        config_override(
+            {
+                "sources": {
+                    "TEST": {
+                        "import_source": "url",
+                        "import_timer": 0,
+                    }
                 }
             }
-        })
+        )
 
-        monkeypatch.setattr('irrd.mirroring.scheduler.RPSLMirrorImportUpdateRunner', MockRunner)
+        monkeypatch.setattr("irrd.mirroring.scheduler.RPSLMirrorImportUpdateRunner", MockRunner)
         MockRunner.run_sleep = True
 
         scheduler = MirrorScheduler()
         scheduler.run()
         # Second run will not start the thread, as the current one is still running
         time.sleep(0.5)
         scheduler.run()
@@ -59,195 +62,230 @@
         scheduler.update_process_state()
         time.sleep(0.5)
         assert len(scheduler.processes.items()) == 1
         scheduler.update_process_state()
         assert len(scheduler.processes.items()) == 0
 
     def test_scheduler_limits_simultaneous_runs(self, monkeypatch, config_override):
-        monkeypatch.setattr('irrd.mirroring.scheduler.ScheduledTaskProcess', MockScheduledTaskProcess)
+        monkeypatch.setattr("irrd.mirroring.scheduler.ScheduledTaskProcess", MockScheduledTaskProcess)
         global thread_run_count
         thread_run_count = 0
 
-        config_override({
-            'sources': {
-                'TEST': {
-                    'import_source': 'url',
-                    'import_timer': 0,
-                },
-                'TEST2': {
-                    'import_source': 'url',
-                    'import_timer': 0,
-                },
-                'TEST3': {
-                    'import_source': 'url',
-                    'import_timer': 0,
-                },
-                'TEST4': {
-                    'import_source': 'url',
-                    'import_timer': 0,
-                },
+        config_override(
+            {
+                "sources": {
+                    "TEST": {
+                        "import_source": "url",
+                        "import_timer": 0,
+                    },
+                    "TEST2": {
+                        "import_source": "url",
+                        "import_timer": 0,
+                    },
+                    "TEST3": {
+                        "import_source": "url",
+                        "import_timer": 0,
+                    },
+                    "TEST4": {
+                        "import_source": "url",
+                        "import_timer": 0,
+                    },
+                }
             }
-        })
+        )
 
-        monkeypatch.setattr('irrd.mirroring.scheduler.RPSLMirrorImportUpdateRunner', MockRunner)
+        monkeypatch.setattr("irrd.mirroring.scheduler.RPSLMirrorImportUpdateRunner", MockRunner)
         MockRunner.run_sleep = False
 
         scheduler = MirrorScheduler()
         scheduler.run()
 
         time.sleep(0.5)
         assert thread_run_count == MAX_SIMULTANEOUS_RUNS
 
     def test_scheduler_runs_roa_import(self, monkeypatch, config_override):
-        monkeypatch.setattr('irrd.mirroring.scheduler.ScheduledTaskProcess', MockScheduledTaskProcess)
+        monkeypatch.setattr("irrd.mirroring.scheduler.ScheduledTaskProcess", MockScheduledTaskProcess)
         global thread_run_count
         thread_run_count = 0
 
-        config_override({
-            'rpki': {
-                'roa_source': 'https://example.com/roa.json'
-            }
-        })
+        config_override({"rpki": {"roa_source": "https://example.com/roa.json"}})
 
-        monkeypatch.setattr('irrd.mirroring.scheduler.ROAImportRunner', MockRunner)
+        monkeypatch.setattr("irrd.mirroring.scheduler.ROAImportRunner", MockRunner)
         MockRunner.run_sleep = True
 
         scheduler = MirrorScheduler()
         scheduler.run()
         # Second run will not start the thread, as the current one is still running
         time.sleep(0.5)
         scheduler.run()
 
         assert thread_run_count == 1
 
     def test_scheduler_runs_scopefilter(self, monkeypatch, config_override):
-        monkeypatch.setattr('irrd.mirroring.scheduler.ScheduledTaskProcess', MockScheduledTaskProcess)
+        monkeypatch.setattr("irrd.mirroring.scheduler.ScheduledTaskProcess", MockScheduledTaskProcess)
         global thread_run_count
         thread_run_count = 0
 
-        config_override({
-            'rpki': {'roa_source': None},
-            'scopefilter': {
-                'prefixes': ['192.0.2.0/24'],
+        config_override(
+            {
+                "rpki": {"roa_source": None},
+                "scopefilter": {
+                    "prefixes": ["192.0.2.0/24"],
+                },
             }
-        })
+        )
 
-        monkeypatch.setattr('irrd.mirroring.scheduler.ScopeFilterUpdateRunner', MockRunner)
+        monkeypatch.setattr("irrd.mirroring.scheduler.ScopeFilterUpdateRunner", MockRunner)
         MockRunner.run_sleep = False
 
         scheduler = MirrorScheduler()
         scheduler.run()
 
         # Second run will not start the thread, as the config hasn't changed
-        config_override({
-            'rpki': {'roa_source': None},
-            'scopefilter': {
-                'prefixes': ['192.0.2.0/24'],
+        config_override(
+            {
+                "rpki": {"roa_source": None},
+                "scopefilter": {
+                    "prefixes": ["192.0.2.0/24"],
+                },
             }
-        })
+        )
         scheduler.run()
         time.sleep(0.2)
         assert thread_run_count == 1
 
-        config_override({
-            'rpki': {'roa_source': None},
-            'scopefilter': {
-                'asns': [23456],
+        config_override(
+            {
+                "rpki": {"roa_source": None},
+                "scopefilter": {
+                    "asns": [23456],
+                },
             }
-        })
+        )
 
         # Should run now, because config has changed
         scheduler.update_process_state()
         scheduler.run()
         time.sleep(0.2)
         assert thread_run_count == 2
 
-        config_override({
-            'rpki': {'roa_source': None},
-            'scopefilter': {
-                'asns': [23456],
-            },
-            'sources': {
-                'TEST': {'scopefilter_excluded': True}
-            },
-        })
+        config_override(
+            {
+                "rpki": {"roa_source": None},
+                "scopefilter": {
+                    "asns": [23456],
+                },
+                "sources": {"TEST": {"scopefilter_excluded": True}},
+            }
+        )
 
         # Should run again, because exclusions have changed
         scheduler.update_process_state()
         scheduler.run()
         time.sleep(0.2)
         assert thread_run_count == 3
 
+    def test_scheduler_runs_route_preference(self, monkeypatch, config_override):
+        monkeypatch.setattr("irrd.mirroring.scheduler.ScheduledTaskProcess", MockScheduledTaskProcess)
+        global thread_run_count
+        thread_run_count = 0
+
+        config_override(
+            {
+                "rpki": {"roa_source": None},
+                "sources": {
+                    "TEST": {"route_object_preference": 200},
+                },
+            }
+        )
+
+        monkeypatch.setattr("irrd.mirroring.scheduler.RoutePreferenceUpdateRunner", MockRunner)
+        MockRunner.run_sleep = True
+
+        scheduler = MirrorScheduler()
+        scheduler.run()
+        # Second run will not start the thread, as the current one is still running
+        time.sleep(0.5)
+        scheduler.run()
+
+        assert thread_run_count == 1
+
     def test_scheduler_import_ignores_timer_not_expired(self, monkeypatch, config_override):
-        monkeypatch.setattr('irrd.mirroring.scheduler.ScheduledTaskProcess', MockScheduledTaskProcess)
+        monkeypatch.setattr("irrd.mirroring.scheduler.ScheduledTaskProcess", MockScheduledTaskProcess)
         global thread_run_count
         thread_run_count = 0
 
-        config_override({
-            'sources': {
-                'TEST': {
-                    'import_source': 'url',
-                    'import_timer': 100,
+        config_override(
+            {
+                "sources": {
+                    "TEST": {
+                        "import_source": "url",
+                        "import_timer": 100,
+                    }
                 }
             }
-        })
+        )
 
-        monkeypatch.setattr('irrd.mirroring.scheduler.RPSLMirrorImportUpdateRunner', MockRunner)
+        monkeypatch.setattr("irrd.mirroring.scheduler.RPSLMirrorImportUpdateRunner", MockRunner)
         MockRunner.run_sleep = False
 
         scheduler = MirrorScheduler()
         scheduler.run()
         time.sleep(0.5)
         assert thread_run_count == 1
 
         # Second run will not start due to timer not expired yet
         time.sleep(0.5)
         scheduler.run()
         assert thread_run_count == 1
 
     def test_scheduler_runs_export(self, monkeypatch, config_override):
-        monkeypatch.setattr('irrd.mirroring.scheduler.ScheduledTaskProcess', MockScheduledTaskProcess)
+        monkeypatch.setattr("irrd.mirroring.scheduler.ScheduledTaskProcess", MockScheduledTaskProcess)
         global thread_run_count
         thread_run_count = 0
 
-        config_override({
-            'sources': {
-                'TEST': {
-                    'export_destination': 'url',
-                    'export_timer': 0,
+        config_override(
+            {
+                "sources": {
+                    "TEST": {
+                        "export_destination": "url",
+                        "export_timer": 0,
+                    }
                 }
             }
-        })
+        )
 
-        monkeypatch.setattr('irrd.mirroring.scheduler.SourceExportRunner', MockRunner)
+        monkeypatch.setattr("irrd.mirroring.scheduler.SourceExportRunner", MockRunner)
         MockRunner.run_sleep = True
 
         scheduler = MirrorScheduler()
         scheduler.run()
         time.sleep(0.5)
         # Second run will not start the thread, as the current one is still running
         scheduler.run()
 
         assert thread_run_count == 1
 
     def test_scheduler_export_ignores_timer_not_expired(self, monkeypatch, config_override):
-        monkeypatch.setattr('irrd.mirroring.scheduler.ScheduledTaskProcess', MockScheduledTaskProcess)
+        monkeypatch.setattr("irrd.mirroring.scheduler.ScheduledTaskProcess", MockScheduledTaskProcess)
         global thread_run_count
         thread_run_count = 0
 
-        config_override({
-            'sources': {
-                'TEST': {
-                    'export_destination': 'url',
-                    'export_timer': 100,
+        config_override(
+            {
+                "sources": {
+                    "TEST": {
+                        "export_destination": "url",
+                        "export_timer": 100,
+                    }
                 }
             }
-        })
+        )
 
-        monkeypatch.setattr('irrd.mirroring.scheduler.SourceExportRunner', MockRunner)
+        monkeypatch.setattr("irrd.mirroring.scheduler.SourceExportRunner", MockRunner)
         MockRunner.run_sleep = False
 
         scheduler = MirrorScheduler()
         scheduler.run()
         time.sleep(0.5)
         assert thread_run_count == 1
 
@@ -258,23 +296,23 @@
 
 
 class TestScheduledTaskProcess:
     def test_task(self):
         global thread_run_count
         thread_run_count = 0
         MockRunner.run_sleep = True
-        ScheduledTaskProcess(runner=MockRunner('TEST'), name='test').run()
+        ScheduledTaskProcess(runner=MockRunner("TEST"), name="test").run()
         assert thread_run_count == 1
 
 
 class MockRunner:
     run_sleep = True
 
     def __init__(self, source):
-        assert source in ['TEST', 'TEST2', 'TEST3', 'TEST4', 'RPKI', 'scopefilter']
+        assert source in ["TEST", "TEST2", "TEST3", "TEST4", "RPKI", "scopefilter", "routepref"]
 
     def run(self):
         global thread_run_count
         thread_run_count += 1
         if self.run_sleep:
             time.sleep(1.5)
```

### Comparing `irrd-4.2.8/irrd/rpki/importer.py` & `irrd-4.3.0/irrd/rpki/importer.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,25 +1,24 @@
+import logging
 from collections import defaultdict
+from typing import Dict, List, Optional, Set
 
 import ujson
-
-import logging
 from IPy import IP, IPSet
-from typing import List, Optional, Dict, Set
 
 from irrd.conf import RPKI_IRR_PSEUDO_SOURCE, get_setting
 from irrd.rpki.status import RPKIStatus
-from irrd.rpsl.parser import RPSLObject, RPSL_ATTRIBUTE_TEXT_WIDTH
+from irrd.rpsl.parser import RPSL_ATTRIBUTE_TEXT_WIDTH, RPSLObject
 from irrd.rpsl.rpsl_objects import RPSL_ROUTE_OBJECT_CLASS_FOR_IP_VERSION
 from irrd.scopefilter.validators import ScopeFilterValidator
 from irrd.storage.database_handler import DatabaseHandler
 from irrd.storage.models import JournalEntryOrigin
 from irrd.utils.validators import parse_as_number
 
-SLURM_TRUST_ANCHOR = 'SLURM file'
+SLURM_TRUST_ANCHOR = "SLURM file"
 
 logger = logging.getLogger(__name__)
 
 
 class ROAParserException(Exception):  # noqa: N818
     pass
 
@@ -32,59 +31,59 @@
     If a slurm_json_str is provided, it is used to filter/amend the ROAs.
 
     Expects all existing ROA and pseudo-IRR objects to be deleted
     already, e.g. with:
         database_handler.delete_all_roa_objects()
         database_handler.delete_all_rpsl_objects_with_journal(RPKI_IRR_PSEUDO_SOURCE)
     """
-    def __init__(self, rpki_json_str: str, slurm_json_str: Optional[str],
-                 database_handler: DatabaseHandler):
+
+    def __init__(self, rpki_json_str: str, slurm_json_str: Optional[str], database_handler: DatabaseHandler):
         self.roa_objs: List[ROA] = []
         self._filtered_asns: Set[int] = set()
         self._filtered_prefixes: IPSet = IPSet()
         self._filtered_combined: Dict[int, IPSet] = defaultdict(IPSet)
 
         self._load_roa_dicts(rpki_json_str)
         if slurm_json_str:
             self._load_slurm(slurm_json_str)
 
         scopefilter_validator = ScopeFilterValidator()
 
         for roa_dict in self._roa_dicts:
             try:
-                _, asn = parse_as_number(roa_dict['asn'], permit_plain=True)
-                prefix = IP(roa_dict['prefix'])
-                ta = roa_dict['ta']
+                _, asn = parse_as_number(roa_dict["asn"], permit_plain=True)
+                prefix = IP(roa_dict["prefix"])
+                ta = roa_dict["ta"]
                 if ta != SLURM_TRUST_ANCHOR:
                     if asn in self._filtered_asns:
                         continue
                     if any([prefix in self._filtered_prefixes]):
                         continue
                     if any([prefix in self._filtered_combined.get(asn, [])]):
                         continue
 
-                roa_obj = ROA(prefix, asn, roa_dict['maxLength'], ta)
+                roa_obj = ROA(prefix, asn, roa_dict["maxLength"], ta)
             except KeyError as ke:
-                msg = f'Unable to parse ROA record: missing key {ke} -- full record: {roa_dict}'
+                msg = f"Unable to parse ROA record: missing key {ke} -- full record: {roa_dict}"
                 logger.error(msg)
                 raise ROAParserException(msg)
             except ValueError as ve:
-                msg = f'Invalid value in ROA or SLURM: {ve}'
+                msg = f"Invalid value in ROA or SLURM: {ve}"
                 logger.error(msg)
                 raise ROAParserException(msg)
 
             roa_obj.save(database_handler, scopefilter_validator)
             self.roa_objs.append(roa_obj)
 
     def _load_roa_dicts(self, rpki_json_str: str) -> None:
         """Load the ROAs from the JSON string into self._roa_dicts"""
         try:
-            self._roa_dicts = ujson.loads(rpki_json_str)['roas']
+            self._roa_dicts = ujson.loads(rpki_json_str)["roas"]
         except ValueError as error:
-            msg = f'Unable to parse ROA input: invalid JSON: {error}'
+            msg = f"Unable to parse ROA input: invalid JSON: {error}"
             logger.error(msg)
             raise ROAParserException(msg)
         except KeyError:
             msg = 'Unable to parse ROA input: root key "roas" not found'
             logger.error(msg)
             raise ROAParserException(msg)
 
@@ -103,64 +102,69 @@
           that ASN and having a prefix that matches or is more specific,
           should be discarded
 
         Prefix assertions are directly loaded into self._roa_dicts.
         This must be called after _load_roa_dicts()
         """
         slurm = ujson.loads(slurm_json_str)
-        version = slurm.get('slurmVersion')
+        version = slurm.get("slurmVersion")
         if version != 1:
-            msg = f'SLURM data has invalid version: {version}'
+            msg = f"SLURM data has invalid version: {version}"
             logger.error(msg)
             raise ROAParserException(msg)
 
-        filters = slurm.get('validationOutputFilters', {}).get('prefixFilters', [])
+        filters = slurm.get("validationOutputFilters", {}).get("prefixFilters", [])
         for item in filters:
-            if 'asn' in item and 'prefix' not in item:
-                self._filtered_asns.add(int(item['asn']))
-            if 'asn' not in item and 'prefix' in item:
-                self._filtered_prefixes.add(IP(item['prefix']))
-            if 'asn' in item and 'prefix' in item:
-                self._filtered_combined[int(item['asn'])].add(IP(item['prefix']))
+            if "asn" in item and "prefix" not in item:
+                self._filtered_asns.add(int(item["asn"]))
+            if "asn" not in item and "prefix" in item:
+                self._filtered_prefixes.add(IP(item["prefix"]))
+            if "asn" in item and "prefix" in item:
+                self._filtered_combined[int(item["asn"])].add(IP(item["prefix"]))
 
-        assertions = slurm.get('locallyAddedAssertions', {}).get('prefixAssertions', [])
+        assertions = slurm.get("locallyAddedAssertions", {}).get("prefixAssertions", [])
         for assertion in assertions:
-            max_length = assertion.get('maxPrefixLength')
+            max_length = assertion.get("maxPrefixLength")
             if max_length is None:
-                max_length = IP(assertion['prefix']).prefixlen()
-            self._roa_dicts.append({
-                'asn': 'AS' + str(assertion['asn']),
-                'prefix': assertion['prefix'],
-                'maxLength': max_length,
-                'ta': SLURM_TRUST_ANCHOR,
-            })
+                max_length = IP(assertion["prefix"]).prefixlen()
+            self._roa_dicts.append(
+                {
+                    "asn": "AS" + str(assertion["asn"]),
+                    "prefix": assertion["prefix"],
+                    "maxLength": max_length,
+                    "ta": SLURM_TRUST_ANCHOR,
+                }
+            )
 
 
 class ROA:
     """
     Representation of a ROA.
 
     This is used when (re-)importing all ROAs, to save the data to the DB,
     and by the BulkRouteROAValidator when validating all existing routes.
     """
+
     def __init__(self, prefix: IP, asn: int, max_length: str, trust_anchor: str):
         try:
             self.prefix = prefix
             self.prefix_str = str(prefix)
             self.asn = asn
             self.max_length = int(max_length)
             self.trust_anchor = trust_anchor
         except ValueError as ve:
-            msg = f'Invalid value in ROA: {ve}'
+            msg = f"Invalid value in ROA: {ve}"
             logger.error(msg)
             raise ROAParserException(msg)
 
         if self.max_length < self.prefix.prefixlen():
-            msg = f'Invalid ROA: prefix size {self.prefix.prefixlen()} is smaller than max length {max_length} in ' \
-                  f'ROA for {self.prefix} / AS{self.asn}'
+            msg = (
+                f"Invalid ROA: prefix size {self.prefix.prefixlen()} is smaller than max length"
+                f" {max_length} in ROA for {self.prefix} / AS{self.asn}"
+            )
             logger.error(msg)
             raise ROAParserException(msg)
 
     def save(self, database_handler: DatabaseHandler, scopefilter_validator: ScopeFilterValidator):
         """
         Save the ROA object to the DB, create a pseudo-IRR object, and save that too.
         """
@@ -175,27 +179,36 @@
             prefix=self.prefix,
             prefix_str=self.prefix_str,
             asn=self.asn,
             max_length=self.max_length,
             trust_anchor=self.trust_anchor,
             scopefilter_validator=scopefilter_validator,
         )
-        database_handler.upsert_rpsl_object(self._rpsl_object, JournalEntryOrigin.pseudo_irr,
-                                            rpsl_guaranteed_no_existing=True)
+        database_handler.upsert_rpsl_object(
+            self._rpsl_object, JournalEntryOrigin.pseudo_irr, rpsl_guaranteed_no_existing=True
+        )
 
 
 class RPSLObjectFromROA(RPSLObject):
     """
     This is an RPSLObject compatible class that represents
     an RPKI pseudo-IRR object. It overrides the API in
     relevant parts.
     """
+
     # noinspection PyMissingConstructor
-    def __init__(self, prefix: IP, prefix_str: str, asn: int, max_length: int, trust_anchor: str,
-                 scopefilter_validator: ScopeFilterValidator):
+    def __init__(
+        self,
+        prefix: IP,
+        prefix_str: str,
+        asn: int,
+        max_length: int,
+        trust_anchor: str,
+        scopefilter_validator: ScopeFilterValidator,
+    ):
         self.prefix = prefix
         self.prefix_str = prefix_str
         self.asn = asn
         self.max_length = max_length
         self.trust_anchor = trust_anchor
 
         self.rpsl_object_class = RPSL_ROUTE_OBJECT_CLASS_FOR_IP_VERSION[self.prefix.version()]
@@ -203,33 +216,36 @@
         self.ip_last = self.prefix.broadcast()
         self.prefix_length = self.prefix.prefixlen()
         self.asn_first = asn
         self.asn_last = asn
         self.rpki_status = RPKIStatus.valid
         self.parsed_data = {
             self.rpsl_object_class: self.prefix_str,
-            'origin': 'AS' + str(self.asn),
-            'source': RPKI_IRR_PSEUDO_SOURCE,
-            'rpki_max_length': max_length,
+            "origin": "AS" + str(self.asn),
+            "source": RPKI_IRR_PSEUDO_SOURCE,
+            "rpki_max_length": max_length,
         }
         self.scopefilter_status, _ = scopefilter_validator.validate_rpsl_object(self)
 
     def source(self):
         return RPKI_IRR_PSEUDO_SOURCE
 
     def pk(self):
-        return f'{self.prefix_str}AS{self.asn}/ML{self.max_length}'
+        return f"{self.prefix_str}AS{self.asn}/ML{self.max_length}"
 
     def render_rpsl_text(self, last_modified=None):
-        object_class_display = f'{self.rpsl_object_class}:'.ljust(RPSL_ATTRIBUTE_TEXT_WIDTH)
-        remarks_fill = RPSL_ATTRIBUTE_TEXT_WIDTH * ' '
-        remarks = get_setting('rpki.pseudo_irr_remarks').replace('\n', '\n' + remarks_fill).strip()
+        object_class_display = f"{self.rpsl_object_class}:".ljust(RPSL_ATTRIBUTE_TEXT_WIDTH)
+        remarks_fill = RPSL_ATTRIBUTE_TEXT_WIDTH * " "
+        remarks = get_setting("rpki.pseudo_irr_remarks").replace("\n", "\n" + remarks_fill).strip()
         remarks = remarks.format(asn=self.asn, prefix=self.prefix_str)
-        rpsl_object_text = f"""
+        rpsl_object_text = (
+            f"""
 {object_class_display}{self.prefix_str}
 descr:          RPKI ROA for {self.prefix_str} / AS{self.asn}
 remarks:        {remarks}
 max-length:     {self.max_length}
 origin:         AS{self.asn}
 source:         {RPKI_IRR_PSEUDO_SOURCE}  # Trust Anchor: {self.trust_anchor}
-""".strip() + '\n'
+""".strip()
+            + "\n"
+        )
         return rpsl_object_text
```

### Comparing `irrd-4.2.8/irrd/rpki/notifications.py` & `irrd-4.3.0/irrd/rpki/notifications.py`

 * *Files 12% similar despite different names*

```diff
@@ -10,70 +10,84 @@
 from irrd.storage.database_handler import DatabaseHandler
 from irrd.storage.queries import RPSLDatabaseQuery
 from irrd.utils.email import send_email
 
 logger = logging.getLogger(__name__)
 
 
-def notify_rpki_invalid_owners(database_handler: DatabaseHandler,
-                               rpsl_dicts_now_invalid: List[Dict[str, str]]) -> int:
+def notify_rpki_invalid_owners(
+    database_handler: DatabaseHandler, rpsl_dicts_now_invalid: List[Dict[str, str]]
+) -> int:
     """
     Notify the owners/contacts of newly RPKI invalid objects.
 
     Expects a list of objects, each a dict with their properties.
     Contacts are resolved as any mnt-nfy, or any email address on any
     tech-c or admin-c, of any maintainer of the object.
     One email is sent per email address.
     """
-    if not get_setting('rpki.notify_invalid_enabled'):
+    if not get_setting("rpki.notify_invalid_enabled"):
         return 0
 
     rpsl_objs = []
     for obj in rpsl_dicts_now_invalid:
-        source = obj['source']
-        authoritative = get_setting(f'sources.{source}.authoritative')
-        if authoritative and obj['rpki_status'] == RPKIStatus.invalid:
-            rpsl_objs.append(rpsl_object_from_text(obj['object_text']))
+        source = obj["source"]
+        authoritative = get_setting(f"sources.{source}.authoritative")
+        if authoritative and obj["rpki_status"] == RPKIStatus.invalid:
+            rpsl_objs.append(rpsl_object_from_text(obj["object_text"]))
 
     if not rpsl_objs:
         return 0
 
-    sources = set([obj.parsed_data['source'] for obj in rpsl_objs])
+    sources = {obj.parsed_data["source"] for obj in rpsl_objs}
     mntner_emails_by_source = {}
     for source in sources:
         # For each source, a multi-step process is run to fill this
         # dict with the contact emails for each mntner.
         mntner_emails = defaultdict(set)
 
         # Step 1: retrieve all relevant maintainers from the DB
-        mntner_pks = set(itertools.chain(*[
-            obj.parsed_data.get('mnt-by', [])
-            for obj in rpsl_objs
-            if obj.parsed_data['source'] == source
-        ]))
-        query = RPSLDatabaseQuery(['rpsl_pk', 'parsed_data']).sources([source]).rpsl_pks(mntner_pks).object_classes(['mntner'])
+        mntner_pks = set(
+            itertools.chain(
+                *[
+                    obj.parsed_data.get("mnt-by", [])
+                    for obj in rpsl_objs
+                    if obj.parsed_data["source"] == source
+                ]
+            )
+        )
+        query = (
+            RPSLDatabaseQuery(["rpsl_pk", "parsed_data"])
+            .sources([source])
+            .rpsl_pks(mntner_pks)
+            .object_classes(["mntner"])
+        )
         mntners = list(database_handler.execute_query(query))
 
         # Step 2: any mnt-nfy on these maintainers is a contact address
         for mntner in mntners:
-            mntner_emails[mntner['rpsl_pk']].update(mntner['parsed_data'].get('mnt-nfy', []))
+            mntner_emails[mntner["rpsl_pk"]].update(mntner["parsed_data"].get("mnt-nfy", []))
 
         # Step 3: extract the contact handles for each maintainer
         mntner_contacts = {
-            m['rpsl_pk']: m['parsed_data'].get('tech-c', []) + m['parsed_data'].get('admin-c', [])
+            m["rpsl_pk"]: m["parsed_data"].get("tech-c", []) + m["parsed_data"].get("admin-c", [])
             for m in mntners
         }
 
         # Step 4: retrieve all these contacts from the DB in bulk,
         # and extract their e-mail addresses
         contact_pks = set(itertools.chain(*mntner_contacts.values()))
-        query = RPSLDatabaseQuery(['rpsl_pk', 'parsed_data']).sources([source]).rpsl_pks(contact_pks).object_classes(['role', 'person'])
+        query = (
+            RPSLDatabaseQuery(["rpsl_pk", "parsed_data"])
+            .sources([source])
+            .rpsl_pks(contact_pks)
+            .object_classes(["role", "person"])
+        )
         contacts = {
-            r['rpsl_pk']: r['parsed_data'].get('e-mail', [])
-            for r in database_handler.execute_query(query)
+            r["rpsl_pk"]: r["parsed_data"].get("e-mail", []) for r in database_handler.execute_query(query)
         }
 
         # Step 5: use the contacts per maintainer, and emails per contact
         # to create a flattened list of emails per maintainer
         for mntner_pk, mntner_contacts in mntner_contacts.items():
             for contact_pk in mntner_contacts:
                 try:
@@ -84,33 +98,33 @@
         mntner_emails_by_source[source] = mntner_emails
 
     # With mntners_emails_by_source filled with per source, per maintainer,
     # all relevant emails, categorise the RPSL objects on which email
     # addresses they need to be sent to.
     objs_per_email: Dict[str, Set[RPSLObject]] = defaultdict(set)
     for rpsl_obj in rpsl_objs:
-        mntners = rpsl_obj.parsed_data.get('mnt-by', [])
-        source = rpsl_obj.parsed_data['source']
+        mntners = rpsl_obj.parsed_data.get("mnt-by", [])
+        source = rpsl_obj.parsed_data["source"]
         for mntner_pk in mntners:
             try:
                 for email in mntner_emails_by_source[source][mntner_pk]:
                     objs_per_email[email].add(rpsl_obj)
             except KeyError:  # pragma: no cover
                 pass
 
-    header_template = get_setting('rpki.notify_invalid_header', '')
-    subject_template = get_setting('rpki.notify_invalid_subject', '').replace('\n', ' ')
+    header_template = get_setting("rpki.notify_invalid_header", "")
+    subject_template = get_setting("rpki.notify_invalid_subject", "").replace("\n", " ")
     for email, objs in objs_per_email.items():
-        sources_str = ', '.join(set([obj.parsed_data['source'] for obj in objs]))
+        sources_str = ", ".join({obj.parsed_data["source"] for obj in objs})
         subject = subject_template.format(sources_str=sources_str, object_count=len(objs))
         body = header_template.format(sources_str=sources_str, object_count=len(objs))
-        body += '\nThe following objects are affected:\n'
-        body += '~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n'
+        body += "\nThe following objects are affected:\n"
+        body += "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n"
         for rpsl_obj in objs:
-            body += rpsl_obj.render_rpsl_text() + '\n'
-        body += '~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~'
+            body += rpsl_obj.render_rpsl_text() + "\n"
+        body += "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
         try:
             send_email(email, subject, body)
         except Exception as e:  # pragma: no cover
-            logger.warning(f'Unable to send RPKI invalid notification to {email}: {e}')
+            logger.warning(f"Unable to send RPKI invalid notification to {email}: {e}")
 
     return len(objs_per_email.keys())
```

### Comparing `irrd-4.2.8/irrd/rpki/tests/test_importer.py` & `irrd-4.3.0/irrd/rpki/tests/test_importer.py`

 * *Files 13% similar despite different names*

```diff
@@ -5,244 +5,280 @@
 import ujson
 
 from irrd.conf import RPKI_IRR_PSEUDO_SOURCE
 from irrd.scopefilter.status import ScopeFilterStatus
 from irrd.scopefilter.validators import ScopeFilterValidator
 from irrd.storage.database_handler import DatabaseHandler
 from irrd.utils.test_utils import flatten_mock_calls
+
 from ..importer import ROADataImporter, ROAParserException
 
 
 @pytest.fixture()
 def mock_scopefilter(monkeypatch):
     mock_scopefilter = Mock(spec=ScopeFilterValidator)
-    monkeypatch.setattr('irrd.rpki.importer.ScopeFilterValidator',
-                        lambda: mock_scopefilter)
-    mock_scopefilter.validate_rpsl_object = lambda obj: (ScopeFilterStatus.out_scope_as, '')
+    monkeypatch.setattr("irrd.rpki.importer.ScopeFilterValidator", lambda: mock_scopefilter)
+    mock_scopefilter.validate_rpsl_object = lambda obj: (ScopeFilterStatus.out_scope_as, "")
 
 
 class TestROAImportProcess:
     def test_valid_process(self, monkeypatch, mock_scopefilter):
         # Note that this test does not mock RPSLObjectFromROA, used
         # for generating the pseudo-IRR object, or the ROA class itself.
 
         mock_dh = Mock(spec=DatabaseHandler)
 
-        rpki_data = ujson.dumps({
-            "roas": [{
-                "asn": "64496",
-                "prefix": "192.0.2.0/24",
-                "maxLength": 26,
-                "ta": "APNIC RPKI Root"
-            }, {
-                "asn": "AS64497",
-                "prefix": "2001:db8::/32",
-                "maxLength": 40,
-                "ta": "RIPE NCC RPKI Root"
-            }, {
-                # Filtered out by SLURM due to origin
-                "asn": "64498",
-                "prefix": "192.0.2.0/24",
-                "maxLength": 32,
-                "ta": "APNIC RPKI Root"
-            }, {
-                # Filtered out by SLURM due to prefix
-                "asn": "AS64496",
-                "prefix": "203.0.113.0/25",
-                "maxLength": 26,
-                "ta": "APNIC RPKI Root"
-            }, {
-                # Filtered out by SLURM due to prefix
-                "asn": "AS64497",
-                "prefix": "203.0.113.0/26",
-                "maxLength": 26,
-                "ta": "APNIC RPKI Root"
-            }, {
-                # Filtered out by SLURM due to prefix plus origin
-                "asn": "AS64497",
-                "prefix": "203.0.113.128/26",
-                "maxLength": 26,
-                "ta": "APNIC RPKI Root"
-            }]
-        })
-
-        slurm_data = ujson.dumps({
-            "slurmVersion": 1,
-            "validationOutputFilters": {
-                "prefixFilters": [
+        rpki_data = ujson.dumps(
+            {
+                "roas": [
+                    {"asn": "64496", "prefix": "192.0.2.0/24", "maxLength": 26, "ta": "APNIC RPKI Root"},
                     {
-                        "prefix": "203.0.113.0/25",
-                        "comment": "All VRPs encompassed by prefix",
+                        "asn": "AS64497",
+                        "prefix": "2001:db8::/32",
+                        "maxLength": 40,
+                        "ta": "RIPE NCC RPKI Root",
                     },
                     {
-                        "asn": 64498,
-                        "comment": "All VRPs matching ASN",
+                        # Filtered out by SLURM due to origin
+                        "asn": "64498",
+                        "prefix": "192.0.2.0/24",
+                        "maxLength": 32,
+                        "ta": "APNIC RPKI Root",
                     },
                     {
-                        "prefix": "203.0.113.128/25",
-                        "asn": 64497,
-                        "comment": "All VRPs encompassed by prefix, matching ASN",
+                        # Filtered out by SLURM due to prefix
+                        "asn": "AS64496",
+                        "prefix": "203.0.113.0/25",
+                        "maxLength": 26,
+                        "ta": "APNIC RPKI Root",
                     },
                     {
-                        # This filters out nothing, the ROA for this prefix has AS 64496
-                        "prefix": "192.0.2.0/24",
-                        "asn": 64497,
-                        "comment": "All VRPs encompassed by prefix, matching ASN",
+                        # Filtered out by SLURM due to prefix
+                        "asn": "AS64497",
+                        "prefix": "203.0.113.0/26",
+                        "maxLength": 26,
+                        "ta": "APNIC RPKI Root",
                     },
                     {
-                        # This should not filter out the assertion for 198.51.100/24
-                        "prefix": "198.51.100.0/24",
-                        "asn": 64496,
-                        "comment": "All VRPs encompassed by prefix, matching ASN",
-                    }
-                ],
-            },
-            "locallyAddedAssertions": {
-                "prefixAssertions": [
-                    {
-                        "asn": 64496,
-                        "prefix": "198.51.100.0/24",
-                        "comment": "My other important route",
+                        # Filtered out by SLURM due to prefix plus origin
+                        "asn": "AS64497",
+                        "prefix": "203.0.113.128/26",
+                        "maxLength": 26,
+                        "ta": "APNIC RPKI Root",
                     },
-                    {
-                        "asn": 64497,
-                        "prefix": "2001:DB8::/32",
-                        "maxPrefixLength": 48,
-                        "comment": "My other important de-aggregated routes",
-                    }
-                ],
+                ]
             }
-        })
+        )
+
+        slurm_data = ujson.dumps(
+            {
+                "slurmVersion": 1,
+                "validationOutputFilters": {
+                    "prefixFilters": [
+                        {
+                            "prefix": "203.0.113.0/25",
+                            "comment": "All VRPs encompassed by prefix",
+                        },
+                        {
+                            "asn": 64498,
+                            "comment": "All VRPs matching ASN",
+                        },
+                        {
+                            "prefix": "203.0.113.128/25",
+                            "asn": 64497,
+                            "comment": "All VRPs encompassed by prefix, matching ASN",
+                        },
+                        {
+                            # This filters out nothing, the ROA for this prefix has AS 64496
+                            "prefix": "192.0.2.0/24",
+                            "asn": 64497,
+                            "comment": "All VRPs encompassed by prefix, matching ASN",
+                        },
+                        {
+                            # This should not filter out the assertion for 198.51.100/24
+                            "prefix": "198.51.100.0/24",
+                            "asn": 64496,
+                            "comment": "All VRPs encompassed by prefix, matching ASN",
+                        },
+                    ],
+                },
+                "locallyAddedAssertions": {
+                    "prefixAssertions": [
+                        {
+                            "asn": 64496,
+                            "prefix": "198.51.100.0/24",
+                            "comment": "My other important route",
+                        },
+                        {
+                            "asn": 64497,
+                            "prefix": "2001:DB8::/32",
+                            "maxPrefixLength": 48,
+                            "comment": "My other important de-aggregated routes",
+                        },
+                    ],
+                },
+            }
+        )
 
         roa_importer = ROADataImporter(rpki_data, slurm_data, mock_dh)
         assert flatten_mock_calls(mock_dh, flatten_objects=True) == [
-            ['insert_roa_object', (),
-                {'ip_version': 4, 'prefix_str': '192.0.2.0/24', 'asn': 64496,
-                 'max_length': 26, 'trust_anchor': 'APNIC RPKI Root'}],
-            ['upsert_rpsl_object',
-             ('route/192.0.2.0/24AS64496/ML26/RPKI', 'JournalEntryOrigin.pseudo_irr'),
-             {'rpsl_guaranteed_no_existing': True}],
-            ['insert_roa_object', (),
-                {'ip_version': 6, 'prefix_str': '2001:db8::/32', 'asn': 64497,
-                 'max_length': 40, 'trust_anchor': 'RIPE NCC RPKI Root'}],
-            ['upsert_rpsl_object',
-             ('route6/2001:db8::/32AS64497/ML40/RPKI', 'JournalEntryOrigin.pseudo_irr'),
-             {'rpsl_guaranteed_no_existing': True}],
-            ['insert_roa_object', (),
-             {'ip_version': 4, 'prefix_str': '198.51.100.0/24', 'asn': 64496,
-              'max_length': 24, 'trust_anchor': 'SLURM file'}],
-            ['upsert_rpsl_object',
-             ('route/198.51.100.0/24AS64496/ML24/RPKI', 'JournalEntryOrigin.pseudo_irr'),
-             {'rpsl_guaranteed_no_existing': True}],
-            ['insert_roa_object', (),
-             {'ip_version': 6, 'prefix_str': '2001:db8::/32', 'asn': 64497,
-              'max_length': 48, 'trust_anchor': 'SLURM file'}],
-            ['upsert_rpsl_object',
-             ('route6/2001:db8::/32AS64497/ML48/RPKI', 'JournalEntryOrigin.pseudo_irr'),
-             {'rpsl_guaranteed_no_existing': True}],
+            [
+                "insert_roa_object",
+                (),
+                {
+                    "ip_version": 4,
+                    "prefix_str": "192.0.2.0/24",
+                    "asn": 64496,
+                    "max_length": 26,
+                    "trust_anchor": "APNIC RPKI Root",
+                },
+            ],
+            [
+                "upsert_rpsl_object",
+                ("route/192.0.2.0/24AS64496/ML26/RPKI", "JournalEntryOrigin.pseudo_irr"),
+                {"rpsl_guaranteed_no_existing": True},
+            ],
+            [
+                "insert_roa_object",
+                (),
+                {
+                    "ip_version": 6,
+                    "prefix_str": "2001:db8::/32",
+                    "asn": 64497,
+                    "max_length": 40,
+                    "trust_anchor": "RIPE NCC RPKI Root",
+                },
+            ],
+            [
+                "upsert_rpsl_object",
+                ("route6/2001:db8::/32AS64497/ML40/RPKI", "JournalEntryOrigin.pseudo_irr"),
+                {"rpsl_guaranteed_no_existing": True},
+            ],
+            [
+                "insert_roa_object",
+                (),
+                {
+                    "ip_version": 4,
+                    "prefix_str": "198.51.100.0/24",
+                    "asn": 64496,
+                    "max_length": 24,
+                    "trust_anchor": "SLURM file",
+                },
+            ],
+            [
+                "upsert_rpsl_object",
+                ("route/198.51.100.0/24AS64496/ML24/RPKI", "JournalEntryOrigin.pseudo_irr"),
+                {"rpsl_guaranteed_no_existing": True},
+            ],
+            [
+                "insert_roa_object",
+                (),
+                {
+                    "ip_version": 6,
+                    "prefix_str": "2001:db8::/32",
+                    "asn": 64497,
+                    "max_length": 48,
+                    "trust_anchor": "SLURM file",
+                },
+            ],
+            [
+                "upsert_rpsl_object",
+                ("route6/2001:db8::/32AS64497/ML48/RPKI", "JournalEntryOrigin.pseudo_irr"),
+                {"rpsl_guaranteed_no_existing": True},
+            ],
         ]
 
         assert roa_importer.roa_objs[0]._rpsl_object.scopefilter_status == ScopeFilterStatus.out_scope_as
         assert roa_importer.roa_objs[0]._rpsl_object.source() == RPKI_IRR_PSEUDO_SOURCE
         assert roa_importer.roa_objs[0]._rpsl_object.parsed_data == {
-            'origin': 'AS64496',
-            'route': '192.0.2.0/24',
-            'rpki_max_length': 26,
-            'source': 'RPKI',
+            "origin": "AS64496",
+            "route": "192.0.2.0/24",
+            "rpki_max_length": 26,
+            "source": "RPKI",
         }
-        assert roa_importer.roa_objs[0]._rpsl_object.render_rpsl_text() == textwrap.dedent("""
+        assert (
+            roa_importer.roa_objs[0]._rpsl_object.render_rpsl_text()
+            == textwrap.dedent(
+                """
             route:          192.0.2.0/24
             descr:          RPKI ROA for 192.0.2.0/24 / AS64496
             remarks:        This AS64496 route object represents routing data retrieved
                             from the RPKI. This route object is the result of an automated
                             RPKI-to-IRR conversion process performed by IRRd.
             max-length:     26
             origin:         AS64496
             source:         RPKI  # Trust Anchor: APNIC RPKI Root
-            """).strip() + '\n'
+            """
+            ).strip()
+            + "\n"
+        )
 
     def test_invalid_rpki_json(self, monkeypatch, mock_scopefilter):
         mock_dh = Mock(spec=DatabaseHandler)
 
         with pytest.raises(ROAParserException) as rpe:
-            ROADataImporter('invalid', None, mock_dh)
+            ROADataImporter("invalid", None, mock_dh)
 
-        assert 'Unable to parse ROA input: invalid JSON: Expected object or value' in str(rpe.value)
+        assert "Unable to parse ROA input: invalid JSON: Expected object or value" in str(rpe.value)
 
-        data = ujson.dumps({'invalid root': 42})
+        data = ujson.dumps({"invalid root": 42})
         with pytest.raises(ROAParserException) as rpe:
             ROADataImporter(data, None, mock_dh)
         assert 'Unable to parse ROA input: root key "roas" not found' in str(rpe.value)
 
         assert flatten_mock_calls(mock_dh) == []
 
     def test_invalid_data_in_roa(self, monkeypatch, mock_scopefilter):
         mock_dh = Mock(spec=DatabaseHandler)
 
-        data = ujson.dumps({
-            "roas": [{
-                "asn": "AS64496",
-                "prefix": "192.0.2.999/24",
-                "maxLength": 26,
-                "ta": "APNIC RPKI Root"
-            }]
-        })
-        with pytest.raises(ROAParserException) as rpe:
-            ROADataImporter(data, None, mock_dh)
-        assert "Invalid value in ROA or SLURM: '192.0.2.999': single byte must be 0 <= byte < 256" in str(rpe.value)
-
-        data = ujson.dumps({
-            "roas": [{
-                "asn": "ASx",
-                "prefix": "192.0.2.0/24",
-                "maxLength": 24,
-                "ta": "APNIC RPKI Root"
-            }]
-        })
-        with pytest.raises(ROAParserException) as rpe:
-            ROADataImporter(data, None, mock_dh)
-        assert 'Invalid AS number ASX: number part is not numeric' in str(rpe.value)
-
-        data = ujson.dumps({
-            "roas": [{
-                "prefix": "192.0.2.0/24",
-                "maxLength": 24,
-                "ta": "APNIC RPKI Root"
-            }]
-        })
+        data = ujson.dumps(
+            {
+                "roas": [
+                    {"asn": "AS64496", "prefix": "192.0.2.999/24", "maxLength": 26, "ta": "APNIC RPKI Root"}
+                ]
+            }
+        )
+        with pytest.raises(ROAParserException) as rpe:
+            ROADataImporter(data, None, mock_dh)
+        assert "Invalid value in ROA or SLURM: '192.0.2.999': single byte must be 0 <= byte < 256" in str(
+            rpe.value
+        )
+
+        data = ujson.dumps(
+            {"roas": [{"asn": "ASx", "prefix": "192.0.2.0/24", "maxLength": 24, "ta": "APNIC RPKI Root"}]}
+        )
+        with pytest.raises(ROAParserException) as rpe:
+            ROADataImporter(data, None, mock_dh)
+        assert "Invalid AS number ASX: number part is not numeric" in str(rpe.value)
+
+        data = ujson.dumps({"roas": [{"prefix": "192.0.2.0/24", "maxLength": 24, "ta": "APNIC RPKI Root"}]})
         with pytest.raises(ROAParserException) as rpe:
             ROADataImporter(data, None, mock_dh)
         assert "Unable to parse ROA record: missing key 'asn'" in str(rpe.value)
 
-        data = ujson.dumps({
-            "roas": [{
-                "asn": "AS64496",
-                "prefix": "192.0.2.0/24",
-                "maxLength": 22,
-                "ta": "APNIC RPKI Root"
-            }]
-        })
-        with pytest.raises(ROAParserException) as rpe:
-            ROADataImporter(data, None, mock_dh)
-        assert 'Invalid ROA: prefix size 24 is smaller than max length 22' in str(rpe.value)
-
-        data = ujson.dumps({
-            "roas": [{
-                "asn": "AS64496",
-                "prefix": "192.0.2.0/24",
-                "maxLength": 'xx',
-                "ta": "APNIC RPKI Root"
-            }]
-        })
+        data = ujson.dumps(
+            {"roas": [{"asn": "AS64496", "prefix": "192.0.2.0/24", "maxLength": 22, "ta": "APNIC RPKI Root"}]}
+        )
+        with pytest.raises(ROAParserException) as rpe:
+            ROADataImporter(data, None, mock_dh)
+        assert "Invalid ROA: prefix size 24 is smaller than max length 22" in str(rpe.value)
+
+        data = ujson.dumps(
+            {
+                "roas": [
+                    {"asn": "AS64496", "prefix": "192.0.2.0/24", "maxLength": "xx", "ta": "APNIC RPKI Root"}
+                ]
+            }
+        )
         with pytest.raises(ROAParserException) as rpe:
             ROADataImporter(data, None, mock_dh)
-        assert 'xx' in str(rpe.value)
+        assert "xx" in str(rpe.value)
 
         assert flatten_mock_calls(mock_dh) == []
 
     def test_invalid_slurm_version(self, monkeypatch, mock_scopefilter):
         mock_dh = Mock(spec=DatabaseHandler)
 
         with pytest.raises(ROAParserException) as rpe:
             ROADataImporter('{"roas": []}', '{"slurmVersion": 2}', mock_dh)
 
-        assert 'SLURM data has invalid version: 2' in str(rpe.value)
+        assert "SLURM data has invalid version: 2" in str(rpe.value)
```

### Comparing `irrd-4.2.8/irrd/rpki/tests/test_notifications.py` & `irrd-4.3.0/irrd/rpki/tests/test_notifications.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,69 +1,96 @@
 # flake8: noqa: W293
 import textwrap
 from unittest.mock import Mock
 
-from ..notifications import notify_rpki_invalid_owners
-from irrd.utils.test_utils import flatten_mock_calls
 from irrd.storage.database_handler import DatabaseHandler
 from irrd.utils.rpsl_samples import SAMPLE_ROUTE, SAMPLE_ROUTE6
-from ..status import RPKIStatus
+from irrd.utils.test_utils import flatten_mock_calls
+
 from ...storage.queries import RPSLDatabaseQuery
 from ...utils.email import send_email
+from ..notifications import notify_rpki_invalid_owners
+from ..status import RPKIStatus
 
 
 class TestNotifyRPKIInvalidOwners:
     def test_notify_regular(self, monkeypatch, config_override):
-        config_override({
-            'sources': {'TEST': {'authoritative': True}},
-            'rpki': {'notify_invalid_enabled': True},
-        })
+        config_override(
+            {
+                "sources": {"TEST": {"authoritative": True}},
+                "rpki": {"notify_invalid_enabled": True},
+            }
+        )
         mock_dh = Mock(spec=DatabaseHandler)
         mock_dq = Mock(spec=RPSLDatabaseQuery)
-        monkeypatch.setattr('irrd.rpki.notifications.RPSLDatabaseQuery', lambda columns: mock_dq)
+        monkeypatch.setattr("irrd.rpki.notifications.RPSLDatabaseQuery", lambda columns: mock_dq)
         mock_email = Mock(spec=send_email)
-        monkeypatch.setattr('irrd.rpki.notifications.send_email', mock_email)
+        monkeypatch.setattr("irrd.rpki.notifications.send_email", mock_email)
 
         rpsl_dicts_now_invalid = [
-            {'source': 'TEST', 'object_text': SAMPLE_ROUTE + 'mnt-by: DOESNOTEXIST-MNT\nMISSING-DATA-MNT\n', 'rpki_status': RPKIStatus.invalid},
-            {'source': 'TEST', 'object_text': SAMPLE_ROUTE6, 'rpki_status': RPKIStatus.valid},  # should be ignored
-            {'source': 'TEST2', 'object_text': SAMPLE_ROUTE6, 'rpki_status': RPKIStatus.invalid},  # should be ignored
+            {
+                "source": "TEST",
+                "object_text": SAMPLE_ROUTE + "mnt-by: DOESNOTEXIST-MNT\nMISSING-DATA-MNT\n",
+                "rpki_status": RPKIStatus.invalid,
+            },
+            {
+                "source": "TEST",
+                "object_text": SAMPLE_ROUTE6,
+                "rpki_status": RPKIStatus.valid,
+            },  # should be ignored
+            {
+                "source": "TEST2",
+                "object_text": SAMPLE_ROUTE6,
+                "rpki_status": RPKIStatus.invalid,
+            },  # should be ignored
         ]
 
-        query_results = iter([
+        query_results = iter(
             [
-                {'rpsl_pk': 'TEST-MNT', 'parsed_data': {
-                    'mnt-nfy': ['mnt-nfy@example.com'],
-                    'tech-c': ['PERSON-TEST', 'DOESNOTEXIST-TEST']
-                }},
-                {'rpsl_pk': 'MISSING-DATA-MNT', 'parsed_data': {}},
-            ],
-            [
-                {'rpsl_pk': 'PERSON-TEST', 'parsed_data': {'e-mail': ['person@xample.com', 'person2@example.com']}},
-                {'rpsl_pk': 'IGNORED-TEST', 'parsed_data': {'e-mail': ['ignored@xample.com']}},
-            ],
-        ])
+                [
+                    {
+                        "rpsl_pk": "TEST-MNT",
+                        "parsed_data": {
+                            "mnt-nfy": ["mnt-nfy@example.com"],
+                            "tech-c": ["PERSON-TEST", "DOESNOTEXIST-TEST"],
+                        },
+                    },
+                    {"rpsl_pk": "MISSING-DATA-MNT", "parsed_data": {}},
+                ],
+                [
+                    {
+                        "rpsl_pk": "PERSON-TEST",
+                        "parsed_data": {"e-mail": ["person@xample.com", "person2@example.com"]},
+                    },
+                    {"rpsl_pk": "IGNORED-TEST", "parsed_data": {"e-mail": ["ignored@xample.com"]}},
+                ],
+            ]
+        )
         mock_dh.execute_query = lambda q: next(query_results)
         notified = notify_rpki_invalid_owners(mock_dh, rpsl_dicts_now_invalid)
         assert notified == 3
 
         assert flatten_mock_calls(mock_dq) == [
-            ['sources', (['TEST'],), {}],
-            ['rpsl_pks', ({'TEST-MNT', 'DOESNOTEXIST-MNT'},), {}],
-            ['object_classes', (['mntner'],), {}],
-            ['sources', (['TEST'],), {}],
-            ['rpsl_pks', ({'PERSON-TEST', 'DOESNOTEXIST-TEST'},), {}],
-            ['object_classes', (['role', 'person'],), {}]]
+            ["sources", (["TEST"],), {}],
+            ["rpsl_pks", ({"TEST-MNT", "DOESNOTEXIST-MNT"},), {}],
+            ["object_classes", (["mntner"],), {}],
+            ["sources", (["TEST"],), {}],
+            ["rpsl_pks", ({"PERSON-TEST", "DOESNOTEXIST-TEST"},), {}],
+            ["object_classes", (["role", "person"],), {}],
+        ]
 
         assert len(mock_email.mock_calls) == 3
         actual_recipients = {call[1][0] for call in mock_email.mock_calls}
-        expected_recipients = {'person@xample.com', 'person2@example.com', 'mnt-nfy@example.com'}
+        expected_recipients = {"person@xample.com", "person2@example.com", "mnt-nfy@example.com"}
         assert actual_recipients == expected_recipients
-        assert mock_email.mock_calls[0][1][1] == 'route(6) objects in TEST marked RPKI invalid'
-        assert mock_email.mock_calls[0][1][2] == textwrap.dedent("""
+        assert mock_email.mock_calls[0][1][1] == "route(6) objects in TEST marked RPKI invalid"
+        assert (
+            mock_email.mock_calls[0][1][2]
+            == textwrap.dedent(
+                """
             This is to notify that 1 route(6) objects for which you are a
             contact have been marked as RPKI invalid. This concerns
             objects in the TEST database.
             
             You have received this message because your e-mail address is
             listed in one or more of the tech-c or admin-c contacts, on
             the maintainer(s) for these route objects.
@@ -94,43 +121,49 @@
             mnt-by:         TEST-MNT
             changed:        changed@example.com 20190701 # comment
             source:         TEST
             remarks:        remark
             mnt-by:         DOESNOTEXIST-MNT
             
             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-        """).strip()
+        """
+            ).strip()
+        )
 
     def test_notify_disabled(self, monkeypatch, config_override):
-        config_override({
-            'sources': {'TEST': {'authoritative': True}},
-            'rpki': {'notify_invalid_enabled': False},
-        })
+        config_override(
+            {
+                "sources": {"TEST": {"authoritative": True}},
+                "rpki": {"notify_invalid_enabled": False},
+            }
+        )
         mock_dh = Mock(spec=DatabaseHandler)
         mock_email = Mock()
-        monkeypatch.setattr('irrd.rpki.notifications.send_email', mock_email)
+        monkeypatch.setattr("irrd.rpki.notifications.send_email", mock_email)
 
         rpsl_dicts_now_invalid = [
-            {'source': 'TEST', 'object_text': SAMPLE_ROUTE6, 'rpki_status': RPKIStatus.invalid},
+            {"source": "TEST", "object_text": SAMPLE_ROUTE6, "rpki_status": RPKIStatus.invalid},
         ]
 
         notified = notify_rpki_invalid_owners(mock_dh, rpsl_dicts_now_invalid)
         assert notified == 0
         assert len(mock_email.mock_calls) == 0
 
     def test_notify_no_relevant_objects(self, monkeypatch, config_override):
-        config_override({
-            'sources': {'TEST': {'authoritative': True}},
-            'rpki': {'notify_invalid_enabled': True},
-        })
+        config_override(
+            {
+                "sources": {"TEST": {"authoritative": True}},
+                "rpki": {"notify_invalid_enabled": True},
+            }
+        )
         mock_dh = Mock(spec=DatabaseHandler)
         mock_email = Mock()
-        monkeypatch.setattr('irrd.rpki.notifications.send_email', mock_email)
+        monkeypatch.setattr("irrd.rpki.notifications.send_email", mock_email)
 
         rpsl_dicts_now_invalid = [
             # should be ignored
-            {'source': 'TEST2', 'object_text': SAMPLE_ROUTE6, 'rpki_status': RPKIStatus.invalid},
+            {"source": "TEST2", "object_text": SAMPLE_ROUTE6, "rpki_status": RPKIStatus.invalid},
         ]
 
         notified = notify_rpki_invalid_owners(mock_dh, rpsl_dicts_now_invalid)
         assert notified == 0
         assert len(mock_email.mock_calls) == 0
```

### Comparing `irrd-4.2.8/irrd/rpki/tests/test_validators.py` & `irrd-4.3.0/irrd/rpki/tests/test_validators.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,328 +1,370 @@
-from IPy import IP
 from unittest.mock import Mock
 
+from IPy import IP
+
 from irrd.conf import RPKI_IRR_PSEUDO_SOURCE
 from irrd.storage.database_handler import DatabaseHandler
-from irrd.storage.queries import RPSLDatabaseQuery, ROADatabaseObjectQuery
+from irrd.storage.queries import ROADatabaseObjectQuery, RPSLDatabaseQuery
 from irrd.utils.test_utils import flatten_mock_calls
+
 from ..importer import ROA
 from ..status import RPKIStatus
 from ..validators import BulkRouteROAValidator, SingleRouteROAValidator
 
 
 class TestBulkRouteROAValidator:
     def test_validate_routes_from_roa_objs(self, monkeypatch, config_override):
-        config_override({
-            'sources': {'TEST1': {}, 'TEST2': {}, RPKI_IRR_PSEUDO_SOURCE: {},
-                        'SOURCE-EXCLUDED': {'rpki_excluded': True}}
-        })
+        config_override(
+            {
+                "sources": {
+                    "TEST1": {},
+                    "TEST2": {},
+                    RPKI_IRR_PSEUDO_SOURCE: {},
+                    "SOURCE-EXCLUDED": {"rpki_excluded": True},
+                }
+            }
+        )
         mock_dh = Mock(spec=DatabaseHandler)
         mock_dq = Mock(spec=RPSLDatabaseQuery)
-        monkeypatch.setattr('irrd.rpki.validators.RPSLDatabaseQuery',
-                            lambda column_names, enable_ordering: mock_dq)
+        monkeypatch.setattr(
+            "irrd.rpki.validators.RPSLDatabaseQuery", lambda column_names, enable_ordering: mock_dq
+        )
 
-        mock_query_result = iter([
+        mock_query_result = iter(
             [
-                {
-                    'pk': 'pk_route_v4_d0_l24',
-                    'rpsl_pk': 'pk_route_v4_d0_l24',
-                    'ip_version': 4,
-                    'ip_first': '192.0.2.0',
-                    'prefix_length': 24,
-                    'asn_first': 65546,
-                    'rpki_status': RPKIStatus.not_found,
-                    'source': 'TEST1',
-                },
-                {
-                    'pk': 'pk_route_v4_d0_l25',
-                    'rpsl_pk': 'pk_route_v4_d0_l25',
-                    'ip_version': 4,
-                    'ip_first': '192.0.2.0',
-                    'prefix_length': 25,
-                    'asn_first': 65546,
-                    'rpki_status': RPKIStatus.not_found,
-                    'source': 'TEST1',
-                },
-                {
-                    # This route is valid, but as the state is already valid,
-                    # it should not be included in the response.
-                    'pk': 'pk_route_v4_d0_l28',
-                    'rpsl_pk': 'pk_route_v4_d0_l28',
-                    'ip_version': 4,
-                    'ip_first': '192.0.2.0',
-                    'prefix_length': 27,
-                    'asn_first': 65546,
-                    'rpki_status': RPKIStatus.valid,
-                    'source': 'TEST1',
-                },
-                {
-                    'pk': 'pk_route_v4_d64_l32',
-                    'rpsl_pk': 'pk_route_v4_d64_l32',
-                    'ip_version': 4,
-                    'ip_first': '192.0.2.64',
-                    'prefix_length': 32,
-                    'asn_first': 65546,
-                    'rpki_status': RPKIStatus.valid,
-                    'source': 'TEST1',
-                },
-                {
-                    'pk': 'pk_route_v4_d128_l25',
-                    'rpsl_pk': 'pk_route_v4_d128_l25',
-                    'ip_version': 4,
-                    'ip_first': '192.0.2.128',
-                    'prefix_length': 25,
-                    'asn_first': 65547,
-                    'rpki_status': RPKIStatus.valid,
-                    'source': 'TEST1',
-                },
-                {
-                    # RPKI invalid, but should be ignored.
-                    'pk': 'pk_route_v4_d128_l26_rpki',
-                    'rpsl_pk': 'pk_route_v4_d128_l26',
-                    'ip_version': 4,
-                    'ip_first': '192.0.2.128',
-                    'prefix_length': 26,
-                    'asn_first': 65547,
-                    'rpki_status': RPKIStatus.invalid,
-                    'source': RPKI_IRR_PSEUDO_SOURCE,
-                },
-                {
-                    # RPKI invalid, but should be not_found because of source.
-                    'pk': 'pk_route_v4_d128_l26_excluded',
-                    'rpsl_pk': 'pk_route_v4_d128_l26_excluded',
-                    'ip_version': 4,
-                    'ip_first': '192.0.2.128',
-                    'prefix_length': 26,
-                    'asn_first': 65547,
-                    'rpki_status': RPKIStatus.valid,
-                    'source': 'SOURCE-EXCLUDED',
-                },
-                {
-                    'pk': 'pk_route_v6',
-                    'rpsl_pk': 'pk_route_v6',
-                    'ip_version': 6,
-                    'ip_first': '2001:db8::',
-                    'prefix_length': 32,
-                    'asn_first': 65547,
-                    'rpki_status': RPKIStatus.invalid,
-                    'source': 'TEST1',
-                },
-                {
-                    # Should not match any ROA - ROAs for a subset
-                    # exist, but those should not be included
-                    'pk': 'pk_route_v4_no_roa',
-                    'rpsl_pk': 'pk_route_v4_no_roa',
-                    'ip_version': 4,
-                    'ip_first': '192.0.2.0',
-                    'prefix_length': 23,
-                    'asn_first': 65549,
-                    'rpki_status': RPKIStatus.valid,
-                    'source': 'TEST1',
-                },
-                {
-                    'pk': 'pk_route_v4_roa_as0',
-                    'rpsl_pk': 'pk_route_v4_roa_as0',
-                    'ip_version': 4,
-                    'ip_first': '203.0.113.1',
-                    'prefix_length': 32,
-                    'asn_first': 65547,
-                    'rpki_status': RPKIStatus.not_found,
-                    'source': 'TEST1',
-                },
-            ], [
-                {
-                    'pk': 'pk_route_v4_d0_l24',
-                    'object_text': 'object text',
-                    'object_class': 'route',
-                },
-                {
-                    'pk': 'pk_route_v4_d0_l25',
-                    'object_text': 'object text',
-                    'object_class': 'route',
-                },
-                {
-                    'pk': 'pk_route_v4_d64_l32',
-                    'object_text': 'object text',
-                    'object_class': 'route',
-                },
-                {
-                    'pk': 'pk_route_v4_d128_l25',
-                    'object_text': 'object text',
-                    'object_class': 'route',
-                },
-                {
-                    'pk': 'pk_route_v4_d128_l26_rpki',
-                    'object_text': 'object text',
-                    'object_class': 'route',
-                },
-                {
-                    'pk': 'pk_route_v4_d128_l26_excluded',
-                    'object_text': 'object text',
-                    'object_class': 'route',
-                },
-                {
-                    'pk': 'pk_route_v6',
-                    'object_text': 'object text',
-                    'object_class': 'route',
-                },
-                {
-                    'pk': 'pk_route_v4_no_roa',
-                    'object_text': 'object text',
-                    'object_class': 'route',
-                },
-                {
-                    'pk': 'pk_route_v4_roa_as0',
-                    'object_text': 'object text',
-                    'object_class': 'route',
-                },
+                [
+                    {
+                        "pk": "pk_route_v4_d0_l24",
+                        "rpsl_pk": "pk_route_v4_d0_l24",
+                        "ip_version": 4,
+                        "ip_first": "192.0.2.0",
+                        "prefix_length": 24,
+                        "asn_first": 65546,
+                        "rpki_status": RPKIStatus.not_found,
+                        "source": "TEST1",
+                    },
+                    {
+                        "pk": "pk_route_v4_d0_l25",
+                        "rpsl_pk": "pk_route_v4_d0_l25",
+                        "ip_version": 4,
+                        "ip_first": "192.0.2.0",
+                        "prefix_length": 25,
+                        "asn_first": 65546,
+                        "rpki_status": RPKIStatus.not_found,
+                        "source": "TEST1",
+                    },
+                    {
+                        # This route is valid, but as the state is already valid,
+                        # it should not be included in the response.
+                        "pk": "pk_route_v4_d0_l28",
+                        "rpsl_pk": "pk_route_v4_d0_l28",
+                        "ip_version": 4,
+                        "ip_first": "192.0.2.0",
+                        "prefix_length": 27,
+                        "asn_first": 65546,
+                        "rpki_status": RPKIStatus.valid,
+                        "source": "TEST1",
+                    },
+                    {
+                        "pk": "pk_route_v4_d64_l32",
+                        "rpsl_pk": "pk_route_v4_d64_l32",
+                        "ip_version": 4,
+                        "ip_first": "192.0.2.64",
+                        "prefix_length": 32,
+                        "asn_first": 65546,
+                        "rpki_status": RPKIStatus.valid,
+                        "source": "TEST1",
+                    },
+                    {
+                        "pk": "pk_route_v4_d128_l25",
+                        "rpsl_pk": "pk_route_v4_d128_l25",
+                        "ip_version": 4,
+                        "ip_first": "192.0.2.128",
+                        "prefix_length": 25,
+                        "asn_first": 65547,
+                        "rpki_status": RPKIStatus.valid,
+                        "source": "TEST1",
+                    },
+                    {
+                        # RPKI invalid, but should be ignored.
+                        "pk": "pk_route_v4_d128_l26_rpki",
+                        "rpsl_pk": "pk_route_v4_d128_l26",
+                        "ip_version": 4,
+                        "ip_first": "192.0.2.128",
+                        "prefix_length": 26,
+                        "asn_first": 65547,
+                        "rpki_status": RPKIStatus.invalid,
+                        "source": RPKI_IRR_PSEUDO_SOURCE,
+                    },
+                    {
+                        # RPKI invalid, but should be not_found because of source.
+                        "pk": "pk_route_v4_d128_l26_excluded",
+                        "rpsl_pk": "pk_route_v4_d128_l26_excluded",
+                        "ip_version": 4,
+                        "ip_first": "192.0.2.128",
+                        "prefix_length": 26,
+                        "asn_first": 65547,
+                        "rpki_status": RPKIStatus.valid,
+                        "source": "SOURCE-EXCLUDED",
+                    },
+                    {
+                        "pk": "pk_route_v6",
+                        "rpsl_pk": "pk_route_v6",
+                        "ip_version": 6,
+                        "ip_first": "2001:db8::",
+                        "prefix_length": 32,
+                        "asn_first": 65547,
+                        "rpki_status": RPKIStatus.invalid,
+                        "source": "TEST1",
+                    },
+                    {
+                        # Should not match any ROA - ROAs for a subset
+                        # exist, but those should not be included
+                        "pk": "pk_route_v4_no_roa",
+                        "rpsl_pk": "pk_route_v4_no_roa",
+                        "ip_version": 4,
+                        "ip_first": "192.0.2.0",
+                        "prefix_length": 23,
+                        "asn_first": 65549,
+                        "rpki_status": RPKIStatus.valid,
+                        "source": "TEST1",
+                    },
+                    {
+                        "pk": "pk_route_v4_roa_as0",
+                        "rpsl_pk": "pk_route_v4_roa_as0",
+                        "ip_version": 4,
+                        "ip_first": "203.0.113.1",
+                        "prefix_length": 32,
+                        "asn_first": 65547,
+                        "rpki_status": RPKIStatus.not_found,
+                        "source": "TEST1",
+                    },
+                ],
+                [
+                    {
+                        "pk": "pk_route_v4_d0_l24",
+                        "object_text": "object text",
+                        "object_class": "route",
+                    },
+                    {
+                        "pk": "pk_route_v4_d0_l25",
+                        "object_text": "object text",
+                        "object_class": "route",
+                    },
+                    {
+                        "pk": "pk_route_v4_d64_l32",
+                        "object_text": "object text",
+                        "object_class": "route",
+                    },
+                    {
+                        "pk": "pk_route_v4_d128_l25",
+                        "object_text": "object text",
+                        "object_class": "route",
+                    },
+                    {
+                        "pk": "pk_route_v4_d128_l26_rpki",
+                        "object_text": "object text",
+                        "object_class": "route",
+                    },
+                    {
+                        "pk": "pk_route_v4_d128_l26_excluded",
+                        "object_text": "object text",
+                        "object_class": "route",
+                    },
+                    {
+                        "pk": "pk_route_v6",
+                        "object_text": "object text",
+                        "object_class": "route",
+                    },
+                    {
+                        "pk": "pk_route_v4_no_roa",
+                        "object_text": "object text",
+                        "object_class": "route",
+                    },
+                    {
+                        "pk": "pk_route_v4_roa_as0",
+                        "object_text": "object text",
+                        "object_class": "route",
+                    },
+                ],
             ]
-        ])
+        )
         mock_dh.execute_query = lambda query: next(mock_query_result)
 
         roas = [
             # Valid for pk_route_v4_d0_l25 and pk_route_v4_d0_l24
             # - the others have incorrect origin or are too small.
-            ROA(IP('192.0.2.0/24'), 65546, '28', 'TEST TA'),
+            ROA(IP("192.0.2.0/24"), 65546, "28", "TEST TA"),
             # Matches the origin of pk_route_v4_d128_l25,
             # but not max_length.
-            ROA(IP('192.0.2.0/24'), 65547, '24', 'TEST TA'),
+            ROA(IP("192.0.2.0/24"), 65547, "24", "TEST TA"),
             # Matches pk_route_v6, but not max_length.
-            ROA(IP('2001:db8::/30'), 65547, '30', 'TEST TA'),
+            ROA(IP("2001:db8::/30"), 65547, "30", "TEST TA"),
             # Matches pk_route_v6, but not on origin.
-            ROA(IP('2001:db8::/32'), 65548, '32', 'TEST TA'),
+            ROA(IP("2001:db8::/32"), 65548, "32", "TEST TA"),
             # Matches pk_route_v6
-            ROA(IP('2001:db8::/32'), 65547, '64', 'TEST TA'),
+            ROA(IP("2001:db8::/32"), 65547, "64", "TEST TA"),
             # Matches no routes, no effect
-            ROA(IP('203.0.113.0/32'), 65547, '32', 'TEST TA'),
+            ROA(IP("203.0.113.0/32"), 65547, "32", "TEST TA"),
             # AS0 can not match
-            ROA(IP('203.0.113.1/32'), 0, '32', 'TEST TA'),
+            ROA(IP("203.0.113.1/32"), 0, "32", "TEST TA"),
         ]
-        result = BulkRouteROAValidator(mock_dh, roas).validate_all_routes(sources=['TEST1'])
+        result = BulkRouteROAValidator(mock_dh, roas).validate_all_routes(sources=["TEST1"])
         new_valid_objs, new_invalid_objs, new_unknown_objs = result
-        assert {o['rpsl_pk'] for o in new_valid_objs} == {'pk_route_v6', 'pk_route_v4_d0_l25', 'pk_route_v4_d0_l24'}
-        assert [o['object_class'] for o in new_valid_objs] == ['route', 'route', 'route']
-        assert [o['object_text'] for o in new_valid_objs] == ['object text', 'object text', 'object text']
-        assert {o['rpsl_pk'] for o in new_invalid_objs} == {'pk_route_v4_d64_l32', 'pk_route_v4_d128_l25', 'pk_route_v4_roa_as0'}
-        assert {o['rpsl_pk'] for o in new_unknown_objs} == {'pk_route_v4_no_roa', 'pk_route_v4_d128_l26_excluded'}
+        assert {o["rpsl_pk"] for o in new_valid_objs} == {
+            "pk_route_v6",
+            "pk_route_v4_d0_l25",
+            "pk_route_v4_d0_l24",
+        }
+        assert [o["object_class"] for o in new_valid_objs] == ["route", "route", "route"]
+        assert [o["object_text"] for o in new_valid_objs] == ["object text", "object text", "object text"]
+        assert {o["rpsl_pk"] for o in new_invalid_objs} == {
+            "pk_route_v4_d64_l32",
+            "pk_route_v4_d128_l25",
+            "pk_route_v4_roa_as0",
+        }
+        assert {o["rpsl_pk"] for o in new_unknown_objs} == {
+            "pk_route_v4_no_roa",
+            "pk_route_v4_d128_l26_excluded",
+        }
 
         assert flatten_mock_calls(mock_dq) == [
-            ['object_classes', (['route', 'route6'],), {}],
-            ['sources', (['TEST1'],), {}],
-            ['pks', (['pk_route_v4_d0_l24', 'pk_route_v4_d0_l25', 'pk_route_v6', 'pk_route_v4_d64_l32', 'pk_route_v4_d128_l25', 'pk_route_v4_roa_as0', 'pk_route_v4_d128_l26_excluded', 'pk_route_v4_no_roa'], ), {}],
+            ["object_classes", (["route", "route6"],), {}],
+            ["sources", (["TEST1"],), {}],
+            [
+                "pks",
+                (
+                    [
+                        "pk_route_v4_d0_l24",
+                        "pk_route_v4_d0_l25",
+                        "pk_route_v6",
+                        "pk_route_v4_d64_l32",
+                        "pk_route_v4_d128_l25",
+                        "pk_route_v4_roa_as0",
+                        "pk_route_v4_d128_l26_excluded",
+                        "pk_route_v4_no_roa",
+                    ],
+                ),
+                {},
+            ],
         ]
 
     def test_validate_routes_with_roa_from_database(self, monkeypatch, config_override):
-        config_override({
-            'sources': {'TEST1': {}, 'TEST2': {}, RPKI_IRR_PSEUDO_SOURCE: {}}
-        })
+        config_override({"sources": {"TEST1": {}, "TEST2": {}, RPKI_IRR_PSEUDO_SOURCE: {}}})
         mock_dh = Mock(spec=DatabaseHandler)
         mock_dq = Mock(spec=RPSLDatabaseQuery)
-        monkeypatch.setattr('irrd.rpki.validators.RPSLDatabaseQuery',
-                            lambda column_names, enable_ordering: mock_dq)
+        monkeypatch.setattr(
+            "irrd.rpki.validators.RPSLDatabaseQuery", lambda column_names, enable_ordering: mock_dq
+        )
         mock_rq = Mock(spec=ROADatabaseObjectQuery)
-        monkeypatch.setattr('irrd.rpki.validators.ROADatabaseObjectQuery',
-                            lambda: mock_rq)
+        monkeypatch.setattr("irrd.rpki.validators.ROADatabaseObjectQuery", lambda: mock_rq)
 
-        mock_query_result = iter([
-            [  # ROAs:
-                {
-                    'prefix': '192.0.2.0/24',
-                    'asn': 65546,
-                    'max_length': 25,
-                    'ip_version': 4,
-                },
-                {
-                    'prefix': '192.0.2.0/24',
-                    'asn': 65547,
-                    'max_length': 24,
-                    'ip_version': 4,
-                },
-            ], [  # RPSL objects:
-                {
-                    'pk': 'pk1',
-                    'rpsl_pk': 'pk_route_v4_d0_l25',
-                    'ip_version': 4,
-                    'ip_first': '192.0.2.0',
-                    'prefix_length': 25,
-                    'asn_first': 65546,
-                    'rpki_status': RPKIStatus.not_found,
-                    'source': 'TEST1',
-                },
-            ], [
-                {
-                    'pk': 'pk1',
-                    'object_class': 'route',
-                    'object_text': 'object text',
-                },
+        mock_query_result = iter(
+            [
+                [  # ROAs:
+                    {
+                        "prefix": "192.0.2.0/24",
+                        "asn": 65546,
+                        "max_length": 25,
+                        "ip_version": 4,
+                    },
+                    {
+                        "prefix": "192.0.2.0/24",
+                        "asn": 65547,
+                        "max_length": 24,
+                        "ip_version": 4,
+                    },
+                ],
+                [  # RPSL objects:
+                    {
+                        "pk": "pk1",
+                        "rpsl_pk": "pk_route_v4_d0_l25",
+                        "ip_version": 4,
+                        "ip_first": "192.0.2.0",
+                        "prefix_length": 25,
+                        "asn_first": 65546,
+                        "rpki_status": RPKIStatus.not_found,
+                        "source": "TEST1",
+                    },
+                ],
+                [
+                    {
+                        "pk": "pk1",
+                        "object_class": "route",
+                        "object_text": "object text",
+                    },
+                ],
             ]
-        ])
+        )
         mock_dh.execute_query = lambda query: next(mock_query_result)
 
-        result = BulkRouteROAValidator(mock_dh).validate_all_routes(sources=['TEST1'])
+        result = BulkRouteROAValidator(mock_dh).validate_all_routes(sources=["TEST1"])
         new_valid_pks, new_invalid_pks, new_unknown_pks = result
-        assert {o['rpsl_pk'] for o in new_valid_pks} == {'pk_route_v4_d0_l25'}
-        assert {o['object_text'] for o in new_valid_pks} == {'object text'}
+        assert {o["rpsl_pk"] for o in new_valid_pks} == {"pk_route_v4_d0_l25"}
+        assert {o["object_text"] for o in new_valid_pks} == {"object text"}
         assert new_invalid_pks == list()
         assert new_unknown_pks == list()
 
         assert flatten_mock_calls(mock_dq) == [
-            ['object_classes', (['route', 'route6'],), {}],
-            ['sources', (['TEST1'],), {}],
-            ['pks', (['pk1'],), {}],
+            ["object_classes", (["route", "route6"],), {}],
+            ["sources", (["TEST1"],), {}],
+            ["pks", (["pk1"],), {}],
         ]
         assert flatten_mock_calls(mock_rq) == []  # No filters applied
 
 
 class TestSingleRouteROAValidator:
     def test_validator_normal_roa(self, monkeypatch, config_override):
-        config_override({
-            'sources': {'SOURCE-EXCLUDED': {'rpki_excluded': True}}
-        })
+        config_override({"sources": {"SOURCE-EXCLUDED": {"rpki_excluded": True}}})
         mock_dh = Mock(spec=DatabaseHandler)
         mock_rq = Mock(spec=ROADatabaseObjectQuery)
-        monkeypatch.setattr('irrd.rpki.validators.ROADatabaseObjectQuery', lambda: mock_rq)
+        monkeypatch.setattr("irrd.rpki.validators.ROADatabaseObjectQuery", lambda: mock_rq)
 
-        roa_response = [{
-            'asn': 65548,
-            'max_length': 25,
-        }]
+        roa_response = [
+            {
+                "asn": 65548,
+                "max_length": 25,
+            }
+        ]
         mock_dh.execute_query = lambda q: roa_response
 
         validator = SingleRouteROAValidator(mock_dh)
-        assert validator.validate_route(IP('192.0.2.0/24'), 65548, 'TEST1') == RPKIStatus.valid
-        assert validator.validate_route(IP('192.0.2.0/24'), 65548, 'SOURCE-EXCLUDED') == RPKIStatus.not_found
-        assert validator.validate_route(IP('192.0.2.0/24'), 65549, 'TEST1') == RPKIStatus.invalid
-        assert validator.validate_route(IP('192.0.2.0/24'), 65549, 'SOURCE-EXCLUDED') == RPKIStatus.not_found
-        assert validator.validate_route(IP('192.0.2.0/26'), 65548, 'TEST1') == RPKIStatus.invalid
+        assert validator.validate_route(IP("192.0.2.0/24"), 65548, "TEST1") == RPKIStatus.valid
+        assert validator.validate_route(IP("192.0.2.0/24"), 65548, "SOURCE-EXCLUDED") == RPKIStatus.not_found
+        assert validator.validate_route(IP("192.0.2.0/24"), 65549, "TEST1") == RPKIStatus.invalid
+        assert validator.validate_route(IP("192.0.2.0/24"), 65549, "SOURCE-EXCLUDED") == RPKIStatus.not_found
+        assert validator.validate_route(IP("192.0.2.0/26"), 65548, "TEST1") == RPKIStatus.invalid
 
         assert flatten_mock_calls(mock_rq) == [
-            ['ip_less_specific_or_exact', (IP('192.0.2.0/24'),), {}],
-            ['ip_less_specific_or_exact', (IP('192.0.2.0/24'),), {}],
-            ['ip_less_specific_or_exact', (IP('192.0.2.0/26'),), {}],
+            ["ip_less_specific_or_exact", (IP("192.0.2.0/24"),), {}],
+            ["ip_less_specific_or_exact", (IP("192.0.2.0/24"),), {}],
+            ["ip_less_specific_or_exact", (IP("192.0.2.0/26"),), {}],
         ]
 
     def test_validator_as0_roa(self, monkeypatch):
         mock_dh = Mock(spec=DatabaseHandler)
         mock_rq = Mock(spec=ROADatabaseObjectQuery)
-        monkeypatch.setattr('irrd.rpki.validators.ROADatabaseObjectQuery', lambda: mock_rq)
+        monkeypatch.setattr("irrd.rpki.validators.ROADatabaseObjectQuery", lambda: mock_rq)
 
-        roa_response = [{
-            'asn': 0,
-            'max_length': 25,
-        }]
+        roa_response = [
+            {
+                "asn": 0,
+                "max_length": 25,
+            }
+        ]
         mock_dh.execute_query = lambda q: roa_response
 
         validator = SingleRouteROAValidator(mock_dh)
-        assert validator.validate_route(IP('192.0.2.0/24'), 65548, 'TEST1') == RPKIStatus.invalid
+        assert validator.validate_route(IP("192.0.2.0/24"), 65548, "TEST1") == RPKIStatus.invalid
 
     def test_validator_no_matching_roa(self, monkeypatch):
         mock_dh = Mock(spec=DatabaseHandler)
         mock_rq = Mock(spec=ROADatabaseObjectQuery)
-        monkeypatch.setattr('irrd.rpki.validators.ROADatabaseObjectQuery', lambda: mock_rq)
+        monkeypatch.setattr("irrd.rpki.validators.ROADatabaseObjectQuery", lambda: mock_rq)
 
         mock_dh.execute_query = lambda q: []
 
         validator = SingleRouteROAValidator(mock_dh)
-        assert validator.validate_route(IP('192.0.2.0/24'), 65548, 'TEST1') == RPKIStatus.not_found
-        assert validator.validate_route(IP('192.0.2.0/24'), 65549, 'TEST1') == RPKIStatus.not_found
-        assert validator.validate_route(IP('192.0.2.0/26'), 65548, 'TEST1') == RPKIStatus.not_found
+        assert validator.validate_route(IP("192.0.2.0/24"), 65548, "TEST1") == RPKIStatus.not_found
+        assert validator.validate_route(IP("192.0.2.0/24"), 65549, "TEST1") == RPKIStatus.not_found
+        assert validator.validate_route(IP("192.0.2.0/26"), 65548, "TEST1") == RPKIStatus.not_found
```

### Comparing `irrd-4.2.8/irrd/rpki/validators.py` & `irrd-4.3.0/irrd/rpki/validators.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,18 +1,19 @@
-import datrie
-from collections import defaultdict
-
 import codecs
 import socket
+from collections import defaultdict
+from typing import Dict, List, Optional, Tuple
+
+import datrie
 from IPy import IP
-from typing import Optional, List, Tuple, Dict
 
 from irrd.conf import RPKI_IRR_PSEUDO_SOURCE, get_setting
 from irrd.storage.database_handler import DatabaseHandler
-from irrd.storage.queries import RPSLDatabaseQuery, ROADatabaseObjectQuery
+from irrd.storage.queries import ROADatabaseObjectQuery, RPSLDatabaseQuery
+
 from .importer import ROA
 from .status import RPKIStatus
 
 # Pregenerated conversion from binary to binary strings for performance
 BYTE_BIN = [bin(byte)[2:].zfill(8) for byte in range(256)]
 
 decode_hex = codecs.getdecoder("hex_codec")
@@ -54,74 +55,82 @@
         when many routes have to be validated, otherwise it's more
         efficient to use SingleRouteROAValidator. The break even
         point is in the order of magnitude of checking 10.000 routes.
         """
         self.database_handler = dh
 
         self.excluded_sources = []
-        for source, settings in get_setting('sources', {}).items():
-            if settings.get('rpki_excluded'):
+        for source, settings in get_setting("sources", {}).items():
+            if settings.get("rpki_excluded"):
                 self.excluded_sources.append(source)
 
-        self.roa_tree4 = datrie.Trie('01')
-        self.roa_tree6 = datrie.Trie('01')
+        self.roa_tree4 = datrie.Trie("01")
+        self.roa_tree6 = datrie.Trie("01")
         if roas is None:
             self._build_roa_tree_from_db()
         else:
             self._build_roa_tree_from_roa_objs(roas)
 
-    def validate_all_routes(self, sources: List[str]=None) -> \
-            Tuple[List[Dict[str, str]], List[Dict[str, str]], List[Dict[str, str]]]:
+    def validate_all_routes(
+        self, sources: Optional[List[str]] = None
+    ) -> Tuple[List[Dict[str, str]], List[Dict[str, str]], List[Dict[str, str]]]:
         """
         Validate all RPSL route/route6 objects.
 
         Retrieves all routes from the DB, and aggregates the validation results.
         Returns a tuple of three sets of RPSL route(6)'s:
         - one with routes that should be set to status VALID, but are not now
         - one with routes that should be set to status INVALID, but are not now
         - one with routes that should be set to status UNKNOWN, but are not now
         Each route is recorded as a dict, which has the fields shown
         in "columns" below.
 
         Routes where their current validation status in the DB matches the new
         validation result, are not included in the return value.
         """
-        columns = ['pk', 'rpsl_pk', 'ip_first', 'prefix_length', 'asn_first', 'source',
-                   'rpki_status', 'scopefilter_status']
+        columns = ["pk", "rpsl_pk", "ip_first", "prefix_length", "asn_first", "source", "rpki_status"]
         q = RPSLDatabaseQuery(column_names=columns, enable_ordering=False)
-        q = q.object_classes(['route', 'route6'])
+        q = q.object_classes(["route", "route6"])
         if sources:
             q = q.sources(sources)
         routes = self.database_handler.execute_query(q)
 
         objs_changed: Dict[RPKIStatus, List[Dict[str, str]]] = defaultdict(list)
 
         for result in routes:
             # RPKI_IRR_PSEUDO_SOURCE objects are ROAs, and don't need validation.
-            if result['source'] == RPKI_IRR_PSEUDO_SOURCE:
+            if result["source"] == RPKI_IRR_PSEUDO_SOURCE:
                 continue
 
-            current_status = result['rpki_status']
-            result['old_status'] = current_status
-            new_status = self.validate_route(result['ip_first'], result['prefix_length'],
-                                             result['asn_first'], result['source'])
+            current_status = result["rpki_status"]
+            result["old_status"] = current_status
+            new_status = self.validate_route(
+                result["ip_first"], result["prefix_length"], result["asn_first"], result["source"]
+            )
             if new_status != current_status:
-                result['rpki_status'] = new_status
+                result["rpki_status"] = new_status
                 objs_changed[new_status].append(result)
 
         # Object text and class are only retrieved for objects with state changes
-        pks_to_enrich = [obj['pk'] for objs in objs_changed.values() for obj in objs]
-        query = RPSLDatabaseQuery(['pk', 'object_text', 'object_class'], enable_ordering=False).pks(pks_to_enrich)
-        rows_per_pk = {row['pk']: row for row in self.database_handler.execute_query(query)}
+        pks_to_enrich = [obj["pk"] for objs in objs_changed.values() for obj in objs]
+        query = RPSLDatabaseQuery(
+            ["pk", "prefix", "object_text", "object_class", "scopefilter_status", "route_preference_status"],
+            enable_ordering=False,
+        ).pks(pks_to_enrich)
+        rows_per_pk = {row["pk"]: row for row in self.database_handler.execute_query(query)}
 
         for rpsl_objs in objs_changed.values():
             for rpsl_obj in rpsl_objs:
-                rpsl_obj.update(rows_per_pk[rpsl_obj['pk']])
+                rpsl_obj.update(rows_per_pk[rpsl_obj["pk"]])
 
-        return objs_changed[RPKIStatus.valid], objs_changed[RPKIStatus.invalid], objs_changed[RPKIStatus.not_found]
+        return (
+            objs_changed[RPKIStatus.valid],
+            objs_changed[RPKIStatus.invalid],
+            objs_changed[RPKIStatus.not_found],
+        )
 
     def validate_route(self, prefix_ip: str, prefix_length: int, prefix_asn: int, source: str) -> RPKIStatus:
         """
         Validate a single route.
 
         A route is valid when at least one ROA is found that covers the prefix,
         with the same origin AS and a match on the max length in the ROA.
@@ -145,60 +154,60 @@
 
     def _build_roa_tree_from_roa_objs(self, roas: List[ROA]):
         """
         Build the tree of all ROAs from ROA objects.
         """
         for roa in roas:
             roa_tree = self.roa_tree6 if roa.prefix.version() == 6 else self.roa_tree4
-            key = roa.prefix.strBin()[:roa.prefix.prefixlen()]
+            key = roa.prefix.strBin()[: roa.prefix.prefixlen()]
             if key in roa_tree:
                 roa_tree[key].append((roa.prefix_str, roa.asn, roa.max_length))
             else:
                 roa_tree[key] = [(roa.prefix_str, roa.asn, roa.max_length)]
 
     def _build_roa_tree_from_db(self):
         """
         Build the tree of all ROAs from the DB.
         """
         roas = self.database_handler.execute_query(ROADatabaseObjectQuery())
         for roa in roas:
-            first_ip, length = roa['prefix'].split('/')
+            first_ip, length = roa["prefix"].split("/")
             ip_version, ip_bin_str = self._ip_to_binary_str(first_ip)
-            key = ip_bin_str[:int(length)]
+            key = ip_bin_str[: int(length)]
             roa_tree = self.roa_tree6 if ip_version == 6 else self.roa_tree4
             if key in roa_tree:
-                roa_tree[key].append((roa['prefix'], roa['asn'], roa['max_length']))
+                roa_tree[key].append((roa["prefix"], roa["asn"], roa["max_length"]))
             else:
-                roa_tree[key] = [(roa['prefix'], roa['asn'], roa['max_length'])]
+                roa_tree[key] = [(roa["prefix"], roa["asn"], roa["max_length"])]
 
     def _ip_to_binary_str(self, ip: str) -> Tuple[int, str]:
         """
         Convert an IP string to a binary string, e.g.
         192.0.2.139 to 11000000000000000000001010001011
         and return the IP version.
         """
-        address_family = socket.AF_INET6 if ':' in ip else socket.AF_INET
+        address_family = socket.AF_INET6 if ":" in ip else socket.AF_INET
         ip_bin = socket.inet_pton(address_family, ip)
-        ip_bin_str = ''.join([BYTE_BIN[b] for b in ip_bin]) + '0'
+        ip_bin_str = "".join([BYTE_BIN[b] for b in ip_bin]) + "0"
         ip_version = 6 if address_family == socket.AF_INET6 else 4
         return ip_version, ip_bin_str
 
 
 class SingleRouteROAValidator:
     def __init__(self, database_handler: DatabaseHandler):
         self.database_handler = database_handler
 
     def validate_route(self, route: IP, asn: int, source: str) -> RPKIStatus:
         """
         Validate a route from a particular source.
         """
-        if get_setting(f'sources.{source}.rpki_excluded'):
+        if get_setting(f"sources.{source}.rpki_excluded"):
             return RPKIStatus.not_found
 
         query = ROADatabaseObjectQuery().ip_less_specific_or_exact(route)
         roas_covering = list(self.database_handler.execute_query(query))
         if not roas_covering:
             return RPKIStatus.not_found
         for roa in roas_covering:
-            if roa['asn'] != 0 and roa['asn'] == asn and route.prefixlen() <= roa['max_length']:
+            if roa["asn"] != 0 and roa["asn"] == asn and route.prefixlen() <= roa["max_length"]:
                 return RPKIStatus.valid
         return RPKIStatus.invalid
```

### Comparing `irrd-4.2.8/irrd/rpsl/fields.py` & `irrd-4.3.0/irrd/rpsl/fields.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,35 +1,61 @@
 import datetime
 import re
-from typing import List, Type, Optional
+from typing import List, Optional, Type
 from urllib.parse import urlparse
 
 from IPy import IP
 
 from irrd.utils.text import clean_ip_value_error
-from irrd.utils.validators import parse_as_number, ValidationError
-from .config import PASSWORD_HASHERS
-from .parser_state import RPSLParserMessages, RPSLFieldParseResult
+from irrd.utils.validators import ValidationError, parse_as_number
+
+from .parser_state import RPSLFieldParseResult, RPSLParserMessages
+from .passwords import get_password_hashers
 
 # The IPv4/IPv6 regexes are for initial screening - not full validators
 
 re_ipv4_prefix = re.compile(r"^\d+\.\d+\.\d+\.\d+/\d+$")
 re_ipv6_prefix = re.compile(r"^[A-F\d:]+/\d+$", re.IGNORECASE)
 
 # This regex is not designed to catch every possible invalid variation,
 # but rather meant to protect against unintentional mistakes.
 #                         # Validate local-part           @ domain         | or IPv4 address        | or IPv6
-re_email = re.compile(r"^[A-Z0-9$!#%&\"*+\/=?^_`{|}~\\.-]+@(([A-Z0-9\\.-]+)|(\[\d+\.\d+\.\d+\.\d+\])|(\[[A-f\d:]+\]))$", re.IGNORECASE)
+re_email = re.compile(
+    r"^[A-Z0-9$!#%&\"*+\/=?^_`{|}~\\.-]+@(([A-Z0-9\\.-]+)|(\[\d+\.\d+\.\d+\.\d+\])|(\[[A-f\d:]+\]))$",
+    re.IGNORECASE,
+)
 
-re_range_operator = re.compile(r"^((\d{1,3}-\d{1,3})|(-)|(\+)|(\d{1,3}))$")
+re_range_operator = re.compile(r"^(?P<start>\d{1,3})-(?P<end>\d{1,3})$|^(-)$|^(\+)$|^(?P<single>\d{1,3})$")
 re_pgpkey = re.compile(r"^PGPKEY-[A-F0-9]{8}$")
-re_dnsname = re.compile(r"^(([A-Z0-9]|[A-Z0-9][A-Z0-9\-]*[A-Z0-9])\.)*([A-Z0-9]|[A-Z0-9][A-Z0-9\-]*[A-Z0-9])$", re.IGNORECASE)
+re_dnsname = re.compile(
+    r"^(([A-Z0-9]|[A-Z0-9][A-Z0-9\-]*[A-Z0-9])\.)*([A-Z0-9]|[A-Z0-9][A-Z0-9\-]*[A-Z0-9])$", re.IGNORECASE
+)
 re_generic_name = re.compile(r"^[A-Z][A-Z0-9_-]*[A-Z0-9]$", re.IGNORECASE)
-reserved_words = ["ANY", "AS-ANY", "RS_ANY", "PEERAS", "AND", "OR", "NOT", "ATOMIC", "FROM", "TO", "AT", "ACTION",
-                  "ACCEPT", "ANNOUNCE", "EXCEPT", "REFINE", "NETWORKS", "INTO", "INBOUND", "OUTBOUND"]
+reserved_words = [
+    "ANY",
+    "AS-ANY",
+    "RS_ANY",
+    "PEERAS",
+    "AND",
+    "OR",
+    "NOT",
+    "ATOMIC",
+    "FROM",
+    "TO",
+    "AT",
+    "ACTION",
+    "ACCEPT",
+    "ANNOUNCE",
+    "EXCEPT",
+    "REFINE",
+    "NETWORKS",
+    "INTO",
+    "INBOUND",
+    "OUTBOUND",
+]
 reserved_prefixes = ["AS-", "RS-", "RTRS-", "FLTR-", "PRNG-"]
 
 """
 Fields for RPSL data.
 
 Note that these objects are instantiated once per attribute during RPSL object
 class loading. Therefore, the instances will be shared between different RPSL
@@ -53,269 +79,330 @@
     modified for this. As parsed_data is indexed in the databases, this is important for searches,
     to match e.g. 'mntner: FOO' to 'mnt-by: foo' - as these values are equivalent.
 
     The extracts property defines which fields from RPSLFieldParseResult are extracted
     by a field, other than the value. The parser only consider these extractions
     for primary key or lookup key fields.
     """
+
     keep_case = True
     extracts: List[str] = []
 
-    def __init__(self, optional: bool=False, multiple: bool=False, primary_key: bool=False, lookup_key: bool=False) -> None:
+    def __init__(
+        self,
+        optional: bool = False,
+        multiple: bool = False,
+        primary_key: bool = False,
+        lookup_key: bool = False,
+    ) -> None:
         self.optional = optional
         self.multiple = multiple
         self.primary_key = primary_key
         self.lookup_key = lookup_key
 
-    def parse(self, value: str, messages: RPSLParserMessages, strict_validation=True) -> Optional[RPSLFieldParseResult]:
+    def parse(
+        self, value: str, messages: RPSLParserMessages, strict_validation=True
+    ) -> Optional[RPSLFieldParseResult]:
         return RPSLFieldParseResult(value)
 
 
 class RPSLFieldListMixin:
     """
     Mixin to allow fields to support list values, like 'AS1, AS2, AS3'.
 
     For example, if you have an RPSLASNumberField that validates a single
     AS number, you can create RPSLASNumbersField that allows a list of
     AS numbers by creating:
 
         class RPSLASNumbersField(RPSLFieldListMixin, RPSLASNumberField):
             pass
     """
-    def parse(self, value: str, messages: RPSLParserMessages, strict_validation=True) -> Optional[RPSLFieldParseResult]:
+
+    def parse(
+        self, value: str, messages: RPSLParserMessages, strict_validation=True
+    ) -> Optional[RPSLFieldParseResult]:
         parse_results = []
-        for single_value in value.split(','):
+        for single_value in value.split(","):
             single_value = single_value.strip()
             if single_value:
                 parse_result = super().parse(single_value, messages, strict_validation)  # type: ignore
                 parse_results.append(parse_result)
         if not all(parse_results):
             return None
         values = [result.value for result in parse_results]
-        return RPSLFieldParseResult(','.join(values), values_list=values)
+        return RPSLFieldParseResult(",".join(values), values_list=values)
 
 
 class RPSLIPv4PrefixField(RPSLTextField):
     """Field for a single IPv4 prefix."""
-    extracts = ['ip_first', 'ip_last', 'prefix', 'prefix_length']
 
-    def parse(self, value: str, messages: RPSLParserMessages, strict_validation=True) -> Optional[RPSLFieldParseResult]:
+    extracts = ["ip_first", "ip_last", "prefix", "prefix_length"]
+
+    def parse(
+        self, value: str, messages: RPSLParserMessages, strict_validation=True
+    ) -> Optional[RPSLFieldParseResult]:
         if not re_ipv4_prefix.match(value):
-            messages.error(f'Invalid address prefix: {value}')
+            messages.error(f"Invalid address prefix: {value}")
             return None
 
         try:
             ip = IP(value, ipversion=4)
         except ValueError as ve:
             clean_error = clean_ip_value_error(ve)
-            messages.error(f'Invalid address prefix: {value}: {clean_error}')
+            messages.error(f"Invalid address prefix: {value}: {clean_error}")
             return None
 
         parsed_ip_str = str(ip)
         if ip.prefixlen() == 32:
-            parsed_ip_str += '/32'
+            parsed_ip_str += "/32"
         if parsed_ip_str != value:
-            messages.info(f'Address prefix {value} was reformatted as {parsed_ip_str}')
-        return RPSLFieldParseResult(parsed_ip_str, ip_first=ip.net(), ip_last=ip.broadcast(),
-                                    prefix=ip, prefix_length=ip.prefixlen())
+            messages.info(f"Address prefix {value} was reformatted as {parsed_ip_str}")
+        return RPSLFieldParseResult(
+            parsed_ip_str, ip_first=ip.net(), ip_last=ip.broadcast(), prefix=ip, prefix_length=ip.prefixlen()
+        )
 
 
 class RPSLIPv4PrefixesField(RPSLFieldListMixin, RPSLIPv4PrefixField):
     """Field for a comma-separated list of IPv4 prefixes."""
+
     pass
 
 
 class RPSLIPv6PrefixField(RPSLTextField):
     """Field for a single IPv6 prefix."""
-    extracts = ['ip_first', 'ip_last', 'prefix', 'prefix_length']
 
-    def parse(self, value: str, messages: RPSLParserMessages, strict_validation=True) -> Optional[RPSLFieldParseResult]:
+    extracts = ["ip_first", "ip_last", "prefix", "prefix_length"]
+
+    def parse(
+        self, value: str, messages: RPSLParserMessages, strict_validation=True
+    ) -> Optional[RPSLFieldParseResult]:
         if not re_ipv6_prefix.match(value):
-            messages.error(f'Invalid address prefix: {value}')
+            messages.error(f"Invalid address prefix: {value}")
             return None
 
         try:
             ip = IP(value, ipversion=6)
         except ValueError as ve:
             clean_error = clean_ip_value_error(ve)
-            messages.error(f'Invalid address prefix: {value}: {clean_error}')
+            messages.error(f"Invalid address prefix: {value}: {clean_error}")
             return None
 
         parsed_ip_str = str(ip)
         if ip.prefixlen() == 128:
-            parsed_ip_str += '/128'
+            parsed_ip_str += "/128"
         if parsed_ip_str != value:
-            messages.info(f'Address prefix {value} was reformatted as {parsed_ip_str}')
-        return RPSLFieldParseResult(parsed_ip_str, ip_first=ip.net(), ip_last=ip.broadcast(),
-                                    prefix=ip, prefix_length=ip.prefixlen())
+            messages.info(f"Address prefix {value} was reformatted as {parsed_ip_str}")
+        return RPSLFieldParseResult(
+            parsed_ip_str, ip_first=ip.net(), ip_last=ip.broadcast(), prefix=ip, prefix_length=ip.prefixlen()
+        )
 
 
 class RPSLIPv6PrefixesField(RPSLFieldListMixin, RPSLIPv6PrefixField):
     """Field for a comma-separated list of IPv6 prefixes."""
+
     pass
 
 
 class RPSLIPv4AddressRangeField(RPSLTextField):
     """
     Field for a range of IPv4 addresses, as used in inetnum keys.
 
     Note that a single IP address is also valid, and that the range does
     not have to align to bitwise boundaries of prefixes.
     """
-    extracts = ['ip_first', 'ip_last']
 
-    def parse(self, value: str, messages: RPSLParserMessages, strict_validation=True) -> Optional[RPSLFieldParseResult]:
-        value = value.replace(',', '')  # #311, process multiline PK correctly
-        if '-' in value:
-            ip1_input, ip2_input = value.split('-', 1)
+    extracts = ["ip_first", "ip_last"]
+
+    def parse(
+        self, value: str, messages: RPSLParserMessages, strict_validation=True
+    ) -> Optional[RPSLFieldParseResult]:
+        value = value.replace(",", "")  # #311, process multiline PK correctly
+        if "-" in value:
+            ip1_input, ip2_input = value.split("-", 1)
         else:
             ip1_input = ip2_input = value
 
         try:
             ip1 = IP(ip1_input)
             ip2 = IP(ip2_input)
         except ValueError as ve:
             clean_error = clean_ip_value_error(ve)
-            messages.error(f'Invalid address range: {value}: {clean_error}')
+            messages.error(f"Invalid address range: {value}: {clean_error}")
             return None
 
         if not ip1.version() == ip2.version() == 4:
-            messages.error(f'Invalid address range: {value}: IP version mismatch')
+            messages.error(f"Invalid address range: {value}: IP version mismatch")
             return None
         if ip1.int() > ip2.int():
-            messages.error(f'Invalid address range: {value}: first IP is higher than second IP')
+            messages.error(f"Invalid address range: {value}: first IP is higher than second IP")
             return None
 
-        if '-' in value:
-            parsed_value = f'{ip1} - {ip2}'
+        if "-" in value:
+            parsed_value = f"{ip1} - {ip2}"
         else:
             parsed_value = str(ip1)
         if parsed_value != value:
-            messages.info(f'Address range {value} was reformatted as {parsed_value}')
+            messages.info(f"Address range {value} was reformatted as {parsed_value}")
         return RPSLFieldParseResult(parsed_value, ip_first=ip1, ip_last=ip2)
 
 
 class RPSLRouteSetMemberField(RPSLTextField):
     """
     Field for the members of a route-set. These can be:
         - A valid name for another route set.
         - A valid IPv4 or IPv6 (depending on ip_version) prefix
         - A valid prefix followed by:
           - ^-
           - ^+
           - ^[integer]
           - ^[integer]-[integer]
     """
+
     keep_case = True
 
     def __init__(self, ip_version: Optional[int], *args, **kwargs) -> None:
         if ip_version and ip_version not in [4, 6]:
-            raise ValueError(f'Invalid IP version: {ip_version}')
+            raise ValueError(f"Invalid IP version: {ip_version}")
         self.ip_version = ip_version
         super().__init__(*args, **kwargs)
 
-    def parse(self, value: str, messages: RPSLParserMessages, strict_validation=True) -> Optional[RPSLFieldParseResult]:
-        if '^' in value:
-            address, range_operator = value.split('^', maxsplit=1)
+    def parse(
+        self, value: str, messages: RPSLParserMessages, strict_validation=True
+    ) -> Optional[RPSLFieldParseResult]:
+        if "^" in value:
+            address, range_operator = value.split("^", maxsplit=1)
             if not range_operator:
-                messages.error(f'Missing range operator in value: {value}')
+                messages.error(f"Missing range operator in value: {value}")
                 return None
         else:
             address = value
-            range_operator = ''
+            range_operator = ""
 
         parse_set_result_messages = RPSLParserMessages()
-        parse_set_result = parse_set_name(['RS-', 'AS-'], address, parse_set_result_messages, strict_validation)
+        parse_set_result = parse_set_name(
+            ["RS-", "AS-"], address, parse_set_result_messages, strict_validation
+        )
         if parse_set_result and not parse_set_result_messages.errors():
             result_value = parse_set_result.value
             if range_operator:
-                result_value += '^' + range_operator
+                result_value += "^" + range_operator
             if result_value != value:
-                messages.info(f'Route set member {value} was reformatted as {result_value}')
+                messages.info(f"Route set member {value} was reformatted as {result_value}")
             return RPSLFieldParseResult(value=result_value)
         try:
             parsed_str, parsed_int = parse_as_number(address)
             result_value = parsed_str
             if range_operator:
-                result_value += '^' + range_operator
+                result_value += "^" + range_operator
             if result_value != value:
-                messages.info(f'Route set member {value} was reformatted as {result_value}')
+                messages.info(f"Route set member {value} was reformatted as {result_value}")
             return RPSLFieldParseResult(value=result_value)
         except ValidationError:
             pass
 
         try:
             ip_version = self.ip_version if self.ip_version else 0
             ip = IP(address, ipversion=ip_version)
         except ValueError as ve:
             clean_error = clean_ip_value_error(ve)
-            messages.error(f'Value is neither a valid set name nor a valid prefix: {address}: {clean_error}')
+            messages.error(f"Value is neither a valid set name nor a valid prefix: {address}: {clean_error}")
             return None
 
-        if range_operator and not re_range_operator.match(range_operator):
-            messages.error(f'Invalid range operator {range_operator} in value: {value}')
-            return None
+        if range_operator:
+            range_operator_match = re_range_operator.match(range_operator)
+            if not range_operator_match:
+                messages.error(f"Invalid range operator {range_operator} in value: {value}")
+                return None
+
+            single_range = range_operator_match.group("single")
+            if single_range and int(single_range) < ip.prefixlen():
+                messages.error(
+                    f"Invalid range operator: operator length ({single_range}) must be equal "
+                    f"to or longer than prefix length ({ip.prefixlen()}) {value}"
+                )
+                return None
+
+            start_range = range_operator_match.group("start")
+            end_range = range_operator_match.group("end")
+            if start_range and int(start_range) < ip.prefixlen():
+                messages.error(
+                    f"Invalid range operator: operator start ({start_range}) must be equal "
+                    f"to or longer than prefix length ({ip.prefixlen()}) {value}"
+                )
+                return None
+            if end_range and int(end_range) < int(start_range):
+                messages.error(
+                    f"Invalid range operator: operator end ({end_range}) must be equal "
+                    f"to or longer than operator start ({start_range}) {value}"
+                )
+                return None
 
         parsed_ip_str = str(ip)
         if ip.version() == 4 and ip.prefixlen() == 32:
-            parsed_ip_str += '/32'
+            parsed_ip_str += "/32"
         if ip.version() == 6 and ip.prefixlen() == 128:
-            parsed_ip_str += '/128'
+            parsed_ip_str += "/128"
         if range_operator:
-            parsed_ip_str += '^' + range_operator
+            parsed_ip_str += "^" + range_operator
 
         if parsed_ip_str != value:
-            messages.info(f'Route set member {value} was reformatted as {parsed_ip_str}')
+            messages.info(f"Route set member {value} was reformatted as {parsed_ip_str}")
         return RPSLFieldParseResult(parsed_ip_str)
 
 
 class RPSLRouteSetMembersField(RPSLFieldListMixin, RPSLRouteSetMemberField):
     pass
 
 
 class RPSLASNumberField(RPSLTextField):
     """Field for a single AS number (in ASxxxx syntax)."""
-    extracts = ['asn']
 
-    def parse(self, value: str, messages: RPSLParserMessages, strict_validation=True) -> Optional[RPSLFieldParseResult]:
+    extracts = ["asn"]
+
+    def parse(
+        self, value: str, messages: RPSLParserMessages, strict_validation=True
+    ) -> Optional[RPSLFieldParseResult]:
         try:
             parsed_str, parsed_int = parse_as_number(value)
         except ValidationError as ve:
             messages.error(str(ve))
             return None
         if parsed_str and parsed_str.upper() != value.upper():
-            messages.info(f'AS number {value} was reformatted as {parsed_str}')
+            messages.info(f"AS number {value} was reformatted as {parsed_str}")
         return RPSLFieldParseResult(parsed_str, asn_first=parsed_int, asn_last=parsed_int)
 
 
 class RPSLASBlockField(RPSLTextField):
     """Field for a block of AS numbers, e.g. AS1 - AS5."""
-    extracts = ['asn_first', 'asn_last']
 
-    def parse(self, value: str, messages: RPSLParserMessages, strict_validation=True) -> Optional[RPSLFieldParseResult]:
-        if '-' not in value:
-            messages.error(f'Invalid AS range: {value}: does not contain a hyphen')
+    extracts = ["asn_first", "asn_last"]
+
+    def parse(
+        self, value: str, messages: RPSLParserMessages, strict_validation=True
+    ) -> Optional[RPSLFieldParseResult]:
+        if "-" not in value:
+            messages.error(f"Invalid AS range: {value}: does not contain a hyphen")
             return None
 
-        as1_raw, as2_raw = map(str.strip, value.split('-', 1))
+        as1_raw, as2_raw = map(str.strip, value.split("-", 1))
 
         try:
             as1_str, as1_int = parse_as_number(as1_raw)
             as2_str, as2_int = parse_as_number(as2_raw)
         except ValidationError as ve:
             messages.error(str(ve))
             return None
 
         if as1_int > as2_int:  # type: ignore
-            messages.error(f'Invalid AS range: {value}: first AS is higher then second AS')
+            messages.error(f"Invalid AS range: {value}: first AS is higher then second AS")
             return None
 
-        parsed_value = f'{as1_str} - {as2_str}'
+        parsed_value = f"{as1_str} - {as2_str}"
         if parsed_value != value:
-            messages.info(f'AS range {value} was reformatted as {parsed_value}')
+            messages.info(f"AS range {value} was reformatted as {parsed_value}")
         return RPSLFieldParseResult(parsed_value, asn_first=as1_int, asn_last=as2_int)
 
 
 class RPSLSetNameField(RPSLTextField):
     """
     Field for set names, i.e. names of objects like route-set, prefix-set, etc.
 
@@ -324,72 +411,87 @@
     Set names can consist of multiple components, e.g. AS65537:RS-FOO. Each
     component must be a valid set name or valid AS number, and one component
     must be a valid set name for this specific set, i.e. start with the given prefix.
 
     The prefix provided is the expected prefix of the set name, e.g. 'RS' for
     a route-set, or 'AS' for an as-set.
     """
+
     keep_case = False
 
     def __init__(self, prefix: str, *args, **kwargs) -> None:
-        self.prefix = prefix + '-'
+        self.prefix = prefix + "-"
         super().__init__(*args, **kwargs)
 
-    def parse(self, value: str, messages: RPSLParserMessages, strict_validation=True) -> Optional[RPSLFieldParseResult]:
+    def parse(
+        self, value: str, messages: RPSLParserMessages, strict_validation=True
+    ) -> Optional[RPSLFieldParseResult]:
         return parse_set_name([self.prefix], value, messages, strict_validation)
 
 
 class RPSLEmailField(RPSLTextField):
     """Field for an e-mail address. Only performs basic validation."""
-    def parse(self, value: str, messages: RPSLParserMessages, strict_validation=True) -> Optional[RPSLFieldParseResult]:
+
+    def parse(
+        self, value: str, messages: RPSLParserMessages, strict_validation=True
+    ) -> Optional[RPSLFieldParseResult]:
         if not re_email.match(value):
-            messages.error(f'Invalid e-mail address: {value}')
+            messages.error(f"Invalid e-mail address: {value}")
             return None
         return RPSLFieldParseResult(value)
 
 
 class RPSLChangedField(RPSLTextField):
     """Field for an changed line. Only performs basic validation for email."""
-    def parse(self, value: str, messages: RPSLParserMessages, strict_validation=True) -> Optional[RPSLFieldParseResult]:
+
+    def parse(
+        self, value: str, messages: RPSLParserMessages, strict_validation=True
+    ) -> Optional[RPSLFieldParseResult]:
         date: Optional[str]
         try:
-            email, date = value.split(' ')
+            email, date = value.split(" ")
         except ValueError:
             email = value
             date = None
 
         if not re_email.match(email):
-            messages.error(f'Invalid e-mail address: {email}')
+            messages.error(f"Invalid e-mail address: {email}")
             return None
         if date:
             try:
-                datetime.datetime.strptime(date, '%Y%m%d')
+                datetime.datetime.strptime(date, "%Y%m%d")
             except ValueError as ve:
-                messages.error(f'Invalid changed date: {date}: {ve}')
+                messages.error(f"Invalid changed date: {date}: {ve}")
                 return None
         return RPSLFieldParseResult(value)
 
 
 class RPSLDNSNameField(RPSLTextField):
     """Field for a DNS name, as used in e.g. inet-rtr names."""
+
     keep_case = False
 
-    def parse(self, value: str, messages: RPSLParserMessages, strict_validation=True) -> Optional[RPSLFieldParseResult]:
+    def parse(
+        self, value: str, messages: RPSLParserMessages, strict_validation=True
+    ) -> Optional[RPSLFieldParseResult]:
         if not re_dnsname.match(value):
-            messages.error(f'Invalid DNS name: {value}')
+            messages.error(f"Invalid DNS name: {value}")
             return None
         return RPSLFieldParseResult(value)
 
 
 class RPSLURLField(RPSLTextField):
     """Field for a URL, as used in e.g. geofeed attribute."""
+
     keep_case = False
-    permitted_schemes = ['http', 'https']
+    permitted_schemes = ["http", "https"]
 
-    def parse(self, value: str, messages: RPSLParserMessages, strict_validation=True) -> Optional[RPSLFieldParseResult]:
+    def parse(
+        self, value: str, messages: RPSLParserMessages, strict_validation=True
+    ) -> Optional[RPSLFieldParseResult]:
         result = urlparse(value)
         if all([result.scheme in self.permitted_schemes, result.netloc]):
             return RPSLFieldParseResult(value)
         messages.error(f'Invalid {"/".join(self.permitted_schemes)} URL: {value}')
         return None
 
 
@@ -403,42 +505,49 @@
     For example, this is used for the as-name attribute of an aut-num, as they
     are allowed to start with 'AS'.
 
     If non_strict_allow_any is set, the parser will allow any value if strict_validation
     is disabled. This is needed on nic-hdl for legacy reasons -
     see https://github.com/irrdnet/irrd/issues/60
     """
+
     keep_case = False
 
-    def __init__(self, allowed_prefixes: List[str]=None, non_strict_allow_any=False, *args, **kwargs) -> None:
+    def __init__(
+        self, allowed_prefixes: Optional[List[str]] = None, non_strict_allow_any=False, *args, **kwargs
+    ) -> None:
         self.non_strict_allow_any = non_strict_allow_any
         if allowed_prefixes:
-            self.allowed_prefixes = [prefix.upper() + '-' for prefix in allowed_prefixes]
+            self.allowed_prefixes = [prefix.upper() + "-" for prefix in allowed_prefixes]
         else:
             self.allowed_prefixes = []
         super().__init__(*args, **kwargs)
 
-    def parse(self, value: str, messages: RPSLParserMessages, strict_validation=True) -> Optional[RPSLFieldParseResult]:
+    def parse(
+        self, value: str, messages: RPSLParserMessages, strict_validation=True
+    ) -> Optional[RPSLFieldParseResult]:
         if not strict_validation and self.non_strict_allow_any:
             return RPSLFieldParseResult(value)
 
         if strict_validation:
             upper_value = value.upper()
             if upper_value in reserved_words:
-                messages.error(f'Invalid name: {value}: this is a reserved word')
+                messages.error(f"Invalid name: {value}: this is a reserved word")
                 return None
 
             for prefix in reserved_prefixes:
                 if upper_value.startswith(prefix) and prefix not in self.allowed_prefixes:
-                    messages.error(f'Invalid name: {value}: {prefix} is a reserved prefix')
+                    messages.error(f"Invalid name: {value}: {prefix} is a reserved prefix")
                     return None
 
         if not re_generic_name.match(value):
-            messages.error(f'Invalid name: {value}: contains invalid characters, does not start with a letter, '
-                           f'or does not end in a letter/digit')
+            messages.error(
+                f"Invalid name: {value}: contains invalid characters, does not start with a letter, "
+                "or does not end in a letter/digit"
+            )
             return None
         return RPSLFieldParseResult(value)
 
 
 class RPSLReferenceField(RPSLTextField):
     """
     Field for a reference to another field.
@@ -451,25 +560,29 @@
     the value must refer to one of these objects (e.g. tech-c can refer to
     role or person).
 
     If the references are strong, this reference is included in reference checks
     on updates, i.e. adding an object with a strong reference to another object
     that does not exist, is a validation failure.
     """
+
     keep_case = False
 
     def __init__(self, referring: List[str], strong=True, *args, **kwargs) -> None:
         from .parser import RPSLObject
+
         self.referring = referring
         self.strong = strong
         self.referring_object_classes: List[Type[RPSLObject]] = []
         self.referring_identifier_fields: List[RPSLTextField] = []
         super().__init__(*args, **kwargs)
 
-    def parse(self, value: str, messages: RPSLParserMessages, strict_validation=True) -> Optional[RPSLFieldParseResult]:
+    def parse(
+        self, value: str, messages: RPSLParserMessages, strict_validation=True
+    ) -> Optional[RPSLFieldParseResult]:
         if not self.referring_identifier_fields:
             self.resolve_references()
 
         referring_field_messages = RPSLParserMessages()
         for identifier_field in self.referring_identifier_fields:
             if identifier_field:
                 parsed_value = identifier_field.parse(value, referring_field_messages, strict_validation)
@@ -477,88 +590,106 @@
                     return parsed_value
 
         messages.merge_messages(referring_field_messages)
         return None
 
     def resolve_references(self):
         from .rpsl_objects import OBJECT_CLASS_MAPPING
+
         for ref in self.referring:
             rpsl_object_class = OBJECT_CLASS_MAPPING[ref]
-            pk_field = [field for field in rpsl_object_class.fields.values() if field.primary_key and field.lookup_key][0]
+            pk_field = [
+                field for field in rpsl_object_class.fields.values() if field.primary_key and field.lookup_key
+            ][0]
             self.referring_object_classes.append(rpsl_object_class)
             self.referring_identifier_fields.append(pk_field)
 
 
 class RPSLReferenceListField(RPSLFieldListMixin, RPSLReferenceField):
     """
     Field for a comma-seperated list of references to another field.
 
     Optionally, ANY can be allowed as a valid option too, instead of a list.
     """
-    def __init__(self, allow_kw_any: bool=False, *args, **kwargs) -> None:
+
+    def __init__(self, allow_kw_any: bool = False, *args, **kwargs) -> None:
         self.allow_kw_any = allow_kw_any
         super().__init__(*args, **kwargs)
 
-    def parse(self, value: str, messages: RPSLParserMessages, strict_validation=True) -> Optional[RPSLFieldParseResult]:
-        if self.allow_kw_any and value.upper() == 'ANY':
-            return RPSLFieldParseResult('ANY', values_list=['ANY'])
+    def parse(
+        self, value: str, messages: RPSLParserMessages, strict_validation=True
+    ) -> Optional[RPSLFieldParseResult]:
+        if self.allow_kw_any and value.upper() == "ANY":
+            return RPSLFieldParseResult("ANY", values_list=["ANY"])
         return super().parse(value, messages, strict_validation)
 
 
 class RPSLAuthField(RPSLTextField):
     """Field for the auth attribute of a mntner."""
-    def parse(self, value: str, messages: RPSLParserMessages, strict_validation=True) -> Optional[RPSLFieldParseResult]:
-        valid_beginnings = [hasher + ' ' for hasher in PASSWORD_HASHERS.keys()]
+
+    def parse(
+        self, value: str, messages: RPSLParserMessages, strict_validation=True
+    ) -> Optional[RPSLFieldParseResult]:
+        hashers = get_password_hashers(permit_legacy=not strict_validation)
+        valid_beginnings = [hasher + " " for hasher in hashers.keys()]
         has_valid_beginning = any(value.upper().startswith(b) for b in valid_beginnings)
-        is_valid_hash = has_valid_beginning and value.count(' ') == 1 and not value.count(',')
+        is_valid_hash = has_valid_beginning and value.count(" ") == 1 and not value.count(",")
         if is_valid_hash or re_pgpkey.match(value.upper()):
             return RPSLFieldParseResult(value)
 
-        hashers = ', '.join(PASSWORD_HASHERS.keys())
-        messages.error(f'Invalid auth attribute: {value}: supported options are {hashers} and PGPKEY-xxxxxxxx')
+        hashers = ", ".join(hashers.keys())
+        messages.error(
+            f"Invalid auth attribute: {value}: supported options are {hashers} and PGPKEY-xxxxxxxx"
+        )
         return None
 
 
-def parse_set_name(prefixes: List[str], value: str, messages: RPSLParserMessages, strict_validation=True) -> Optional[RPSLFieldParseResult]:
+def parse_set_name(
+    prefixes: List[str], value: str, messages: RPSLParserMessages, strict_validation=True
+) -> Optional[RPSLFieldParseResult]:
     assert all([prefix in reserved_prefixes for prefix in prefixes])
-    input_components = value.split(':')
+    input_components = value.split(":")
     output_components: List[str] = []
-    prefix_display = '/'.join(prefixes)
+    prefix_display = "/".join(prefixes)
 
     if strict_validation and len(input_components) > 5:
-        messages.error('Set names can have a maximum of five components.')
+        messages.error("Set names can have a maximum of five components.")
         return None
 
     if strict_validation and not any([c.upper().startswith(tuple(prefixes)) for c in input_components]):
-        messages.error(f'Invalid set name {value}: at least one component must be '
-                       f'an actual set name (i.e. start with {prefix_display})')
+        messages.error(
+            f"Invalid set name {value}: at least one component must be "
+            f"an actual set name (i.e. start with {prefix_display})"
+        )
         return None
 
     for component in input_components:
         if strict_validation and component.upper() in reserved_words:
-            messages.error(f'Invalid set name {value}: component {component} is a reserved word')
+            messages.error(f"Invalid set name {value}: component {component} is a reserved word")
             return None
 
         parsed_as_number = None
         try:
             parsed_as_number, _ = parse_as_number(component)
         except ValidationError:
             pass
         if not re_generic_name.match(component.upper()) and not parsed_as_number:
             messages.error(
-                f'Invalid set {value}: component {component} is not a valid AS number nor a valid set name'
+                f"Invalid set {value}: component {component} is not a valid AS number nor a valid set name"
             )
             return None
         if strict_validation and not parsed_as_number and not component.upper().startswith(tuple(prefixes)):
-            messages.error(f'Invalid set {value}: component {component} is not a valid AS number, '
-                           f'nor does it start with {prefix_display}')
+            messages.error(
+                f"Invalid set {value}: component {component} is not a valid AS number, "
+                f"nor does it start with {prefix_display}"
+            )
             return None
 
         if parsed_as_number:
             output_components.append(parsed_as_number)
         else:
             output_components.append(component)
 
-    parsed_value = ':'.join(output_components)
+    parsed_value = ":".join(output_components)
     if parsed_value != value:
-        messages.info(f'Set name {value} was reformatted as {parsed_value}')
+        messages.info(f"Set name {value} was reformatted as {parsed_value}")
     return RPSLFieldParseResult(parsed_value)
```

### Comparing `irrd-4.2.8/irrd/rpsl/parser.py` & `irrd-4.3.0/irrd/rpsl/parser.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,90 +1,106 @@
 import datetime
 import itertools
 import json
 import re
-from collections import OrderedDict, Counter
-from typing import Dict, List, Optional, Tuple, Any, Set
+from collections import Counter, OrderedDict
+from typing import Any, Dict, List, Optional, Set, Tuple
 
 from IPy import IP
 
+from irrd.routepref.status import RoutePreferenceStatus
 from irrd.rpki.status import RPKIStatus
 from irrd.rpsl.parser_state import RPSLParserMessages
 from irrd.scopefilter.status import ScopeFilterStatus
 from irrd.utils.text import splitline_unicodesafe
-from .fields import RPSLTextField
+
 from ..conf import get_setting
+from .fields import RPSLTextField
 
 RPSL_ATTRIBUTE_TEXT_WIDTH = 16
 TypeRPSLObjectData = List[Tuple[str, str, List[str]]]
 
 
 class RPSLObjectMeta(type):
     """
     Meta class for RPSLObject (and all subclasses) for performance enhancement.
 
     As RPSLObject is instantiated once per object parsed, __init__ should be
     kept as small as possible. This metaclass pre-calculates some derived data
     from the fields defined by a subclass of RPSLObject, for optimised parsing speed.
     """
+
     def __init__(cls, name, bases, clsdict):  # noqa: N805
         super().__init__(name, bases, clsdict)
-        fields = clsdict.get('fields')
+        fields = clsdict.get("fields")
         if fields:
             cls.rpsl_object_class = list(fields.keys())[0]
             cls.pk_fields = [field[0] for field in fields.items() if field[1].primary_key]
             cls.lookup_fields = [field[0] for field in fields.items() if field[1].lookup_key]
             cls.attrs_allowed = [field[0] for field in fields.items()]
             cls.attrs_required = [field[0] for field in fields.items() if not field[1].optional]
             cls.attrs_multiple = [field[0] for field in fields.items() if field[1].multiple]
-            cls.field_extracts = list(itertools.chain(
-                *[field[1].extracts for field in fields.items() if field[1].primary_key or field[1].lookup_key]
-            ))
-            cls.referring_strong_fields = [(field[0], field[1].referring) for field in fields.items() if hasattr(field[1], 'referring') and getattr(field[1], 'strong')]
+            cls.field_extracts = list(
+                itertools.chain(
+                    *[
+                        field[1].extracts
+                        for field in fields.items()
+                        if field[1].primary_key or field[1].lookup_key
+                    ]
+                )
+            )
+            cls.referring_strong_fields = [
+                (field[0], field[1].referring)
+                for field in fields.items()
+                if hasattr(field[1], "referring") and getattr(field[1], "strong")
+            ]
 
 
 class RPSLObject(metaclass=RPSLObjectMeta):
     """
     Base class for RPSL objects.
 
     To clean an RPSL object in string form, the best option is not to instance
     this or a subclass, but instead call rpsl_object_from_text() which
     automatically derives the correct class.
 
     This class should not be instanced directly - instead subclasses should be
     made for each RPSL type with the appropriate fields defined. Note that any
     subclasses should also be added to OBJECT_CLASS_MAPPING.
     """
+
     fields: Dict[str, RPSLTextField] = OrderedDict()
     rpsl_object_class: str
     pk_fields: List[str] = []
     attrs_allowed: List[str] = []
     attrs_required: List[str] = []
     attrs_multiple: List[str] = []
     ip_first: IP = None
     ip_last: IP = None
     asn_first: Optional[int] = None
     asn_last: Optional[int] = None
     prefix: IP = None
     prefix_length: Optional[int] = None
     rpki_status: RPKIStatus = RPKIStatus.not_found
     scopefilter_status: ScopeFilterStatus = ScopeFilterStatus.in_scope
+    route_preference_status: RoutePreferenceStatus = RoutePreferenceStatus.visible
+    pk_asn_segment: Optional[str] = None
     default_source: Optional[str] = None  # noqa: E704 (flake8 bug)
-    # Whether this object has a relation to RPKI ROA data, and therefore RPKI
-    # checks should be performed in certain scenarios. Enabled for route/route6.
-    rpki_relevant = False
+    # Shortcut for whether this object is a route-like object, and therefore
+    # should be included in RPKI and route preference status. Enabled for route/route6.
+    is_route = False
     # Fields whose values are discarded during parsing
     discarded_fields: List[str] = []
     # Fields that are ignored in validation even
     # for authoritative objects (see #587 for example).
-    ignored_validation_fields: List[str] = ['last-modified']
+    ignored_validation_fields: List[str] = ["last-modified"]
 
-    _re_attr_name = re.compile(r'^[a-z0-9_-]+$')
+    _re_attr_name = re.compile(r"^[a-z0-9_-]+$")
 
-    def __init__(self, from_text: Optional[str]=None, strict_validation=True, default_source=None) -> None:
+    def __init__(self, from_text: Optional[str] = None, strict_validation=True, default_source=None) -> None:
         """
         Create a new RPSL object, optionally instantiated from a string.
 
         Optionally, you can set/unset strict validation. This means all
         attribute values are validated, and attribute presence/absence is
         verified. Non-strict validation is limited to primary and lookup
         keys.
@@ -106,22 +122,22 @@
     def pk(self) -> str:
         """Get the primary key value of an RPSL object. The PK is always converted to uppercase."""
         if len(self.pk_fields) == 1:
             return self.parsed_data.get(self.pk_fields[0], "").upper()
         composite_values = []
         for field in self.pk_fields:
             composite_values.append(self.parsed_data.get(field, ""))
-        return ''.join(composite_values).upper()
+        return "".join(composite_values).upper()
 
     def source(self) -> str:
         """Shortcut to retrieve object source"""
         try:
-            return self.parsed_data['source']
+            return self.parsed_data["source"]
         except KeyError:
-            raise ValueError('RPSL object has no known source')
+            raise ValueError("RPSL object has no known source")
 
     def ip_version(self) -> Optional[int]:
         """
         Get the IP version to which this object relates, or None for
         e.g. person or as-block objects.
         """
         if self.ip_first:
@@ -154,72 +170,73 @@
         """
         Get a set of field names under which other objects refer to
         this object. E.g. for a person object, this would typically
         return {'zone-c', 'admin-c', 'tech-c'}.
         """
         result = set()
         from irrd.rpsl.rpsl_objects import OBJECT_CLASS_MAPPING
+
         for rpsl_object in OBJECT_CLASS_MAPPING.values():
             for field_name, field in rpsl_object.fields.items():
-                if self.rpsl_object_class in getattr(field, 'referring', []) and getattr(field, 'strong'):
+                if self.rpsl_object_class in getattr(field, "referring", []) and getattr(field, "strong"):
                     result.add(field_name)
         return result
 
-    def render_rpsl_text(self, last_modified: datetime.datetime=None) -> str:
+    def render_rpsl_text(self, last_modified: Optional[datetime.datetime] = None) -> str:
         """
         Render the RPSL object as an RPSL string.
         If last_modified is provided, removes existing last-modified:
         attributes and adds a new one with that timestamp, if self.source()
         is authoritative.
         """
         output = ""
-        authoritative = get_setting(f'sources.{self.source()}.authoritative')
+        authoritative = get_setting(f"sources.{self.source()}.authoritative")
         for attr, value, continuation_chars in self._object_data:
-            if authoritative and last_modified and attr == 'last-modified':
+            if authoritative and last_modified and attr == "last-modified":
                 continue
-            attr_display = f'{attr}:'.ljust(RPSL_ATTRIBUTE_TEXT_WIDTH)
+            attr_display = f"{attr}:".ljust(RPSL_ATTRIBUTE_TEXT_WIDTH)
             value_lines = list(splitline_unicodesafe(value))
             if not value_lines:
-                output += f'{attr}:\n'
+                output += f"{attr}:\n"
             for idx, line in enumerate(value_lines):
                 if idx == 0:
                     output += attr_display + line
                 else:
                     continuation_char = continuation_chars[idx - 1]
                     # Override the continuation char for empty lines #298
                     if not line:
-                        continuation_char = '+'
-                    output += continuation_char + (RPSL_ATTRIBUTE_TEXT_WIDTH - 1) * ' ' + line
-                output += '\n'
+                        continuation_char = "+"
+                    output += continuation_char + (RPSL_ATTRIBUTE_TEXT_WIDTH - 1) * " " + line
+                output += "\n"
         if authoritative and last_modified:
-            output += 'last-modified:'.ljust(RPSL_ATTRIBUTE_TEXT_WIDTH)
-            output += last_modified.replace(microsecond=0).isoformat().replace('+00:00', 'Z')
-            output += '\n'
+            output += "last-modified:".ljust(RPSL_ATTRIBUTE_TEXT_WIDTH)
+            output += last_modified.replace(microsecond=0).isoformat().replace("+00:00", "Z")
+            output += "\n"
         return output
 
     def generate_template(self):
         """Generate a template in text form of the main attributes of all fields."""
         template = ""
         max_name_width = max(len(k) for k in self.fields.keys())
         for name, field in self.fields.items():
-            mandatory = '[optional] ' if field.optional else '[mandatory]'
-            single = '[multiple]' if field.multiple else '[single]  '
+            mandatory = "[optional] " if field.optional else "[mandatory]"
+            single = "[multiple]" if field.multiple else "[single]  "
             metadata = []
             if field.primary_key and field.lookup_key:
-                metadata.append('primary/look-up key')
+                metadata.append("primary/look-up key")
             elif field.primary_key:
-                metadata.append('primary key')
+                metadata.append("primary key")
             elif field.lookup_key:
-                metadata.append('look-up key')
-            if getattr(field, 'referring', []):
-                reference_type = 'strong' if getattr(field, 'strong') else 'weak'
-                metadata.append(f'{reference_type} references ' + '/'.join(field.referring))
-            metadata_str = ', '.join(metadata)
-            name_padding = (max_name_width - len(name)) * ' '
-            template += f'{name}: {name_padding}  {mandatory}  {single}  [{metadata_str}]\n'
+                metadata.append("look-up key")
+            if getattr(field, "referring", []):
+                reference_type = "strong" if getattr(field, "strong") else "weak"
+                metadata.append(f"{reference_type} references " + "/".join(field.referring))
+            metadata_str = ", ".join(metadata)
+            name_padding = (max_name_width - len(name)) * " "
+            template += f"{name}: {name_padding}  {mandatory}  {single}  [{metadata_str}]\n"
         return template
 
     def clean(self) -> bool:
         """
         Additional cleaning steps for some objects.
         """
         return True
@@ -239,45 +256,51 @@
         which is distinct from an attribute occurring multiple times.
 
         The parse result is internally stored in self._object_data. This is a
         list of 3-tuples, where each tuple contains the attribute name,
         attribute value, and the continuation characters. The continuation
         characters are needed to reconstruct the original object into a string.
         """
-        continuation_chars = (' ', '+', '\t')
+        continuation_chars = (" ", "+", "\t")
         current_attr = None
         current_value = ""
         current_continuation_chars: List[str] = []
 
         for line_no, line in enumerate(splitline_unicodesafe(text.strip())):
             if not line:
-                self.messages.error(f'Line {line_no+1}: encountered empty line in the middle of object: [{line}]')
+                self.messages.error(
+                    f"Line {line_no+1}: encountered empty line in the middle of object: [{line}]"
+                )
                 return
 
             if not line.startswith(continuation_chars):
                 if current_attr and current_attr not in self.discarded_fields:
                     # Encountering a new attribute requires saving the previous attribute data first, if any,
                     # which can't be done earlier as line continuation means we can't know earlier whether
                     # the attribute is finished.
                     self._object_data.append((current_attr, current_value, current_continuation_chars))
 
-                if ':' not in line:
-                    self.messages.error(f'Line {line_no+1}: line is neither continuation nor valid attribute [{line}]')
+                if ":" not in line:
+                    self.messages.error(
+                        f"Line {line_no+1}: line is neither continuation nor valid attribute [{line}]"
+                    )
                     return
-                current_attr, current_value = line.split(':', maxsplit=1)
+                current_attr, current_value = line.split(":", maxsplit=1)
                 current_attr = current_attr.lower()
                 current_value = current_value.strip()
                 current_continuation_chars = []
 
                 if current_attr not in self.attrs_allowed and not self._re_attr_name.match(current_attr):
-                    self.messages.error(f'Line {line_no+1}: encountered malformed attribute name: [{current_attr}]')
+                    self.messages.error(
+                        f"Line {line_no+1}: encountered malformed attribute name: [{current_attr}]"
+                    )
                     return
             else:
                 # Whitespace between the continuation character and the start of the data is not significant.
-                current_value += '\n' + line[1:].strip()
+                current_value += "\n" + line[1:].strip()
                 current_continuation_chars += line[0]
         if current_attr and current_attr not in self.discarded_fields:
             self._object_data.append((current_attr, current_value, current_continuation_chars))
 
     def _validate_object(self) -> None:
         """
         Validate an object. The strictness depends on self.strict_validation
@@ -302,28 +325,31 @@
         attrs_present = Counter([attr[0] for attr in self._object_data])
 
         if self.strict_validation:
             for attr_name, count in attrs_present.items():
                 if attr_name in self.ignored_validation_fields:
                     continue
                 if attr_name not in self.attrs_allowed:
-                    self.messages.error(f'Unrecognised attribute {attr_name} on object {self.rpsl_object_class}')
+                    self.messages.error(
+                        f"Unrecognised attribute {attr_name} on object {self.rpsl_object_class}"
+                    )
                 if count > 1 and attr_name not in self.attrs_multiple:
                     self.messages.error(
-                        f'Attribute "{attr_name}" on object {self.rpsl_object_class} occurs multiple times, but is '
-                        f'only allowed once')
+                        f'Attribute "{attr_name}" on object {self.rpsl_object_class} occurs multiple times,'
+                        " but is only allowed once"
+                    )
             for attr_required in self.attrs_required:
                 if attr_required not in attrs_present:
                     self.messages.error(
                         f'Mandatory attribute "{attr_required}" on object {self.rpsl_object_class} is missing'
                     )
         else:
             required_fields = self.pk_fields
             if not self.default_source:
-                required_fields = required_fields + ['source']
+                required_fields = required_fields + ["source"]
             for attr_pk in required_fields:
                 if attr_pk not in attrs_present:
                     self.messages.error(
                         f'Primary key attribute "{attr_pk}" on object {self.rpsl_object_class} is missing'
                     )
 
     def _parse_attribute_data(self, allow_invalid_metadata=False) -> None:
@@ -346,15 +372,17 @@
                 normalised_value = self._normalise_rpsl_value(value)
 
                 # We always parse all fields, but only care about errors if we're running
                 # in strict validation mode, if the field is primary or lookup, or if it's
                 # the source field. In all other cases, the field parsing is best effort.
                 # In all these other cases we pass a new parser messages object to the
                 # field parser, so that we basically discard any errors.
-                raise_errors = self.strict_validation or field.primary_key or field.lookup_key or attr_name == 'source'
+                raise_errors = (
+                    self.strict_validation or field.primary_key or field.lookup_key or attr_name == "source"
+                )
                 field_messages = self.messages if raise_errors else RPSLParserMessages()
                 parsed_value = field.parse(normalised_value, field_messages, self.strict_validation)
 
                 if parsed_value:
                     parsed_value_str = parsed_value.value
                     if parsed_value_str != normalised_value:
                         # Note: this replacement can be incomplete: if the normalised value is not contained in the
@@ -376,32 +404,34 @@
                         if field.multiple:
                             if attr_name in self.parsed_data:
                                 self.parsed_data[attr_name].append(parsed_value_str)
                             else:
                                 self.parsed_data[attr_name] = [parsed_value_str]
                         else:
                             if attr_name in self.parsed_data:
-                                self.parsed_data[attr_name] = '\n' + parsed_value_str
+                                self.parsed_data[attr_name] = "\n" + parsed_value_str
                             else:
                                 self.parsed_data[attr_name] = parsed_value_str
 
                     # Some fields provide additional metadata about the resources to
                     # which this object pertains.
                     if field.primary_key or field.lookup_key:
-                        for attr in 'ip_first', 'ip_last', 'asn_first', 'asn_last', 'prefix', 'prefix_length':
+                        for attr in "ip_first", "ip_last", "asn_first", "asn_last", "prefix", "prefix_length":
                             attr_value = getattr(parsed_value, attr, None)
                             if attr_value:
                                 existing_attr_value = getattr(self, attr, None)
                                 if existing_attr_value and not allow_invalid_metadata:  # pragma: no cover
-                                    raise ValueError(f'Parsing of {parsed_value.value} reads {attr_value} for {attr},'
-                                                     f'but value {existing_attr_value} is already set.')
+                                    raise ValueError(
+                                        f"Parsing of {parsed_value.value} reads {attr_value} for {attr},"
+                                        f"but value {existing_attr_value} is already set."
+                                    )
                                 setattr(self, attr, attr_value)
 
-        if 'source' not in self.parsed_data and self.default_source:
-            self.parsed_data['source'] = self.default_source
+        if "source" not in self.parsed_data and self.default_source:
+            self.parsed_data["source"] = self.default_source
 
     def _normalise_rpsl_value(self, value: str) -> str:
         """
         Normalise an RPSL attribute value to its significant parts
         in a consistent format.
 
         For example, the following is valid in RPSL:
@@ -414,48 +444,48 @@
         This value will be normalised by this method to:
             192.0.2.0 - 192.0.2.1
         to be used for further validation and extraction of primary keys.
         """
         normalized_lines = []
         # The shortcuts below are functionally inconsequential, but significantly improve performance,
         # as most values are single line without comments, and this method is called extremely often.
-        if '\n' not in value:
-            if '#' in value:
-                return value.split('#')[0].strip()
+        if "\n" not in value:
+            if "#" in value:
+                return value.split("#")[0].strip()
             return value.strip()
         for line in splitline_unicodesafe(value):
-            parsed_line = line.split('#')[0].strip('\n\t, ')
+            parsed_line = line.split("#")[0].strip("\n\t, ")
             if parsed_line:
                 normalized_lines.append(parsed_line)
-        return ','.join(normalized_lines)
+        return ",".join(normalized_lines)
 
     def _update_attribute_value(self, attribute, new_values):
         """
         Update the value of an attribute in the internal state and in
         parsed_data.
 
         This is used for key-cert objects, where e.g. owner lines are
         derived from other data in the object.
 
         All existing occurences of the attribute are removed, new items
         are always inserted at line 2 of the object.
         """
         if isinstance(new_values, str):
             new_values = [new_values]
-        self.parsed_data[attribute] = '\n'.join(new_values)
+        self.parsed_data[attribute] = "\n".join(new_values)
 
         self._object_data = list(filter(lambda a: a[0] != attribute, self._object_data))
         insert_idx = 1
         for new_value in new_values:
             self._object_data.insert(insert_idx, (attribute, new_value, []))
             insert_idx += 1
 
     def __repr__(self):
-        source = self.parsed_data.get('source', '')
-        return f'{self.rpsl_object_class}/{self.pk()}/{source}'
+        source = self.parsed_data.get("source", "")
+        return f"{self.rpsl_object_class}/{self.pk()}/{source}"
 
     def __key(self):
         return self.rpsl_object_class, self.pk(), json.dumps(self.parsed_data, sort_keys=True)
 
     def __hash__(self):
         return hash(self.__key())
```

### Comparing `irrd-4.2.8/irrd/rpsl/parser_state.py` & `irrd-4.3.0/irrd/rpsl/parser_state.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,49 +1,58 @@
-from typing import TypeVar, List
+from typing import List, Optional, TypeVar
 
 from IPy import IP
 
-RPSLParserMessagesType = TypeVar('RPSLParserMessagesType', bound='RPSLParserMessages')
+RPSLParserMessagesType = TypeVar("RPSLParserMessagesType", bound="RPSLParserMessages")
 
 
 class RPSLParserMessages:
-    levels = ['INFO', 'ERROR']
+    levels = ["INFO", "ERROR"]
 
     def __init__(self) -> None:
         self._messages: List[tuple] = []
 
     def __str__(self) -> str:
-        messages_str = [f'{msg[0]}: {msg[1]}' for msg in self._messages]
-        return '\n'.join(messages_str)
+        messages_str = [f"{msg[0]}: {msg[1]}" for msg in self._messages]
+        return "\n".join(messages_str)
 
     def messages(self) -> List[str]:
         return [msg[1] for msg in self._messages]
 
     def infos(self) -> List[str]:
-        return [msg[1] for msg in self._messages if msg[0] == 'INFO']
+        return [msg[1] for msg in self._messages if msg[0] == "INFO"]
 
     def errors(self) -> List[str]:
-        return [msg[1] for msg in self._messages if msg[0] == 'ERROR']
+        return [msg[1] for msg in self._messages if msg[0] == "ERROR"]
 
     def info(self, msg: str) -> None:
-        self._message('INFO', msg)
+        self._message("INFO", msg)
 
     def error(self, msg: str) -> None:
-        self._message('ERROR', msg)
+        self._message("ERROR", msg)
 
     def merge_messages(self, other_messages: RPSLParserMessagesType) -> None:
         self._messages += other_messages._messages
 
     def _message(self, level: str, message: str) -> None:
         self._messages.append((level, message))
 
 
 class RPSLFieldParseResult:
-    def __init__(self, value: str, values_list: List[str]=None, ip_first: IP=None, ip_last: IP=None,
-                 prefix: IP=None, prefix_length: int=None, asn_first: int=None, asn_last: int=None) -> None:
+    def __init__(
+        self,
+        value: str,
+        values_list: Optional[List[str]] = None,
+        ip_first: Optional[IP] = None,
+        ip_last: Optional[IP] = None,
+        prefix: Optional[IP] = None,
+        prefix_length: Optional[int] = None,
+        asn_first: Optional[int] = None,
+        asn_last: Optional[int] = None,
+    ) -> None:
         self.value = value
         self.values_list = values_list
         self.ip_first = ip_first
         self.ip_last = ip_last
         self.prefix = prefix
         self.prefix_length = prefix_length
         self.asn_first = asn_first
```

### Comparing `irrd-4.2.8/irrd/rpsl/tests/test_fields.py` & `irrd-4.3.0/irrd/rpsl/tests/test_fields.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,17 +1,30 @@
 from IPy import IP
 from pytest import raises
 
-from ..fields import (RPSLIPv4PrefixField, RPSLIPv4PrefixesField, RPSLIPv6PrefixField,
-                      RPSLIPv6PrefixesField, RPSLIPv4AddressRangeField, RPSLASNumberField,
-                      RPSLASBlockField,
-                      RPSLSetNameField, RPSLEmailField, RPSLDNSNameField, RPSLGenericNameField,
-                      RPSLReferenceField,
-                      RPSLReferenceListField, RPSLTextField, RPSLAuthField, RPSLRouteSetMemberField,
-                      RPSLChangedField, RPSLURLField)
+from ..fields import (
+    RPSLASBlockField,
+    RPSLASNumberField,
+    RPSLAuthField,
+    RPSLChangedField,
+    RPSLDNSNameField,
+    RPSLEmailField,
+    RPSLGenericNameField,
+    RPSLIPv4AddressRangeField,
+    RPSLIPv4PrefixesField,
+    RPSLIPv4PrefixField,
+    RPSLIPv6PrefixesField,
+    RPSLIPv6PrefixField,
+    RPSLReferenceField,
+    RPSLReferenceListField,
+    RPSLRouteSetMemberField,
+    RPSLSetNameField,
+    RPSLTextField,
+    RPSLURLField,
+)
 from ..parser_state import RPSLParserMessages
 
 
 def assert_validation_err(expected_errors, callable, *args, **kwargs):
     __tracebackhide__ = True
 
     if isinstance(expected_errors, str):
@@ -27,382 +40,408 @@
         for error in errors:
             if expected_error in error:
                 matched_errors.append(error)
                 matched_expected_errors.append(expected_error)
 
     expected_errors = [e for e in expected_errors if e not in matched_expected_errors]
     errors = [e for e in errors if e not in matched_errors]
-    assert len(errors) == 0, f'unexpected error messages in: {messages.errors()}'
-    assert len(expected_errors) == 0, f'did not find error messages: {expected_errors}'
+    assert len(errors) == 0, f"unexpected error messages in: {messages.errors()}"
+    assert len(expected_errors) == 0, f"did not find error messages: {expected_errors}"
 
 
 def test_rpsl_text_field():
     field = RPSLTextField()
     messages = RPSLParserMessages()
-    assert field.parse('AS-FOO$', messages).value, 'AS-FOO$'
+    assert field.parse("AS-FOO$", messages).value, "AS-FOO$"
     assert not messages.errors()
 
 
 def test_ipv4_prefix_field():
     field = RPSLIPv4PrefixField()
     messages = RPSLParserMessages()
 
-    parse_result = field.parse('192.0.2.0/24', messages)
-    assert parse_result.value == '192.0.2.0/24'
-    assert parse_result.ip_first == IP('192.0.2.0')
-    assert parse_result.ip_last == IP('192.0.2.255')
+    parse_result = field.parse("192.0.2.0/24", messages)
+    assert parse_result.value == "192.0.2.0/24"
+    assert parse_result.ip_first == IP("192.0.2.0")
+    assert parse_result.ip_last == IP("192.0.2.255")
     assert parse_result.prefix_length == 24
-    assert field.parse('192.00.02.0/25', messages).value == '192.0.2.0/25'
-    assert field.parse('192.0.2.0/32', messages).value == '192.0.2.0/32'
+    assert field.parse("192.00.02.0/25", messages).value == "192.0.2.0/25"
+    assert field.parse("192.0.2.0/32", messages).value == "192.0.2.0/32"
     assert not messages.errors()
-    assert messages.infos() == ['Address prefix 192.00.02.0/25 was reformatted as 192.0.2.0/25']
+    assert messages.infos() == ["Address prefix 192.00.02.0/25 was reformatted as 192.0.2.0/25"]
 
     # 192.0.2/24 is generally seen as a valid prefix, but RFC 2622 does not allow this notation.
-    assert_validation_err('Invalid address prefix', field.parse, '192.0.2/24')
-    assert_validation_err('Invalid address prefix', field.parse, '555.555.555.555/24')
-    assert_validation_err('Invalid address prefix', field.parse, 'foo')
-    assert_validation_err('Invalid address prefix', field.parse, '2001::/32')
-    assert_validation_err('Invalid address prefix', field.parse, '192.0.2.0/16')
+    assert_validation_err("Invalid address prefix", field.parse, "192.0.2/24")
+    assert_validation_err("Invalid address prefix", field.parse, "555.555.555.555/24")
+    assert_validation_err("Invalid address prefix", field.parse, "foo")
+    assert_validation_err("Invalid address prefix", field.parse, "2001::/32")
+    assert_validation_err("Invalid address prefix", field.parse, "192.0.2.0/16")
 
 
 def test_ipv4_prefixes_field():
     field = RPSLIPv4PrefixesField()
     messages = RPSLParserMessages()
-    assert field.parse('192.0.2.0/24', messages).value == '192.0.2.0/24'
+    assert field.parse("192.0.2.0/24", messages).value == "192.0.2.0/24"
     # Technically, the trailing comma is not RFC-compliant.
     # However, it's used in some cases when the list is broken over
     # multiple lines, and accepting it is harmless.
-    parse_result = field.parse('192.0.2.0/24, 192.00.02.0/25, ', messages)
-    assert parse_result.value == '192.0.2.0/24,192.0.2.0/25'
-    assert parse_result.values_list == ['192.0.2.0/24', '192.0.2.0/25']
+    parse_result = field.parse("192.0.2.0/24, 192.00.02.0/25, ", messages)
+    assert parse_result.value == "192.0.2.0/24,192.0.2.0/25"
+    assert parse_result.values_list == ["192.0.2.0/24", "192.0.2.0/25"]
     assert not messages.errors()
-    assert messages.infos() == ['Address prefix 192.00.02.0/25 was reformatted as 192.0.2.0/25']
+    assert messages.infos() == ["Address prefix 192.00.02.0/25 was reformatted as 192.0.2.0/25"]
 
-    assert_validation_err('Invalid address prefix', field.parse, '192.0.2.0/24, 192.0.2/16')
+    assert_validation_err("Invalid address prefix", field.parse, "192.0.2.0/24, 192.0.2/16")
 
 
 def test_ipv6_prefix_field():
     field = RPSLIPv6PrefixField()
     messages = RPSLParserMessages()
 
-    parse_result = field.parse('12AB:0000:0000:CD30:0000:0000:0000:0000/60', messages)
-    assert parse_result.value == '12ab:0:0:cd30::/60'
-    assert parse_result.ip_first == IP('12ab:0:0:cd30::')
-    assert parse_result.ip_last == IP('12ab::cd3f:ffff:ffff:ffff:ffff')
+    parse_result = field.parse("12AB:0000:0000:CD30:0000:0000:0000:0000/60", messages)
+    assert parse_result.value == "12ab:0:0:cd30::/60"
+    assert parse_result.ip_first == IP("12ab:0:0:cd30::")
+    assert parse_result.ip_last == IP("12ab::cd3f:ffff:ffff:ffff:ffff")
     assert parse_result.prefix_length == 60
 
-    assert field.parse('12ab::cd30:0:0:0:0/60', messages).value == '12ab:0:0:cd30::/60'
-    assert field.parse('12AB:0:0:CD30::/60', messages).value == '12ab:0:0:cd30::/60'
-    assert field.parse('12ab:0:0:cd30::/128', messages).value == '12ab:0:0:cd30::/128'
+    assert field.parse("12ab::cd30:0:0:0:0/60", messages).value == "12ab:0:0:cd30::/60"
+    assert field.parse("12AB:0:0:CD30::/60", messages).value == "12ab:0:0:cd30::/60"
+    assert field.parse("12ab:0:0:cd30::/128", messages).value == "12ab:0:0:cd30::/128"
     assert not messages.errors()
     assert messages.infos() == [
-        'Address prefix 12AB:0000:0000:CD30:0000:0000:0000:0000/60 was reformatted as 12ab:0:0:cd30::/60',
-        'Address prefix 12ab::cd30:0:0:0:0/60 was reformatted as 12ab:0:0:cd30::/60',
-        'Address prefix 12AB:0:0:CD30::/60 was reformatted as 12ab:0:0:cd30::/60',
+        "Address prefix 12AB:0000:0000:CD30:0000:0000:0000:0000/60 was reformatted as 12ab:0:0:cd30::/60",
+        "Address prefix 12ab::cd30:0:0:0:0/60 was reformatted as 12ab:0:0:cd30::/60",
+        "Address prefix 12AB:0:0:CD30::/60 was reformatted as 12ab:0:0:cd30::/60",
     ]
 
-    assert_validation_err('Invalid address prefix', field.parse, 'foo')
-    assert_validation_err('Invalid address prefix', field.parse, 'foo/bar')
-    assert_validation_err('invalid hexlet', field.parse, '2001525::/32')
-    assert_validation_err('should have 8 hextets', field.parse, '12AB:0:0:CD3/60')
-    assert_validation_err('Invalid address prefix', field.parse, '12AB::CD30/60')
-    assert_validation_err('Invalid address prefix', field.parse, '12AB::CD3/60')
-    assert_validation_err('Invalid address prefix', field.parse, '192.0.2.0/16')
+    assert_validation_err("Invalid address prefix", field.parse, "foo")
+    assert_validation_err("Invalid address prefix", field.parse, "foo/bar")
+    assert_validation_err("invalid hexlet", field.parse, "2001525::/32")
+    assert_validation_err("should have 8 hextets", field.parse, "12AB:0:0:CD3/60")
+    assert_validation_err("Invalid address prefix", field.parse, "12AB::CD30/60")
+    assert_validation_err("Invalid address prefix", field.parse, "12AB::CD3/60")
+    assert_validation_err("Invalid address prefix", field.parse, "192.0.2.0/16")
 
 
 def test_ipv6_prefixes_field():
     field = RPSLIPv6PrefixesField()
     messages = RPSLParserMessages()
-    assert field.parse('12AB:0:0:CD30::/60', messages).value == '12ab:0:0:cd30::/60'
-    assert field.parse('12AB:0:0:CD30::/60, 2001:DB8::0/64', messages).value == '12ab:0:0:cd30::/60,2001:db8::/64'
+    assert field.parse("12AB:0:0:CD30::/60", messages).value == "12ab:0:0:cd30::/60"
+    assert (
+        field.parse("12AB:0:0:CD30::/60, 2001:DB8::0/64", messages).value
+        == "12ab:0:0:cd30::/60,2001:db8::/64"
+    )
     assert not messages.errors()
 
-    assert_validation_err('Invalid address prefix', field.parse, 'foo')
-    assert_validation_err('Invalid address prefix', field.parse, 'foo/bar')
-    assert_validation_err('invalid hexlet', field.parse, '2001:db8::/32, 2001525::/32')
-    assert_validation_err('should have 8 hextets', field.parse, '12AB:0:0:CD3/60')
-    assert_validation_err('Invalid address prefix', field.parse, '12AB::CD30/60')
-    assert_validation_err('Invalid address prefix', field.parse, '12AB::CD3/60')
-    assert_validation_err('Invalid address prefix', field.parse, '192.0.2.0/16')
+    assert_validation_err("Invalid address prefix", field.parse, "foo")
+    assert_validation_err("Invalid address prefix", field.parse, "foo/bar")
+    assert_validation_err("invalid hexlet", field.parse, "2001:db8::/32, 2001525::/32")
+    assert_validation_err("should have 8 hextets", field.parse, "12AB:0:0:CD3/60")
+    assert_validation_err("Invalid address prefix", field.parse, "12AB::CD30/60")
+    assert_validation_err("Invalid address prefix", field.parse, "12AB::CD3/60")
+    assert_validation_err("Invalid address prefix", field.parse, "192.0.2.0/16")
 
 
 def test_ipv4_address_range_field():
     field = RPSLIPv4AddressRangeField()
     messages = RPSLParserMessages()
 
-    parse_result = field.parse('192.0.02.0', messages)
-    assert parse_result.value == '192.0.2.0'
-    assert parse_result.ip_first == IP('192.0.2.0')
-    assert parse_result.ip_last == IP('192.0.2.0')
-
-    parse_result = field.parse('192.0.2.0 - 192.0.2.126', messages)
-    assert parse_result.value == '192.0.2.0 - 192.0.2.126'
-
-    parse_result = field.parse('192.0.2.0 -192.0.02.126', messages)
-    assert parse_result.value == '192.0.2.0 - 192.0.2.126'
-    assert parse_result.ip_first == IP('192.0.2.0')
-    assert parse_result.ip_last == IP('192.0.2.126')
+    parse_result = field.parse("192.0.02.0", messages)
+    assert parse_result.value == "192.0.2.0"
+    assert parse_result.ip_first == IP("192.0.2.0")
+    assert parse_result.ip_last == IP("192.0.2.0")
+
+    parse_result = field.parse("192.0.2.0 - 192.0.2.126", messages)
+    assert parse_result.value == "192.0.2.0 - 192.0.2.126"
+
+    parse_result = field.parse("192.0.2.0 -192.0.02.126", messages)
+    assert parse_result.value == "192.0.2.0 - 192.0.2.126"
+    assert parse_result.ip_first == IP("192.0.2.0")
+    assert parse_result.ip_last == IP("192.0.2.126")
 
     assert not messages.errors()
     assert messages.infos() == [
-        'Address range 192.0.02.0 was reformatted as 192.0.2.0',
-        'Address range 192.0.2.0 -192.0.02.126 was reformatted as 192.0.2.0 - 192.0.2.126',
+        "Address range 192.0.02.0 was reformatted as 192.0.2.0",
+        "Address range 192.0.2.0 -192.0.02.126 was reformatted as 192.0.2.0 - 192.0.2.126",
     ]
 
-    assert_validation_err('Invalid address', field.parse, '192.0.1.5555 - 192.0.2.0')
-    assert_validation_err('IP version mismatch', field.parse, '192.0.2.0 - 2001:db8::')
-    assert_validation_err('first IP is higher', field.parse, '192.0.2.1 - 192.0.2.0')
-    assert_validation_err('IP version mismatch', field.parse, '2001:db8::0 - 2001:db8::1')
+    assert_validation_err("Invalid address", field.parse, "192.0.1.5555 - 192.0.2.0")
+    assert_validation_err("IP version mismatch", field.parse, "192.0.2.0 - 2001:db8::")
+    assert_validation_err("first IP is higher", field.parse, "192.0.2.1 - 192.0.2.0")
+    assert_validation_err("IP version mismatch", field.parse, "2001:db8::0 - 2001:db8::1")
 
 
 def test_route_set_members_field():
     with raises(ValueError):
         RPSLRouteSetMemberField(ip_version=2)
 
     field = RPSLRouteSetMemberField(ip_version=4)
     messages = RPSLParserMessages()
 
-    assert field.parse('192.0.2.0/24^12-23', messages).value == '192.0.2.0/24^12-23'
-    assert field.parse('AS065537:RS-TEST^32', messages).value == 'AS65537:RS-TEST^32'
-    assert field.parse('AS065537^32', messages).value == 'AS65537^32'
-    assert field.parse('192.0.2.0/25^+', messages).value == '192.0.2.0/25^+'
-    assert field.parse('192.0.2.0/25^32', messages).value == '192.0.2.0/25^32'
-    assert field.parse('192.00.02.0/25^-', messages).value == '192.0.2.0/25^-'
-    assert field.parse('192.0.02.0/32', messages).value == '192.0.2.0/32'
+    assert field.parse("192.0.2.0/24^24-25", messages).value == "192.0.2.0/24^24-25"
+    assert field.parse("AS065537:RS-TEST^32", messages).value == "AS65537:RS-TEST^32"
+    assert field.parse("AS065537^32", messages).value == "AS65537^32"
+    assert field.parse("192.0.2.0/25^+", messages).value == "192.0.2.0/25^+"
+    assert field.parse("192.0.2.0/25^32", messages).value == "192.0.2.0/25^32"
+    assert field.parse("192.00.02.0/25^-", messages).value == "192.0.2.0/25^-"
+    assert field.parse("192.0.02.0/32", messages).value == "192.0.2.0/32"
     assert not messages.errors()
     assert messages.infos() == [
-        'Route set member AS065537:RS-TEST^32 was reformatted as AS65537:RS-TEST^32',
-        'Route set member AS065537^32 was reformatted as AS65537^32',
-        'Route set member 192.00.02.0/25^- was reformatted as 192.0.2.0/25^-',
-        'Route set member 192.0.02.0/32 was reformatted as 192.0.2.0/32',
+        "Route set member AS065537:RS-TEST^32 was reformatted as AS65537:RS-TEST^32",
+        "Route set member AS065537^32 was reformatted as AS65537^32",
+        "Route set member 192.00.02.0/25^- was reformatted as 192.0.2.0/25^-",
+        "Route set member 192.0.02.0/32 was reformatted as 192.0.2.0/32",
     ]
 
-    assert_validation_err('Value is neither a valid set name nor a valid prefix', field.parse, 'AS65537:TEST')
-    assert_validation_err('Missing range operator', field.parse, '192.0.2.0/32^')
-    assert_validation_err('Invalid range operator', field.parse, '192.0.2.0/32^x')
-    assert_validation_err('Invalid range operator', field.parse, '192.0.2.0/32^-32')
-    assert_validation_err('Invalid range operator', field.parse, '192.0.2.0/32^32-')
-    assert_validation_err('Invalid range operator', field.parse, '192.0.2.0/32^24+32')
+    assert_validation_err("Value is neither a valid set name nor a valid prefix", field.parse, "AS65537:TEST")
+    assert_validation_err("Missing range operator", field.parse, "192.0.2.0/24^")
+    assert_validation_err("Invalid range operator", field.parse, "192.0.2.0/24^x")
+    assert_validation_err("Invalid range operator", field.parse, "192.0.2.0/24^-32")
+    assert_validation_err("Invalid range operator", field.parse, "192.0.2.0/24^32-")
+    assert_validation_err("Invalid range operator", field.parse, "192.0.2.0/24^24+32")
+    assert_validation_err("operator length (23) must be equal ", field.parse, "192.0.2.0/24^23")
+    assert_validation_err("operator start (23) must be equal ", field.parse, "192.0.2.0/24^23-32")
+    assert_validation_err("operator end (30) must be equal", field.parse, "192.0.2.0/24^32-30")
 
     field = RPSLRouteSetMemberField(ip_version=None)
     messages = RPSLParserMessages()
 
-    assert field.parse('192.0.2.0/24^12-23', messages).value == '192.0.2.0/24^12-23'
-    assert field.parse('12ab:0:0:cd30::/128^12-23', messages).value == '12ab:0:0:cd30::/128^12-23'
-    assert field.parse('AS65537:RS-TEST', messages).value == 'AS65537:RS-TEST'
-
-    assert field.parse('192.0.2.0/25^+', messages).value == '192.0.2.0/25^+'
-    assert field.parse('192.0.2.0/25^32', messages).value == '192.0.2.0/25^32'
-    assert field.parse('12ab:00:0:cd30::/60^-', messages).value == '12ab:0:0:cd30::/60^-'
-    assert field.parse('12ab:0:0:cd30::/60', messages).value == '12ab:0:0:cd30::/60'
+    assert field.parse("192.0.2.0/24^24-25", messages).value == "192.0.2.0/24^24-25"
+    assert field.parse("12ab:0:0:cd30::/128", messages).value == "12ab:0:0:cd30::/128"
+    assert field.parse("12ab:0:0:cd30::/64^120-128", messages).value == "12ab:0:0:cd30::/64^120-128"
+    assert field.parse("AS65537:RS-TEST", messages).value == "AS65537:RS-TEST"
+
+    assert field.parse("192.0.2.0/25^+", messages).value == "192.0.2.0/25^+"
+    assert field.parse("192.0.2.0/25^32", messages).value == "192.0.2.0/25^32"
+    assert field.parse("12ab:00:0:cd30::/60^-", messages).value == "12ab:0:0:cd30::/60^-"
+    assert field.parse("12ab:0:0:cd30::/60", messages).value == "12ab:0:0:cd30::/60"
     assert not messages.errors()
     assert messages.infos() == [
-        'Route set member 12ab:00:0:cd30::/60^- was reformatted as 12ab:0:0:cd30::/60^-',
+        "Route set member 12ab:00:0:cd30::/60^- was reformatted as 12ab:0:0:cd30::/60^-",
     ]
 
-    assert_validation_err('Invalid range operator', field.parse, '192.0.2.0/32^24+32')
-    assert_validation_err('Invalid range operator', field.parse, '12ab:0:0:cd30::/60^24+32')
+    assert_validation_err("Invalid range operator", field.parse, "192.0.2.0/32^24+32")
+    assert_validation_err("Invalid range operator", field.parse, "12ab:0:0:cd30::/60^24+32")
 
 
 def test_validate_as_number_field():
     field = RPSLASNumberField()
     messages = RPSLParserMessages()
 
-    parse_result = field.parse('AS065537', messages)
-    assert parse_result.value == 'AS65537'
+    parse_result = field.parse("AS065537", messages)
+    assert parse_result.value == "AS65537"
     assert parse_result.asn_first == 65537
     assert parse_result.asn_last == 65537
     assert not messages.errors()
-    assert messages.infos() == ['AS number AS065537 was reformatted as AS65537']
+    assert messages.infos() == ["AS number AS065537 was reformatted as AS65537"]
 
-    assert_validation_err('not numeric', field.parse, 'ASxxxx')
-    assert_validation_err('not numeric', field.parse, 'AS2345💩')
-    assert_validation_err('must start with', field.parse, '💩AS2345')
+    assert_validation_err("not numeric", field.parse, "ASxxxx")
+    assert_validation_err("not numeric", field.parse, "AS2345💩")
+    assert_validation_err("must start with", field.parse, "💩AS2345")
 
 
 def test_validate_as_block_field():
     field = RPSLASBlockField()
     messages = RPSLParserMessages()
 
-    parse_result = field.parse('AS001- AS200', messages)
-    assert parse_result.value == 'AS1 - AS200'
+    parse_result = field.parse("AS001- AS200", messages)
+    assert parse_result.value == "AS1 - AS200"
     assert parse_result.asn_first == 1
     assert parse_result.asn_last == 200
 
-    assert field.parse('AS200-AS0200', messages).value == 'AS200 - AS200'
+    assert field.parse("AS200-AS0200", messages).value == "AS200 - AS200"
     assert not messages.errors()
     assert messages.infos() == [
-        'AS range AS001- AS200 was reformatted as AS1 - AS200',
-        'AS range AS200-AS0200 was reformatted as AS200 - AS200'
+        "AS range AS001- AS200 was reformatted as AS1 - AS200",
+        "AS range AS200-AS0200 was reformatted as AS200 - AS200",
     ]
 
-    assert_validation_err('does not contain a hyphen', field.parse, 'AS65537')
-    assert_validation_err('number part is not numeric', field.parse, 'ASxxxx - ASyyyy')
-    assert_validation_err('Invalid AS number', field.parse, 'AS-FOO - AS-BAR')
-    assert_validation_err('Invalid AS range', field.parse, 'AS300 - AS200')
+    assert_validation_err("does not contain a hyphen", field.parse, "AS65537")
+    assert_validation_err("number part is not numeric", field.parse, "ASxxxx - ASyyyy")
+    assert_validation_err("Invalid AS number", field.parse, "AS-FOO - AS-BAR")
+    assert_validation_err("Invalid AS range", field.parse, "AS300 - AS200")
 
 
 def test_validate_set_name_field():
-    field = RPSLSetNameField(prefix='AS')
+    field = RPSLSetNameField(prefix="AS")
     messages = RPSLParserMessages()
-    assert field.parse('AS-FOO', messages).value == 'AS-FOO'
-    assert field.parse('AS01:AS-FOO', messages).value == 'AS1:AS-FOO'
-    assert field.parse('AS1:AS-FOO:AS3', messages).value == 'AS1:AS-FOO:AS3'
-    assert field.parse('AS01:AS-3', messages).value == 'AS1:AS-3'
+    assert field.parse("AS-FOO", messages).value == "AS-FOO"
+    assert field.parse("AS01:AS-FOO", messages).value == "AS1:AS-FOO"
+    assert field.parse("AS1:AS-FOO:AS3", messages).value == "AS1:AS-FOO:AS3"
+    assert field.parse("AS01:AS-3", messages).value == "AS1:AS-3"
     assert not messages.errors()
     assert messages.infos() == [
-        'Set name AS01:AS-FOO was reformatted as AS1:AS-FOO',
-        'Set name AS01:AS-3 was reformatted as AS1:AS-3'
+        "Set name AS01:AS-FOO was reformatted as AS1:AS-FOO",
+        "Set name AS01:AS-3 was reformatted as AS1:AS-3",
     ]
 
-    long_set = 'AS1:AS-B:AS-C:AS-D:AS-E:AS-F'
-    assert_validation_err('at least one component must be an actual set name', field.parse, 'AS1',)
-    assert_validation_err('at least one component must be an actual set name', field.parse, 'AS1:AS3')
-    assert_validation_err('not a valid AS number, nor does it start with AS-', field.parse, 'AS1:AS-FOO:RS-FORBIDDEN')
-    assert_validation_err('not a valid AS number nor a valid set name', field.parse, ':AS-FOO')
-    assert_validation_err('not a valid AS number nor a valid set name', field.parse, 'AS-FOO:')
-    assert_validation_err('can have a maximum of five components', field.parse, long_set)
-    assert_validation_err('reserved word', field.parse, 'AS1:AS-ANY')
+    long_set = "AS1:AS-B:AS-C:AS-D:AS-E:AS-F"
+    assert_validation_err(
+        "at least one component must be an actual set name",
+        field.parse,
+        "AS1",
+    )
+    assert_validation_err("at least one component must be an actual set name", field.parse, "AS1:AS3")
+    assert_validation_err(
+        "not a valid AS number, nor does it start with AS-", field.parse, "AS1:AS-FOO:RS-FORBIDDEN"
+    )
+    assert_validation_err("not a valid AS number nor a valid set name", field.parse, ":AS-FOO")
+    assert_validation_err("not a valid AS number nor a valid set name", field.parse, "AS-FOO:")
+    assert_validation_err("can have a maximum of five components", field.parse, long_set)
+    assert_validation_err("reserved word", field.parse, "AS1:AS-ANY")
 
-    assert field.parse('AS-ANY', messages, strict_validation=False).value == 'AS-ANY'
+    assert field.parse("AS-ANY", messages, strict_validation=False).value == "AS-ANY"
     assert field.parse(long_set, messages, strict_validation=False).value == long_set
 
-    field = RPSLSetNameField(prefix='RS')
+    field = RPSLSetNameField(prefix="RS")
     messages = RPSLParserMessages()
-    assert field.parse('RS-FOO', messages).value == 'RS-FOO'
-    assert field.parse('AS1:RS-FOO', messages).value == 'AS1:RS-FOO'
-    assert field.parse('AS1:RS-FOO:AS3', messages).value == 'AS1:RS-FOO:AS3'
-    assert field.parse('AS1:RS-3', messages).value == 'AS1:RS-3'
+    assert field.parse("RS-FOO", messages).value == "RS-FOO"
+    assert field.parse("AS1:RS-FOO", messages).value == "AS1:RS-FOO"
+    assert field.parse("AS1:RS-FOO:AS3", messages).value == "AS1:RS-FOO:AS3"
+    assert field.parse("AS1:RS-3", messages).value == "AS1:RS-3"
     assert not messages.errors()
 
-    assert_validation_err('at least one component must be an actual set name', field.parse, 'AS1:AS-FOO')
+    assert_validation_err("at least one component must be an actual set name", field.parse, "AS1:AS-FOO")
 
 
 def test_validate_email_field():
     field = RPSLEmailField()
     messages = RPSLParserMessages()
-    assert field.parse('foo.bar@example.asia', messages).value == 'foo.bar@example.asia'
-    assert field.parse('foo.bar@[192.0.2.1]', messages).value == 'foo.bar@[192.0.2.1]'
-    assert field.parse('foo.bar@[2001:db8::1]', messages).value == 'foo.bar@[2001:db8::1]'
+    assert field.parse("foo.bar@example.asia", messages).value == "foo.bar@example.asia"
+    assert field.parse("foo.bar@[192.0.2.1]", messages).value == "foo.bar@[192.0.2.1]"
+    assert field.parse("foo.bar@[2001:db8::1]", messages).value == "foo.bar@[2001:db8::1]"
     assert not messages.errors()
 
-    assert_validation_err('Invalid e-mail', field.parse, 'foo.bar+baz@')
-    assert_validation_err('Invalid e-mail', field.parse, 'a§§@example.com')
-    assert_validation_err('Invalid e-mail', field.parse, 'a@[192.0.2.2.2]')
+    assert_validation_err("Invalid e-mail", field.parse, "foo.bar+baz@")
+    assert_validation_err("Invalid e-mail", field.parse, "a§§@example.com")
+    assert_validation_err("Invalid e-mail", field.parse, "a@[192.0.2.2.2]")
 
 
 def test_validate_changed_field():
     field = RPSLChangedField()
     messages = RPSLParserMessages()
-    assert field.parse('foo.bar@example.asia', messages).value == 'foo.bar@example.asia'
-    assert field.parse('foo.bar@[192.0.2.1] 20190701', messages).value == 'foo.bar@[192.0.2.1] 20190701'
-    assert field.parse('foo.bar@[2001:db8::1] 19980101', messages).value == 'foo.bar@[2001:db8::1] 19980101'
+    assert field.parse("foo.bar@example.asia", messages).value == "foo.bar@example.asia"
+    assert field.parse("foo.bar@[192.0.2.1] 20190701", messages).value == "foo.bar@[192.0.2.1] 20190701"
+    assert field.parse("foo.bar@[2001:db8::1] 19980101", messages).value == "foo.bar@[2001:db8::1] 19980101"
     assert not messages.errors()
 
-    assert_validation_err('Invalid e-mail', field.parse, 'foo.bar+baz@')
-    assert_validation_err('Invalid changed date', field.parse, 'foo.bar@example.com 20191301')
-    assert_validation_err('Invalid e-mail', field.parse, '\nfoo.bar@example.com \n20190701')
-    assert_validation_err('Invalid changed date', field.parse, 'foo.bar@example.com \n20190701')
-    assert_validation_err('Invalid changed date', field.parse, 'foo.bar@example.com 20190701\n')
+    assert_validation_err("Invalid e-mail", field.parse, "foo.bar+baz@")
+    assert_validation_err("Invalid changed date", field.parse, "foo.bar@example.com 20191301")
+    assert_validation_err("Invalid e-mail", field.parse, "\nfoo.bar@example.com \n20190701")
+    assert_validation_err("Invalid changed date", field.parse, "foo.bar@example.com \n20190701")
+    assert_validation_err("Invalid changed date", field.parse, "foo.bar@example.com 20190701\n")
 
 
 def test_validate_dns_name_field():
     field = RPSLDNSNameField()
     messages = RPSLParserMessages()
-    assert field.parse('foo.bar.baz', messages).value == 'foo.bar.baz'
+    assert field.parse("foo.bar.baz", messages).value == "foo.bar.baz"
     assert not messages.errors()
 
-    assert_validation_err('Invalid DNS name', field.parse, 'foo.bar+baz@')
+    assert_validation_err("Invalid DNS name", field.parse, "foo.bar+baz@")
 
 
 def test_validate_url_field():
     field = RPSLURLField()
     messages = RPSLParserMessages()
-    assert field.parse('http://example.com', messages).value == 'http://example.com'
-    assert field.parse('https://example.com', messages).value == 'https://example.com'
+    assert field.parse("http://example.com", messages).value == "http://example.com"
+    assert field.parse("https://example.com", messages).value == "https://example.com"
     assert not messages.errors()
 
-    assert_validation_err('Invalid http/https URL', field.parse, 'ftp://test')
-    assert_validation_err('Invalid http/https URL', field.parse, 'test')
-    assert_validation_err('Invalid http/https URL', field.parse, 'test')
+    assert_validation_err("Invalid http/https URL", field.parse, "ftp://test")
+    assert_validation_err("Invalid http/https URL", field.parse, "test")
+    assert_validation_err("Invalid http/https URL", field.parse, "test")
 
 
 def test_validate_generic_name_field():
     field = RPSLGenericNameField()
     messages = RPSLParserMessages()
-    assert field.parse('MAINT-FOO', messages).value == 'MAINT-FOO'
-    assert field.parse('FOO-MNT', messages).value == 'FOO-MNT'
-    assert field.parse('FOO-MN_T2', messages).value == 'FOO-MN_T2'
+    assert field.parse("MAINT-FOO", messages).value == "MAINT-FOO"
+    assert field.parse("FOO-MNT", messages).value == "FOO-MNT"
+    assert field.parse("FOO-MN_T2", messages).value == "FOO-MN_T2"
     assert not messages.errors()
 
-    assert_validation_err('reserved word', field.parse, 'any')
-    assert_validation_err('reserved prefix', field.parse, 'As-FOO')
-    assert_validation_err('invalid character', field.parse, 'FoO$BAR')
-    assert_validation_err('invalid character', field.parse, 'FOOBAR-')
-    assert_validation_err('invalid character', field.parse, 'FOO💩BAR')
+    assert_validation_err("reserved word", field.parse, "any")
+    assert_validation_err("reserved prefix", field.parse, "As-FOO")
+    assert_validation_err("invalid character", field.parse, "FoO$BAR")
+    assert_validation_err("invalid character", field.parse, "FOOBAR-")
+    assert_validation_err("invalid character", field.parse, "FOO💩BAR")
 
-    assert field.parse('AS-FOO', messages, strict_validation=False).value == 'AS-FOO'
-    assert field.parse('FOO BAR', messages, strict_validation=False) is None
+    assert field.parse("AS-FOO", messages, strict_validation=False).value == "AS-FOO"
+    assert field.parse("FOO BAR", messages, strict_validation=False) is None
 
-    field = RPSLGenericNameField(allowed_prefixes=['as'])
+    field = RPSLGenericNameField(allowed_prefixes=["as"])
     messages = RPSLParserMessages()
-    assert field.parse('As-FOO', messages).value == 'As-FOO'
+    assert field.parse("As-FOO", messages).value == "As-FOO"
     assert not messages.errors()
 
-    assert_validation_err('reserved prefix', field.parse, 'FLTr-FOO')
+    assert_validation_err("reserved prefix", field.parse, "FLTr-FOO")
 
     field = RPSLGenericNameField(non_strict_allow_any=True)
-    assert field.parse('FOO BAR', messages, strict_validation=False).value == 'FOO BAR'
-    assert_validation_err('invalid character', field.parse, 'FOO BAR')
+    assert field.parse("FOO BAR", messages, strict_validation=False).value == "FOO BAR"
+    assert_validation_err("invalid character", field.parse, "FOO BAR")
 
 
 def test_rpsl_reference_field():
-    field = RPSLReferenceField(referring=['person'])
+    field = RPSLReferenceField(referring=["person"])
     messages = RPSLParserMessages()
-    assert field.parse('SR123-NTT', messages).value == 'SR123-NTT'
+    assert field.parse("SR123-NTT", messages).value == "SR123-NTT"
     assert not messages.errors()
 
-    assert_validation_err('RS- is a reserved prefix', field.parse, 'RS-1234')
-    assert_validation_err('Invalid name', field.parse, 'foo$$')
+    assert_validation_err("RS- is a reserved prefix", field.parse, "RS-1234")
+    assert_validation_err("Invalid name", field.parse, "foo$$")
 
-    field = RPSLReferenceField(referring=['aut-num', 'as-set'])
+    field = RPSLReferenceField(referring=["aut-num", "as-set"])
     messages = RPSLParserMessages()
-    assert field.parse('AS01234', messages).value == 'AS1234'
-    assert field.parse('AS-FOO', messages).value == 'AS-FOO'
+    assert field.parse("AS01234", messages).value == "AS1234"
+    assert field.parse("AS-FOO", messages).value == "AS-FOO"
     assert not messages.errors()
 
-    assert_validation_err(['Invalid AS number', 'start with AS-'], field.parse, 'RS-1234')
-    assert_validation_err(['Invalid AS number', 'start with AS-'], field.parse, 'RS-1234')
-    assert_validation_err(['Invalid AS number', 'at least one component must be an actual set name (i.e. start with AS-'], field.parse, 'FOOBAR')
+    assert_validation_err(["Invalid AS number", "start with AS-"], field.parse, "RS-1234")
+    assert_validation_err(["Invalid AS number", "start with AS-"], field.parse, "RS-1234")
+    assert_validation_err(
+        ["Invalid AS number", "at least one component must be an actual set name (i.e. start with AS-"],
+        field.parse,
+        "FOOBAR",
+    )
 
 
 def test_rpsl_references_field():
-    field = RPSLReferenceListField(referring=['aut-num'])
+    field = RPSLReferenceListField(referring=["aut-num"])
     messages = RPSLParserMessages()
-    assert field.parse('AS1234', messages).value == 'AS1234'
-    assert field.parse('AS01234, AS04567', messages).value == 'AS1234,AS4567'
+    assert field.parse("AS1234", messages).value == "AS1234"
+    assert field.parse("AS01234, AS04567", messages).value == "AS1234,AS4567"
     assert not messages.errors()
 
-    assert_validation_err('Invalid AS number', field.parse, 'ANY')
+    assert_validation_err("Invalid AS number", field.parse, "ANY")
 
-    field = RPSLReferenceListField(referring=['aut-num'], allow_kw_any=True)
+    field = RPSLReferenceListField(referring=["aut-num"], allow_kw_any=True)
     messages = RPSLParserMessages()
-    assert field.parse('AS1234', messages).value == 'AS1234'
-    assert field.parse('AS01234, AS04567', messages).value == 'AS1234,AS4567'
-    assert field.parse('any', messages).value == 'ANY'
+    assert field.parse("AS1234", messages).value == "AS1234"
+    assert field.parse("AS01234, AS04567", messages).value == "AS1234,AS4567"
+    assert field.parse("any", messages).value == "ANY"
     assert not messages.errors()
 
-    assert_validation_err('Invalid AS number', field.parse, 'AS1234, any')
+    assert_validation_err("Invalid AS number", field.parse, "AS1234, any")
 
 
-def test_rpsl_auth_field():
+def test_rpsl_auth_field(config_override):
     field = RPSLAuthField()
     messages = RPSLParserMessages()
-    assert field.parse('CRYPT-PW hashhash', messages).value == 'CRYPT-PW hashhash'
-    assert field.parse('MD5-pw hashhash', messages).value == 'MD5-pw hashhash'
-    assert field.parse('PGPKEY-AABB0011', messages).value == 'PGPKEY-AABB0011'
+    assert field.parse("MD5-pw hashhash", messages).value == "MD5-pw hashhash"
+    assert field.parse("bcrypt-pw hashhash", messages).value == "bcrypt-pw hashhash"
+    assert field.parse("PGPKEY-AABB0011", messages).value == "PGPKEY-AABB0011"
     assert not messages.errors()
 
-    assert_validation_err('Invalid auth attribute', field.parse, 'PGPKEY-XX')
-    assert_validation_err('Invalid auth attribute', field.parse, 'PGPKEY-AABB00112233')
-    assert_validation_err('Invalid auth attribute', field.parse, 'ARGON-PW hashhash')
-    assert_validation_err('Invalid auth attribute', field.parse, 'CRYPT-PWhashhash')
+    assert_validation_err("Invalid auth attribute", field.parse, "PGPKEY-XX")
+    assert_validation_err("Invalid auth attribute", field.parse, "PGPKEY-AABB00112233")
+    assert_validation_err("Invalid auth attribute", field.parse, "ARGON-PW hashhash")
+    assert_validation_err("Invalid auth attribute", field.parse, "BCRYPT-PWhashhash")
+
+    assert_validation_err("Invalid auth attribute", field.parse, "CRYPT-PW hashhash")
+    assert field.parse("CRYPT-PW hashhash", messages, strict_validation=False).value == "CRYPT-PW hashhash"
+
+    config_override({"auth": {"password_hashers": {"crypt-pw": "enabled"}}})
+    assert field.parse("CRYPT-PW hashhash", messages).value == "CRYPT-PW hashhash"
+
+    config_override({"auth": {"password_hashers": {"crypt-pw": "disabled"}}})
+    assert field.parse("CRYPT-PW hashhash", messages, strict_validation=False) is None
```

### Comparing `irrd-4.2.8/irrd/rpsl/tests/test_rpsl_objects.py` & `irrd-4.3.0/irrd/rpsl/tests/test_rpsl_objects.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,323 +1,364 @@
 import datetime
 
 import pytest
 from IPy import IP
 from pytest import raises
 from pytz import timezone
 
-from irrd.conf import PASSWORD_HASH_DUMMY_VALUE
-from irrd.utils.rpsl_samples import (object_sample_mapping, SAMPLE_MALFORMED_EMPTY_LINE,
-                                     SAMPLE_MALFORMED_ATTRIBUTE_NAME,
-                                     SAMPLE_UNKNOWN_CLASS, SAMPLE_MISSING_MANDATORY_ATTRIBUTE,
-                                     SAMPLE_MALFORMED_SOURCE,
-                                     SAMPLE_MALFORMED_PK, SAMPLE_UNKNOWN_ATTRIBUTE,
-                                     SAMPLE_INVALID_MULTIPLE_ATTRIBUTE,
-                                     KEY_CERT_SIGNED_MESSAGE_VALID, KEY_CERT_SIGNED_MESSAGE_INVALID,
-                                     KEY_CERT_SIGNED_MESSAGE_CORRUPT,
-                                     KEY_CERT_SIGNED_MESSAGE_WRONG_KEY,
-                                     TEMPLATE_ROUTE_OBJECT,
-                                     TEMPLATE_PERSON_OBJECT,
-                                     SAMPLE_LINE_NEITHER_CONTINUATION_NOR_ATTR,
-                                     SAMPLE_MISSING_SOURCE, SAMPLE_ROUTE)
+from irrd.conf import AUTH_SET_CREATION_COMMON_KEY, PASSWORD_HASH_DUMMY_VALUE
+from irrd.utils.rpsl_samples import (
+    KEY_CERT_SIGNED_MESSAGE_CORRUPT,
+    KEY_CERT_SIGNED_MESSAGE_INVALID,
+    KEY_CERT_SIGNED_MESSAGE_VALID,
+    KEY_CERT_SIGNED_MESSAGE_WRONG_KEY,
+    SAMPLE_INVALID_MULTIPLE_ATTRIBUTE,
+    SAMPLE_LINE_NEITHER_CONTINUATION_NOR_ATTR,
+    SAMPLE_MALFORMED_ATTRIBUTE_NAME,
+    SAMPLE_MALFORMED_EMPTY_LINE,
+    SAMPLE_MALFORMED_PK,
+    SAMPLE_MALFORMED_SOURCE,
+    SAMPLE_MISSING_MANDATORY_ATTRIBUTE,
+    SAMPLE_MISSING_SOURCE,
+    SAMPLE_ROUTE,
+    SAMPLE_UNKNOWN_ATTRIBUTE,
+    SAMPLE_UNKNOWN_CLASS,
+    TEMPLATE_PERSON_OBJECT,
+    TEMPLATE_ROUTE_OBJECT,
+    object_sample_mapping,
+)
 
 from ..parser import UnknownRPSLObjectClassException
-from ..rpsl_objects import (RPSLAsBlock, RPSLAsSet, RPSLAutNum, RPSLDomain, RPSLFilterSet, RPSLInetRtr,
-                            RPSLInet6Num, RPSLInetnum, RPSLKeyCert, RPSLMntner, RPSLPeeringSet,
-                            RPSLPerson, RPSLRole, RPSLRoute, RPSLRouteSet, RPSLRoute6, RPSLRtrSet,
-                            OBJECT_CLASS_MAPPING, rpsl_object_from_text)
+from ..rpsl_objects import (
+    OBJECT_CLASS_MAPPING,
+    RPSLAsBlock,
+    RPSLAsSet,
+    RPSLAutNum,
+    RPSLDomain,
+    RPSLFilterSet,
+    RPSLInet6Num,
+    RPSLInetnum,
+    RPSLInetRtr,
+    RPSLKeyCert,
+    RPSLMntner,
+    RPSLPeeringSet,
+    RPSLPerson,
+    RPSLRole,
+    RPSLRoute,
+    RPSLRoute6,
+    RPSLRouteSet,
+    RPSLRtrSet,
+    rpsl_object_from_text,
+)
 
 
 class TestRPSLParsingGeneric:
     # Most malformed objects are tested without strict validation, as they should always fail.
     def test_unknown_class(self):
         with raises(UnknownRPSLObjectClassException) as ve:
             rpsl_object_from_text(SAMPLE_UNKNOWN_CLASS)
-        assert 'unknown object class' in str(ve.value)
+        assert "unknown object class" in str(ve.value)
 
     def test_malformed_empty_line(self):
         obj = rpsl_object_from_text(SAMPLE_MALFORMED_EMPTY_LINE, strict_validation=False)
-        assert len(obj.messages.errors()) == 1, f'Unexpected extra errors: {obj.messages.errors()}'
-        assert 'encountered empty line' in obj.messages.errors()[0]
+        assert len(obj.messages.errors()) == 1, f"Unexpected extra errors: {obj.messages.errors()}"
+        assert "encountered empty line" in obj.messages.errors()[0]
 
         with raises(ValueError):
             obj.source()
 
     def test_malformed_attribute_name(self):
         obj = rpsl_object_from_text(SAMPLE_MALFORMED_ATTRIBUTE_NAME, strict_validation=False)
-        assert len(obj.messages.errors()) == 1, f'Unexpected extra errors: {obj.messages.errors()}'
-        assert 'malformed attribute name' in obj.messages.errors()[0]
+        assert len(obj.messages.errors()) == 1, f"Unexpected extra errors: {obj.messages.errors()}"
+        assert "malformed attribute name" in obj.messages.errors()[0]
 
     def test_missing_mandatory_attribute(self):
         obj = rpsl_object_from_text(SAMPLE_MISSING_MANDATORY_ATTRIBUTE, strict_validation=True)
-        assert len(obj.messages.errors()) == 1, f'Unexpected extra errors: {obj.messages.errors()}'
+        assert len(obj.messages.errors()) == 1, f"Unexpected extra errors: {obj.messages.errors()}"
         assert 'Mandatory attribute "mnt-by" on object route is missing' in obj.messages.errors()[0]
 
         obj = rpsl_object_from_text(SAMPLE_MISSING_MANDATORY_ATTRIBUTE, strict_validation=False)
-        assert len(obj.messages.errors()) == 0, f'Unexpected extra errors: {obj.messages.errors()}'
+        assert len(obj.messages.errors()) == 0, f"Unexpected extra errors: {obj.messages.errors()}"
 
     def test_unknown_atribute(self):
         obj = rpsl_object_from_text(SAMPLE_UNKNOWN_ATTRIBUTE, strict_validation=True)
-        assert len(obj.messages.errors()) == 1, f'Unexpected extra errors: {obj.messages.errors()}'
-        assert 'Unrecognised attribute' in obj.messages.errors()[0]
+        assert len(obj.messages.errors()) == 1, f"Unexpected extra errors: {obj.messages.errors()}"
+        assert "Unrecognised attribute" in obj.messages.errors()[0]
 
         obj = rpsl_object_from_text(SAMPLE_UNKNOWN_ATTRIBUTE, strict_validation=False)
-        assert len(obj.messages.errors()) == 0, f'Unexpected extra errors: {obj.messages.errors()}'
+        assert len(obj.messages.errors()) == 0, f"Unexpected extra errors: {obj.messages.errors()}"
 
     def test_invalid_multiple_attribute(self):
         obj = rpsl_object_from_text(SAMPLE_INVALID_MULTIPLE_ATTRIBUTE, strict_validation=True)
-        assert len(obj.messages.errors()) == 1, f'Unexpected extra errors: {obj.messages.errors()}'
-        assert 'occurs multiple times' in obj.messages.errors()[0]
+        assert len(obj.messages.errors()) == 1, f"Unexpected extra errors: {obj.messages.errors()}"
+        assert "occurs multiple times" in obj.messages.errors()[0]
 
         obj = rpsl_object_from_text(SAMPLE_INVALID_MULTIPLE_ATTRIBUTE, strict_validation=False)
-        assert len(obj.messages.errors()) == 0, f'Unexpected extra errors: {obj.messages.errors()}'
+        assert len(obj.messages.errors()) == 0, f"Unexpected extra errors: {obj.messages.errors()}"
 
     def test_malformed_pk(self):
         obj = rpsl_object_from_text(SAMPLE_MALFORMED_PK, strict_validation=True)
-        assert len(obj.messages.errors()) == 1, f'Unexpected extra errors: {obj.messages.errors()}'
-        assert 'Invalid address prefix: not-a-prefix' in obj.messages.errors()[0]
+        assert len(obj.messages.errors()) == 1, f"Unexpected extra errors: {obj.messages.errors()}"
+        assert "Invalid address prefix: not-a-prefix" in obj.messages.errors()[0]
 
         # A primary key field should also be tested in non-strict mode
         obj = rpsl_object_from_text(SAMPLE_MALFORMED_PK, strict_validation=False)
-        assert len(obj.messages.errors()) == 1, f'Unexpected extra errors: {obj.messages.errors()}'
-        assert 'Invalid address prefix: not-a-prefix' in obj.messages.errors()[0]
+        assert len(obj.messages.errors()) == 1, f"Unexpected extra errors: {obj.messages.errors()}"
+        assert "Invalid address prefix: not-a-prefix" in obj.messages.errors()[0]
 
     def test_malformed_source(self):
         obj = rpsl_object_from_text(SAMPLE_MALFORMED_SOURCE, strict_validation=False)
-        assert len(obj.messages.errors()) == 1, f'Unexpected extra errors: {obj.messages.errors()}'
-        assert 'contains invalid characters' in obj.messages.errors()[0]
+        assert len(obj.messages.errors()) == 1, f"Unexpected extra errors: {obj.messages.errors()}"
+        assert "contains invalid characters" in obj.messages.errors()[0]
 
     def test_missing_source_optional_default_source(self):
-        obj = rpsl_object_from_text(SAMPLE_MISSING_SOURCE, strict_validation=False, default_source='TEST')
-        assert len(obj.messages.errors()) == 0, f'Unexpected extra errors: {obj.messages.errors()}'
-        assert obj.source() == 'TEST'
+        obj = rpsl_object_from_text(SAMPLE_MISSING_SOURCE, strict_validation=False, default_source="TEST")
+        assert len(obj.messages.errors()) == 0, f"Unexpected extra errors: {obj.messages.errors()}"
+        assert obj.source() == "TEST"
 
         obj = rpsl_object_from_text(SAMPLE_MISSING_SOURCE, strict_validation=False)
-        assert len(obj.messages.errors()) == 1, f'Unexpected extra errors: {obj.messages.errors()}'
+        assert len(obj.messages.errors()) == 1, f"Unexpected extra errors: {obj.messages.errors()}"
         assert 'attribute "source" on object route is missing' in obj.messages.errors()[0]
 
     def test_line_neither_continuation_nor_attribute(self):
         obj = rpsl_object_from_text(SAMPLE_LINE_NEITHER_CONTINUATION_NOR_ATTR, strict_validation=False)
-        assert len(obj.messages.errors()) == 1, f'Unexpected extra errors: {obj.messages.errors()}'
-        assert 'line is neither continuation nor valid attribute' in obj.messages.errors()[0]
+        assert len(obj.messages.errors()) == 1, f"Unexpected extra errors: {obj.messages.errors()}"
+        assert "line is neither continuation nor valid attribute" in obj.messages.errors()[0]
 
     def test_double_object_297(self):
-        obj = rpsl_object_from_text(SAMPLE_ROUTE + ' \n' + SAMPLE_ROUTE)
-        assert len(obj.messages.errors()) == 3, f'Unexpected extra errors: {obj.messages.errors()}'
+        obj = rpsl_object_from_text(SAMPLE_ROUTE + " \n" + SAMPLE_ROUTE)
+        assert len(obj.messages.errors()) == 3, f"Unexpected extra errors: {obj.messages.errors()}"
         assert 'Attribute "route" on object route occurs multiple times' in obj.messages.errors()[0]
 
 
 class TestRPSLAsBlock:
     def test_has_mapping(self):
         obj = RPSLAsBlock()
         assert OBJECT_CLASS_MAPPING[obj.rpsl_object_class] == obj.__class__
 
     def test_parse(self):
         rpsl_text = object_sample_mapping[RPSLAsBlock().rpsl_object_class]
         obj = rpsl_object_from_text(rpsl_text)
         assert obj.__class__ == RPSLAsBlock
         assert not obj.messages.errors()
-        assert obj.pk() == 'AS65536 - AS65538'
+        assert obj.pk() == "AS65536 - AS65538"
         assert obj.asn_first == 65536
         assert obj.asn_last == 65538
         # Field parsing will cause our object to look slightly different than the original, hence the replace()
-        assert obj.render_rpsl_text() == rpsl_text.replace('as065538', 'AS65538')
+        assert obj.render_rpsl_text() == rpsl_text.replace("as065538", "AS65538")
 
 
 class TestRPSLAsSet:
     def test_has_mapping(self):
         obj = RPSLAsSet()
         assert OBJECT_CLASS_MAPPING[obj.rpsl_object_class] == obj.__class__
 
     def test_parse(self):
         rpsl_text = object_sample_mapping[RPSLAsSet().rpsl_object_class]
         obj = rpsl_object_from_text(rpsl_text)
         assert obj.__class__ == RPSLAsSet
         assert obj.clean_for_create()
         assert not obj.messages.errors()
-        assert obj.pk() == 'AS65537:AS-SETTEST'
+        assert obj.pk() == "AS65537:AS-SETTEST"
         assert obj.referred_strong_objects() == [
-            ('admin-c', ['role', 'person'], ['PERSON-TEST']),
-            ('tech-c', ['role', 'person'], ['PERSON-TEST']),
-            ('mnt-by', ['mntner'], ['TEST-MNT'])
+            ("admin-c", ["role", "person"], ["PERSON-TEST"]),
+            ("tech-c", ["role", "person"], ["PERSON-TEST"]),
+            ("mnt-by", ["mntner"], ["TEST-MNT"]),
         ]
         assert obj.references_strong_inbound() == set()
-        assert obj.source() == 'TEST'
+        assert obj.source() == "TEST"
+        assert obj.pk_asn_segment == "AS65537"
 
-        assert obj.parsed_data['members'] == ['AS65538', 'AS65539', 'AS65537', 'AS-OTHERSET']
+        assert obj.parsed_data["members"] == ["AS65538", "AS65539", "AS65537", "AS-OTHERSET"]
         # Field parsing will cause our object to look slightly different than the original, hence the replace()
-        assert obj.render_rpsl_text() == rpsl_text.replace('AS65538, AS65539', 'AS65538,AS65539')
+        assert obj.render_rpsl_text() == rpsl_text.replace("AS65538, AS65539", "AS65538,AS65539")
 
     def test_clean_for_create(self, config_override):
         rpsl_text = object_sample_mapping[RPSLAsSet().rpsl_object_class]
-        rpsl_text = rpsl_text.replace('AS65537:AS-SETTEST', 'AS-SETTEST')
+        rpsl_text = rpsl_text.replace("AS65537:AS-SETTEST", "AS-SETTEST")
         obj = rpsl_object_from_text(rpsl_text)
         assert obj.__class__ == RPSLAsSet
         assert not obj.messages.errors()
         assert not obj.clean_for_create()
-        assert 'AS set names must be hierarchical and the first ' in obj.messages.errors()[0]
+        assert not obj.pk_asn_segment
+        assert "as-set names must be hierarchical and the first " in obj.messages.errors()[0]
 
-        config_override({'compatibility': {'permit_non_hierarchical_as_set_name': True}})
+        config_override({"auth": {"set_creation": {"as-set": {"prefix_required": False}}}})
         obj = rpsl_object_from_text(rpsl_text)
         assert obj.clean_for_create()
+        assert not obj.pk_asn_segment
+
+        config_override(
+            {"auth": {"set_creation": {AUTH_SET_CREATION_COMMON_KEY: {"prefix_required": False}}}}
+        )
+        obj = rpsl_object_from_text(rpsl_text)
+        assert obj.clean_for_create()
+        assert not obj.pk_asn_segment
 
 
 class TestRPSLAutNum:
     def test_has_mapping(self):
         obj = RPSLAutNum()
         assert OBJECT_CLASS_MAPPING[obj.rpsl_object_class] == obj.__class__
 
     def test_parse(self):
         rpsl_text = object_sample_mapping[RPSLAutNum().rpsl_object_class]
         obj = rpsl_object_from_text(rpsl_text)
         assert obj.__class__ == RPSLAutNum
         assert not obj.messages.errors()
-        assert obj.pk() == 'AS65537'
+        assert obj.pk() == "AS65537"
         assert obj.asn_first == 65537
         assert obj.asn_last == 65537
         assert obj.ip_version() is None
         assert obj.references_strong_inbound() == set()
         # Field parsing will cause our object to look slightly different than the original, hence the replace()
-        assert obj.render_rpsl_text() == rpsl_text.replace('as065537', 'AS65537')
+        assert obj.render_rpsl_text() == rpsl_text.replace("as065537", "AS65537")
 
 
 class TestRPSLDomain:
     def test_has_mapping(self):
         obj = RPSLDomain()
         assert OBJECT_CLASS_MAPPING[obj.rpsl_object_class] == obj.__class__
 
     def test_parse(self):
         rpsl_text = object_sample_mapping[RPSLDomain().rpsl_object_class]
         obj = rpsl_object_from_text(rpsl_text)
         assert obj.__class__ == RPSLDomain
         assert not obj.messages.errors()
-        assert obj.pk() == '2.0.192.IN-ADDR.ARPA'
-        assert obj.parsed_data['source'] == 'TEST'
+        assert obj.pk() == "2.0.192.IN-ADDR.ARPA"
+        assert obj.parsed_data["source"] == "TEST"
         assert obj.render_rpsl_text() == rpsl_text
 
 
 class TestRPSLFilterSet:
     def test_has_mapping(self):
         obj = RPSLFilterSet()
         assert OBJECT_CLASS_MAPPING[obj.rpsl_object_class] == obj.__class__
 
     def test_parse(self):
         rpsl_text = object_sample_mapping[RPSLFilterSet().rpsl_object_class]
         obj = rpsl_object_from_text(rpsl_text)
         assert obj.__class__ == RPSLFilterSet
         assert not obj.messages.errors()
-        assert obj.pk() == 'FLTR-SETTEST'
-        assert obj.render_rpsl_text() == rpsl_text.replace('\t', '+')  # #298
+        assert obj.pk() == "FLTR-SETTEST"
+        assert obj.render_rpsl_text() == rpsl_text.replace("\t", "+")  # #298
 
 
 class TestRPSLInetRtr:
     def test_has_mapping(self):
         obj = RPSLInetRtr()
         assert OBJECT_CLASS_MAPPING[obj.rpsl_object_class] == obj.__class__
 
     def test_parse(self):
         rpsl_text = object_sample_mapping[RPSLInetRtr().rpsl_object_class]
         obj = rpsl_object_from_text(rpsl_text)
         assert obj.__class__ == RPSLInetRtr
         assert not obj.messages.errors()
-        assert obj.pk() == 'RTR.EXAMPLE.COM'
-        assert obj.parsed_data['inet-rtr'] == 'RTR.EXAMPLE.COM'
+        assert obj.pk() == "RTR.EXAMPLE.COM"
+        assert obj.parsed_data["inet-rtr"] == "RTR.EXAMPLE.COM"
         assert obj.render_rpsl_text() == rpsl_text
 
 
 class TestRPSLInet6Num:
     def test_has_mapping(self):
         obj = RPSLInet6Num()
         assert OBJECT_CLASS_MAPPING[obj.rpsl_object_class] == obj.__class__
 
     def test_parse(self):
         rpsl_text = object_sample_mapping[RPSLInet6Num().rpsl_object_class]
         obj = rpsl_object_from_text(rpsl_text)
         assert obj.__class__ == RPSLInet6Num
         assert not obj.messages.errors()
-        assert obj.pk() == '2001:DB8::/48'
-        assert obj.ip_first == IP('2001:db8::')
-        assert obj.ip_last == IP('2001:db8::ffff:ffff:ffff:ffff:ffff')
+        assert obj.pk() == "2001:DB8::/48"
+        assert obj.ip_first == IP("2001:db8::")
+        assert obj.ip_last == IP("2001:db8::ffff:ffff:ffff:ffff:ffff")
         assert obj.ip_version() == 6
         assert obj.render_rpsl_text() == rpsl_text
 
 
 class TestRPSLInetnum:
     def test_has_mapping(self):
         obj = RPSLInetnum()
         assert OBJECT_CLASS_MAPPING[obj.rpsl_object_class] == obj.__class__
 
     def test_parse(self):
         rpsl_text = object_sample_mapping[RPSLInetnum().rpsl_object_class]
         obj = rpsl_object_from_text(rpsl_text)
         assert obj.__class__ == RPSLInetnum
         assert not obj.messages.errors()
-        assert obj.pk() == '192.0.2.0 - 192.0.2.255'
-        assert obj.ip_first == IP('192.0.2.0')
-        assert obj.ip_last == IP('192.0.2.255')
+        assert obj.pk() == "192.0.2.0 - 192.0.2.255"
+        assert obj.ip_first == IP("192.0.2.0")
+        assert obj.ip_last == IP("192.0.2.255")
         assert obj.ip_version() == 4
         assert obj.references_strong_inbound() == set()
         # Field parsing will cause our object to look slightly different than the original, hence the replace()
-        assert obj.render_rpsl_text() == rpsl_text.replace('192.0.02.255', '192.0.2.255')
+        assert obj.render_rpsl_text() == rpsl_text.replace("192.0.02.255", "192.0.2.255")
 
 
 class TestRPSLKeyCert:
     """
     The tests for KeyCert objects intentionally do not mock gnupg, meaning these
     tests call the actual gpg binary, as the test has little value when gpg is
     mocked out.
     """
+
     def test_has_mapping(self):
         obj = RPSLKeyCert()
         assert OBJECT_CLASS_MAPPING[obj.rpsl_object_class] == obj.__class__
 
-    @pytest.mark.usefixtures('tmp_gpg_dir')  # noqa: F811
+    @pytest.mark.usefixtures("tmp_gpg_dir")  # noqa: F811
     def test_parse_parse(self):
         rpsl_text = object_sample_mapping[RPSLKeyCert().rpsl_object_class]
 
         # Mangle the fingerprint/owner/method lines to ensure the parser correctly re-generates them
-        mangled_rpsl_text = rpsl_text.replace('8626 1D8D BEBD A4F5 4692  D64D A838 3BA7 80F2 38C6', 'fingerprint')
-        mangled_rpsl_text = mangled_rpsl_text.replace('sasha', 'foo').replace('method:         PGP', 'method: test')
+        mangled_rpsl_text = rpsl_text.replace(
+            "8626 1D8D BEBD A4F5 4692  D64D A838 3BA7 80F2 38C6", "fingerprint"
+        )
+        mangled_rpsl_text = mangled_rpsl_text.replace("sasha", "foo").replace(
+            "method:         PGP", "method: test"
+        )
 
-        expected_text = rpsl_text.replace('                \n', '+               \n')  # #298
+        expected_text = rpsl_text.replace("                \n", "+               \n")  # #298
         obj = rpsl_object_from_text(mangled_rpsl_text)
         assert obj.__class__ == RPSLKeyCert
         assert not obj.messages.errors()
-        assert obj.pk() == 'PGPKEY-80F238C6'
+        assert obj.pk() == "PGPKEY-80F238C6"
         assert obj.render_rpsl_text() == expected_text
-        assert obj.parsed_data['fingerpr'] == '8626 1D8D BEBD A4F5 4692  D64D A838 3BA7 80F2 38C6'
+        assert obj.parsed_data["fingerpr"] == "8626 1D8D BEBD A4F5 4692  D64D A838 3BA7 80F2 38C6"
 
-    @pytest.mark.usefixtures('tmp_gpg_dir')  # noqa: F811
+    @pytest.mark.usefixtures("tmp_gpg_dir")  # noqa: F811
     def test_parse_incorrect_object_name(self, tmp_gpg_dir):
         rpsl_text = object_sample_mapping[RPSLKeyCert().rpsl_object_class]
-        obj = rpsl_object_from_text(rpsl_text.replace('PGPKEY-80F238C6', 'PGPKEY-80F23816'))
+        obj = rpsl_object_from_text(rpsl_text.replace("PGPKEY-80F238C6", "PGPKEY-80F23816"))
 
         errors = obj.messages.errors()
-        assert len(errors) == 1, f'Unexpected multiple errors: {errors}'
-        assert 'does not match key fingerprint' in errors[0]
+        assert len(errors) == 1, f"Unexpected multiple errors: {errors}"
+        assert "does not match key fingerprint" in errors[0]
 
-    @pytest.mark.usefixtures('tmp_gpg_dir')  # noqa: F811
+    @pytest.mark.usefixtures("tmp_gpg_dir")  # noqa: F811
     def test_parse_missing_key(self, tmp_gpg_dir):
         rpsl_text = object_sample_mapping[RPSLKeyCert().rpsl_object_class]
-        obj = rpsl_object_from_text(rpsl_text.replace('certif:', 'remarks:'), strict_validation=True)
+        obj = rpsl_object_from_text(rpsl_text.replace("certif:", "remarks:"), strict_validation=True)
 
         errors = obj.messages.errors()
-        assert len(errors) == 1, f'Unexpected multiple errors: {errors}'
+        assert len(errors) == 1, f"Unexpected multiple errors: {errors}"
         assert 'Mandatory attribute "certif" on object key-cert is missing' in errors[0]
 
-    @pytest.mark.usefixtures('tmp_gpg_dir')  # noqa: F811
+    @pytest.mark.usefixtures("tmp_gpg_dir")  # noqa: F811
     def test_parse_invalid_key(self, tmp_gpg_dir):
         rpsl_text = object_sample_mapping[RPSLKeyCert().rpsl_object_class]
-        obj = rpsl_object_from_text(rpsl_text.replace('mQINBFnY7YoBEADH5ooPsoR9G', 'foo'), strict_validation=True)
+        obj = rpsl_object_from_text(
+            rpsl_text.replace("mQINBFnY7YoBEADH5ooPsoR9G", "foo"), strict_validation=True
+        )
 
         errors = obj.messages.errors()
-        assert len(errors) == 1, f'Unexpected multiple errors: {errors}'
-        assert 'Unable to read public PGP key: key corrupt or multiple keys provided: No valid data found' in errors[0]
+        assert len(errors) == 1, f"Unexpected multiple errors: {errors}"
+        assert "Unable to read public PGP key: key corrupt or multiple keys provided" in errors[0]
 
-    @pytest.mark.usefixtures('tmp_gpg_dir')  # noqa: F811
+    @pytest.mark.usefixtures("tmp_gpg_dir")  # noqa: F811
     def test_verify(self, tmp_gpg_dir):
         rpsl_text = object_sample_mapping[RPSLKeyCert().rpsl_object_class]
         obj = rpsl_object_from_text(rpsl_text)
 
         assert obj.verify(KEY_CERT_SIGNED_MESSAGE_VALID)
         assert not obj.verify(KEY_CERT_SIGNED_MESSAGE_INVALID)
         assert not obj.verify(KEY_CERT_SIGNED_MESSAGE_CORRUPT)
@@ -325,77 +366,83 @@
 
 
 class TestRPSLMntner:
     def test_has_mapping(self):
         obj = RPSLMntner()
         assert OBJECT_CLASS_MAPPING[obj.rpsl_object_class] == obj.__class__
 
-    def test_parse(self):
+    def test_parse(self, config_override):
+        config_override({"auth": {"password_hashers": {"crypt-pw": "enabled"}}})
         rpsl_text = object_sample_mapping[RPSLMntner().rpsl_object_class]
         obj = rpsl_object_from_text(rpsl_text)
         assert obj.__class__ == RPSLMntner
 
         assert not obj.messages.errors()
-        assert obj.pk() == 'TEST-MNT'
-        assert obj.parsed_data['mnt-by'] == ['TEST-MNT', 'OTHER1-MNT', 'OTHER2-MNT']
+        assert obj.pk() == "TEST-MNT"
+        assert obj.parsed_data["mnt-by"] == ["TEST-MNT", "OTHER1-MNT", "OTHER2-MNT"]
         assert obj.render_rpsl_text() == rpsl_text
-        assert obj.references_strong_inbound() == {'mnt-by'}
+        assert obj.references_strong_inbound() == {"mnt-by"}
 
-    def test_parse_invalid_partial_dummy_hash(self):
+    def test_parse_invalid_partial_dummy_hash(self, config_override):
+        config_override({"auth": {"password_hashers": {"crypt-pw": "enabled"}}})
         rpsl_text = object_sample_mapping[RPSLMntner().rpsl_object_class]
-        rpsl_text = rpsl_text.replace('LEuuhsBJNFV0Q', PASSWORD_HASH_DUMMY_VALUE)
+        rpsl_text = rpsl_text.replace("LEuuhsBJNFV0Q", PASSWORD_HASH_DUMMY_VALUE)
         obj = rpsl_object_from_text(rpsl_text)
         assert obj.__class__ == RPSLMntner
         assert obj.messages.errors() == [
-            'Either all password auth hashes in a submitted mntner must be dummy objects, or none.'
+            "Either all password auth hashes in a submitted mntner must be dummy objects, or none."
         ]
 
-    @pytest.mark.usefixtures('tmp_gpg_dir')  # noqa: F811
+    @pytest.mark.usefixtures("tmp_gpg_dir")  # noqa: F811
     def test_verify(self, tmp_gpg_dir):
         rpsl_text = object_sample_mapping[RPSLMntner().rpsl_object_class]
         # Unknown hashes and invalid hashes should simply be ignored.
-        obj = rpsl_object_from_text(rpsl_text + 'auth: UNKNOWN_HASH foo\nauth: MD5-PW 💩')
-
-        assert obj.verify_auth(['crypt-password'])
-        assert obj.verify_auth(['md5-password'])
-        assert obj.verify_auth(['md5-password'], 'PGPKey-80F238C6')
-        assert not obj.verify_auth(['other-password'])
+        # Strict validation set to False to allow legacy mode for CRYPT-PW
+        obj = rpsl_object_from_text(
+            rpsl_text + "auth: UNKNOWN_HASH foo\nauth: MD5-PW 💩", strict_validation=False
+        )
+
+        assert obj.verify_auth(["crypt-password"])
+        assert obj.verify_auth(["md5-password"])
+        assert obj.verify_auth(["bcrypt-password"])
+        assert obj.verify_auth(["md5-password"], "PGPKey-80F238C6")
+        assert not obj.verify_auth(["other-password"])
         assert not obj.verify_auth([KEY_CERT_SIGNED_MESSAGE_CORRUPT])
         assert not obj.verify_auth([KEY_CERT_SIGNED_MESSAGE_WRONG_KEY])
 
 
 class TestRPSLPeeringSet:
     def test_has_mapping(self):
         obj = RPSLPeeringSet()
         assert OBJECT_CLASS_MAPPING[obj.rpsl_object_class] == obj.__class__
 
     def test_parse(self):
         rpsl_text = object_sample_mapping[RPSLPeeringSet().rpsl_object_class]
         obj = rpsl_object_from_text(rpsl_text)
         assert obj.__class__ == RPSLPeeringSet
         assert not obj.messages.errors()
-        assert obj.pk() == 'PRNG-SETTEST'
-        assert obj.parsed_data['tech-c'] == ['PERSON-TEST', 'DUMY2-TEST']
+        assert obj.pk() == "PRNG-SETTEST"
+        assert obj.parsed_data["tech-c"] == ["PERSON-TEST", "DUMY2-TEST"]
         assert obj.render_rpsl_text() == rpsl_text
 
 
 class TestRPSLPerson:
     def test_has_mapping(self):
         obj = RPSLPerson()
         assert OBJECT_CLASS_MAPPING[obj.rpsl_object_class] == obj.__class__
 
     def test_parse(self):
         rpsl_text = object_sample_mapping[RPSLPerson().rpsl_object_class]
         obj = rpsl_object_from_text(rpsl_text)
         assert obj.__class__ == RPSLPerson
         assert not obj.messages.errors()
-        assert obj.pk() == 'PERSON-TEST'
-        assert obj.parsed_data['nic-hdl'] == 'PERSON-TEST'
+        assert obj.pk() == "PERSON-TEST"
+        assert obj.parsed_data["nic-hdl"] == "PERSON-TEST"
         assert obj.render_rpsl_text() == rpsl_text
-        assert obj.references_strong_inbound() == {'admin-c', 'tech-c', 'zone-c'}
+        assert obj.references_strong_inbound() == {"admin-c", "tech-c", "zone-c"}
 
     def test_generate_template(self):
         template = RPSLPerson().generate_template()
         assert template == TEMPLATE_PERSON_OBJECT
 
 
 class TestRPSLRole:
@@ -404,52 +451,52 @@
         assert OBJECT_CLASS_MAPPING[obj.rpsl_object_class] == obj.__class__
 
     def test_parse(self):
         rpsl_text = object_sample_mapping[RPSLRole().rpsl_object_class]
         obj = rpsl_object_from_text(rpsl_text)
         assert obj.__class__ == RPSLRole
         assert not obj.messages.errors()
-        assert obj.pk() == 'ROLE-TEST'
+        assert obj.pk() == "ROLE-TEST"
         assert obj.render_rpsl_text() == rpsl_text
-        assert obj.references_strong_inbound() == {'admin-c', 'tech-c', 'zone-c'}
+        assert obj.references_strong_inbound() == {"admin-c", "tech-c", "zone-c"}
 
 
 class TestRPSLRoute:
     def test_has_mapping(self):
         obj = RPSLRoute()
         assert OBJECT_CLASS_MAPPING[obj.rpsl_object_class] == obj.__class__
 
     def test_parse(self):
         rpsl_text = object_sample_mapping[RPSLRoute().rpsl_object_class]
         obj = rpsl_object_from_text(rpsl_text)
         assert obj.__class__ == RPSLRoute
         assert not obj.messages.errors()
-        assert obj.pk() == '192.0.2.0/24AS65537'
-        assert obj.ip_first == IP('192.0.2.0')
-        assert obj.ip_last == IP('192.0.2.255')
-        assert obj.prefix == IP('192.0.2.0/24')
+        assert obj.pk() == "192.0.2.0/24AS65537"
+        assert obj.ip_first == IP("192.0.2.0")
+        assert obj.ip_last == IP("192.0.2.255")
+        assert obj.prefix == IP("192.0.2.0/24")
         assert obj.prefix_length == 24
         assert obj.asn_first == 65537
         assert obj.asn_last == 65537
         assert obj.ip_version() == 4
         assert obj.references_strong_inbound() == set()
 
-        expected_text = rpsl_text.replace('  192.0.02.0/24', '  192.0.2.0/24')
-        expected_text = expected_text.replace('rpki-ov-state: valid  # should be discarded\n', '')
+        expected_text = rpsl_text.replace("  192.0.02.0/24", "  192.0.2.0/24")
+        expected_text = expected_text.replace("rpki-ov-state: valid  # should be discarded\n", "")
         assert obj.render_rpsl_text() == expected_text
 
     def test_missing_pk_nonstrict(self):
         # In non-strict mode, the parser should not fail validation for missing
         # attributes, except for those part of the PK. Route is one of the few
         # objects that has two PK attributes.
-        missing_pk_route = 'route: 192.0.2.0/24'
+        missing_pk_route = "route: 192.0.2.0/24"
         obj = rpsl_object_from_text(missing_pk_route, strict_validation=False)
         assert obj.__class__ == RPSLRoute
         errors = obj.messages.errors()
-        assert len(errors) == 2, f'Unexpected extra errors: {errors}'
+        assert len(errors) == 2, f"Unexpected extra errors: {errors}"
         assert 'Primary key attribute "origin" on object route is missing' in errors[0]
         assert 'Primary key attribute "source" on object route is missing' in errors[1]
 
     def test_generate_template(self):
         template = RPSLRoute().generate_template()
         assert template == TEMPLATE_ROUTE_OBJECT
 
@@ -460,76 +507,74 @@
         assert OBJECT_CLASS_MAPPING[obj.rpsl_object_class] == obj.__class__
 
     def test_parse(self):
         rpsl_text = object_sample_mapping[RPSLRouteSet().rpsl_object_class]
         obj = rpsl_object_from_text(rpsl_text)
         assert obj.__class__ == RPSLRouteSet
         assert not obj.messages.errors()
-        assert obj.pk() == 'RS-TEST'
-        assert obj.parsed_data['mp-members'] == ['2001:db8::/48']
-        assert obj.render_rpsl_text() == rpsl_text.replace('2001:0dB8::/48', '2001:db8::/48')
+        assert obj.pk() == "RS-TEST"
+        assert obj.parsed_data["mp-members"] == ["2001:db8::/48"]
+        assert obj.render_rpsl_text() == rpsl_text.replace("2001:0dB8::/48", "2001:db8::/48")
         assert obj.references_strong_inbound() == set()
 
 
 class TestRPSLRoute6:
     def test_has_mapping(self):
         obj = RPSLRoute6()
         assert OBJECT_CLASS_MAPPING[obj.rpsl_object_class] == obj.__class__
 
     def test_parse(self):
         rpsl_text = object_sample_mapping[RPSLRoute6().rpsl_object_class]
         obj = rpsl_object_from_text(rpsl_text)
         assert obj.__class__ == RPSLRoute6
         assert not obj.messages.errors()
-        assert obj.pk() == '2001:DB8::/48AS65537'
-        assert obj.ip_first == IP('2001:db8::')
-        assert obj.ip_last == IP('2001:db8::ffff:ffff:ffff:ffff:ffff')
-        assert obj.prefix == IP('2001:db8::/48')
+        assert obj.pk() == "2001:DB8::/48AS65537"
+        assert obj.ip_first == IP("2001:db8::")
+        assert obj.ip_last == IP("2001:db8::ffff:ffff:ffff:ffff:ffff")
+        assert obj.prefix == IP("2001:db8::/48")
         assert obj.prefix_length == 48
         assert obj.asn_first == 65537
         assert obj.asn_last == 65537
         assert obj.ip_version() == 6
-        assert obj.parsed_data['mnt-by'] == ['TEST-MNT']
+        assert obj.parsed_data["mnt-by"] == ["TEST-MNT"]
         assert obj.render_rpsl_text() == rpsl_text
         assert obj.references_strong_inbound() == set()
 
 
 class TestRPSLRtrSet:
     def test_has_mapping(self):
         obj = RPSLRtrSet()
         assert OBJECT_CLASS_MAPPING[obj.rpsl_object_class] == obj.__class__
 
     def test_parse(self):
         rpsl_text = object_sample_mapping[RPSLRtrSet().rpsl_object_class]
         obj = rpsl_object_from_text(rpsl_text)
         assert obj.__class__ == RPSLRtrSet
         assert not obj.messages.errors()
-        assert obj.pk() == 'RTRS-SETTEST'
-        assert obj.parsed_data['rtr-set'] == 'RTRS-SETTEST'
+        assert obj.pk() == "RTRS-SETTEST"
+        assert obj.parsed_data["rtr-set"] == "RTRS-SETTEST"
         assert obj.referred_strong_objects() == [
-            ('admin-c', ['role', 'person'], ['PERSON-TEST']),
-            ('tech-c', ['role', 'person'], ['PERSON-TEST']),
-            ('mnt-by', ['mntner'], ['TEST-MNT'])
+            ("admin-c", ["role", "person"], ["PERSON-TEST"]),
+            ("tech-c", ["role", "person"], ["PERSON-TEST"]),
+            ("mnt-by", ["mntner"], ["TEST-MNT"]),
         ]
         assert obj.references_strong_inbound() == set()
         assert obj.render_rpsl_text() == rpsl_text
 
 
 class TestLastModified:
     def test_authoritative(self, config_override):
-        config_override({
-            'sources': {'TEST': {'authoritative': True}}
-        })
+        config_override({"sources": {"TEST": {"authoritative": True}}})
         rpsl_text = object_sample_mapping[RPSLRtrSet().rpsl_object_class]
-        obj = rpsl_object_from_text(rpsl_text + 'last-modified: old-value\n')
+        obj = rpsl_object_from_text(rpsl_text + "last-modified: old-value\n")
         assert not obj.messages.errors()
-        last_modified = datetime.datetime(2020, 1, 1, tzinfo=timezone('UTC'))
-        expected_text = rpsl_text + 'last-modified:  2020-01-01T00:00:00Z\n'
+        last_modified = datetime.datetime(2020, 1, 1, tzinfo=timezone("UTC"))
+        expected_text = rpsl_text + "last-modified:  2020-01-01T00:00:00Z\n"
         assert obj.render_rpsl_text(last_modified=last_modified) == expected_text
 
     def test_not_authoritative(self):
         rpsl_text = object_sample_mapping[RPSLRtrSet().rpsl_object_class]
-        obj = rpsl_object_from_text(rpsl_text + 'last-modified: old-value\n')
+        obj = rpsl_object_from_text(rpsl_text + "last-modified: old-value\n")
         assert not obj.messages.errors()
-        last_modified = datetime.datetime(2020, 1, 1, tzinfo=timezone('UTC'))
-        expected_text = rpsl_text + 'last-modified:  old-value\n'
+        last_modified = datetime.datetime(2020, 1, 1, tzinfo=timezone("UTC"))
+        expected_text = rpsl_text + "last-modified:  old-value\n"
         assert obj.render_rpsl_text(last_modified=last_modified) == expected_text
```

### Comparing `irrd-4.2.8/irrd/scopefilter/tests/test_scopefilter.py` & `irrd-4.3.0/irrd/scopefilter/tests/test_scopefilter.py`

 * *Files 11% similar despite different names*

```diff
@@ -2,217 +2,229 @@
 
 import pytest
 from IPy import IP
 
 from irrd.rpsl.rpsl_objects import rpsl_object_from_text
 from irrd.storage.database_handler import DatabaseHandler
 from irrd.storage.queries import RPSLDatabaseQuery
-from irrd.utils.rpsl_samples import SAMPLE_AUT_NUM, SAMPLE_ROUTE, SAMPLE_INETNUM
+from irrd.utils.rpsl_samples import SAMPLE_AUT_NUM, SAMPLE_INETNUM, SAMPLE_ROUTE
 from irrd.utils.test_utils import flatten_mock_calls
+
 from ..status import ScopeFilterStatus
 from ..validators import ScopeFilterValidator
 
 
 class TestScopeFilterValidator:
     def test_validate(self, config_override):
-        config_override({
-            'scopefilter': {
-                'asns': [
-                    '23456',
-                    '10-20',
-                ],
-                'prefixes': [
-                    '10/8',
-                    '192.168.0.0/24'
-                ],
-            },
-            'sources': {'TEST-EXCLUDED': {'scopefilter_excluded': True}}
-        })
+        config_override(
+            {
+                "scopefilter": {
+                    "asns": [
+                        "23456",
+                        "10-20",
+                    ],
+                    "prefixes": ["10/8", "192.168.0.0/24"],
+                },
+                "sources": {"TEST-EXCLUDED": {"scopefilter_excluded": True}},
+            }
+        )
 
         validator = ScopeFilterValidator()
-        assert validator.validate('TEST', IP('192.0.2/24')) == ScopeFilterStatus.in_scope
-        assert validator.validate('TEST', IP('192.168/24')) == ScopeFilterStatus.out_scope_prefix
-        assert validator.validate('TEST', IP('10.2.1/24')) == ScopeFilterStatus.out_scope_prefix
-        assert validator.validate('TEST', IP('192/8')) == ScopeFilterStatus.out_scope_prefix
-
-        assert validator.validate('TEST', asn=9) == ScopeFilterStatus.in_scope
-        assert validator.validate('TEST', asn=21) == ScopeFilterStatus.in_scope
-        assert validator.validate('TEST', asn=20) == ScopeFilterStatus.out_scope_as
-        assert validator.validate('TEST', asn=10) == ScopeFilterStatus.out_scope_as
-        assert validator.validate('TEST', asn=15) == ScopeFilterStatus.out_scope_as
-        assert validator.validate('TEST', asn=23456) == ScopeFilterStatus.out_scope_as
+        assert validator.validate("TEST", IP("192.0.2/24")) == ScopeFilterStatus.in_scope
+        assert validator.validate("TEST", IP("192.168/24")) == ScopeFilterStatus.out_scope_prefix
+        assert validator.validate("TEST", IP("10.2.1/24")) == ScopeFilterStatus.out_scope_prefix
+        assert validator.validate("TEST", IP("192/8")) == ScopeFilterStatus.out_scope_prefix
+
+        assert validator.validate("TEST", asn=9) == ScopeFilterStatus.in_scope
+        assert validator.validate("TEST", asn=21) == ScopeFilterStatus.in_scope
+        assert validator.validate("TEST", asn=20) == ScopeFilterStatus.out_scope_as
+        assert validator.validate("TEST", asn=10) == ScopeFilterStatus.out_scope_as
+        assert validator.validate("TEST", asn=15) == ScopeFilterStatus.out_scope_as
+        assert validator.validate("TEST", asn=23456) == ScopeFilterStatus.out_scope_as
 
-        assert validator.validate('TEST-EXCLUDED', IP('192/8')) == ScopeFilterStatus.in_scope
-        assert validator.validate('TEST-EXCLUDED', asn=20) == ScopeFilterStatus.in_scope
+        assert validator.validate("TEST-EXCLUDED", IP("192/8")) == ScopeFilterStatus.in_scope
+        assert validator.validate("TEST-EXCLUDED", asn=20) == ScopeFilterStatus.in_scope
 
         # Override to no filter
         config_override({})
         validator.load_filters()
-        assert validator.validate('TEST', IP('192.168/24')) == ScopeFilterStatus.in_scope
-        assert validator.validate('TEST', asn=20) == ScopeFilterStatus.in_scope
+        assert validator.validate("TEST", IP("192.168/24")) == ScopeFilterStatus.in_scope
+        assert validator.validate("TEST", asn=20) == ScopeFilterStatus.in_scope
 
     def test_invalid_input(self):
         validator = ScopeFilterValidator()
         with pytest.raises(ValueError) as ve:
-            validator.validate('TEST')
-        assert 'must be provided asn or prefix' in str(ve.value)
+            validator.validate("TEST")
+        assert "must be provided asn or prefix" in str(ve.value)
 
     def test_validate_rpsl_object(self, config_override):
         validator = ScopeFilterValidator()
         route_obj = rpsl_object_from_text(SAMPLE_ROUTE)
-        assert validator.validate_rpsl_object(route_obj) == (ScopeFilterStatus.in_scope, '')
+        assert validator.validate_rpsl_object(route_obj) == (ScopeFilterStatus.in_scope, "")
         autnum_obj = rpsl_object_from_text(SAMPLE_AUT_NUM)
-        assert validator.validate_rpsl_object(autnum_obj) == (ScopeFilterStatus.in_scope, '')
+        assert validator.validate_rpsl_object(autnum_obj) == (ScopeFilterStatus.in_scope, "")
 
-        config_override({
-            'scopefilter': {
-                'asns': ['65537'],
-            },
-        })
+        config_override(
+            {
+                "scopefilter": {
+                    "asns": ["65537"],
+                },
+            }
+        )
         validator.load_filters()
         result = validator.validate_rpsl_object(route_obj)
-        assert result == (ScopeFilterStatus.out_scope_as, 'ASN 65537 is out of scope')
+        assert result == (ScopeFilterStatus.out_scope_as, "ASN 65537 is out of scope")
         result = validator.validate_rpsl_object(autnum_obj)
-        assert result == (ScopeFilterStatus.out_scope_as, 'ASN 65537 is out of scope')
+        assert result == (ScopeFilterStatus.out_scope_as, "ASN 65537 is out of scope")
 
-        config_override({
-            'scopefilter': {
-                'prefixes': ['192.0.2.0/32'],
-            },
-        })
+        config_override(
+            {
+                "scopefilter": {
+                    "prefixes": ["192.0.2.0/32"],
+                },
+            }
+        )
         validator.load_filters()
         result = validator.validate_rpsl_object(route_obj)
-        assert result == (ScopeFilterStatus.out_scope_prefix, 'prefix 192.0.2.0/24 is out of scope')
+        assert result == (ScopeFilterStatus.out_scope_prefix, "prefix 192.0.2.0/24 is out of scope")
 
-        config_override({
-            'scopefilter': {
-                'prefix': ['0/0'],
-            },
-        })
+        config_override(
+            {
+                "scopefilter": {
+                    "prefix": ["0/0"],
+                },
+            }
+        )
         validator.load_filters()
 
         # Ignored object class
         result = validator.validate_rpsl_object(rpsl_object_from_text(SAMPLE_INETNUM))
-        assert result == (ScopeFilterStatus.in_scope, '')
+        assert result == (ScopeFilterStatus.in_scope, "")
 
     def test_validate_all_rpsl_objects(self, config_override, monkeypatch):
         mock_dh = Mock(spec=DatabaseHandler)
         mock_dq = Mock(spec=RPSLDatabaseQuery)
-        monkeypatch.setattr('irrd.scopefilter.validators.RPSLDatabaseQuery',
-                            lambda column_names=None, enable_ordering=True: mock_dq)
+        monkeypatch.setattr(
+            "irrd.scopefilter.validators.RPSLDatabaseQuery",
+            lambda column_names=None, enable_ordering=True: mock_dq,
+        )
+
+        config_override(
+            {
+                "scopefilter": {
+                    "asns": [
+                        "23456",
+                    ],
+                    "prefixes": [
+                        "192.0.2.0/25",
+                    ],
+                },
+            }
+        )
 
-        config_override({
-            'scopefilter': {
-                'asns': [
-                    '23456',
+        mock_query_result = iter(
+            [
+                [
+                    {
+                        # Should become in_scope
+                        "pk": "192.0.2.128/25,AS65547",
+                        "rpsl_pk": "192.0.2.128/25,AS65547",
+                        "prefix": "192.0.2.128/25",
+                        "asn_first": 65547,
+                        "source": "TEST",
+                        "object_class": "route",
+                        "scopefilter_status": ScopeFilterStatus.out_scope_prefix,
+                    },
+                    {
+                        # Should become out_scope_prefix
+                        "pk": "192.0.2.0/25,AS65547",
+                        "rpsl_pk": "192.0.2.0/25,AS65547",
+                        "prefix": "192.0.2.0/25",
+                        "asn_first": 65547,
+                        "source": "TEST",
+                        "object_class": "route",
+                        "scopefilter_status": ScopeFilterStatus.in_scope,
+                    },
+                    {
+                        # Should become out_scope_as
+                        "pk": "192.0.2.128/25,AS65547",
+                        "rpsl_pk": "192.0.2.128/25,AS65547",
+                        "prefix": "192.0.2.128/25",
+                        "asn_first": 23456,
+                        "source": "TEST",
+                        "object_class": "route",
+                        "scopefilter_status": ScopeFilterStatus.out_scope_prefix,
+                    },
+                    {
+                        # Should become out_scope_as
+                        "pk": "AS65547",
+                        "rpsl_pk": "AS65547",
+                        "asn_first": 23456,
+                        "source": "TEST",
+                        "object_class": "aut-num",
+                        "object_text": "text",
+                        "scopefilter_status": ScopeFilterStatus.in_scope,
+                    },
+                    {
+                        # Should not change
+                        "pk": "192.0.2.128/25,AS65548",
+                        "rpsl_pk": "192.0.2.128/25,AS65548",
+                        "prefix": "192.0.2.128/25",
+                        "asn_first": 65548,
+                        "source": "TEST",
+                        "object_class": "route",
+                        "scopefilter_status": ScopeFilterStatus.in_scope,
+                    },
                 ],
-                'prefixes': [
-                    '192.0.2.0/25',
+                [
+                    {
+                        "pk": "192.0.2.128/25,AS65547",
+                        "object_text": "text-192.0.2.128/25,AS65547",
+                    },
+                    {
+                        "pk": "192.0.2.0/25,AS65547",
+                        "object_text": "text-192.0.2.0/25,AS65547",
+                    },
+                    {
+                        "pk": "192.0.2.128/25,AS65547",
+                        "object_text": "text-192.0.2.128/25,AS65547",
+                    },
+                    {
+                        "pk": "AS65547",
+                        "object_text": "text-AS65547",
+                    },
                 ],
-            },
-        })
-
-        mock_query_result = iter([
-            [
-                {
-                    # Should become in_scope
-                    'pk': '192.0.2.128/25,AS65547',
-                    'rpsl_pk': '192.0.2.128/25,AS65547',
-                    'ip_first': '192.0.2.128',
-                    'prefix_length': 25,
-                    'asn_first': 65547,
-                    'source': 'TEST',
-                    'object_class': 'route',
-                    'scopefilter_status': ScopeFilterStatus.out_scope_prefix,
-                },
-                {
-                    # Should become out_scope_prefix
-                    'pk': '192.0.2.0/25,AS65547',
-                    'rpsl_pk': '192.0.2.0/25,AS65547',
-                    'ip_first': '192.0.2.0',
-                    'prefix_length': 25,
-                    'asn_first': 65547,
-                    'source': 'TEST',
-                    'object_class': 'route',
-                    'scopefilter_status': ScopeFilterStatus.in_scope,
-                },
-                {
-                    # Should become out_scope_as
-                    'pk': '192.0.2.128/25,AS65547',
-                    'rpsl_pk': '192.0.2.128/25,AS65547',
-                    'ip_first': '192.0.2.128',
-                    'prefix_length': 25,
-                    'asn_first': 23456,
-                    'source': 'TEST',
-                    'object_class': 'route',
-                    'scopefilter_status': ScopeFilterStatus.out_scope_prefix,
-                },
-                {
-                    # Should become out_scope_as
-                    'pk': 'AS65547',
-                    'rpsl_pk': 'AS65547',
-                    'asn_first': 23456,
-                    'source': 'TEST',
-                    'object_class': 'aut-num',
-                    'object_text': 'text',
-                    'scopefilter_status': ScopeFilterStatus.in_scope,
-                },
-                {
-                    # Should not change
-                    'pk': '192.0.2.128/25,AS65548',
-                    'rpsl_pk': '192.0.2.128/25,AS65548',
-                    'ip_first': '192.0.2.128',
-                    'prefix_length': 25,
-                    'asn_first': 65548,
-                    'source': 'TEST',
-                    'object_class': 'route',
-                    'scopefilter_status': ScopeFilterStatus.in_scope,
-                },
-            ],
-            [
-                {
-                    'pk': '192.0.2.128/25,AS65547',
-                    'object_text': 'text-192.0.2.128/25,AS65547',
-                },
-                {
-                    'pk': '192.0.2.0/25,AS65547',
-                    'object_text': 'text-192.0.2.0/25,AS65547',
-                },
-                {
-                    'pk': '192.0.2.128/25,AS65547',
-                    'object_text': 'text-192.0.2.128/25,AS65547',
-                },
-                {
-                    'pk': 'AS65547',
-                    'object_text': 'text-AS65547',
-                },
             ]
-        ])
+        )
         mock_dh.execute_query = lambda query: next(mock_query_result)
 
         validator = ScopeFilterValidator()
         result = validator.validate_all_rpsl_objects(mock_dh)
         now_in_scope, now_out_scope_as, now_out_scope_prefix = result
 
         assert len(now_in_scope) == 1
         assert len(now_out_scope_as) == 2
         assert len(now_out_scope_prefix) == 1
 
-        assert now_in_scope[0]['rpsl_pk'] == '192.0.2.128/25,AS65547'
-        assert now_in_scope[0]['old_status'] == ScopeFilterStatus.out_scope_prefix
-        assert now_in_scope[0]['object_text'] == 'text-192.0.2.128/25,AS65547'
-
-        assert now_out_scope_as[0]['rpsl_pk'] == '192.0.2.128/25,AS65547'
-        assert now_out_scope_as[0]['old_status'] == ScopeFilterStatus.out_scope_prefix
-        assert now_out_scope_as[0]['object_text'] == 'text-192.0.2.128/25,AS65547'
-        assert now_out_scope_as[1]['rpsl_pk'] == 'AS65547'
-        assert now_out_scope_as[1]['old_status'] == ScopeFilterStatus.in_scope
-        assert now_out_scope_as[1]['object_text'] == 'text-AS65547'
-
-        assert now_out_scope_prefix[0]['rpsl_pk'] == '192.0.2.0/25,AS65547'
-        assert now_out_scope_prefix[0]['old_status'] == ScopeFilterStatus.in_scope
-        assert now_out_scope_prefix[0]['object_text'] == 'text-192.0.2.0/25,AS65547'
+        assert now_in_scope[0]["rpsl_pk"] == "192.0.2.128/25,AS65547"
+        assert now_in_scope[0]["old_status"] == ScopeFilterStatus.out_scope_prefix
+        assert now_in_scope[0]["object_text"] == "text-192.0.2.128/25,AS65547"
+
+        assert now_out_scope_as[0]["rpsl_pk"] == "192.0.2.128/25,AS65547"
+        assert now_out_scope_as[0]["old_status"] == ScopeFilterStatus.out_scope_prefix
+        assert now_out_scope_as[0]["object_text"] == "text-192.0.2.128/25,AS65547"
+        assert now_out_scope_as[1]["rpsl_pk"] == "AS65547"
+        assert now_out_scope_as[1]["old_status"] == ScopeFilterStatus.in_scope
+        assert now_out_scope_as[1]["object_text"] == "text-AS65547"
+
+        assert now_out_scope_prefix[0]["rpsl_pk"] == "192.0.2.0/25,AS65547"
+        assert now_out_scope_prefix[0]["old_status"] == ScopeFilterStatus.in_scope
+        assert now_out_scope_prefix[0]["object_text"] == "text-192.0.2.0/25,AS65547"
 
         assert flatten_mock_calls(mock_dq) == [
-            ['object_classes', (['route', 'route6', 'aut-num'],), {}],
-            ['pks', (['192.0.2.128/25,AS65547', '192.0.2.0/25,AS65547', '192.0.2.128/25,AS65547', 'AS65547'],), {}],
+            ["object_classes", (["route", "route6", "aut-num"],), {}],
+            [
+                "pks",
+                (["192.0.2.128/25,AS65547", "192.0.2.0/25,AS65547", "192.0.2.128/25,AS65547", "AS65547"],),
+                {},
+            ],
         ]
```

### Comparing `irrd-4.2.8/irrd/scopefilter/validators.py` & `irrd-4.3.0/irrd/scopefilter/validators.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,16 +1,17 @@
 from collections import defaultdict
-from typing import Optional, Tuple, List, Dict
+from typing import Dict, List, Optional, Tuple
 
 from IPy import IP
 
 from irrd.conf import get_setting
 from irrd.rpsl.parser import RPSLObject
 from irrd.storage.database_handler import DatabaseHandler
 from irrd.storage.queries import RPSLDatabaseQuery
+
 from .status import ScopeFilterStatus
 
 
 class ScopeFilterValidator:
     """
     The scope filter validator validates whether prefixes, ASNs or RPSL
     objects fall within the configured scope filter.
@@ -20,36 +21,38 @@
         self.load_filters()
 
     def load_filters(self):
         """
         (Re)load the local cache of the configured filters.
         Also called by __init__
         """
-        prefixes = get_setting('scopefilter.prefixes', [])
+        prefixes = get_setting("scopefilter.prefixes", [])
         self.filtered_prefixes = [IP(prefix) for prefix in prefixes]
 
         self.filtered_asns = set()
         self.filtered_asn_ranges = set()
-        asn_filters = get_setting('scopefilter.asns', [])
+        asn_filters = get_setting("scopefilter.asns", [])
         for asn_filter in asn_filters:
-            if '-' in str(asn_filter):
-                start, end = asn_filter.split('-')
+            if "-" in str(asn_filter):
+                start, end = asn_filter.split("-")
                 self.filtered_asn_ranges.add((int(start), int(end)))
             else:
                 self.filtered_asns.add(int(asn_filter))
 
-    def validate(self, source: str, prefix: Optional[IP]=None, asn: Optional[int]=None) -> ScopeFilterStatus:
+    def validate(
+        self, source: str, prefix: Optional[IP] = None, asn: Optional[int] = None
+    ) -> ScopeFilterStatus:
         """
         Validate a prefix and/or ASN, for a particular source.
         Returns a tuple of a ScopeFilterStatus and an explanation string.
         """
         if not prefix and asn is None:
-            raise ValueError('Scope Filter validator must be provided asn or prefix')
+            raise ValueError("Scope Filter validator must be provided asn or prefix")
 
-        if get_setting(f'sources.{source}.scopefilter_excluded'):
+        if get_setting(f"sources.{source}.scopefilter_excluded"):
             return ScopeFilterStatus.in_scope
 
         if prefix:
             for filtered_prefix in self.filtered_prefixes:
                 if prefix.version() == filtered_prefix.version() and filtered_prefix.overlaps(prefix):
                     return ScopeFilterStatus.out_scope_prefix
 
@@ -58,94 +61,99 @@
                 return ScopeFilterStatus.out_scope_as
             for range_start, range_end in self.filtered_asn_ranges:
                 if range_start <= asn <= range_end:
                     return ScopeFilterStatus.out_scope_as
 
         return ScopeFilterStatus.in_scope
 
-    def _validate_rpsl_data(self, source: str, object_class: str, prefix: Optional[IP],
-                            asn_first: Optional[int]) -> Tuple[ScopeFilterStatus, str]:
+    def _validate_rpsl_data(
+        self, source: str, object_class: str, prefix: Optional[IP], asn_first: Optional[int]
+    ) -> Tuple[ScopeFilterStatus, str]:
         """
         Validate whether a particular set of RPSL data is in scope.
         Returns a ScopeFilterStatus.
         """
         out_of_scope = [ScopeFilterStatus.out_scope_prefix, ScopeFilterStatus.out_scope_as]
-        if object_class not in ['route', 'route6', 'aut-num']:
-            return ScopeFilterStatus.in_scope, ''
+        if object_class not in ["route", "route6", "aut-num"]:
+            return ScopeFilterStatus.in_scope, ""
 
         if prefix:
             prefix_state = self.validate(source, prefix)
             if prefix_state in out_of_scope:
-                return prefix_state, f'prefix {prefix} is out of scope'
+                return prefix_state, f"prefix {prefix} is out of scope"
 
         if asn_first is not None:
             asn_state = self.validate(source, asn=asn_first)
             if asn_state in out_of_scope:
-                return asn_state, f'ASN {asn_first} is out of scope'
+                return asn_state, f"ASN {asn_first} is out of scope"
 
-        return ScopeFilterStatus.in_scope, ''
+        return ScopeFilterStatus.in_scope, ""
 
     def validate_rpsl_object(self, rpsl_object: RPSLObject) -> Tuple[ScopeFilterStatus, str]:
         """
         Validate whether an RPSLObject is in scope.
         Returns a tuple of a ScopeFilterStatus and an explanation string.
         """
         return self._validate_rpsl_data(
             rpsl_object.source(),
             rpsl_object.rpsl_object_class,
             rpsl_object.prefix,
             rpsl_object.asn_first,
         )
 
-    def validate_all_rpsl_objects(self, database_handler: DatabaseHandler) -> \
-            Tuple[List[Dict[str, str]], List[Dict[str, str]], List[Dict[str, str]]]:
+    def validate_all_rpsl_objects(
+        self, database_handler: DatabaseHandler
+    ) -> Tuple[List[Dict[str, str]], List[Dict[str, str]], List[Dict[str, str]]]:
         """
         Apply the scope filter to all relevant objects.
 
         Retrieves all routes from the DB, and aggregates the validation results.
         Returns a tuple of three sets:
         - one with routes that should be set to status in_scope, but are not now
         - one with routes that should be set to status out_scope_as, but are not now
         - one with routes that should be set to status out_scope_prefix, but are not now
         Each object is recorded as a dict, which has the fields shown
         in "columns" below.
 
         Objects where their current status in the DB matches the new
         validation result, are not included in the return value.
         """
-        columns = ['pk', 'rpsl_pk', 'ip_first', 'prefix_length', 'asn_first', 'source', 'object_class',
-                   'scopefilter_status', 'rpki_status']
+        columns = ["pk", "rpsl_pk", "prefix", "asn_first", "source", "object_class", "scopefilter_status"]
 
         objs_changed: Dict[ScopeFilterStatus, List[Dict[str, str]]] = defaultdict(list)
 
         q = RPSLDatabaseQuery(column_names=columns, enable_ordering=False)
-        q = q.object_classes(['route', 'route6', 'aut-num'])
+        q = q.object_classes(["route", "route6", "aut-num"])
         results = database_handler.execute_query(q)
 
         for result in results:
-            current_status = result['scopefilter_status']
-            result['old_status'] = current_status
+            current_status = result["scopefilter_status"]
+            result["old_status"] = current_status
             prefix = None
-            if result.get('ip_first'):
-                prefix = IP(result['ip_first'] + '/' + str(result['prefix_length']))
+            if result.get("prefix"):
+                prefix = IP(result["prefix"])
             new_status, _ = self._validate_rpsl_data(
-                result['source'],
-                result['object_class'],
+                result["source"],
+                result["object_class"],
                 prefix,
-                result['asn_first'],
+                result["asn_first"],
             )
             if new_status != current_status:
-                result['scopefilter_status'] = new_status
+                result["scopefilter_status"] = new_status
                 objs_changed[new_status].append(result)
 
         # Object text is only retrieved for objects with state changes
-        pks_to_enrich = [obj['pk'] for objs in objs_changed.values() for obj in objs]
-        query = RPSLDatabaseQuery(['pk', 'object_text'], enable_ordering=False).pks(pks_to_enrich)
-        rows_per_pk = {row['pk']: row for row in database_handler.execute_query(query)}
+        pks_to_enrich = [obj["pk"] for objs in objs_changed.values() for obj in objs]
+        query = RPSLDatabaseQuery(
+            ["pk", "object_text", "rpki_status", "route_preference_status"], enable_ordering=False
+        ).pks(pks_to_enrich)
+        rows_per_pk = {row["pk"]: row for row in database_handler.execute_query(query)}
 
         for rpsl_objs in objs_changed.values():
             for rpsl_obj in rpsl_objs:
-                rpsl_obj.update(rows_per_pk[rpsl_obj['pk']])
+                rpsl_obj.update(rows_per_pk[rpsl_obj["pk"]])
 
-        return (objs_changed[ScopeFilterStatus.in_scope],
-                objs_changed[ScopeFilterStatus.out_scope_as],
-                objs_changed[ScopeFilterStatus.out_scope_prefix])
+        return (
+            objs_changed[ScopeFilterStatus.in_scope],
+            objs_changed[ScopeFilterStatus.out_scope_as],
+            objs_changed[ScopeFilterStatus.out_scope_prefix],
+        )
```

### Comparing `irrd-4.2.8/irrd/scripts/database_downgrade.py` & `irrd-4.3.0/irrd/scripts/database_downgrade.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,37 +1,46 @@
 #!/usr/bin/env python
 # flake8: noqa: E402
+import argparse
 import sys
+from pathlib import Path
 
-import argparse
 from alembic import command
 from alembic.config import Config
-from pathlib import Path
 
 irrd_root = str(Path(__file__).resolve().parents[2])
 sys.path.append(irrd_root)
 
-from irrd.conf import config_init, CONFIG_PATH_DEFAULT
+from irrd.conf import CONFIG_PATH_DEFAULT, config_init
 
 
 def run(version):
     alembic_cfg = Config()
-    alembic_cfg.set_main_option('script_location', f'{irrd_root}/irrd/storage/alembic')
+    alembic_cfg.set_main_option("script_location", f"{irrd_root}/irrd/storage/alembic")
     command.downgrade(alembic_cfg, version)
-    print(f'Downgrade successful, or already on this version.')
+    print(f"Downgrade successful, or already on this version.")
 
 
 def main():  # pragma: no cover
     description = """Downgrade the IRRd SQL database to a particular version by running database migrations. See release notes."""
     parser = argparse.ArgumentParser(description=description)
-    parser.add_argument('--config', dest='config_file_path', type=str,
-                        help=f'use a different IRRd config file (default: {CONFIG_PATH_DEFAULT})')
-    parser.add_argument('--version', dest='version', type=str, required=True,
-                        help=f'version to downgrade to (see release notes)')
+    parser.add_argument(
+        "--config",
+        dest="config_file_path",
+        type=str,
+        help=f"use a different IRRd config file (default: {CONFIG_PATH_DEFAULT})",
+    )
+    parser.add_argument(
+        "--version",
+        dest="version",
+        type=str,
+        required=True,
+        help=f"version to downgrade to (see release notes)",
+    )
     args = parser.parse_args()
 
     config_init(args.config_file_path)
     run(args.version)
 
 
-if __name__ == '__main__':  # pragma: no cover
+if __name__ == "__main__":  # pragma: no cover
     main()
```

### Comparing `irrd-4.2.8/irrd/scripts/database_upgrade.py` & `irrd-4.3.0/irrd/scripts/database_upgrade.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,37 +1,46 @@
 #!/usr/bin/env python
 # flake8: noqa: E402
+import argparse
 import sys
+from pathlib import Path
 
-import argparse
 from alembic import command
 from alembic.config import Config
-from pathlib import Path
 
 irrd_root = str(Path(__file__).resolve().parents[2])
 sys.path.append(irrd_root)
 
-from irrd.conf import config_init, CONFIG_PATH_DEFAULT
+from irrd.conf import CONFIG_PATH_DEFAULT, config_init
 
 
 def run(version):
     alembic_cfg = Config()
-    alembic_cfg.set_main_option('script_location', f'{irrd_root}/irrd/storage/alembic')
+    alembic_cfg.set_main_option("script_location", f"{irrd_root}/irrd/storage/alembic")
     command.upgrade(alembic_cfg, version)
-    print(f'Upgrade successful, or already on latest version.')
+    print(f"Upgrade successful, or already on latest version.")
 
 
 def main():  # pragma: no cover
     description = """Upgrade the IRRd SQL database to a particular version by running database migrations."""
     parser = argparse.ArgumentParser(description=description)
-    parser.add_argument('--config', dest='config_file_path', type=str,
-                        help=f'use a different IRRd config file (default: {CONFIG_PATH_DEFAULT})')
-    parser.add_argument('--version', dest='version', type=str, default='head',
-                        help=f'version to upgrade to (default: head, i.e. latest)')
+    parser.add_argument(
+        "--config",
+        dest="config_file_path",
+        type=str,
+        help=f"use a different IRRd config file (default: {CONFIG_PATH_DEFAULT})",
+    )
+    parser.add_argument(
+        "--version",
+        dest="version",
+        type=str,
+        default="head",
+        help=f"version to upgrade to (default: head, i.e. latest)",
+    )
     args = parser.parse_args()
 
     config_init(args.config_file_path)
     run(args.version)
 
 
-if __name__ == '__main__':  # pragma: no cover
+if __name__ == "__main__":  # pragma: no cover
     main()
```

### Comparing `irrd-4.2.8/irrd/scripts/load_database.py` & `irrd-4.3.0/irrd/scripts/load_database.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,73 +1,83 @@
 #!/usr/bin/env python
 # flake8: noqa: E402
 import argparse
 import logging
 import sys
-
 from pathlib import Path
 
-
 """
 Load an RPSL file into the database.
 """
 
 logger = logging.getLogger(__name__)
 sys.path.append(str(Path(__file__).resolve().parents[2]))
 
+from irrd.conf import CONFIG_PATH_DEFAULT, config_init, get_setting
+from irrd.mirroring.parsers import MirrorFileImportParser
 from irrd.rpki.validators import BulkRouteROAValidator
 from irrd.storage.database_handler import DatabaseHandler
-from irrd.mirroring.parsers import MirrorFileImportParser
-from irrd.conf import config_init, CONFIG_PATH_DEFAULT, get_setting
 
 
 def load(source, filename, serial) -> int:
-    if any([
-        get_setting(f'sources.{source}.import_source'),
-        get_setting(f'sources.{source}.import_serial_source')
-    ]):
-        print(f'Error: to use this command, import_source and import_serial_source '
-              f'for source {source} must not be set.')
+    if any(
+        [
+            get_setting(f"sources.{source}.import_source"),
+            get_setting(f"sources.{source}.import_serial_source"),
+        ]
+    ):
+        print(
+            "Error: to use this command, import_source and import_serial_source "
+            f"for source {source} must not be set."
+        )
         return 2
 
     dh = DatabaseHandler()
     roa_validator = BulkRouteROAValidator(dh)
     dh.delete_all_rpsl_objects_with_journal(source)
     dh.disable_journaling()
     parser = MirrorFileImportParser(
-        source=source, filename=filename, serial=serial, database_handler=dh,
-        direct_error_return=True, roa_validator=roa_validator)
+        source=source,
+        filename=filename,
+        serial=serial,
+        database_handler=dh,
+        direct_error_return=True,
+        roa_validator=roa_validator,
+    )
     error = parser.run_import()
     if error:
         dh.rollback()
     else:
         dh.commit()
     dh.close()
     if error:
-        print(f'Error occurred while processing object:\n{error}')
+        print(f"Error occurred while processing object:\n{error}")
         return 1
     return 0
 
 
 def main():  # pragma: no cover
     description = """Load an RPSL file into the database."""
     parser = argparse.ArgumentParser(description=description)
-    parser.add_argument('--config', dest='config_file_path', type=str,
-                        help=f'use a different IRRd config file (default: {CONFIG_PATH_DEFAULT})')
-    parser.add_argument('--serial', dest='serial', type=int,
-                        help=f'serial number (optional)')
-    parser.add_argument('--source', dest='source', type=str, required=True,
-                        help=f'name of the source, e.g. NTTCOM')
-    parser.add_argument('input_file', type=str,
-                        help='the name of a file to read')
+    parser.add_argument(
+        "--config",
+        dest="config_file_path",
+        type=str,
+        help=f"use a different IRRd config file (default: {CONFIG_PATH_DEFAULT})",
+    )
+    parser.add_argument("--serial", dest="serial", type=int, help=f"serial number (optional)")
+    parser.add_argument(
+        "--source", dest="source", type=str, required=True, help=f"name of the source, e.g. NTTCOM"
+    )
+    parser.add_argument("input_file", type=str, help="the name of a file to read")
     args = parser.parse_args()
 
     config_init(args.config_file_path)
-    if get_setting('database_readonly'):
-        print('Unable to run, because database_readonly is set')
+    if get_setting("database_readonly"):
+        print("Unable to run, because database_readonly is set")
         sys.exit(-1)
 
     sys.exit(load(args.source, args.input_file, args.serial))
 
 
-if __name__ == '__main__':  # pragma: no cover
+if __name__ == "__main__":  # pragma: no cover
     main()
```

### Comparing `irrd-4.2.8/irrd/scripts/load_pgp_keys.py` & `irrd-4.3.0/irrd/scripts/load_pgp_keys.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,54 +1,57 @@
 #!/usr/bin/env python
 # flake8: noqa: E402
 import argparse
 import logging
 import sys
-
 from pathlib import Path
 
 """
 Load an RPSL file into the database.
 """
 
 logger = logging.getLogger(__name__)
 sys.path.append(str(Path(__file__).resolve().parents[2]))
 
-from irrd.conf import config_init, CONFIG_PATH_DEFAULT
-from irrd.storage.queries import RPSLDatabaseQuery
-from irrd.storage.database_handler import DatabaseHandler
+from irrd.conf import CONFIG_PATH_DEFAULT, config_init
 from irrd.rpsl.rpsl_objects import rpsl_object_from_text
+from irrd.storage.database_handler import DatabaseHandler
+from irrd.storage.queries import RPSLDatabaseQuery
+
 
 def load_pgp_keys(source: str) -> None:
     dh = DatabaseHandler()
-    query = RPSLDatabaseQuery(column_names=['rpsl_pk', 'object_text'])
-    query = query.sources([source]).object_classes(['key-cert'])
+    query = RPSLDatabaseQuery(column_names=["rpsl_pk", "object_text"])
+    query = query.sources([source]).object_classes(["key-cert"])
     keycerts = dh.execute_query(query)
 
     for keycert in keycerts:
         rpsl_pk = keycert["rpsl_pk"]
-        print(f'Loading key-cert {rpsl_pk}')
+        print(f"Loading key-cert {rpsl_pk}")
         # Parsing the keycert in strict mode will load it into the GPG keychain
-        result = rpsl_object_from_text(keycert['object_text'], strict_validation=True)
+        result = rpsl_object_from_text(keycert["object_text"], strict_validation=True)
         if result.messages.errors():
-            print(f'Errors in PGP key {rpsl_pk}: {result.messages.errors()}')
+            print(f"Errors in PGP key {rpsl_pk}: {result.messages.errors()}")
 
-    print('All valid key-certs loaded into the GnuPG keychain.')
+    print("All valid key-certs loaded into the GnuPG keychain.")
     dh.close()
 
 
 def main():  # pragma: no cover
     description = """Load all PGP keys from key-cert objects for a specific source into the GnuPG keychain."""
     parser = argparse.ArgumentParser(description=description)
-    parser.add_argument('--config', dest='config_file_path', type=str,
-                        help=f'use a different IRRd config file (default: {CONFIG_PATH_DEFAULT})')
-    parser.add_argument('source', type=str,
-                        help='the name of the source for which to load PGP keys')
+    parser.add_argument(
+        "--config",
+        dest="config_file_path",
+        type=str,
+        help=f"use a different IRRd config file (default: {CONFIG_PATH_DEFAULT})",
+    )
+    parser.add_argument("source", type=str, help="the name of the source for which to load PGP keys")
     args = parser.parse_args()
 
     config_init(args.config_file_path)
 
     load_pgp_keys(args.source)
 
 
-if __name__ == '__main__':  # pragma: no cover
+if __name__ == "__main__":  # pragma: no cover
     main()
```

### Comparing `irrd-4.2.8/irrd/scripts/load_test.py` & `irrd-4.3.0/irrd/scripts/load_test.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,53 +1,51 @@
 #!/usr/bin/env python
 # flake8: noqa: E402
 
 """
 A simple load tester for IRRd.
 Sends random !g queries.
 """
-import time
-
 import argparse
 import random
 import socket
+import time
 
 
 def main(host, port, count):
-    queries = [b'!!\n']
+    queries = [b"!!\n"]
     for i in range(count):
         asn = random.randrange(1, 50000)
-        query = f'!gAS{asn}\n'.encode('ascii')
+        query = f"!gAS{asn}\n".encode("ascii")
         queries.append(query)
-    queries.append(b'!q\n')
+    queries.append(b"!q\n")
 
     s = socket.socket()
     s.settimeout(600)
     s.connect((host, port))
 
-    queries_str = b''.join(queries)
+    queries_str = b"".join(queries)
     s.sendall(queries_str)
 
     start_time = time.perf_counter()
     while 1:
-        data = s.recv(1024*1024)
+        data = s.recv(1024 * 1024)
         if not data:
             break
 
     elapsed = time.perf_counter() - start_time
     time_per_query = elapsed / count * 1000
     qps = int(count / elapsed)
-    print(f'Ran {count} queries in {elapsed}s, time per query {time_per_query} ms, {qps} qps')
+    print(f"Ran {count} queries in {elapsed}s, time per query {time_per_query} ms, {qps} qps")
 
 
-if __name__ == '__main__':  # pragma: no cover
+if __name__ == "__main__":  # pragma: no cover
     description = """A simple load tester for IRRd. Sends random !g queries."""
     parser = argparse.ArgumentParser(description=description)
-    parser.add_argument('--count', dest='count', type=int, default=5000,
-                        help=f'number of queries to run (default: 5000)')
-    parser.add_argument('host', type=str,
-                        help='hostname of instance')
-    parser.add_argument('port', type=int,
-                        help='port of instance')
+    parser.add_argument(
+        "--count", dest="count", type=int, default=5000, help=f"number of queries to run (default: 5000)"
+    )
+    parser.add_argument("host", type=str, help="hostname of instance")
+    parser.add_argument("port", type=int, help="port of instance")
     args = parser.parse_args()
 
     main(args.host, args.port, args.count)
```

### Comparing `irrd-4.2.8/irrd/scripts/mirror_force_reload.py` & `irrd-4.3.0/irrd/scripts/mirror_force_reload.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,45 +1,47 @@
 #!/usr/bin/env python
 # flake8: noqa: E402
 import argparse
 import logging
 import sys
-
 from pathlib import Path
 
 """
 Load an RPSL file into the database.
 """
 
 logger = logging.getLogger(__name__)
 sys.path.append(str(Path(__file__).resolve().parents[2]))
 
-from irrd.conf import config_init, CONFIG_PATH_DEFAULT, get_setting
-
+from irrd.conf import CONFIG_PATH_DEFAULT, config_init, get_setting
 from irrd.storage.database_handler import DatabaseHandler
 
+
 def set_force_reload(source) -> None:
     dh = DatabaseHandler()
     dh.set_force_reload(source)
     dh.commit()
     dh.close()
 
 
 def main():  # pragma: no cover
     description = """Force a full reload for a mirror."""
     parser = argparse.ArgumentParser(description=description)
-    parser.add_argument('--config', dest='config_file_path', type=str,
-                        help=f'use a different IRRd config file (default: {CONFIG_PATH_DEFAULT})')
-    parser.add_argument('source', type=str,
-                        help='the name of the source to reload')
+    parser.add_argument(
+        "--config",
+        dest="config_file_path",
+        type=str,
+        help=f"use a different IRRd config file (default: {CONFIG_PATH_DEFAULT})",
+    )
+    parser.add_argument("source", type=str, help="the name of the source to reload")
     args = parser.parse_args()
 
     config_init(args.config_file_path)
-    if get_setting('database_readonly'):
-        print('Unable to run, because database_readonly is set')
+    if get_setting("database_readonly"):
+        print("Unable to run, because database_readonly is set")
         sys.exit(-1)
 
     set_force_reload(args.source)
 
 
-if __name__ == '__main__':  # pragma: no cover
+if __name__ == "__main__":  # pragma: no cover
     main()
```

### Comparing `irrd-4.2.8/irrd/scripts/query_qa_comparison.py` & `irrd-4.3.0/irrd/scripts/query_qa_comparison.py`

 * *Files 10% similar despite different names*

```diff
@@ -3,32 +3,30 @@
 """
 This script will run a list of queries against two IRRD instances,
 and report significant results.
 """
 
 import argparse
 import difflib
-import sys
-
 import re
-from IPy import IP
-from ordered_set import OrderedSet
+import sys
 from pathlib import Path
 from typing import Optional
 
+from IPy import IP
+from ordered_set import OrderedSet
 
 sys.path.append(str(Path(__file__).resolve().parents[2]))
 
 from irrd.rpsl.rpsl_objects import rpsl_object_from_text
-from irrd.utils.text import splitline_unicodesafe, split_paragraphs_rpsl
-from irrd.utils.whois_client import whois_query_irrd, whois_query, WhoisQueryError
-
+from irrd.utils.text import split_paragraphs_rpsl, splitline_unicodesafe
+from irrd.utils.whois_client import WhoisQueryError, whois_query, whois_query_irrd
 
-SSP_QUERIES = ['!6', '!g', '!i']
-ASDOT_RE = re.compile(r'as\d+\.\d*', flags=re.IGNORECASE)
+SSP_QUERIES = ["!6", "!g", "!i"]
+ASDOT_RE = re.compile(r"as\d+\.\d*", flags=re.IGNORECASE)
 
 
 class QueryComparison:
     queries_run = 0
     queries_different = 0
     queries_both_error = 0
     queries_invalid = 0
@@ -36,167 +34,173 @@
 
     def __init__(self, input_file, host_reference, port_reference, host_tested, port_tested):
         self.host_reference = host_reference
         self.port_reference = port_reference
         self.host_tested = host_tested
         self.port_tested = port_tested
 
-        if input_file == '-':
+        if input_file == "-":
             f = sys.stdin
         else:
-            f = open(input_file, encoding='utf-8', errors='backslashreplace')
+            f = open(input_file, encoding="utf-8", errors="backslashreplace")
 
         for query in f.readlines():
-            query = query.strip() + '\n'
-            if query == '!!\n':
+            query = query.strip() + "\n"
+            if query == "!!\n":
                 continue
             self.queries_run += 1
             error_reference = None
             error_tested = None
             response_reference = None
             response_tested = None
 
             # ignore version or singular source queries
-            if query.lower().startswith('!v') or query.lower().startswith('!s'):
+            if query.lower().startswith("!v") or query.lower().startswith("!s"):
                 continue
 
-            if (query.startswith('-x') and not query.startswith('-x ')) or re.search(ASDOT_RE, query):
+            if (query.startswith("-x") and not query.startswith("-x ")) or re.search(ASDOT_RE, query):
                 self.queries_invalid += 1
                 continue
 
             # ignore queries asking for NRTM data or mirror serial status
-            if query.lower().startswith('-g ') or query.lower().startswith('!j'):
+            if query.lower().startswith("-g ") or query.lower().startswith("!j"):
                 self.queries_mirror += 1
                 continue
 
-            if query.startswith('!'):  # IRRD style query
+            if query.startswith("!"):  # IRRD style query
                 try:
                     response_reference = whois_query_irrd(self.host_reference, self.port_reference, query)
                 except ConnectionError as ce:
                     error_reference = str(ce)
                 except WhoisQueryError as wqe:
                     error_reference = str(wqe)
                 except ValueError:
-                    print(f'Query response to {query} invalid')
+                    print(f"Query response to {query} invalid")
                     continue
                 try:
                     response_tested = whois_query_irrd(self.host_tested, self.port_tested, query)
                 except WhoisQueryError as wqe:
                     error_tested = str(wqe)
                 except ValueError:
-                    print(f'Query response to {query} invalid')
+                    print(f"Query response to {query} invalid")
                     continue
 
             else:  # RIPE style query
                 try:
                     response_reference = whois_query(self.host_reference, self.port_reference, query)
                 except ConnectionError as ce:
                     error_reference = str(ce)
                 response_tested = whois_query(self.host_tested, self.port_tested, query)
 
             # If both produce error messages, don't compare them
             both_error = error_reference and error_tested
-            both_comment = (response_reference and response_tested and
-                            response_reference.strip() and response_tested.strip() and
-                            response_reference.strip()[0] == '%' and response_tested.strip()[0] == '%')
+            both_comment = (
+                response_reference
+                and response_tested
+                and response_reference.strip()
+                and response_tested.strip()
+                and response_reference.strip()[0] == "%"
+                and response_tested.strip()[0] == "%"
+            )
             if both_error or both_comment:
                 self.queries_both_error += 1
                 continue
 
             try:
                 cleaned_reference = self.clean(query, response_reference)
             except ValueError as ve:
-                print(f'Invalid reference response to query {query.strip()}: {response_reference}: {ve}')
+                print(f"Invalid reference response to query {query.strip()}: {response_reference}: {ve}")
                 continue
 
             try:
                 cleaned_tested = self.clean(query, response_tested)
             except ValueError as ve:
-                print(f'Invalid tested response to query {query.strip()}: {response_tested}: {ve}')
+                print(f"Invalid tested response to query {query.strip()}: {response_tested}: {ve}")
                 continue
 
             if cleaned_reference != cleaned_tested:
                 self.queries_different += 1
                 self.write_inconsistency_report(query, cleaned_reference, cleaned_tested)
 
-        print(f'Ran {self.queries_run} objects, {self.queries_different} had different results, '
-              f'{self.queries_both_error} produced errors on both instances, '
-              f'{self.queries_invalid} invalid queries were skipped, '
-              f'{self.queries_mirror} NRTM queries were skipped')
+        print(
+            f"Ran {self.queries_run} objects, {self.queries_different} had different results, "
+            f"{self.queries_both_error} produced errors on both instances, "
+            f"{self.queries_invalid} invalid queries were skipped, "
+            f"{self.queries_mirror} NRTM queries were skipped"
+        )
 
     def clean(self, query: str, response: Optional[str]) -> Optional[str]:
         """Clean the query response, so that the text can be compared."""
         if not response:
             return response
         irr_query = query[:2].lower()
         response = response.strip().lower()
 
         cleaned_result_list = None
-        if irr_query in SSP_QUERIES or (irr_query == '!r' and query.lower().strip().endswith(',o')):
-            cleaned_result_list = response.split(' ')
-        if irr_query in ['!6', '!g'] and cleaned_result_list:
+        if irr_query in SSP_QUERIES or (irr_query == "!r" and query.lower().strip().endswith(",o")):
+            cleaned_result_list = response.split(" ")
+        if irr_query in ["!6", "!g"] and cleaned_result_list:
             cleaned_result_list = [str(IP(ip)) for ip in cleaned_result_list]
         if cleaned_result_list:
-            return ' '.join(sorted(list(set(cleaned_result_list))))
+            return " ".join(sorted(list(set(cleaned_result_list))))
         else:
             new_responses = []
             for paragraph in split_paragraphs_rpsl(response):
                 rpsl_obj = rpsl_object_from_text(paragraph.strip(), strict_validation=False)
                 new_responses.append(rpsl_obj)
 
-            new_responses.sort(key=lambda i: i.parsed_data.get('source', '') + i.rpsl_object_class + i.pk())
+            new_responses.sort(key=lambda i: i.parsed_data.get("source", "") + i.rpsl_object_class + i.pk())
             texts = [r.render_rpsl_text() for r in new_responses]
-            return '\n'.join(OrderedSet(texts))
+            return "\n".join(OrderedSet(texts))
 
     def write_inconsistency_report(self, query, cleaned_reference, cleaned_tested):
         """Write a report to disk with details of the query response inconsistency."""
-        report = open(f'qout/QR {query.strip().replace("/", "S")[:30]}', 'w')
+        report = open(f'qout/QR {query.strip().replace("/", "S")[:30]}', "w")
         diff_str = self.render_diff(query, cleaned_reference, cleaned_tested)
-        report.write(query.strip() + '\n')
-        report.write('\n=================================================================\n')
+        report.write(query.strip() + "\n")
+        report.write("\n=================================================================\n")
         if diff_str:
-            report.write(f'~~~~~~~~~[ diff clean ref->tst ]~~~~~~~~~\n')
-            report.write(diff_str + '\n')
-        report.write(f'~~~~~~~~~[ clean ref {self.host_reference}:{self.port_reference} ]~~~~~~~~~\n')
-        report.write(str(cleaned_reference) + '\n')
-        report.write(f'~~~~~~~~~[ clean tst {self.host_tested}:{self.port_tested} ]~~~~~~~~~\n')
-        report.write(str(cleaned_tested) + '\n')
-        report.write('\n=================================================================\n')
+            report.write(f"~~~~~~~~~[ diff clean ref->tst ]~~~~~~~~~\n")
+            report.write(diff_str + "\n")
+        report.write(f"~~~~~~~~~[ clean ref {self.host_reference}:{self.port_reference} ]~~~~~~~~~\n")
+        report.write(str(cleaned_reference) + "\n")
+        report.write(f"~~~~~~~~~[ clean tst {self.host_tested}:{self.port_tested} ]~~~~~~~~~\n")
+        report.write(str(cleaned_tested) + "\n")
+        report.write("\n=================================================================\n")
         report.close()
 
     def render_diff(self, query: str, cleaned_reference: str, cleaned_tested: str) -> Optional[str]:
         """Produce a diff between the results, either by line or with queries like !i, by element returned."""
         if not cleaned_reference or not cleaned_tested:
             return None
 
         irr_query = query[:2].lower()
-        if irr_query in SSP_QUERIES or (irr_query == '!r' and query.lower().strip().endswith(',o')):
-            diff_input_reference = list(cleaned_reference.split(' '))
-            diff_input_tested = list(cleaned_tested.split(' '))
+        if irr_query in SSP_QUERIES or (irr_query == "!r" and query.lower().strip().endswith(",o")):
+            diff_input_reference = list(cleaned_reference.split(" "))
+            diff_input_tested = list(cleaned_tested.split(" "))
         else:
             diff_input_reference = list(splitline_unicodesafe(cleaned_reference))
             diff_input_tested = list(splitline_unicodesafe(cleaned_tested))
-        diff = list(difflib.unified_diff(diff_input_reference, diff_input_tested, lineterm=''))
-        diff_str = '\n'.join(diff[2:])  # skip the lines from the diff which would have filenames
+        diff = list(difflib.unified_diff(diff_input_reference, diff_input_tested, lineterm=""))
+        diff_str = "\n".join(diff[2:])  # skip the lines from the diff which would have filenames
         return diff_str
 
 
 def main():  # pragma: no cover
     description = """Run a list of queries against two IRRD instances, and report significant results."""
     parser = argparse.ArgumentParser(description=description)
-    parser.add_argument('input_file', type=str,
-                        help='the name of a file to read containing queries, or - for stdin')
-    parser.add_argument('host_reference', type=str,
-                        help='host/IP of the reference IRRD server')
-    parser.add_argument('port_reference', type=int,
-                        help='port for the reference IRRD server')
-    parser.add_argument('host_tested', type=str,
-                        help='host/IP of the tested IRRD server')
-    parser.add_argument('port_tested', type=int,
-                        help='port for the tested IRRD server')
+    parser.add_argument(
+        "input_file", type=str, help="the name of a file to read containing queries, or - for stdin"
+    )
+    parser.add_argument("host_reference", type=str, help="host/IP of the reference IRRD server")
+    parser.add_argument("port_reference", type=int, help="port for the reference IRRD server")
+    parser.add_argument("host_tested", type=str, help="host/IP of the tested IRRD server")
+    parser.add_argument("port_tested", type=int, help="port for the tested IRRD server")
     args = parser.parse_args()
 
-    QueryComparison(args.input_file, args.host_reference, args.port_reference, args.host_tested, args.port_tested)
+    QueryComparison(
+        args.input_file, args.host_reference, args.port_reference, args.host_tested, args.port_tested
+    )
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     main()
```

### Comparing `irrd-4.2.8/irrd/scripts/rpsl_read.py` & `irrd-4.3.0/irrd/scripts/rpsl_read.py`

 * *Files 10% similar despite different names*

```diff
@@ -2,26 +2,25 @@
 # flake8: noqa: E402
 """
 This is a helper script to run RPSL data through the parser and, optionally,
 insert it into the database.
 """
 import argparse
 import sys
-
 from pathlib import Path
 from typing import Optional, Set
 
 from irrd.storage.models import JournalEntryOrigin
 
 sys.path.append(str(Path(__file__).resolve().parents[2]))
 
 from irrd.conf import CONFIG_PATH_DEFAULT, config_init, get_setting
-from irrd.storage.database_handler import DatabaseHandler
 from irrd.rpsl.parser import UnknownRPSLObjectClassException
 from irrd.rpsl.rpsl_objects import rpsl_object_from_text
+from irrd.storage.database_handler import DatabaseHandler
 from irrd.utils.text import split_paragraphs_rpsl
 
 
 class RPSLParse:
     obj_parsed = 0
     obj_errors = 0
     obj_unknown = 0
@@ -30,78 +29,90 @@
 
     def main(self, filename, strict_validation, database, show_info=True):
         self.show_info = show_info
         if database:
             self.database_handler = DatabaseHandler()
             self.database_handler.disable_journaling()
 
-        if filename == '-':  # pragma: no cover
+        if filename == "-":  # pragma: no cover
             f = sys.stdin
         else:
-            f = open(filename, encoding='utf-8', errors='backslashreplace')
+            f = open(filename, encoding="utf-8", errors="backslashreplace")
 
         for paragraph in split_paragraphs_rpsl(f):
             self.parse_object(paragraph, strict_validation)
 
-        print(f'Processed {self.obj_parsed} objects, {self.obj_errors} with errors')
+        print(f"Processed {self.obj_parsed} objects, {self.obj_errors} with errors")
         if self.obj_unknown:
-            unknown_formatted = ', '.join(self.unknown_object_classes)
-            print(f'Ignored {self.obj_unknown} objects due to unknown object classes: {unknown_formatted}')
+            unknown_formatted = ", ".join(self.unknown_object_classes)
+            print(f"Ignored {self.obj_unknown} objects due to unknown object classes: {unknown_formatted}")
 
         if self.database_handler:
             self.database_handler.commit()
             self.database_handler.close()
 
     def parse_object(self, rpsl_text, strict_validation):
         try:
             self.obj_parsed += 1
             obj = rpsl_object_from_text(rpsl_text.strip(), strict_validation=strict_validation)
             if (obj.messages.messages() and self.show_info) or obj.messages.errors():
                 if obj.messages.errors():
                     self.obj_errors += 1
 
                 print(rpsl_text.strip())
-                print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
+                print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
                 print(obj.messages)
-                print('\n=======================================\n')
+                print("\n=======================================\n")
 
             if self.database_handler and obj and not obj.messages.errors():
                 self.database_handler.upsert_rpsl_object(obj, JournalEntryOrigin.mirror)
 
         except UnknownRPSLObjectClassException as e:
             self.obj_unknown += 1
-            self.unknown_object_classes.add(str(e).split(':')[1].strip())
+            self.unknown_object_classes.add(str(e).split(":")[1].strip())
         except Exception as e:  # pragma: no cover
-            print('=======================================')
+            print("=======================================")
             print(rpsl_text)
-            print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
+            print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
             raise e
 
 
 def main():  # pragma: no cover
     description = """Run RPSL data through the IRRD processor. For each object that resulted in messages emitted by
                      the parser, the object is printed followed by the messages. Optionally, insert objects into
                      the database."""
     parser = argparse.ArgumentParser(description=description)
-    parser.add_argument('--config', dest='config_file_path', type=str,
-                        help=f'use a different IRRd config file (default: {CONFIG_PATH_DEFAULT})')
-    parser.add_argument('--hide-info', dest='hide_info', action='store_true',
-                        help='hide INFO messages')
-    parser.add_argument('--strict', dest='strict_validation', action='store_true',
-                        help='use strict validation (errors on e.g. unknown or missing attributes)')
-    parser.add_argument('--database-destructive-overwrite', dest='database', action='store_true',
-                        help='insert all valid objects into the IRRD database - OVERWRITING ANY EXISTING ENTRIES, if '
-                             'they have the same RPSL primary key and source')
-    parser.add_argument('input_file', type=str,
-                        help='the name of a file to read, or - for stdin')
+    parser.add_argument(
+        "--config",
+        dest="config_file_path",
+        type=str,
+        help=f"use a different IRRd config file (default: {CONFIG_PATH_DEFAULT})",
+    )
+    parser.add_argument("--hide-info", dest="hide_info", action="store_true", help="hide INFO messages")
+    parser.add_argument(
+        "--strict",
+        dest="strict_validation",
+        action="store_true",
+        help="use strict validation (errors on e.g. unknown or missing attributes)",
+    )
+    parser.add_argument(
+        "--database-destructive-overwrite",
+        dest="database",
+        action="store_true",
+        help=(
+            "insert all valid objects into the IRRD database - OVERWRITING ANY EXISTING ENTRIES, if "
+            "they have the same RPSL primary key and source"
+        ),
+    )
+    parser.add_argument("input_file", type=str, help="the name of a file to read, or - for stdin")
     args = parser.parse_args()
 
     config_init(args.config_file_path)
-    if get_setting('database_readonly'):
-        print('Unable to run, because database_readonly is set')
+    if get_setting("database_readonly"):
+        print("Unable to run, because database_readonly is set")
         sys.exit(-1)
-        
+
     RPSLParse().main(args.input_file, args.strict_validation, args.database, not args.hide_info)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     main()
```

### Comparing `irrd-4.2.8/irrd/scripts/set_last_modified_auth.py` & `irrd-4.3.0/irrd/scripts/set_last_modified_auth.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,61 +1,68 @@
 #!/usr/bin/env python
 # flake8: noqa: E402
 import argparse
 import logging
 import sys
 from pathlib import Path
 
-
 """
 Set last-modified attribute on all authoritative objects.
 """
 
 logger = logging.getLogger(__name__)
 sys.path.append(str(Path(__file__).resolve().parents[2]))
 
-from irrd.storage.database_handler import DatabaseHandler
-from irrd.conf import config_init, CONFIG_PATH_DEFAULT, get_setting
+from irrd.conf import CONFIG_PATH_DEFAULT, config_init, get_setting
 from irrd.rpsl.rpsl_objects import rpsl_object_from_text
+from irrd.storage.database_handler import DatabaseHandler
 from irrd.storage.models import RPSLDatabaseObject
 from irrd.storage.queries import RPSLDatabaseQuery
 
+
 def set_last_modified():
     dh = DatabaseHandler()
-    auth_sources = [k for k, v in get_setting('sources').items() if v.get('authoritative')]
-    q = RPSLDatabaseQuery(column_names=['pk', 'object_text', 'updated'], enable_ordering=False)
+    auth_sources = [k for k, v in get_setting("sources").items() if v.get("authoritative")]
+    q = RPSLDatabaseQuery(column_names=["pk", "object_text", "updated"], enable_ordering=False)
     q = q.sources(auth_sources)
 
     results = list(dh.execute_query(q))
-    print(f'Updating {len(results)} objects in sources {auth_sources}')
+    print(f"Updating {len(results)} objects in sources {auth_sources}")
     for result in results:
-        rpsl_obj = rpsl_object_from_text(result['object_text'], strict_validation=False)
+        rpsl_obj = rpsl_object_from_text(result["object_text"], strict_validation=False)
         if rpsl_obj.messages.errors():  # pragma: no cover
-            print(f'Failed to process {rpsl_obj}: {rpsl_obj.messages.errors()}')
+            print(f"Failed to process {rpsl_obj}: {rpsl_obj.messages.errors()}")
             continue
-        new_text = rpsl_obj.render_rpsl_text(result['updated'])
-        stmt = RPSLDatabaseObject.__table__.update().where(
-            RPSLDatabaseObject.__table__.c.pk == result['pk']).values(
-            object_text=new_text,
+        new_text = rpsl_obj.render_rpsl_text(result["updated"])
+        stmt = (
+            RPSLDatabaseObject.__table__.update()
+            .where(RPSLDatabaseObject.__table__.c.pk == result["pk"])
+            .values(
+                object_text=new_text,
+            )
         )
         dh.execute_statement(stmt)
     dh.commit()
     dh.close()
 
 
 def main():  # pragma: no cover
     description = """Set last-modified attribute on all authoritative objects."""
     parser = argparse.ArgumentParser(description=description)
-    parser.add_argument('--config', dest='config_file_path', type=str,
-                        help=f'use a different IRRd config file (default: {CONFIG_PATH_DEFAULT})')
+    parser.add_argument(
+        "--config",
+        dest="config_file_path",
+        type=str,
+        help=f"use a different IRRd config file (default: {CONFIG_PATH_DEFAULT})",
+    )
     args = parser.parse_args()
 
     config_init(args.config_file_path)
-    if get_setting('database_readonly'):
-        print('Unable to run, because database_readonly is set')
+    if get_setting("database_readonly"):
+        print("Unable to run, because database_readonly is set")
         sys.exit(-1)
 
     sys.exit(set_last_modified())
 
 
-if __name__ == '__main__':  # pragma: no cover
+if __name__ == "__main__":  # pragma: no cover
     main()
```

### Comparing `irrd-4.2.8/irrd/scripts/submit_changes.py` & `irrd-4.3.0/irrd/scripts/submit_changes.py`

 * *Files 9% similar despite different names*

```diff
@@ -9,32 +9,35 @@
 The message is always read from stdin.
 
 Prints a report of the results, which would otherwise
 be sent to a user by e-mail.
 """
 import argparse
 import sys
-
 from pathlib import Path
 
 sys.path.append(str(Path(__file__).resolve().parents[2]))
 
-from irrd.conf import config_init, CONFIG_PATH_DEFAULT
+from irrd.conf import CONFIG_PATH_DEFAULT, config_init
 from irrd.updates.handler import ChangeSubmissionHandler
 
 
 def main(data):
     handler = ChangeSubmissionHandler().load_text_blob(data)
     print(handler.submitter_report_human())
 
 
-if __name__ == '__main__':  # pragma: no cover
+if __name__ == "__main__":  # pragma: no cover
     description = """Process a raw update message, i.e. without email headers. Authentication is still checked, 
                      but PGP is not supported. Message is always read from stdin, and a report is printed to stdout."""
     parser = argparse.ArgumentParser(description=description)
-    parser.add_argument('--config', dest='config_file_path', type=str,
-                        help=f'use a different IRRd config file (default: {CONFIG_PATH_DEFAULT})')
+    parser.add_argument(
+        "--config",
+        dest="config_file_path",
+        type=str,
+        help=f"use a different IRRd config file (default: {CONFIG_PATH_DEFAULT})",
+    )
     args = parser.parse_args()
 
     config_init(args.config_file_path)
 
     main(sys.stdin.read())
```

### Comparing `irrd-4.2.8/irrd/scripts/submit_email.py` & `irrd-4.3.0/irrd/scripts/submit_email.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,47 +1,52 @@
 #!/usr/bin/env python
 # flake8: noqa: E402
 
-import sys
-
 import argparse
 import logging
+import sys
 from pathlib import Path
 
 """
 Submit a raw e-mail message, i.e. with e-mail headers.
 The message is always read from stdin.
 
 A report on the results will be sent to the user by e-mail.
 """
 
 logger = logging.getLogger(__name__)
 sys.path.append(str(Path(__file__).resolve().parents[2]))
 
-from irrd.conf import config_init, CONFIG_PATH_DEFAULT
+from irrd.conf import CONFIG_PATH_DEFAULT, config_init
 from irrd.updates.email import handle_email_submission
 
 
 def run(data):
     try:
         handle_email_submission(data)
     except Exception as exc:
-        logger.critical(f'An exception occurred while attempting to process the following email: {data}', exc_info=exc)
-        print('An internal error occurred while processing this email.')
+        logger.critical(
+            f"An exception occurred while attempting to process the following email: {data}", exc_info=exc
+        )
+        print("An internal error occurred while processing this email.")
 
 
 def main():  # pragma: no cover
     description = """Process a raw email message with requested changes. Authentication is checked, message
                      is always read from stdin. A report is sent to the user by email, along with any
                      notifications to mntners and others."""
     parser = argparse.ArgumentParser(description=description)
-    parser.add_argument('--config', dest='config_file_path', type=str,
-                        help=f'use a different IRRd config file (default: {CONFIG_PATH_DEFAULT})')
+    parser.add_argument(
+        "--config",
+        dest="config_file_path",
+        type=str,
+        help=f"use a different IRRd config file (default: {CONFIG_PATH_DEFAULT})",
+    )
     args = parser.parse_args()
 
     config_init(args.config_file_path)
 
     run(sys.stdin.read())
 
 
-if __name__ == '__main__':  # pragma: no cover
+if __name__ == "__main__":  # pragma: no cover
     main()
```

### Comparing `irrd-4.2.8/irrd/scripts/tests/test_load_database.py` & `irrd-4.3.0/irrd/scripts/tests/test_load_database.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,65 +1,72 @@
 from unittest.mock import Mock
 
 from irrd.utils.test_utils import flatten_mock_calls
+
 from ..load_database import load
 
 
 def test_load_database_success(capsys, monkeypatch):
     mock_dh = Mock()
-    monkeypatch.setattr('irrd.scripts.load_database.DatabaseHandler', lambda: mock_dh)
+    monkeypatch.setattr("irrd.scripts.load_database.DatabaseHandler", lambda: mock_dh)
     mock_roa_validator = Mock()
-    monkeypatch.setattr('irrd.scripts.load_database.BulkRouteROAValidator', lambda dh: mock_roa_validator)
+    monkeypatch.setattr("irrd.scripts.load_database.BulkRouteROAValidator", lambda dh: mock_roa_validator)
     mock_parser = Mock()
-    monkeypatch.setattr('irrd.scripts.load_database.MirrorFileImportParser', lambda *args, **kwargs: mock_parser)
+    monkeypatch.setattr(
+        "irrd.scripts.load_database.MirrorFileImportParser", lambda *args, **kwargs: mock_parser
+    )
 
     mock_parser.run_import = lambda: None
 
-    assert load('TEST', 'test.db', 42) == 0
+    assert load("TEST", "test.db", 42) == 0
     assert flatten_mock_calls(mock_dh) == [
-        ['delete_all_rpsl_objects_with_journal', ('TEST',), {}],
-        ['disable_journaling', (), {}],
-        ['commit', (), {}],
-        ['close', (), {}]
+        ["delete_all_rpsl_objects_with_journal", ("TEST",), {}],
+        ["disable_journaling", (), {}],
+        ["commit", (), {}],
+        ["close", (), {}],
     ]
 
     # run_import() call is not included here
     assert flatten_mock_calls(mock_parser) == []
     assert not capsys.readouterr().out
 
 
 def test_load_database_import_error(capsys, monkeypatch, caplog):
     mock_dh = Mock()
-    monkeypatch.setattr('irrd.scripts.load_database.DatabaseHandler', lambda: mock_dh)
+    monkeypatch.setattr("irrd.scripts.load_database.DatabaseHandler", lambda: mock_dh)
     mock_roa_validator = Mock()
-    monkeypatch.setattr('irrd.scripts.load_database.BulkRouteROAValidator', lambda dh: mock_roa_validator)
+    monkeypatch.setattr("irrd.scripts.load_database.BulkRouteROAValidator", lambda dh: mock_roa_validator)
     mock_parser = Mock()
-    monkeypatch.setattr('irrd.scripts.load_database.MirrorFileImportParser', lambda *args, **kwargs: mock_parser)
+    monkeypatch.setattr(
+        "irrd.scripts.load_database.MirrorFileImportParser", lambda *args, **kwargs: mock_parser
+    )
 
-    mock_parser.run_import = lambda: 'object-parsing-error'
+    mock_parser.run_import = lambda: "object-parsing-error"
 
-    assert load('TEST', 'test.db', 42) == 1
+    assert load("TEST", "test.db", 42) == 1
     assert flatten_mock_calls(mock_dh) == [
-        ['delete_all_rpsl_objects_with_journal', ('TEST',), {}],
-        ['disable_journaling', (), {}],
-        ['rollback', (), {}],
-        ['close', (), {}]
+        ["delete_all_rpsl_objects_with_journal", ("TEST",), {}],
+        ["disable_journaling", (), {}],
+        ["rollback", (), {}],
+        ["close", (), {}],
     ]
 
     # run_import() call is not included here
     assert flatten_mock_calls(mock_parser) == []
 
-    assert 'object-parsing-error' not in caplog.text
+    assert "object-parsing-error" not in caplog.text
     stdout = capsys.readouterr().out
-    assert 'Error occurred while processing object:\nobject-parsing-error' in stdout
+    assert "Error occurred while processing object:\nobject-parsing-error" in stdout
 
 
 def test_reject_import_source_set(capsys, config_override):
-    config_override({
-        'sources': {
-            'TEST': {'import_source': 'import-url'}
-        },
-    })
-    assert load('TEST', 'test.db', 42) == 2
+    config_override(
+        {
+            "sources": {"TEST": {"import_source": "import-url"}},
+        }
+    )
+    assert load("TEST", "test.db", 42) == 2
     stdout = capsys.readouterr().out
-    assert 'Error: to use this command, import_source and import_serial_' \
-           'source for source TEST must not be set.' in stdout
+    assert (
+        "Error: to use this command, import_source and import_serial_source for source TEST must not be set."
+        in stdout
+    )
```

### Comparing `irrd-4.2.8/irrd/scripts/tests/test_mirror_force_reload.py` & `irrd-4.3.0/irrd/scripts/tests/test_mirror_force_reload.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,17 +1,18 @@
 from unittest.mock import Mock
 
 from irrd.utils.test_utils import flatten_mock_calls
+
 from ..mirror_force_reload import set_force_reload
 
 
 def test_set_force_reload(capsys, monkeypatch):
     mock_dh = Mock()
-    monkeypatch.setattr('irrd.scripts.mirror_force_reload.DatabaseHandler', lambda: mock_dh)
+    monkeypatch.setattr("irrd.scripts.mirror_force_reload.DatabaseHandler", lambda: mock_dh)
 
-    set_force_reload('TEST')
+    set_force_reload("TEST")
     assert flatten_mock_calls(mock_dh) == [
-        ['set_force_reload', ('TEST', ), {}],
-        ['commit', (), {}],
-        ['close', (), {}]
+        ["set_force_reload", ("TEST",), {}],
+        ["commit", (), {}],
+        ["close", (), {}],
     ]
     assert not capsys.readouterr().out
```

### Comparing `irrd-4.2.8/irrd/scripts/tests/test_rpsl_read.py` & `irrd-4.3.0/irrd/scripts/tests/test_rpsl_read.py`

 * *Files 6% similar despite different names*

```diff
@@ -38,44 +38,44 @@
 remarks:        remark
 
 """
 
 
 def test_rpsl_read(capsys, tmpdir, monkeypatch):
     mock_database_handler = Mock()
-    monkeypatch.setattr('irrd.scripts.rpsl_read.DatabaseHandler', lambda: mock_database_handler)
+    monkeypatch.setattr("irrd.scripts.rpsl_read.DatabaseHandler", lambda: mock_database_handler)
 
-    tmp_file = tmpdir + '/rpsl_parse_test.rpsl'
-    fh = open(tmp_file, 'w')
+    tmp_file = tmpdir + "/rpsl_parse_test.rpsl"
+    fh = open(tmp_file, "w")
     fh.write(TEST_DATA)
     fh.close()
 
     RPSLParse().main(filename=tmp_file, strict_validation=True, database=True)
     captured = capsys.readouterr().out
-    assert 'ERROR: Unrecognised attribute unknown-obj on object as-block' in captured
-    assert 'INFO: AS range AS65536 - as065538 was reformatted as AS65536 - AS65538' in captured
-    assert 'Processed 3 objects, 1 with errors' in captured
-    assert 'Ignored 1 objects due to unknown object classes: foo-block' in captured
-
-    assert mock_database_handler.mock_calls[0][0] == 'disable_journaling'
-    assert mock_database_handler.mock_calls[1][0] == 'upsert_rpsl_object'
-    assert mock_database_handler.mock_calls[1][1][0].pk() == 'AS65536 - AS65538'
-    assert mock_database_handler.mock_calls[2][0] == 'commit'
+    assert "ERROR: Unrecognised attribute unknown-obj on object as-block" in captured
+    assert "INFO: AS range AS65536 - as065538 was reformatted as AS65536 - AS65538" in captured
+    assert "Processed 3 objects, 1 with errors" in captured
+    assert "Ignored 1 objects due to unknown object classes: foo-block" in captured
+
+    assert mock_database_handler.mock_calls[0][0] == "disable_journaling"
+    assert mock_database_handler.mock_calls[1][0] == "upsert_rpsl_object"
+    assert mock_database_handler.mock_calls[1][1][0].pk() == "AS65536 - AS65538"
+    assert mock_database_handler.mock_calls[2][0] == "commit"
     mock_database_handler.reset_mock()
 
     RPSLParse().main(filename=tmp_file, strict_validation=False, database=True)
     captured = capsys.readouterr().out
-    assert 'ERROR: Unrecognised attribute unknown-obj on object as-block' not in captured
-    assert 'INFO: AS range AS65536 - as065538 was reformatted as AS65536 - AS65538' in captured
-    assert 'Processed 3 objects, 0 with errors' in captured
-    assert 'Ignored 1 objects due to unknown object classes: foo-block' in captured
-
-    assert mock_database_handler.mock_calls[0][0] == 'disable_journaling'
-    assert mock_database_handler.mock_calls[1][0] == 'upsert_rpsl_object'
-    assert mock_database_handler.mock_calls[1][1][0].pk() == 'AS65536 - AS65538'
-    assert mock_database_handler.mock_calls[2][0] == 'upsert_rpsl_object'
-    assert mock_database_handler.mock_calls[2][1][0].pk() == 'AS65536 - AS65538'
-    assert mock_database_handler.mock_calls[3][0] == 'commit'
+    assert "ERROR: Unrecognised attribute unknown-obj on object as-block" not in captured
+    assert "INFO: AS range AS65536 - as065538 was reformatted as AS65536 - AS65538" in captured
+    assert "Processed 3 objects, 0 with errors" in captured
+    assert "Ignored 1 objects due to unknown object classes: foo-block" in captured
+
+    assert mock_database_handler.mock_calls[0][0] == "disable_journaling"
+    assert mock_database_handler.mock_calls[1][0] == "upsert_rpsl_object"
+    assert mock_database_handler.mock_calls[1][1][0].pk() == "AS65536 - AS65538"
+    assert mock_database_handler.mock_calls[2][0] == "upsert_rpsl_object"
+    assert mock_database_handler.mock_calls[2][1][0].pk() == "AS65536 - AS65538"
+    assert mock_database_handler.mock_calls[3][0] == "commit"
     mock_database_handler.reset_mock()
 
     RPSLParse().main(filename=tmp_file, strict_validation=False, database=False)
     assert not mock_database_handler.mock_calls
```

### Comparing `irrd-4.2.8/irrd/scripts/tests/test_set_last_modified_auth.py` & `irrd-4.3.0/irrd/scripts/tests/test_set_last_modified_auth.py`

 * *Files 16% similar despite different names*

```diff
@@ -2,47 +2,47 @@
 import uuid
 from unittest.mock import Mock
 
 from pytz import timezone
 
 from irrd.utils.rpsl_samples import SAMPLE_RTR_SET
 from irrd.utils.test_utils import flatten_mock_calls
+
 from ..set_last_modified_auth import set_last_modified
 
 
 def test_set_last_modified(capsys, monkeypatch, config_override):
-    config_override({
-        'sources': {
-            'TEST': {'authoritative': True},
-            'TEST2': {},
+    config_override(
+        {
+            "sources": {
+                "TEST": {"authoritative": True},
+                "TEST2": {},
+            }
         }
-    })
+    )
     mock_dh = Mock()
-    monkeypatch.setattr('irrd.scripts.set_last_modified_auth.DatabaseHandler', lambda: mock_dh)
+    monkeypatch.setattr("irrd.scripts.set_last_modified_auth.DatabaseHandler", lambda: mock_dh)
     mock_dq = Mock()
-    monkeypatch.setattr('irrd.scripts.set_last_modified_auth.RPSLDatabaseQuery', lambda column_names, enable_ordering: mock_dq)
+    monkeypatch.setattr(
+        "irrd.scripts.set_last_modified_auth.RPSLDatabaseQuery", lambda column_names, enable_ordering: mock_dq
+    )
 
     object_pk = uuid.uuid4()
     mock_query_result = [
         {
-            'pk': object_pk,
-            'object_text': SAMPLE_RTR_SET + 'last-modified: old\n',
-            'updated': datetime.datetime(2020, 1, 1, tzinfo=timezone('UTC')),
+            "pk": object_pk,
+            "object_text": SAMPLE_RTR_SET + "last-modified: old\n",
+            "updated": datetime.datetime(2020, 1, 1, tzinfo=timezone("UTC")),
         },
     ]
     mock_dh.execute_query = lambda query: mock_query_result
 
     set_last_modified()
 
-    assert flatten_mock_calls(mock_dq) == [
-        ['sources', (['TEST'],), {}]
-    ]
-    assert mock_dh.mock_calls[0][0] == 'execute_statement'
+    assert flatten_mock_calls(mock_dq) == [["sources", (["TEST"],), {}]]
+    assert mock_dh.mock_calls[0][0] == "execute_statement"
     statement = mock_dh.mock_calls[0][1][0]
-    new_text = statement.parameters['object_text']
-    assert new_text == SAMPLE_RTR_SET + 'last-modified:  2020-01-01T00:00:00Z\n'
+    new_text = statement.parameters["object_text"]
+    assert new_text == SAMPLE_RTR_SET + "last-modified:  2020-01-01T00:00:00Z\n"
 
-    assert flatten_mock_calls(mock_dh)[1:] == [
-        ['commit', (), {}],
-        ['close', (), {}]
-    ]
+    assert flatten_mock_calls(mock_dh)[1:] == [["commit", (), {}], ["close", (), {}]]
     assert capsys.readouterr().out == "Updating 1 objects in sources ['TEST']\n"
```

### Comparing `irrd-4.2.8/irrd/scripts/tests/test_submit_email.py` & `irrd-4.3.0/irrd/scripts/tests/test_submit_email.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,23 +1,23 @@
 from unittest.mock import Mock
 
 from ..submit_email import run
 
 
 def test_submit_email_success(capsys, monkeypatch):
     mock_handle_email = Mock()
-    monkeypatch.setattr('irrd.scripts.submit_email.handle_email_submission', lambda data: mock_handle_email)
-    mock_handle_email.user_report = lambda: 'output'
+    monkeypatch.setattr("irrd.scripts.submit_email.handle_email_submission", lambda data: mock_handle_email)
+    mock_handle_email.user_report = lambda: "output"
 
-    run('test input')
+    run("test input")
 
 
 def test_submit_email_fail(capsys, monkeypatch, caplog):
-    mock_handle_email = Mock(side_effect=Exception('expected-test-error'))
-    monkeypatch.setattr('irrd.scripts.submit_email.handle_email_submission', mock_handle_email)
+    mock_handle_email = Mock(side_effect=Exception("expected-test-error"))
+    monkeypatch.setattr("irrd.scripts.submit_email.handle_email_submission", mock_handle_email)
 
-    run('test input')
+    run("test input")
 
-    assert 'expected-test-error' in caplog.text
+    assert "expected-test-error" in caplog.text
     stdout = capsys.readouterr().out
-    assert 'An internal error occurred' in stdout
-    assert 'expected-test-error' not in stdout
+    assert "An internal error occurred" in stdout
+    assert "expected-test-error" not in stdout
```

### Comparing `irrd-4.2.8/irrd/scripts/tests/test_submit_update.py` & `irrd-4.3.0/irrd/scripts/tests/test_submit_update.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 from unittest.mock import Mock
 
-from ..submit_changes import main
 from ...updates.handler import ChangeSubmissionHandler
+from ..submit_changes import main
 
 
 def test_submit_changes(capsys, monkeypatch):
     mock_update_handler = Mock(spec=ChangeSubmissionHandler)
-    monkeypatch.setattr('irrd.scripts.submit_changes.ChangeSubmissionHandler', lambda: mock_update_handler)
+    monkeypatch.setattr("irrd.scripts.submit_changes.ChangeSubmissionHandler", lambda: mock_update_handler)
     mock_update_handler.load_text_blob = lambda data: mock_update_handler
-    mock_update_handler.submitter_report_human = lambda: 'output'
+    mock_update_handler.submitter_report_human = lambda: "output"
 
-    main('test input')
+    main("test input")
     captured = capsys.readouterr().out
-    assert captured == 'output\n'
+    assert captured == "output\n"
```

### Comparing `irrd-4.2.8/irrd/scripts/tests/test_update_database.py` & `irrd-4.3.0/irrd/scripts/tests/test_update_database.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,61 +1,66 @@
 from unittest.mock import Mock
 
 from irrd.utils.test_utils import flatten_mock_calls
+
 from ..update_database import update
 
 
 def test_update_database_success(capsys, monkeypatch):
     mock_dh = Mock()
-    monkeypatch.setattr('irrd.scripts.update_database.DatabaseHandler', lambda enable_preload_update=False: mock_dh)
+    monkeypatch.setattr(
+        "irrd.scripts.update_database.DatabaseHandler", lambda enable_preload_update=False: mock_dh
+    )
     mock_roa_validator = Mock()
-    monkeypatch.setattr('irrd.scripts.update_database.BulkRouteROAValidator', lambda dh: mock_roa_validator)
+    monkeypatch.setattr("irrd.scripts.update_database.BulkRouteROAValidator", lambda dh: mock_roa_validator)
     mock_parser = Mock()
-    monkeypatch.setattr('irrd.scripts.update_database.MirrorUpdateFileImportParser', lambda *args, **kwargs: mock_parser)
+    monkeypatch.setattr(
+        "irrd.scripts.update_database.MirrorUpdateFileImportParser", lambda *args, **kwargs: mock_parser
+    )
 
     mock_parser.run_import = lambda: None
 
-    assert update('TEST', 'test.db') == 0
-    assert flatten_mock_calls(mock_dh) == [
-        ['commit', (), {}],
-        ['close', (), {}]
-    ]
+    assert update("TEST", "test.db") == 0
+    assert flatten_mock_calls(mock_dh) == [["commit", (), {}], ["close", (), {}]]
 
     # run_import() call is not included here
     assert flatten_mock_calls(mock_parser) == []
     assert not capsys.readouterr().out
 
 
 def test_update_database_import_error(capsys, monkeypatch, caplog):
     mock_dh = Mock()
-    monkeypatch.setattr('irrd.scripts.update_database.DatabaseHandler', lambda enable_preload_update=False: mock_dh)
+    monkeypatch.setattr(
+        "irrd.scripts.update_database.DatabaseHandler", lambda enable_preload_update=False: mock_dh
+    )
     mock_roa_validator = Mock()
-    monkeypatch.setattr('irrd.scripts.update_database.BulkRouteROAValidator', lambda dh: mock_roa_validator)
+    monkeypatch.setattr("irrd.scripts.update_database.BulkRouteROAValidator", lambda dh: mock_roa_validator)
     mock_parser = Mock()
-    monkeypatch.setattr('irrd.scripts.update_database.MirrorUpdateFileImportParser', lambda *args, **kwargs: mock_parser)
+    monkeypatch.setattr(
+        "irrd.scripts.update_database.MirrorUpdateFileImportParser", lambda *args, **kwargs: mock_parser
+    )
 
-    mock_parser.run_import = lambda: 'object-parsing-error'
+    mock_parser.run_import = lambda: "object-parsing-error"
 
-    assert update('TEST', 'test.db') == 1
-    assert flatten_mock_calls(mock_dh) == [
-        ['rollback', (), {}],
-        ['close', (), {}]
-    ]
+    assert update("TEST", "test.db") == 1
+    assert flatten_mock_calls(mock_dh) == [["rollback", (), {}], ["close", (), {}]]
 
     # run_import() call is not included here
     assert flatten_mock_calls(mock_parser) == []
 
-    assert 'object-parsing-error' not in caplog.text
+    assert "object-parsing-error" not in caplog.text
     stdout = capsys.readouterr().out
-    assert 'Error occurred while processing object:\nobject-parsing-error' in stdout
+    assert "Error occurred while processing object:\nobject-parsing-error" in stdout
 
 
 def test_reject_import_source_set(capsys, config_override):
-    config_override({
-        'sources': {
-            'TEST': {'import_source': 'import-url'}
-        },
-    })
-    assert update('TEST', 'test.db') == 2
+    config_override(
+        {
+            "sources": {"TEST": {"import_source": "import-url"}},
+        }
+    )
+    assert update("TEST", "test.db") == 2
     stdout = capsys.readouterr().out
-    assert 'Error: to use this command, import_source and import_serial_' \
-           'source for source TEST must not be set.' in stdout
+    assert (
+        "Error: to use this command, import_source and import_serial_source for source TEST must not be set."
+        in stdout
+    )
```

### Comparing `irrd-4.2.8/irrd/scripts/update_database.py` & `irrd-4.3.0/irrd/scripts/update_database.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,68 +1,75 @@
 #!/usr/bin/env python
 # flake8: noqa: E402
 import argparse
 import logging
 import sys
-
 from pathlib import Path
 
-
 """
 Update a database based on a RPSL file.
 """
 
 logger = logging.getLogger(__name__)
 sys.path.append(str(Path(__file__).resolve().parents[2]))
 
+from irrd.conf import CONFIG_PATH_DEFAULT, config_init, get_setting
+from irrd.mirroring.parsers import MirrorUpdateFileImportParser
 from irrd.rpki.validators import BulkRouteROAValidator
 from irrd.storage.database_handler import DatabaseHandler
-from irrd.mirroring.parsers import MirrorUpdateFileImportParser
-from irrd.conf import config_init, CONFIG_PATH_DEFAULT, get_setting
+
 
 def update(source, filename) -> int:
-    if any([
-        get_setting(f'sources.{source}.import_source'),
-        get_setting(f'sources.{source}.import_serial_source')
-    ]):
-        print(f'Error: to use this command, import_source and import_serial_source '
-              f'for source {source} must not be set.')
+    if any(
+        [
+            get_setting(f"sources.{source}.import_source"),
+            get_setting(f"sources.{source}.import_serial_source"),
+        ]
+    ):
+        print(
+            "Error: to use this command, import_source and import_serial_source "
+            f"for source {source} must not be set."
+        )
         return 2
 
     dh = DatabaseHandler()
     roa_validator = BulkRouteROAValidator(dh)
     parser = MirrorUpdateFileImportParser(
-        source, filename, database_handler=dh,
-        direct_error_return=True, roa_validator=roa_validator)
+        source, filename, database_handler=dh, direct_error_return=True, roa_validator=roa_validator
+    )
     error = parser.run_import()
     if error:
         dh.rollback()
     else:
         dh.commit()
     dh.close()
     if error:
-        print(f'Error occurred while processing object:\n{error}')
+        print(f"Error occurred while processing object:\n{error}")
         return 1
     return 0
 
 
 def main():  # pragma: no cover
     description = """Update a database based on a RPSL file."""
     parser = argparse.ArgumentParser(description=description)
-    parser.add_argument('--config', dest='config_file_path', type=str,
-                        help=f'use a different IRRd config file (default: {CONFIG_PATH_DEFAULT})')
-    parser.add_argument('--source', dest='source', type=str, required=True,
-                        help=f'name of the source, e.g. NTTCOM')
-    parser.add_argument('input_file', type=str,
-                        help='the name of a file to read')
+    parser.add_argument(
+        "--config",
+        dest="config_file_path",
+        type=str,
+        help=f"use a different IRRd config file (default: {CONFIG_PATH_DEFAULT})",
+    )
+    parser.add_argument(
+        "--source", dest="source", type=str, required=True, help=f"name of the source, e.g. NTTCOM"
+    )
+    parser.add_argument("input_file", type=str, help="the name of a file to read")
     args = parser.parse_args()
 
     config_init(args.config_file_path)
-    if get_setting('database_readonly'):
-        print('Unable to run, because database_readonly is set')
+    if get_setting("database_readonly"):
+        print("Unable to run, because database_readonly is set")
         sys.exit(-1)
 
     sys.exit(update(args.source, args.input_file))
 
 
-if __name__ == '__main__':  # pragma: no cover
+if __name__ == "__main__":  # pragma: no cover
     main()
```

### Comparing `irrd-4.2.8/irrd/server/access_check.py` & `irrd-4.3.0/irrd/server/access_check.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,45 +1,52 @@
 import logging
+import sys
+
 from IPy import IP
 
 from irrd.conf import get_setting
 
+STARLETTE_TEST_CLIENT_HOST = "testclient"
+
 logger = logging.getLogger(__name__)
 
 
 def is_client_permitted(ip: str, access_list_setting: str, default_deny=True, log=True) -> bool:
     """
     Determine whether a client is permitted to access an interface,
     based on the value of the setting of access_list_setting.
     If default_deny is True, an unset or empty access list will lead to denial.
 
     IPv6-mapped IPv4 addresses are unmapped to regular IPv4 addresses before processing.
     """
-    try:
-        client_ip = IP(ip)
-    except (ValueError, AttributeError) as e:
-        if log:
-            logger.error(f'Rejecting request as client IP could not be read from '
-                         f'{ip}: {e}')
-        return False
+    bypass_auth = ip == STARLETTE_TEST_CLIENT_HOST and getattr(sys, "_called_from_test")  # type: ignore
+
+    client_ip = None
+    if not bypass_auth:
+        try:
+            client_ip = IP(ip)
+        except (ValueError, AttributeError) as e:
+            if log:
+                logger.error(f"Rejecting request as client IP could not be read from {ip}: {e}")
+            return False
 
-    if client_ip.version() == 6:
+    if client_ip and client_ip.version() == 6:
         try:
             client_ip = client_ip.v46map()
         except ValueError:
             pass
 
     access_list_name = get_setting(access_list_setting)
-    access_list = get_setting(f'access_lists.{access_list_name}')
+    access_list = get_setting(f"access_lists.{access_list_name}")
 
     if not access_list_name or not access_list:
         if default_deny:
             if log:
-                logger.info(f'Rejecting request, access list empty or undefined: {client_ip}')
+                logger.info(f"Rejecting request, access list empty or undefined: {client_ip}")
             return False
         else:
             return True
 
-    allowed = any([client_ip in IP(allowed) for allowed in access_list])
+    allowed = bypass_auth or any([client_ip in IP(allowed) for allowed in access_list])
     if not allowed and log:
-        logger.info(f'Rejecting request, IP not in access list {access_list_name}: {client_ip}')
+        logger.info(f"Rejecting request, IP not in access list {access_list_name}: {client_ip}")
     return allowed
```

### Comparing `irrd-4.2.8/irrd/server/graphql/extensions.py` & `irrd-4.3.0/irrd/server/graphql/extensions.py`

 * *Files 16% similar despite different names*

```diff
@@ -11,39 +11,40 @@
 class QueryMetadataExtension(Extension):
     """
     Ariadne extension to add query metadata.
     - Returns the execution time
     - Returns SQL queries if SQL trace was enabled
     - Logs the query and execution time
     """
+
     def __init__(self):
         self.start_timestamp = None
         self.end_timestamp = None
 
     def request_started(self, context):
         self.start_timestamp = time.perf_counter()
 
     def format(self, context):
         data = {}
         if self.start_timestamp:
-            data['execution'] = time.perf_counter() - self.start_timestamp
-        if 'sql_queries' in context:
-            data['sql_query_count'] = len(context['sql_queries'])
-            data['sql_queries'] = context['sql_queries']
+            data["execution"] = time.perf_counter() - self.start_timestamp
+        if "sql_queries" in context:
+            data["sql_query_count"] = len(context["sql_queries"])
+            data["sql_queries"] = context["sql_queries"]
 
-        query = context['request']._json
-        if context['request']._json.get('operationName') != 'IntrospectionQuery':
+        query = context["request"]._json
+        if context["request"]._json.get("operationName") != "IntrospectionQuery":
             # Reformat the query to make it fit neatly on a single log line
-            query['query'] = query['query'].replace(' ', '').replace('\n', ' ').replace('\t', '')
-            client = context['request'].client.host
+            query["query"] = query["query"].replace(" ", "").replace("\n", " ").replace("\t", "")
+            client = context["request"].client.host
             logger.info(f'{client} ran query in {data.get("execution")}s: {query}')
         return data
 
 
-def error_formatter(error: GraphQLError, debug: bool=False):
+def error_formatter(error: GraphQLError, debug: bool = False):
     """
     Custom Ariadne error formatter. A generic text is used if the
     server is not in debug mode and the original error is a
     different error (as in, a different kind of exception raised
     during processing).
     """
     if debug or not error.original_error or isinstance(error.original_error, GraphQLError):
```

### Comparing `irrd-4.2.8/irrd/server/graphql/resolvers.py` & `irrd-4.3.0/irrd/server/graphql/resolvers.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,24 +1,26 @@
 from collections import OrderedDict
-from typing import Set, Dict, Optional, List
+from typing import Dict, List, Optional, Set
 
 import ariadne
 import graphql
+from graphql import GraphQLError, GraphQLResolveInfo
 from IPy import IP
-from graphql import GraphQLResolveInfo, GraphQLError
 
-from irrd.conf import get_setting, RPKI_IRR_PSEUDO_SOURCE
+from irrd.conf import RPKI_IRR_PSEUDO_SOURCE, get_setting
+from irrd.routepref.status import RoutePreferenceStatus
 from irrd.rpki.status import RPKIStatus
 from irrd.rpsl.rpsl_objects import OBJECT_CLASS_MAPPING, lookup_field_names
 from irrd.scopefilter.status import ScopeFilterStatus
 from irrd.server.access_check import is_client_permitted
-from irrd.storage.queries import RPSLDatabaseQuery, RPSLDatabaseJournalQuery
-from irrd.utils.text import snake_to_camel_case, remove_auth_hashes
-from .schema_generator import SchemaGenerator
+from irrd.storage.queries import RPSLDatabaseJournalQuery, RPSLDatabaseQuery
+from irrd.utils.text import remove_auth_hashes, snake_to_camel_case
+
 from ..query_resolver import QueryResolver
+from .schema_generator import SchemaGenerator
 
 """
 Resolvers resolve GraphQL queries, usually by translating them
 to a database query and then translating the results to an
 appropriate format for GraphQL.
 """
 
@@ -28,301 +30,323 @@
 
 
 def resolve_rpsl_object_type(obj: Dict[str, str], *_) -> str:
     """
     Find the GraphQL name for an object given its object class.
     (GraphQL names match RPSL class names.)
     """
-    return OBJECT_CLASS_MAPPING[obj.get('objectClass', obj.get('object_class', ''))].__name__
+    return OBJECT_CLASS_MAPPING[obj.get("objectClass", obj.get("object_class", ""))].__name__
 
 
 @ariadne.convert_kwargs_to_snake_case
 def resolve_rpsl_objects(_, info: GraphQLResolveInfo, **kwargs):
     """
     Resolve a `rpslObjects` query. This query has a considerable
     number of parameters, each of which is applied to an RPSL
     database query.
     """
     low_specificity_kwargs = {
-        'object_class', 'rpki_status', 'scope_filter_status', 'sources', 'sql_trace'
+        "object_class",
+        "rpki_status",
+        "scope_filter_status",
+        "route_preference_status",
+        "sources",
+        "sql_trace",
     }
     # A query is sufficiently specific if it has other fields than listed above,
     # except that rpki_status is sufficient if it is exclusively selecting on
     # valid or invalid.
-    low_specificity = all([
-        not (set(kwargs.keys()) - low_specificity_kwargs),
-        kwargs.get('rpki_status', []) not in [[RPKIStatus.valid], [RPKIStatus.invalid]],
-    ])
+    low_specificity = all(
+        [
+            not (set(kwargs.keys()) - low_specificity_kwargs),
+            kwargs.get("rpki_status", []) not in [[RPKIStatus.valid], [RPKIStatus.invalid]],
+        ]
+    )
     if low_specificity:
-        raise ValueError('Your query must be more specific.')
+        raise ValueError("Your query must be more specific.")
 
-    if kwargs.get('sql_trace'):
-        info.context['sql_trace'] = True
+    if kwargs.get("sql_trace"):
+        info.context["sql_trace"] = True
 
     query = RPSLDatabaseQuery(
-        column_names=_columns_for_graphql_selection(info),
-        ordered_by_sources=False,
-        enable_ordering=False
+        column_names=_columns_for_graphql_selection(info), ordered_by_sources=False, enable_ordering=False
     )
 
-    if 'record_limit' in kwargs:
-        query.limit(kwargs['record_limit'])
-    if 'rpsl_pk' in kwargs:
-        query.rpsl_pks(kwargs['rpsl_pk'])
-    if 'object_class' in kwargs:
-        query.object_classes(kwargs['object_class'])
-    if 'asn' in kwargs:
-        query.asns_first(kwargs['asn'])
-    if 'text_search' in kwargs:
-        query.text_search(kwargs['text_search'])
-    if 'rpki_status' in kwargs:
-        query.rpki_status(kwargs['rpki_status'])
+    if "record_limit" in kwargs:
+        query.limit(kwargs["record_limit"])
+    if "rpsl_pk" in kwargs:
+        query.rpsl_pks(kwargs["rpsl_pk"])
+    if "object_class" in kwargs:
+        query.object_classes(kwargs["object_class"])
+    if "asn" in kwargs:
+        query.asns_first(kwargs["asn"])
+    if "text_search" in kwargs:
+        query.text_search(kwargs["text_search"])
+    if "rpki_status" in kwargs:
+        query.rpki_status(kwargs["rpki_status"])
     else:
         query.rpki_status([RPKIStatus.not_found, RPKIStatus.valid])
-    if 'scope_filter_status' in kwargs:
-        query.scopefilter_status(kwargs['scope_filter_status'])
+    if "scope_filter_status" in kwargs:
+        query.scopefilter_status(kwargs["scope_filter_status"])
     else:
         query.scopefilter_status([ScopeFilterStatus.in_scope])
+    if "route_preference_status" in kwargs:
+        query.route_preference_status(kwargs["route_preference_status"])
+    else:
+        query.route_preference_status([RoutePreferenceStatus.visible])
 
-    all_valid_sources = set(get_setting('sources', {}).keys())
-    if get_setting('rpki.roa_source'):
+    all_valid_sources = set(get_setting("sources", {}).keys())
+    if get_setting("rpki.roa_source"):
         all_valid_sources.add(RPKI_IRR_PSEUDO_SOURCE)
-    sources_default = set(get_setting('sources_default', []))
+    sources_default = set(get_setting("sources_default", []))
 
-    if 'sources' in kwargs:
-        query.sources(kwargs['sources'])
+    if "sources" in kwargs:
+        query.sources(kwargs["sources"])
     elif sources_default and sources_default != all_valid_sources:
         query.sources(list(sources_default))
 
     # All other parameters are generic lookup fields, like `members`
     for attr, value in kwargs.items():
-        attr = attr.replace('_', '-')
+        attr = attr.replace("_", "-")
         if attr in lookup_fields:
             query.lookup_attrs_in([attr], value)
 
-    ip_filters = [
-        'ip_exact', 'ip_less_specific', 'ip_more_specific', 'ip_less_specific_one_level', 'ip_any'
-    ]
+    ip_filters = ["ip_exact", "ip_less_specific", "ip_more_specific", "ip_less_specific_one_level", "ip_any"]
     for ip_filter in ip_filters:
         if ip_filter in kwargs:
             getattr(query, ip_filter)(IP(kwargs[ip_filter]))
 
     return _rpsl_db_query_to_graphql_out(query, info)
 
 
 def resolve_rpsl_object_mnt_by_objs(rpsl_object, info: GraphQLResolveInfo):
     """Resolve mntByObjs on RPSL objects"""
-    return _resolve_subquery(rpsl_object, info, ['mntner'], pk_field='mntBy')
+    return _resolve_subquery(rpsl_object, info, ["mntner"], pk_field="mntBy")
 
 
 def resolve_rpsl_object_adminc_objs(rpsl_object, info: GraphQLResolveInfo):
     """Resolve adminCObjs on RPSL objects"""
-    return _resolve_subquery(rpsl_object, info, ['role', 'person'], pk_field='adminC')
+    return _resolve_subquery(rpsl_object, info, ["role", "person"], pk_field="adminC")
 
 
 def resolve_rpsl_object_techc_objs(rpsl_object, info: GraphQLResolveInfo):
     """Resolve techCObjs on RPSL objects"""
-    return _resolve_subquery(rpsl_object, info, ['role', 'person'], pk_field='techC')
+    return _resolve_subquery(rpsl_object, info, ["role", "person"], pk_field="techC")
 
 
 def resolve_rpsl_object_members_by_ref_objs(rpsl_object, info: GraphQLResolveInfo):
     """Resolve mbrsByRefObjs on RPSL objects"""
-    return _resolve_subquery(rpsl_object, info, ['mntner'], pk_field='mbrsByRef')
+    return _resolve_subquery(rpsl_object, info, ["mntner"], pk_field="mbrsByRef")
 
 
 def resolve_rpsl_object_member_of_objs(rpsl_object, info: GraphQLResolveInfo):
     """Resolve memberOfObjs on RPSL objects"""
-    object_klass = OBJECT_CLASS_MAPPING[rpsl_object['objectClass']]
-    sub_object_classes = object_klass.fields['member-of'].referring   # type: ignore
-    return _resolve_subquery(rpsl_object, info, sub_object_classes, pk_field='memberOf')
+    object_klass = OBJECT_CLASS_MAPPING[rpsl_object["objectClass"]]
+    sub_object_classes = object_klass.fields["member-of"].referring  # type: ignore
+    return _resolve_subquery(rpsl_object, info, sub_object_classes, pk_field="memberOf")
 
 
 def resolve_rpsl_object_members_objs(rpsl_object, info: GraphQLResolveInfo):
     """Resolve membersObjs on RPSL objects"""
-    object_klass = OBJECT_CLASS_MAPPING[rpsl_object['objectClass']]
-    sub_object_classes = object_klass.fields['members'].referring   # type: ignore
+    object_klass = OBJECT_CLASS_MAPPING[rpsl_object["objectClass"]]
+    sub_object_classes = object_klass.fields["members"].referring  # type: ignore
     # The reference to an aut-num should not be fully resolved, as the
     # reference is very weak.
-    if 'aut-num' in sub_object_classes:
-        sub_object_classes.remove('aut-num')
-    if 'inet-rtr' in sub_object_classes:
-        sub_object_classes.remove('inet-rtr')
-    return _resolve_subquery(rpsl_object, info, sub_object_classes, 'members', sticky_source=False)
+    if "aut-num" in sub_object_classes:
+        sub_object_classes.remove("aut-num")
+    if "inet-rtr" in sub_object_classes:
+        sub_object_classes.remove("inet-rtr")
+    return _resolve_subquery(rpsl_object, info, sub_object_classes, "members", sticky_source=False)
 
 
-def _resolve_subquery(rpsl_object, info: GraphQLResolveInfo, object_classes: List[str], pk_field: str, sticky_source=True):
+def _resolve_subquery(
+    rpsl_object, info: GraphQLResolveInfo, object_classes: List[str], pk_field: str, sticky_source=True
+):
     """
     Resolve a subquery, like techCobjs, on an RPSL object, considering
     a number of object classes, extracting the PK from pk_field.
     If sticky_source is set, the referred object must be from the same source.
     """
     pks = rpsl_object.get(pk_field)
     if not pks:
         return []
     if not isinstance(pks, list):
         pks = [pks]
     query = RPSLDatabaseQuery(
-        column_names=_columns_for_graphql_selection(info),
-        ordered_by_sources=False,
-        enable_ordering=False
+        column_names=_columns_for_graphql_selection(info), ordered_by_sources=False, enable_ordering=False
     )
     query.object_classes(object_classes).rpsl_pks(pks)
     if sticky_source:
-        query.sources([rpsl_object['source']])
+        query.sources([rpsl_object["source"]])
     return _rpsl_db_query_to_graphql_out(query, info)
 
 
 def resolve_rpsl_object_journal(rpsl_object, info: GraphQLResolveInfo):
     """
     Resolve a journal subquery on an RPSL object.
     """
-    database_handler = info.context['request'].app.state.database_handler
+    database_handler = info.context["request"].app.state.database_handler
     access_list = f"sources.{rpsl_object['source']}.nrtm_access_list"
-    if not is_client_permitted(info.context['request'].client.host, access_list):
+    if not is_client_permitted(info.context["request"].client.host, access_list):
         raise GraphQLError(f"Access to journal denied for source {rpsl_object['source']}")
 
     query = RPSLDatabaseJournalQuery()
-    query.sources([rpsl_object['source']]).rpsl_pk(rpsl_object['rpslPk'])
+    query.sources([rpsl_object["source"]]).rpsl_pk(rpsl_object["rpslPk"])
     for row in database_handler.execute_query(query, refresh_on_error=True):
         response = {snake_to_camel_case(k): v for k, v in row.items()}
-        response['operation'] = response['operation'].name
-        if response['origin']:
-            response['origin'] = response['origin'].name
-        if response['objectText']:
-            response['objectText'] = remove_auth_hashes(response['objectText'])
+        response["operation"] = response["operation"].name
+        if response["origin"]:
+            response["origin"] = response["origin"].name
+        if response["objectText"]:
+            response["objectText"] = remove_auth_hashes(response["objectText"])
         yield response
 
 
 def _rpsl_db_query_to_graphql_out(query: RPSLDatabaseQuery, info: GraphQLResolveInfo):
     """
     Given an RPSL database query, execute it and clean up the output
     to be suitable to return to GraphQL.
 
     Main changes are:
     - Enum handling
     - Adding the asn and prefix fields if applicable
     - Ensuring the right fields are returned as a list of strings or a string
     """
-    database_handler = info.context['request'].app.state.database_handler
-    if info.context.get('sql_trace'):
-        if 'sql_queries' not in info.context:
-            info.context['sql_queries'] = [repr(query)]
+    database_handler = info.context["request"].app.state.database_handler
+    if info.context.get("sql_trace"):
+        if "sql_queries" not in info.context:
+            info.context["sql_queries"] = [repr(query)]
         else:
-            info.context['sql_queries'].append(repr(query))
+            info.context["sql_queries"].append(repr(query))
 
     for row in database_handler.execute_query(query, refresh_on_error=True):
-        graphql_result = {snake_to_camel_case(k): v for k, v in row.items() if k != 'parsed_data'}
-        if 'object_text' in row:
-            graphql_result['objectText'] = remove_auth_hashes(row['object_text'])
-        if 'rpki_status' in row:
-            graphql_result['rpkiStatus'] = row['rpki_status']
-        if row.get('ip_first') is not None and row.get('prefix_length'):
-            graphql_result['prefix'] = row['ip_first'] + '/' + str(row['prefix_length'])
-        if row.get('asn_first') is not None and row.get('asn_first') == row.get('asn_last'):
-            graphql_result['asn'] = row['asn_first']
+        graphql_result = {snake_to_camel_case(k): v for k, v in row.items() if k != "parsed_data"}
+        if "object_text" in row:
+            graphql_result["objectText"] = remove_auth_hashes(row["object_text"])
+        if row.get("ip_first") is not None and row.get("prefix_length"):
+            graphql_result["prefix"] = row["ip_first"] + "/" + str(row["prefix_length"])
+        if row.get("asn_first") is not None and row.get("asn_first") == row.get("asn_last"):
+            graphql_result["asn"] = row["asn_first"]
 
         object_type = resolve_rpsl_object_type(row)
-        for key, value in row.get('parsed_data', dict()).items():
-            if key == 'auth':
+        for key, value in row.get("parsed_data", dict()).items():
+            if key == "auth":
                 value = [remove_auth_hashes(v) for v in value]
             graphql_type = schema.graphql_types[object_type][key]
-            if graphql_type == 'String' and isinstance(value, list):
-                value = '\n'.join(value)
+            if graphql_type == "String" and isinstance(value, list):
+                value = "\n".join(value)
             graphql_result[snake_to_camel_case(key)] = value
         yield graphql_result
 
 
 @ariadne.convert_kwargs_to_snake_case
-def resolve_database_status(_, info: GraphQLResolveInfo, sources: Optional[List[str]]=None):
+def resolve_database_status(_, info: GraphQLResolveInfo, sources: Optional[List[str]] = None):
     """Resolve a databaseStatus query"""
     query_resolver = QueryResolver(
-        info.context['request'].app.state.preloader,
-        info.context['request'].app.state.database_handler
+        info.context["request"].app.state.preloader, info.context["request"].app.state.database_handler
     )
     for name, data in query_resolver.database_status(sources=sources).items():
         camel_case_data = OrderedDict(data)
-        camel_case_data['source'] = name
+        camel_case_data["source"] = name
         for key, value in data.items():
             camel_case_data[snake_to_camel_case(key)] = value
         yield camel_case_data
 
 
 @ariadne.convert_kwargs_to_snake_case
-def resolve_asn_prefixes(_, info: GraphQLResolveInfo, asns: List[int], ip_version: Optional[int]=None, sources: Optional[List[str]]=None):
+def resolve_asn_prefixes(
+    _,
+    info: GraphQLResolveInfo,
+    asns: List[int],
+    ip_version: Optional[int] = None,
+    sources: Optional[List[str]] = None,
+):
     """Resolve an asnPrefixes query"""
     query_resolver = QueryResolver(
-        info.context['request'].app.state.preloader,
-        info.context['request'].app.state.database_handler
+        info.context["request"].app.state.preloader, info.context["request"].app.state.database_handler
     )
     query_resolver.set_query_sources(sources)
     for asn in asns:
-        yield dict(
-            asn=asn,
-            prefixes=list(query_resolver.routes_for_origin(f'AS{asn}', ip_version))
-        )
+        yield dict(asn=asn, prefixes=list(query_resolver.routes_for_origin(f"AS{asn}", ip_version)))
 
 
 @ariadne.convert_kwargs_to_snake_case
-def resolve_as_set_prefixes(_, info: GraphQLResolveInfo, set_names: List[str], sources: Optional[List[str]]=None, ip_version: Optional[int]=None, exclude_sets: Optional[List[str]]=None, sql_trace: bool=False):
+def resolve_as_set_prefixes(
+    _,
+    info: GraphQLResolveInfo,
+    set_names: List[str],
+    sources: Optional[List[str]] = None,
+    ip_version: Optional[int] = None,
+    exclude_sets: Optional[List[str]] = None,
+    sql_trace: bool = False,
+):
     """Resolve an asSetPrefixes query"""
     query_resolver = QueryResolver(
-        info.context['request'].app.state.preloader,
-        info.context['request'].app.state.database_handler
+        info.context["request"].app.state.preloader, info.context["request"].app.state.database_handler
     )
     if sql_trace:
         query_resolver.enable_sql_trace()
     set_names_set = {i.upper() for i in set_names}
     exclude_sets_set = {i.upper() for i in exclude_sets} if exclude_sets else set()
     query_resolver.set_query_sources(sources)
     for set_name in set_names_set:
         prefixes = list(query_resolver.routes_for_as_set(set_name, ip_version, exclude_sets=exclude_sets_set))
         yield dict(rpslPk=set_name, prefixes=prefixes)
     if sql_trace:
-        info.context['sql_queries'] = query_resolver.retrieve_sql_trace()
+        info.context["sql_queries"] = query_resolver.retrieve_sql_trace()
 
 
 @ariadne.convert_kwargs_to_snake_case
-def resolve_recursive_set_members(_, info: GraphQLResolveInfo, set_names: List[str], depth: int=0, sources: Optional[List[str]]=None, exclude_sets: Optional[List[str]]=None, sql_trace: bool=False):
+def resolve_recursive_set_members(
+    _,
+    info: GraphQLResolveInfo,
+    set_names: List[str],
+    depth: int = 0,
+    sources: Optional[List[str]] = None,
+    exclude_sets: Optional[List[str]] = None,
+    sql_trace: bool = False,
+):
     """Resolve an recursiveSetMembers query"""
     query_resolver = QueryResolver(
-        info.context['request'].app.state.preloader,
-        info.context['request'].app.state.database_handler
+        info.context["request"].app.state.preloader, info.context["request"].app.state.database_handler
     )
     if sql_trace:
         query_resolver.enable_sql_trace()
     set_names_set = {i.upper() for i in set_names}
     exclude_sets_set = {i.upper() for i in exclude_sets} if exclude_sets else set()
     query_resolver.set_query_sources(sources)
     for set_name in set_names_set:
-        results = query_resolver.members_for_set_per_source(set_name, exclude_sets=exclude_sets_set, depth=depth, recursive=True)
+        results = query_resolver.members_for_set_per_source(
+            set_name, exclude_sets=exclude_sets_set, depth=depth, recursive=True
+        )
         for source, members in results.items():
             yield dict(rpslPk=set_name, rootSource=source, members=members)
     if sql_trace:
-        info.context['sql_queries'] = query_resolver.retrieve_sql_trace()
+        info.context["sql_queries"] = query_resolver.retrieve_sql_trace()
 
 
 def _columns_for_graphql_selection(info: GraphQLResolveInfo) -> Set[str]:
     """
     Based on the selected GraphQL fields, determine which database
     columns should be retrieved.
     """
     # Some columns are always retrieved
-    columns = {'object_class', 'source', 'parsed_data', 'rpsl_pk'}
+    columns = {"object_class", "source", "parsed_data", "rpsl_pk"}
     fields = _collect_predicate_names(info.field_nodes[0].selection_set.selections)  # type: ignore
     requested_fields = {ariadne.convert_camel_case_to_snake(f) for f in fields}
 
     for field in requested_fields:
         if field in RPSLDatabaseQuery().columns:
             columns.add(field)
-        if field == 'asn':
-            columns.add('asn_first')
-            columns.add('asn_last')
-        if field == 'prefix':
-            columns.add('ip_first')
-            columns.add('prefix_length')
+        if field == "asn":
+            columns.add("asn_first")
+            columns.add("asn_last")
+        if field == "prefix":
+            columns.add("ip_first")
+            columns.add("prefix_length")
     return columns
 
 
 # https://github.com/mirumee/ariadne/issues/287
 def _collect_predicate_names(selections):  # pragma: no cover
     predicates = []
     for selection in selections:
```

### Comparing `irrd-4.2.8/irrd/server/graphql/schema_builder.py` & `irrd-4.3.0/irrd/server/graphql/schema_builder.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,21 +1,29 @@
-from IPy import IP
 from ariadne import make_executable_schema
 from asgiref.sync import sync_to_async as sta
 from graphql import GraphQLError
+from IPy import IP
 
-from .resolvers import (resolve_rpsl_objects, resolve_rpsl_object_type,
-                        resolve_database_status, resolve_rpsl_object_mnt_by_objs,
-                        resolve_rpsl_object_member_of_objs, resolve_rpsl_object_members_by_ref_objs,
-                        resolve_rpsl_object_members_objs, resolve_rpsl_object_adminc_objs,
-                        resolve_asn_prefixes, resolve_as_set_prefixes,
-                        resolve_recursive_set_members, resolve_rpsl_object_techc_objs,
-                        resolve_rpsl_object_journal)
-from .schema_generator import SchemaGenerator
 from ...utils.text import clean_ip_value_error
+from .resolvers import (
+    resolve_as_set_prefixes,
+    resolve_asn_prefixes,
+    resolve_database_status,
+    resolve_recursive_set_members,
+    resolve_rpsl_object_adminc_objs,
+    resolve_rpsl_object_journal,
+    resolve_rpsl_object_member_of_objs,
+    resolve_rpsl_object_members_by_ref_objs,
+    resolve_rpsl_object_members_objs,
+    resolve_rpsl_object_mnt_by_objs,
+    resolve_rpsl_object_techc_objs,
+    resolve_rpsl_object_type,
+    resolve_rpsl_objects,
+)
+from .schema_generator import SchemaGenerator
 
 
 def build_executable_schema():
     """
     Build an executable schema.
     This takes the schema from the schema generator, and attaches
     the resolvers for each field. It also sets up custom parsing
@@ -31,37 +39,37 @@
     schema.query_type.set_field("asnPrefixes", sta(resolve_asn_prefixes, False))
     schema.query_type.set_field("asSetPrefixes", sta(resolve_as_set_prefixes, False))
     schema.query_type.set_field("recursiveSetMembers", sta(resolve_recursive_set_members, False))
 
     schema.rpsl_object_type.set_field("mntByObjs", sta(resolve_rpsl_object_mnt_by_objs, False))
     schema.rpsl_object_type.set_field("journal", sta(resolve_rpsl_object_journal, False))
     for object_type in schema.object_types:
-        if 'adminCObjs' in schema.graphql_types[object_type.name]:
+        if "adminCObjs" in schema.graphql_types[object_type.name]:
             object_type.set_field("adminCObjs", sta(resolve_rpsl_object_adminc_objs, False))
     for object_type in schema.object_types:
-        if 'techCObjs' in schema.graphql_types[object_type.name]:
+        if "techCObjs" in schema.graphql_types[object_type.name]:
             object_type.set_field("techCObjs", sta(resolve_rpsl_object_techc_objs, False))
     for object_type in schema.object_types:
-        if 'mbrsByRefObjs' in schema.graphql_types[object_type.name]:
+        if "mbrsByRefObjs" in schema.graphql_types[object_type.name]:
             object_type.set_field("mbrsByRefObjs", sta(resolve_rpsl_object_members_by_ref_objs, False))
     for object_type in schema.object_types:
-        if 'memberOfObjs' in schema.graphql_types[object_type.name]:
+        if "memberOfObjs" in schema.graphql_types[object_type.name]:
             object_type.set_field("memberOfObjs", sta(resolve_rpsl_object_member_of_objs, False))
     for object_type in schema.object_types:
-        if 'membersObjs' in schema.graphql_types[object_type.name]:
+        if "membersObjs" in schema.graphql_types[object_type.name]:
             object_type.set_field("membersObjs", sta(resolve_rpsl_object_members_objs, False))
 
     @schema.asn_scalar_type.value_parser
     def parse_asn_scalar(value):
         try:
             return int(value)
         except ValueError:
-            raise GraphQLError(f'Invalid ASN: {value}; must be numeric')
+            raise GraphQLError(f"Invalid ASN: {value}; must be numeric")
 
     @schema.ip_scalar_type.value_parser
     def parse_ip_scalar(value):
         try:
             return IP(value)
         except ValueError as ve:
-            raise GraphQLError(f'Invalid IP: {value}: {clean_ip_value_error(ve)}')
+            raise GraphQLError(f"Invalid IP: {value}: {clean_ip_value_error(ve)}")
 
     return make_executable_schema(schema.type_defs, *schema.object_types)
```

### Comparing `irrd-4.2.8/irrd/server/graphql/schema_generator.py` & `irrd-4.3.0/irrd/server/graphql/schema_generator.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,16 +1,23 @@
 from collections import OrderedDict, defaultdict
-from typing import Optional, Dict, Tuple, List
+from typing import Dict, List, Optional, Tuple
 
 import ariadne
 
+from irrd.routepref.status import RoutePreferenceStatus
 from irrd.rpki.status import RPKIStatus
-from irrd.rpsl.fields import RPSLFieldListMixin, RPSLTextField, RPSLReferenceField
-from irrd.rpsl.rpsl_objects import (lookup_field_names, OBJECT_CLASS_MAPPING, RPSLAutNum,
-                                    RPSLInetRtr, RPSLPerson, RPSLRole)
+from irrd.rpsl.fields import RPSLFieldListMixin, RPSLReferenceField, RPSLTextField
+from irrd.rpsl.rpsl_objects import (
+    OBJECT_CLASS_MAPPING,
+    RPSLAutNum,
+    RPSLInetRtr,
+    RPSLPerson,
+    RPSLRole,
+    lookup_field_names,
+)
 from irrd.scopefilter.status import ScopeFilterStatus
 from irrd.utils.text import snake_to_camel_case
 
 
 class SchemaGenerator:
     def __init__(self):
         """
@@ -34,36 +41,40 @@
         self._set_rpsl_query_fields()
         self._set_rpsl_object_interface_schema()
         self._set_rpsl_contact_schema()
         self._set_rpsl_object_schemas()
         self._set_enums()
 
         schema = self.enums
-        schema += """
+        schema += (
+            """
             scalar ASN
             scalar IP
 
             schema {
               query: Query
             }
 
             type Query {
-              rpslObjects(""" + self.rpsl_query_fields + """): [RPSLObject!]
+              rpslObjects("""
+            + self.rpsl_query_fields
+            + """): [RPSLObject!]
               databaseStatus(sources: [String!]): [DatabaseStatus]
               asnPrefixes(asns: [ASN!]!, ipVersion: Int, sources: [String!]): [ASNPrefixes!]
               asSetPrefixes(setNames: [String!]!, ipVersion: Int, sources: [String!], excludeSets: [String!], sqlTrace: Boolean): [AsSetPrefixes!]
               recursiveSetMembers(setNames: [String!]!, depth: Int, sources: [String!], excludeSets: [String!], sqlTrace: Boolean): [SetMembers!]
             }
 
             type DatabaseStatus {
                 source: String!
                 authoritative: Boolean!
                 objectClassFilter: [String!]
                 rpkiRovFilter: Boolean!
                 scopefilterEnabled: Boolean!
+                routePreference: Int
                 localJournalKept: Boolean!
                 serialOldestJournal: Int
                 serialNewestJournal: Int
                 serialLastExport: Int
                 serialNewestMirror: Int
                 lastUpdate: String
                 synchronisedSerials: Boolean!
@@ -92,202 +103,214 @@
 
             type SetMembers {
                 rpslPk: String!
                 rootSource: String!
                 members: [String!]
             }
         """
+        )
         schema += self.rpsl_object_interface_schema
         schema += self.rpsl_contact_schema
-        schema += ''.join(self.rpsl_object_schemas.values())
-        schema += 'union RPSLContactUnion = RPSLPerson | RPSLRole'
+        schema += "".join(self.rpsl_object_schemas.values())
+        schema += "union RPSLContactUnion = RPSLPerson | RPSLRole"
 
         self.type_defs = ariadne.gql(schema)
 
         self.query_type = ariadne.QueryType()
         self.rpsl_object_type = ariadne.InterfaceType("RPSLObject")
         self.rpsl_contact_union_type = ariadne.UnionType("RPSLContactUnion")
         self.asn_scalar_type = ariadne.ScalarType("ASN")
         self.ip_scalar_type = ariadne.ScalarType("IP")
-        self.object_types = [self.query_type, self.rpsl_object_type, self.rpsl_contact_union_type,
-                             self.asn_scalar_type, self.ip_scalar_type]
+        self.object_types = [
+            self.query_type,
+            self.rpsl_object_type,
+            self.rpsl_contact_union_type,
+            self.asn_scalar_type,
+            self.ip_scalar_type,
+        ]
 
         for name in self.rpsl_object_schemas.keys():
             self.object_types.append(ariadne.ObjectType(name))
 
         self.object_types.append(ariadne.ObjectType("ASNPrefixes"))
         self.object_types.append(ariadne.ObjectType("AsSetPrefixes"))
         self.object_types.append(ariadne.ObjectType("SetMembers"))
         self.object_types.append(ariadne.EnumType("RPKIStatus", RPKIStatus))
         self.object_types.append(ariadne.EnumType("ScopeFilterStatus", ScopeFilterStatus))
+        self.object_types.append(ariadne.EnumType("RoutePreferenceStatus", RoutePreferenceStatus))
 
     def _set_rpsl_query_fields(self):
         """
         Create a sub-schema for the fields that can be queried for RPSL objects.
         This includes all fields from all objects, along with a few
         special fields.
         """
-        string_list_fields = {'rpsl_pk', 'sources', 'object_class'}.union(lookup_field_names())
-        params = [snake_to_camel_case(p) + ': [String!]' for p in sorted(string_list_fields)]
+        string_list_fields = {"rpsl_pk", "sources", "object_class"}.union(lookup_field_names())
+        params = [snake_to_camel_case(p) + ": [String!]" for p in sorted(string_list_fields)]
         params += [
-            'ipExact: IP',
-            'ipLessSpecific: IP',
-            'ipLessSpecificOneLevel: IP',
-            'ipMoreSpecific: IP',
-            'ipAny: IP',
-            'asn: [ASN!]',
-            'rpkiStatus: [RPKIStatus!]',
-            'scopeFilterStatus: [ScopeFilterStatus!]',
-            'textSearch: String',
-            'recordLimit: Int',
-            'sqlTrace: Boolean',
+            "ipExact: IP",
+            "ipLessSpecific: IP",
+            "ipLessSpecificOneLevel: IP",
+            "ipMoreSpecific: IP",
+            "ipAny: IP",
+            "asn: [ASN!]",
+            "rpkiStatus: [RPKIStatus!]",
+            "scopeFilterStatus: [ScopeFilterStatus!]",
+            "routePreferenceStatus: [RoutePreferenceStatus!]",
+            "textSearch: String",
+            "recordLimit: Int",
+            "sqlTrace: Boolean",
         ]
-        self.rpsl_query_fields = ', '.join(params)
+        self.rpsl_query_fields = ", ".join(params)
 
     def _set_enums(self):
         """
-        Create the schema for enums, current RPKI and scope filter status.
+        Create the schema for enums of RPKI, scope filter and route preference..
         """
-        self.enums = ''
-        for enum in [RPKIStatus, ScopeFilterStatus]:
-            self.enums += f'enum {enum.__name__} {{\n'
+        self.enums = ""
+        for enum in [RPKIStatus, ScopeFilterStatus, RoutePreferenceStatus]:
+            self.enums += f"enum {enum.__name__} {{\n"
             for value in enum:
-                self.enums += f'    {value.name}\n'
-            self.enums += '}\n\n'
+                self.enums += f"    {value.name}\n"
+            self.enums += "}\n\n"
 
     def _set_rpsl_object_interface_schema(self):
         """
         Create the schema for RPSLObject, which contains only fields that
         are common to every known RPSL object, along with meta
         """
         common_fields = None
         for rpsl_object_class in OBJECT_CLASS_MAPPING.values():
             if common_fields is None:
                 common_fields = set(rpsl_object_class.fields.keys())
             else:
                 common_fields = common_fields.intersection(set(rpsl_object_class.fields.keys()))
         common_fields = list(common_fields)
-        common_fields = ['rpslPk', 'objectClass', 'objectText', 'updated'] + common_fields
+        common_fields = ["rpslPk", "objectClass", "objectText", "updated"] + common_fields
         common_field_dict = self._dict_for_common_fields(common_fields)
-        common_field_dict['journal'] = '[RPSLJournalEntry]'
-        schema = self._generate_schema_str('RPSLObject', 'interface', common_field_dict)
+        common_field_dict["journal"] = "[RPSLJournalEntry]"
+        schema = self._generate_schema_str("RPSLObject", "interface", common_field_dict)
         self.rpsl_object_interface_schema = schema
 
     def _set_rpsl_contact_schema(self):
         """
         Create the schema for RPSLContact. This contains shared fields between
         RPSLPerson and RPSLRole, as they are so similar.
         """
         common_fields = set(RPSLPerson.fields.keys()).intersection(set(RPSLRole.fields.keys()))
-        common_fields = common_fields.union({'rpslPk', 'objectClass', 'objectText', 'updated'})
+        common_fields = common_fields.union({"rpslPk", "objectClass", "objectText", "updated"})
         common_field_dict = self._dict_for_common_fields(list(common_fields))
-        schema = self._generate_schema_str('RPSLContact', 'interface', common_field_dict)
+        schema = self._generate_schema_str("RPSLContact", "interface", common_field_dict)
         self.rpsl_contact_schema = schema
 
     def _dict_for_common_fields(self, common_fields: List[str]):
         common_field_dict = OrderedDict()
         for field_name in sorted(common_fields):
             try:
                 # These fields are present in all relevant object, so this is a safe check
                 rpsl_field = RPSLPerson.fields[field_name]
                 graphql_type = self._graphql_type_for_rpsl_field(rpsl_field)
 
-                reference_name, reference_type = self._grapql_type_for_reference_field(
-                    field_name, rpsl_field)
+                reference_name, reference_type = self._grapql_type_for_reference_field(field_name, rpsl_field)
                 if reference_name and reference_type:
                     common_field_dict[reference_name] = reference_type
             except KeyError:
-                graphql_type = 'String'
+                graphql_type = "String"
             common_field_dict[snake_to_camel_case(field_name)] = graphql_type
         return common_field_dict
 
     def _set_rpsl_object_schemas(self):
         """
         Create the schemas for each specific RPSL object class.
         Each of these implements RPSLObject, and RPSLPerson/RPSLRole
         implement RPSLContact as well.
         """
         self.graphql_types = defaultdict(dict)
         schemas = OrderedDict()
         for object_class, klass in OBJECT_CLASS_MAPPING.items():
             object_name = klass.__name__
             graphql_fields = OrderedDict()
-            graphql_fields['rpslPk'] = 'String'
-            graphql_fields['objectClass'] = 'String'
-            graphql_fields['objectText'] = 'String'
-            graphql_fields['updated'] = 'String'
-            graphql_fields['journal'] = '[RPSLJournalEntry]'
+            graphql_fields["rpslPk"] = "String"
+            graphql_fields["objectClass"] = "String"
+            graphql_fields["objectText"] = "String"
+            graphql_fields["updated"] = "String"
+            graphql_fields["journal"] = "[RPSLJournalEntry]"
             for field_name, field in klass.fields.items():
                 graphql_type = self._graphql_type_for_rpsl_field(field)
                 graphql_fields[snake_to_camel_case(field_name)] = graphql_type
                 self.graphql_types[snake_to_camel_case(object_name)][field_name] = graphql_type
 
                 reference_name, reference_type = self._grapql_type_for_reference_field(field_name, field)
                 if reference_name and reference_type:
                     graphql_fields[reference_name] = reference_type
                     self.graphql_types[object_name][reference_name] = reference_type
 
             for field_name in klass.field_extracts:
-                if field_name.startswith('asn'):
-                    graphql_type = 'ASN'
-                elif field_name == 'prefix':
-                    graphql_type = 'IP'
-                elif field_name == 'prefix_length':
-                    graphql_type = 'Int'
+                if field_name.startswith("asn"):
+                    graphql_type = "ASN"
+                elif field_name == "prefix":
+                    graphql_type = "IP"
+                elif field_name == "prefix_length":
+                    graphql_type = "Int"
                 else:
-                    graphql_type = 'String'
+                    graphql_type = "String"
                 graphql_fields[snake_to_camel_case(field_name)] = graphql_type
-            if klass.rpki_relevant:
-                graphql_fields['rpkiStatus'] = 'RPKIStatus'
-                graphql_fields['rpkiMaxLength'] = 'Int'
-                self.graphql_types[object_name]['rpki_max_length'] = 'Int'
-            implements = 'RPSLContact & RPSLObject' if klass in [RPSLPerson, RPSLRole] else 'RPSLObject'
-            schema = self._generate_schema_str(object_name, 'type', graphql_fields, implements)
+            if klass.is_route:
+                graphql_fields["rpkiStatus"] = "RPKIStatus"
+                graphql_fields["rpkiMaxLength"] = "Int"
+                self.graphql_types[object_name]["rpki_max_length"] = "Int"
+                graphql_fields["routePreferenceStatus"] = "RoutePreferenceStatus"
+            implements = "RPSLContact & RPSLObject" if klass in [RPSLPerson, RPSLRole] else "RPSLObject"
+            schema = self._generate_schema_str(object_name, "type", graphql_fields, implements)
             schemas[object_name] = schema
         self.rpsl_object_schemas = schemas
 
     def _graphql_type_for_rpsl_field(self, field: RPSLTextField) -> str:
         """
         Return the GraphQL type for a regular RPSL field.
         This is always a list of strings if the field is a list and/or
         can occur multiple times.
         """
         if RPSLFieldListMixin in field.__class__.__bases__ or field.multiple:
-            return '[String!]'
-        return 'String'
+            return "[String!]"
+        return "String"
 
-    def _grapql_type_for_reference_field(self, field_name: str, rpsl_field: RPSLTextField) -> Tuple[Optional[str], Optional[str]]:
+    def _grapql_type_for_reference_field(
+        self, field_name: str, rpsl_field: RPSLTextField
+    ) -> Tuple[Optional[str], Optional[str]]:
         """
         Return the GraphQL name and type for a reference field.
         For example, for a field "admin-c" that refers to person/role,
         returns ('adminC', '[RPSLContactUnion!]').
         Some fields are excluded because they are syntactical references,
         not real references.
         """
-        if isinstance(rpsl_field, RPSLReferenceField) and getattr(rpsl_field, 'referring', None):
+        if isinstance(rpsl_field, RPSLReferenceField) and getattr(rpsl_field, "referring", None):
             rpsl_field.resolve_references()
-            graphql_name = snake_to_camel_case(field_name) + 'Objs'
+            graphql_name = snake_to_camel_case(field_name) + "Objs"
             grapql_referring = set(rpsl_field.referring_object_classes)
             if RPSLAutNum in grapql_referring:
                 grapql_referring.remove(RPSLAutNum)
             if RPSLInetRtr in grapql_referring:
                 grapql_referring.remove(RPSLInetRtr)
             if grapql_referring == {RPSLPerson, RPSLRole}:
-                graphql_type = '[RPSLContactUnion!]'
+                graphql_type = "[RPSLContactUnion!]"
             else:
-                graphql_type = '[' + grapql_referring.pop().__name__ + '!]'
+                graphql_type = "[" + grapql_referring.pop().__name__ + "!]"
             return graphql_name, graphql_type
         return None, None
 
-    def _generate_schema_str(self, name: str, graphql_type: str, fields: Dict[str, str], implements: Optional[str]=None) -> str:
+    def _generate_schema_str(
+        self, name: str, graphql_type: str, fields: Dict[str, str], implements: Optional[str] = None
+    ) -> str:
         """
         Generate a schema string for a given name, object type and dict of fields.
         """
-        schema = f'{graphql_type} {name} '
+        schema = f"{graphql_type} {name} "
         if implements:
-            schema += f'implements {implements} '
-        schema += '{\n'
+            schema += f"implements {implements} "
+        schema += "{\n"
 
         for field, field_type in fields.items():
-            schema += f'  {field}: {field_type}\n'
-        schema += '}\n\n'
+            schema += f"  {field}: {field_type}\n"
+        schema += "}\n\n"
         return schema
```

### Comparing `irrd-4.2.8/irrd/server/graphql/tests/test_extensions.py` & `irrd-4.3.0/irrd/server/graphql/tests/test_extensions.py`

 * *Files 12% similar despite different names*

```diff
@@ -3,42 +3,44 @@
 
 from ..extensions import QueryMetadataExtension, error_formatter
 
 
 def test_query_metedata_extension(caplog):
     extension = QueryMetadataExtension()
 
-    mock_request = HTTPConnection({
-        'type': 'http',
-        'client': ('127.0.0.1', '8000'),
-    })
+    mock_request = HTTPConnection(
+        {
+            "type": "http",
+            "client": ("127.0.0.1", "8000"),
+        }
+    )
     mock_request._json = {
-        'operationName': 'operation',
-        'query': 'graphql query',
+        "operationName": "operation",
+        "query": "graphql query",
     }
     context = {
-        'sql_queries': ['sql query'],
-        'request': mock_request,
+        "sql_queries": ["sql query"],
+        "request": mock_request,
     }
     extension.request_started(context)
     result = extension.format(context)
-    assert '127.0.0.1 ran query in ' in caplog.text
+    assert "127.0.0.1 ran query in " in caplog.text
     assert ": {'operationName': 'operation', 'query': 'graphqlquery'}" in caplog.text
-    assert result['execution'] < 3
-    assert result['sql_query_count'] == 1
-    assert result['sql_queries'] == ['sql query']
+    assert result["execution"] < 3
+    assert result["sql_query_count"] == 1
+    assert result["sql_queries"] == ["sql query"]
 
 
 def test_error_formatter():
     # Regular GraphQL error should always be passed
-    error = GraphQLError(message='error')
+    error = GraphQLError(message="error")
     result = error_formatter(error)
-    assert result['message'] == 'error'
+    assert result["message"] == "error"
 
     # If original_error is something else, hide except when in debug mode
-    error = GraphQLError(message='error', original_error=ValueError())
+    error = GraphQLError(message="error", original_error=ValueError())
     result = error_formatter(error)
-    assert result['message'] == 'Internal server error'
+    assert result["message"] == "Internal server error"
 
     result = error_formatter(error, debug=True)
-    assert result['message'] == 'error'
-    assert result['extensions'] == {'exception': None}
+    assert result["message"] == "error"
+    assert result["extensions"] == {"exception": None}
```

### Comparing `irrd-4.2.8/irrd/server/graphql/tests/test_schema_generator.py` & `irrd-4.3.0/irrd/server/graphql/tests/test_schema_generator.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,50 +1,58 @@
 from ..schema_generator import SchemaGenerator
 
 
 def test_schema_generator():
     # This test will need updating if changes are made to RPSL types.
     generator = SchemaGenerator()
-    assert generator.graphql_types['RPSLAsBlock']['descr'] == '[String!]'
-    assert generator.graphql_types['RPSLAsBlock']['techCObjs'] == '[RPSLContactUnion!]'
-    assert generator.graphql_types['RPSLRtrSet']['rtr-set'] == 'String'
-    assert generator.type_defs == """enum RPKIStatus {
+    assert generator.graphql_types["RPSLAsBlock"]["descr"] == "[String!]"
+    assert generator.graphql_types["RPSLAsBlock"]["techCObjs"] == "[RPSLContactUnion!]"
+    assert generator.graphql_types["RPSLRtrSet"]["rtr-set"] == "String"
+    assert (
+        generator.type_defs
+        == """enum RPKIStatus {
     valid
     invalid
     not_found
 }
 
 enum ScopeFilterStatus {
     in_scope
     out_scope_as
     out_scope_prefix
 }
 
+enum RoutePreferenceStatus {
+    visible
+    suppressed
+}
+
 
             scalar ASN
             scalar IP
 
             schema {
               query: Query
             }
 
             type Query {
-              rpslObjects(adminC: [String!], mbrsByRef: [String!], memberOf: [String!], members: [String!], mntBy: [String!], mpMembers: [String!], objectClass: [String!], origin: [String!], person: [String!], role: [String!], rpslPk: [String!], sources: [String!], techC: [String!], zoneC: [String!], ipExact: IP, ipLessSpecific: IP, ipLessSpecificOneLevel: IP, ipMoreSpecific: IP, ipAny: IP, asn: [ASN!], rpkiStatus: [RPKIStatus!], scopeFilterStatus: [ScopeFilterStatus!], textSearch: String, recordLimit: Int, sqlTrace: Boolean): [RPSLObject!]
+              rpslObjects(adminC: [String!], mbrsByRef: [String!], memberOf: [String!], members: [String!], mntBy: [String!], mpMembers: [String!], objectClass: [String!], origin: [String!], person: [String!], role: [String!], rpslPk: [String!], sources: [String!], techC: [String!], zoneC: [String!], ipExact: IP, ipLessSpecific: IP, ipLessSpecificOneLevel: IP, ipMoreSpecific: IP, ipAny: IP, asn: [ASN!], rpkiStatus: [RPKIStatus!], scopeFilterStatus: [ScopeFilterStatus!], routePreferenceStatus: [RoutePreferenceStatus!], textSearch: String, recordLimit: Int, sqlTrace: Boolean): [RPSLObject!]
               databaseStatus(sources: [String!]): [DatabaseStatus]
               asnPrefixes(asns: [ASN!]!, ipVersion: Int, sources: [String!]): [ASNPrefixes!]
               asSetPrefixes(setNames: [String!]!, ipVersion: Int, sources: [String!], excludeSets: [String!], sqlTrace: Boolean): [AsSetPrefixes!]
               recursiveSetMembers(setNames: [String!]!, depth: Int, sources: [String!], excludeSets: [String!], sqlTrace: Boolean): [SetMembers!]
             }
 
             type DatabaseStatus {
                 source: String!
                 authoritative: Boolean!
                 objectClassFilter: [String!]
                 rpkiRovFilter: Boolean!
                 scopefilterEnabled: Boolean!
+                routePreference: Int
                 localJournalKept: Boolean!
                 serialOldestJournal: Int
                 serialNewestJournal: Int
                 serialLastExport: Int
                 serialNewestMirror: Int
                 lastUpdate: String
                 synchronisedSerials: Boolean!
@@ -465,14 +473,15 @@
   ipFirst: String
   ipLast: String
   prefix: IP
   prefixLength: Int
   asn: ASN
   rpkiStatus: RPKIStatus
   rpkiMaxLength: Int
+  routePreferenceStatus: RoutePreferenceStatus
 }
 
 type RPSLRouteSet implements RPSLObject {
   rpslPk: String
   objectClass: String
   objectText: String
   updated: String
@@ -527,14 +536,15 @@
   ipFirst: String
   ipLast: String
   prefix: IP
   prefixLength: Int
   asn: ASN
   rpkiStatus: RPKIStatus
   rpkiMaxLength: Int
+  routePreferenceStatus: RoutePreferenceStatus
 }
 
 type RPSLRtrSet implements RPSLObject {
   rpslPk: String
   objectClass: String
   objectText: String
   updated: String
@@ -556,7 +566,8 @@
   mntBy: [String!]
   mntByObjs: [RPSLMntner!]
   changed: [String!]
   source: String
 }
 
 union RPSLContactUnion = RPSLPerson | RPSLRole"""
+    )
```

### Comparing `irrd-4.2.8/irrd/server/http/app.py` & `irrd-4.3.0/irrd/server/http/app.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,25 +1,35 @@
 import logging
 import os
 import signal
 
 from ariadne.asgi import GraphQL
+from ariadne.asgi.handlers import GraphQLHTTPHandler
 from setproctitle import setproctitle
 from starlette.applications import Starlette
 from starlette.middleware import Middleware
-from starlette.routing import Mount
+from starlette.routing import Mount, Route, WebSocketRoute
 from starlette.types import ASGIApp, Receive, Scope, Send
 
 # Relative imports are not allowed in this file
 from irrd import ENV_MAIN_PROCESS_PID
 from irrd.conf import config_init
 from irrd.server.graphql import ENV_UVICORN_WORKER_CONFIG_PATH
-from irrd.server.graphql.extensions import error_formatter, QueryMetadataExtension
+from irrd.server.graphql.extensions import QueryMetadataExtension, error_formatter
 from irrd.server.graphql.schema_builder import build_executable_schema
-from irrd.server.http.endpoints import StatusEndpoint, WhoisQueryEndpoint, ObjectSubmissionEndpoint
+from irrd.server.http.endpoints import (
+    ObjectSubmissionEndpoint,
+    StatusEndpoint,
+    SuspensionSubmissionEndpoint,
+    WhoisQueryEndpoint,
+)
+from irrd.server.http.event_stream import (
+    EventStreamEndpoint,
+    EventStreamInitialDownloadEndpoint,
+)
 from irrd.storage.database_handler import DatabaseHandler
 from irrd.storage.preload import Preloader
 from irrd.utils.process_support import memory_trim, set_traceback_handler
 
 logger = logging.getLogger(__name__)
 
 """
@@ -42,42 +52,52 @@
     global app
     config_path = os.getenv(ENV_UVICORN_WORKER_CONFIG_PATH)
     config_init(config_path)
     try:
         app.state.database_handler = DatabaseHandler(readonly=True)
         app.state.preloader = Preloader(enable_queries=True)
     except Exception as e:
-        logger.critical(f'HTTP worker failed to initialise preloader or database, '
-                        f'unable to start, terminating IRRd, traceback follows: {e}', exc_info=e)
+        logger.critical(
+            (
+                "HTTP worker failed to initialise preloader or database, "
+                f"unable to start, terminating IRRd, traceback follows: {e}"
+            ),
+            exc_info=e,
+        )
         main_pid = os.getenv(ENV_MAIN_PROCESS_PID)
         if main_pid:
             os.kill(int(main_pid), signal.SIGTERM)
         else:
-            logger.error('Failed to terminate IRRd, unable to find main process PID')
+            logger.error("Failed to terminate IRRd, unable to find main process PID")
         return
 
 
 async def shutdown():
     global app
     app.state.database_handler.close()
     app.state.preloader = None
 
 
 graphql = GraphQL(
     build_executable_schema(),
     debug=False,
-    extensions=[QueryMetadataExtension],
+    http_handler=GraphQLHTTPHandler(
+        extensions=[QueryMetadataExtension],
+    ),
     error_formatter=error_formatter,
 )
 
 routes = [
     Mount("/v1/status", StatusEndpoint),
     Mount("/v1/whois", WhoisQueryEndpoint),
     Mount("/v1/submit", ObjectSubmissionEndpoint),
+    Mount("/v1/suspension", SuspensionSubmissionEndpoint),
     Mount("/graphql", graphql),
+    WebSocketRoute("/v1/event-stream/", EventStreamEndpoint),
+    Route("/v1/event-stream/initial/", EventStreamInitialDownloadEndpoint),
 ]
 
 
 class MemoryTrimMiddleware:
     def __init__(self, app: ASGIApp) -> None:
         self.app = app
```

### Comparing `irrd-4.2.8/irrd/server/http/endpoints.py` & `irrd-4.3.0/irrd/server/http/endpoints.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,57 +1,59 @@
+import json
 import logging
 import time
 from json import JSONDecodeError
 
 import pydantic
 from asgiref.sync import sync_to_async
 from starlette.endpoints import HTTPEndpoint
 from starlette.requests import Request
-from starlette.responses import PlainTextResponse, Response, JSONResponse
+from starlette.responses import JSONResponse, PlainTextResponse, Response
 
 from irrd.server.access_check import is_client_permitted
 from irrd.updates.handler import ChangeSubmissionHandler
-from irrd.utils.validators import RPSLChangeSubmission
-from .status_generator import StatusGenerator
+from irrd.utils.validators import RPSLChangeSubmission, RPSLSuspensionSubmission
+
 from ..whois.query_parser import WhoisQueryParser
 from ..whois.query_response import WhoisQueryResponseType
+from .status_generator import StatusGenerator
 
 logger = logging.getLogger(__name__)
 
 
 class StatusEndpoint(HTTPEndpoint):
     def get(self, request: Request) -> Response:
-        if not is_client_permitted(request.client.host, 'server.http.status_access_list'):
-            return PlainTextResponse('Access denied', status_code=403)
+        assert request.client
+        if not is_client_permitted(request.client.host, "server.http.status_access_list"):
+            return PlainTextResponse("Access denied", status_code=403)
 
         response = StatusGenerator().generate_status()
         return PlainTextResponse(response)
 
 
 class WhoisQueryEndpoint(HTTPEndpoint):
     def get(self, request: Request) -> Response:
+        assert request.client
         start_time = time.perf_counter()
-        if 'q' not in request.query_params:
+        if "q" not in request.query_params:
             return PlainTextResponse('Missing required query parameter "q"', status_code=400)
-        client_str = request.client.host + ':' + str(request.client.port)
-        query = request.query_params['q']
+        client_str = request.client.host + ":" + str(request.client.port)
+        query = request.query_params["q"]
 
         parser = WhoisQueryParser(
-            request.client.host,
-            client_str,
-            request.app.state.preloader,
-            request.app.state.database_handler
+            request.client.host, client_str, request.app.state.preloader, request.app.state.database_handler
         )
         response = parser.handle_query(query)
         response.clean_response()
 
         elapsed = time.perf_counter() - start_time
         length = len(response.result) if response.result else 0
-        logger.info(f'{client_str}: sent answer to HTTP query, elapsed {elapsed:.9f}s, '
-                    f'{length} chars: {query}')
+        logger.info(
+            f"{client_str}: sent answer to HTTP query, elapsed {elapsed:.9f}s, {length} chars: {query}"
+        )
 
         if response.response_type == WhoisQueryResponseType.ERROR_INTERNAL:
             return PlainTextResponse(response.result, status_code=500)
         if response.response_type == WhoisQueryResponseType.ERROR_USER:
             return PlainTextResponse(response.result, status_code=400)
         if response.result:
             return PlainTextResponse(response.result)
@@ -63,23 +65,47 @@
     async def post(self, request: Request) -> Response:
         return await self._handle_submission(request, delete=False)
 
     async def delete(self, request: Request) -> Response:
         return await self._handle_submission(request, delete=True)
 
     async def _handle_submission(self, request: Request, delete=False):
+        assert request.client
         try:
-            json = await request.json()
-            data = RPSLChangeSubmission.parse_obj(json)
+            request_json = await request.json()
+            data = RPSLChangeSubmission.parse_obj(request_json)
         except (JSONDecodeError, pydantic.ValidationError) as error:
             return PlainTextResponse(str(error), status_code=400)
 
-        request_meta = {
-            'HTTP-client-IP': request.client.host,
-            'HTTP-User-Agent': request.headers.get('User-Agent'),
-        }
+        try:
+            meta_json = request.headers["X-irrd-metadata"]
+            request_meta = json.loads(meta_json)
+        except (JSONDecodeError, KeyError):
+            request_meta = {}
+
+        request_meta["HTTP-client-IP"] = request.client.host
+        request_meta["HTTP-User-Agent"] = request.headers.get("User-Agent")
+
         handler = ChangeSubmissionHandler()
         await sync_to_async(handler.load_change_submission)(
             data=data, delete=delete, request_meta=request_meta
         )
         await sync_to_async(handler.send_notification_target_reports)()
         return JSONResponse(handler.submitter_report_json())
+
+
+class SuspensionSubmissionEndpoint(HTTPEndpoint):
+    async def post(self, request: Request) -> Response:
+        assert request.client
+        try:
+            json = await request.json()
+            data = RPSLSuspensionSubmission.parse_obj(json)
+        except (JSONDecodeError, pydantic.ValidationError) as error:
+            return PlainTextResponse(str(error), status_code=400)
+
+        request_meta = {
+            "HTTP-client-IP": request.client.host,
+            "HTTP-User-Agent": request.headers.get("User-Agent"),
+        }
+        handler = ChangeSubmissionHandler()
+        await sync_to_async(handler.load_suspension_submission)(data=data, request_meta=request_meta)
+        return JSONResponse(handler.submitter_report_json())
```

### Comparing `irrd-4.2.8/irrd/server/http/server.py` & `irrd-4.3.0/irrd/server/http/server.py`

 * *Files 16% similar despite different names*

```diff
@@ -4,44 +4,45 @@
 """
 import os
 import sys
 from pathlib import Path
 
 import uvicorn
 from setproctitle import setproctitle
-from uvicorn import subprocess
+from uvicorn import _subprocess
 
 sys.path.append(str(Path(__file__).resolve().parents[3]))
 
 from irrd import __version__
-from irrd.conf import get_setting, get_configuration
-
+from irrd.conf import get_configuration, get_setting
 from irrd.server.graphql import ENV_UVICORN_WORKER_CONFIG_PATH
 
 
 def run_http_server(config_path: str):
-    setproctitle('irrd-http-server-manager')
+    setproctitle("irrd-http-server-manager")
     configuration = get_configuration()
     assert configuration
     os.environ[ENV_UVICORN_WORKER_CONFIG_PATH] = config_path
     uvicorn.run(
         app="irrd.server.http.app:app",
-        host=get_setting('server.http.interface'),
-        port=get_setting('server.http.port'),
-        workers=get_setting('server.http.workers'),
-        forwarded_allow_ips=get_setting('server.http.forwarded_allowed_ips'),
-        headers=[['Server', f'IRRd {__version__}']],
+        host=get_setting("server.http.interface"),
+        port=get_setting("server.http.port"),
+        workers=get_setting("server.http.workers"),
+        forwarded_allow_ips=get_setting("server.http.forwarded_allowed_ips"),
+        headers=[("Server", f"IRRd {__version__}")],
         log_config=configuration.logging_config,
+        ws_ping_interval=60,
+        ws_ping_timeout=60,
     )
 
 
 def subprocess_started(config, target, sockets, stdin_fileno):
     """
     Uvicorns default method attempts magic with stdin which
     does not work with the IRRd daemon setup. This is an
     override that skips that step.
     """
     config.configure_logging()
     target(sockets=sockets)
 
 
-uvicorn.subprocess.subprocess_started = subprocess_started
+uvicorn._subprocess.subprocess_started = subprocess_started
```

### Comparing `irrd-4.2.8/irrd/server/http/status_generator.py` & `irrd-4.3.0/irrd/server/http/status_generator.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,9 +1,8 @@
 import logging
-import socket
 import textwrap
 from typing import Optional
 
 from beautifultable import BeautifulTable
 
 from irrd import __version__
 from irrd.conf import get_setting
@@ -12,114 +11,121 @@
 from irrd.storage.queries import DatabaseStatusQuery, RPSLDatabaseObjectStatisticsQuery
 from irrd.utils.whois_client import whois_query_source_status
 
 logger = logging.getLogger(__name__)
 
 
 class StatusGenerator:
-
     def generate_status(self) -> str:
         """
         Generate a human-readable overview of database status.
         """
         database_handler = DatabaseHandler()
 
         statistics_query = RPSLDatabaseObjectStatisticsQuery()
         self.statistics_results = list(database_handler.execute_query(statistics_query))
         status_query = DatabaseStatusQuery()
         self.status_results = list(database_handler.execute_query(status_query))
 
         results = [
             self._generate_header(),
             self._generate_statistics_table(),
-            self._generate_source_detail(database_handler)
+            self._generate_source_detail(database_handler),
         ]
         database_handler.close()
-        return '\n\n'.join(results)
+        return "\n\n".join(results)
 
     def _generate_header(self) -> str:
         """
         Generate the header of the report, containing basic info like version
         and time until the next mirror update.
         """
-        return textwrap.dedent(f"""
+        return textwrap.dedent(
+            f"""
         IRRD version {__version__}
         Listening on {get_setting('server.whois.interface')} port {get_setting('server.whois.port')}
-        """).lstrip()
+        """
+        ).lstrip()
 
     def _generate_statistics_table(self) -> str:
         """
         Generate a table with an overview of basic stats for each database.
         """
         table = BeautifulTable(default_alignment=BeautifulTable.ALIGN_RIGHT)
-        table.column_headers = ['source', 'total obj', 'rt obj', 'aut-num obj', 'serial', 'last export']
-        table.column_alignments['source'] = BeautifulTable.ALIGN_LEFT
-        table.left_border_char = table.right_border_char = ''
-        table.right_border_char = table.bottom_border_char = ''
-        table.row_separator_char = ''
-        table.column_separator_char = '  '
+        table.column_headers = ["source", "total obj", "rt obj", "aut-num obj", "serial", "last export"]
+        table.column_alignments["source"] = BeautifulTable.ALIGN_LEFT
+        table.left_border_char = table.right_border_char = ""
+        table.right_border_char = table.bottom_border_char = ""
+        table.row_separator_char = ""
+        table.column_separator_char = "  "
 
         for status_result in self.status_results:
-            source = status_result['source'].upper()
+            source = status_result["source"].upper()
             total_obj, route_obj, autnum_obj = self._statistics_for_source(source)
-            serial = status_result['serial_newest_seen']
-            last_export = status_result['serial_last_export']
+            serial = status_result["serial_newest_seen"]
+            last_export = status_result["serial_last_export"]
             if not last_export:
-                last_export = ''
+                last_export = ""
             table.append_row([source, total_obj, route_obj, autnum_obj, serial, last_export])
 
         total_obj, route_obj, autnum_obj = self._statistics_for_source(None)
-        table.append_row(['TOTAL', total_obj, route_obj, autnum_obj, '', ''])
+        table.append_row(["TOTAL", total_obj, route_obj, autnum_obj, "", ""])
 
         return str(table)
 
     def _statistics_for_source(self, source: Optional[str]):
         """
         Extract counts of total objects, route objects and aut-num objects,
         from the results of a previous SQL query.
         If source is None, all sources are counted.
         """
         if source:
-            source_statistics = [s for s in self.statistics_results if s['source'] == source]
+            source_statistics = [s for s in self.statistics_results if s["source"] == source]
         else:
             source_statistics = self.statistics_results
 
-        total_obj = sum([s['count'] for s in source_statistics])
-        route_obj = sum([s['count'] for s in source_statistics if s['object_class'] == 'route'])
-        autnum_obj = sum([s['count'] for s in source_statistics if s['object_class'] == 'aut-num'])
+        total_obj = sum([s["count"] for s in source_statistics])
+        route_obj = sum([s["count"] for s in source_statistics if s["object_class"] == "route"])
+        autnum_obj = sum([s["count"] for s in source_statistics if s["object_class"] == "aut-num"])
         return total_obj, route_obj, autnum_obj
 
     def _generate_source_detail(self, database_handler: DatabaseHandler) -> str:
         """
         Generate status details for each database.
 
         This includes local configuration, local database status metadata,
         and serial information queried from the remote NRTM host,
         queried by _generate_remote_status_info().
         :param database_handler:
         """
-        result_txt = ''
+        result_txt = ""
         for status_result in self.status_results:
-            source = status_result['source'].upper()
-            keep_journal = 'Yes' if get_setting(f'sources.{source}.keep_journal') else 'No'
-            authoritative = 'Yes' if get_setting(f'sources.{source}.authoritative') else 'No'
-            object_class_filter = get_setting(f'sources.{source}.object_class_filter')
-            rpki_enabled = get_setting('rpki.roa_source') and not get_setting(f'sources.{source}.rpki_excluded')
-            rpki_enabled_str = 'Yes' if rpki_enabled else 'No'
-            scopefilter_enabled = get_setting('scopefilter') and not get_setting(f'sources.{source}.scopefilter_excluded')
-            scopefilter_enabled_str = 'Yes' if scopefilter_enabled else 'No'
-            synchronised_serials_str = 'Yes' if is_serial_synchronised(database_handler, source) else 'No'
+            source = status_result["source"].upper()
+            keep_journal = "Yes" if get_setting(f"sources.{source}.keep_journal") else "No"
+            authoritative = "Yes" if get_setting(f"sources.{source}.authoritative") else "No"
+            object_class_filter = get_setting(f"sources.{source}.object_class_filter")
+            rpki_enabled = get_setting("rpki.roa_source") and not get_setting(
+                f"sources.{source}.rpki_excluded"
+            )
+            rpki_enabled_str = "Yes" if rpki_enabled else "No"
+            scopefilter_enabled = get_setting("scopefilter") and not get_setting(
+                f"sources.{source}.scopefilter_excluded"
+            )
+            scopefilter_enabled_str = "Yes" if scopefilter_enabled else "No"
+            synchronised_serials_str = "Yes" if is_serial_synchronised(database_handler, source) else "No"
+            route_object_preference = get_setting(f"sources.{source}.route_object_preference")
 
-            nrtm_host = get_setting(f'sources.{source}.nrtm_host')
-            nrtm_port = int(get_setting(f'sources.{source}.nrtm_port', DEFAULT_SOURCE_NRTM_PORT))
+            nrtm_host = get_setting(f"sources.{source}.nrtm_host")
+            nrtm_port = int(get_setting(f"sources.{source}.nrtm_port", DEFAULT_SOURCE_NRTM_PORT))
 
             remote_information = self._generate_remote_status_info(nrtm_host, nrtm_port, source)
-            remote_information = textwrap.indent(remote_information, ' ' * 16)
+            remote_information = textwrap.indent(remote_information, " " * 16)
 
-            result_txt += textwrap.dedent(f"""
+            result_txt += textwrap.dedent(
+                f"""
             Status for {source}
             -------------------
             Local information:
                 Authoritative: {authoritative}
                 Object class filter: {object_class_filter}
                 Oldest serial seen: {status_result['serial_oldest_seen']}
                 Newest serial seen: {status_result['serial_newest_seen']}
@@ -129,47 +135,57 @@
                 Newest serial number mirrored: {status_result['serial_newest_mirror']}
                 Synchronised NRTM serials: {synchronised_serials_str}
                 Last update: {status_result['updated']}
                 Local journal kept: {keep_journal}
                 Last import error occurred at: {status_result['last_error_timestamp']}
                 RPKI validation enabled: {rpki_enabled_str}
                 Scope filter enabled: {scopefilter_enabled_str}
+                Route object preference: {route_object_preference}
 
             Remote information:{remote_information}
-            """)
+            """
+            )
         return result_txt
 
     def _generate_remote_status_info(self, nrtm_host: Optional[str], nrtm_port: int, source: str) -> str:
         """
         Determine the remote status.
 
         If NRTM is configured, this will include querying the NRTM
         source for serial information. Various error states will produce
         an appropriate remote status message for the report.
         """
         if nrtm_host:
             try:
                 source_status = whois_query_source_status(nrtm_host, nrtm_port, source)
                 mirrorable, mirror_serial_oldest, mirror_serial_newest, mirror_export_serial = source_status
-                mirrorable_str = 'Yes' if mirrorable else 'No'
+                mirrorable_str = "Yes" if mirrorable else "No"
 
-                return textwrap.dedent(f"""
+                return textwrap.dedent(
+                    f"""
                     NRTM host: {nrtm_host} port {nrtm_port}
                     Mirrorable: {mirrorable_str}
                     Oldest journal serial number: {mirror_serial_oldest}
                     Newest journal serial number: {mirror_serial_newest}
                     Last export at serial number: {mirror_export_serial}
-                    """)
+                    """
+                )
             except ValueError:
-                return textwrap.dedent(f"""
+                return textwrap.dedent(
+                    f"""
                     NRTM host: {nrtm_host} port {nrtm_port}
                     Remote status query unsupported or query failed
-                    """)
-            except (socket.timeout, ConnectionError):
-                return textwrap.dedent(f"""
+                    """
+                )
+            except OSError:
+                return textwrap.dedent(
+                    f"""
                     NRTM host: {nrtm_host} port {nrtm_port}
                     Unable to reach remote server for status query
-                    """)
+                    """
+                )
         else:
-            return textwrap.dedent("""
+            return textwrap.dedent(
+                """
                 No NRTM host configured.
-                """)
+                """
+            )
```

### Comparing `irrd-4.2.8/irrd/server/http/tests/test_endpoints.py` & `irrd-4.3.0/irrd/server/http/tests/test_endpoints.py`

 * *Files 27% similar despite different names*

```diff
@@ -3,189 +3,242 @@
 import ujson
 from starlette.requests import HTTPConnection
 from starlette.testclient import TestClient
 
 from irrd.storage.database_handler import DatabaseHandler
 from irrd.storage.preload import Preloader
 from irrd.updates.handler import ChangeSubmissionHandler
-from irrd.utils.validators import RPSLChangeSubmission
+from irrd.utils.validators import RPSLChangeSubmission, RPSLSuspensionSubmission
+
+from ...whois.query_parser import WhoisQueryParser
+from ...whois.query_response import (
+    WhoisQueryResponse,
+    WhoisQueryResponseMode,
+    WhoisQueryResponseType,
+)
 from ..app import app
 from ..endpoints import StatusEndpoint, WhoisQueryEndpoint
 from ..status_generator import StatusGenerator
-from ...whois.query_parser import WhoisQueryParser
-from ...whois.query_response import WhoisQueryResponse, WhoisQueryResponseType, \
-    WhoisQueryResponseMode
 
 
 class TestStatusEndpoint:
     def setup_method(self):
-        self.mock_request = HTTPConnection({
-            'type': 'http',
-            'client': ('127.0.0.1', '8000'),
-        })
+        self.mock_request = HTTPConnection(
+            {
+                "type": "http",
+                "client": ("127.0.0.1", "8000"),
+            }
+        )
         self.endpoint = StatusEndpoint(scope=self.mock_request, receive=None, send=None)
 
     def test_status_no_access_list(self):
         response = self.endpoint.get(self.mock_request)
         assert response.status_code == 403
-        assert response.body == b'Access denied'
+        assert response.body == b"Access denied"
 
     def test_status_access_list_permitted(self, config_override, monkeypatch):
-        config_override({
-            'server': {
-                'http': {
-                    'status_access_list': 'test_access_list',
-                }
-            },
-            'access_lists': {
-                'test_access_list': {
-                    '127.0.0.0/25',
-                }
-            },
-        })
+        config_override(
+            {
+                "server": {
+                    "http": {
+                        "status_access_list": "test_access_list",
+                    }
+                },
+                "access_lists": {
+                    "test_access_list": {
+                        "127.0.0.0/25",
+                    }
+                },
+            }
+        )
 
         mock_database_status_generator = Mock(spec=StatusGenerator)
-        monkeypatch.setattr('irrd.server.http.endpoints.StatusGenerator',
-                            lambda: mock_database_status_generator)
-        mock_database_status_generator.generate_status = lambda: 'status'
+        monkeypatch.setattr(
+            "irrd.server.http.endpoints.StatusGenerator", lambda: mock_database_status_generator
+        )
+        mock_database_status_generator.generate_status = lambda: "status"
 
         response = self.endpoint.get(self.mock_request)
         assert response.status_code == 200
-        assert response.body == b'status'
+        assert response.body == b"status"
 
     def test_status_access_list_denied(self, config_override):
-        config_override({
-            'server': {
-                'http': {
-                    'status_access_list': 'test_access_list',
-                }
-            },
-            'access_lists': {
-                'test_access_list': {
-                    '192.0.2.0/25',
-                }
-            },
-        })
+        config_override(
+            {
+                "server": {
+                    "http": {
+                        "status_access_list": "test_access_list",
+                    }
+                },
+                "access_lists": {
+                    "test_access_list": {
+                        "192.0.2.0/25",
+                    }
+                },
+            }
+        )
         response = self.endpoint.get(self.mock_request)
         assert response.status_code == 403
-        assert response.body == b'Access denied'
+        assert response.body == b"Access denied"
 
 
 class TestWhoisQueryEndpoint:
     def test_query_endpoint(self, monkeypatch):
         mock_query_parser = Mock(spec=WhoisQueryParser)
-        monkeypatch.setattr('irrd.server.http.endpoints.WhoisQueryParser',
-                            lambda client_ip, client_str, preloader, database_handler: mock_query_parser)
-        app = Mock(state=Mock(
-            database_handler=Mock(spec=DatabaseHandler),
-            preloader=Mock(spec=Preloader),
-        ))
-        mock_request = HTTPConnection({
-            'type': 'http',
-            'client': ('127.0.0.1', '8000'),
-            'app': app,
-            'query_string': '',
-        })
+        monkeypatch.setattr(
+            "irrd.server.http.endpoints.WhoisQueryParser",
+            lambda client_ip, client_str, preloader, database_handler: mock_query_parser,
+        )
+        app = Mock(
+            state=Mock(
+                database_handler=Mock(spec=DatabaseHandler),
+                preloader=Mock(spec=Preloader),
+            )
+        )
+        mock_request = HTTPConnection(
+            {
+                "type": "http",
+                "client": ("127.0.0.1", "8000"),
+                "app": app,
+                "query_string": "",
+            }
+        )
         endpoint = WhoisQueryEndpoint(scope=mock_request, receive=None, send=None)
 
         result = endpoint.get(mock_request)
         assert result.status_code == 400
-        assert result.body.startswith(b'Missing required query')
+        assert result.body.startswith(b"Missing required query")
 
-        mock_request = HTTPConnection({
-            'type': 'http',
-            'client': ('127.0.0.1', '8000'),
-            'app': app,
-            'query_string': 'q=query',
-        })
+        mock_request = HTTPConnection(
+            {
+                "type": "http",
+                "client": ("127.0.0.1", "8000"),
+                "app": app,
+                "query_string": "q=query",
+            }
+        )
 
         mock_query_parser.handle_query = lambda query: WhoisQueryResponse(
             response_type=WhoisQueryResponseType.SUCCESS,
             mode=WhoisQueryResponseMode.IRRD,  # irrelevant
-            result=f'result {query} 🦄'
+            result=f"result {query} 🦄",
         )
         result = endpoint.get(mock_request)
         assert result.status_code == 200
-        assert result.body.decode('utf-8') == 'result query 🦄'
+        assert result.body.decode("utf-8") == "result query 🦄"
 
         mock_query_parser.handle_query = lambda query: WhoisQueryResponse(
             response_type=WhoisQueryResponseType.KEY_NOT_FOUND,
             mode=WhoisQueryResponseMode.IRRD,  # irrelevant
-            result='',
+            result="",
         )
         result = endpoint.get(mock_request)
         assert result.status_code == 204
         assert not result.body
 
         mock_query_parser.handle_query = lambda query: WhoisQueryResponse(
             response_type=WhoisQueryResponseType.ERROR_USER,
             mode=WhoisQueryResponseMode.IRRD,  # irrelevant
-            result=f'result {query} 🦄'
+            result=f"result {query} 🦄",
         )
         result = endpoint.get(mock_request)
         assert result.status_code == 400
-        assert result.body.decode('utf-8') == 'result query 🦄'
+        assert result.body.decode("utf-8") == "result query 🦄"
 
         mock_query_parser.handle_query = lambda query: WhoisQueryResponse(
             response_type=WhoisQueryResponseType.ERROR_INTERNAL,
             mode=WhoisQueryResponseMode.IRRD,  # irrelevant
-            result=f'result {query} 🦄'
+            result=f"result {query} 🦄",
         )
         result = endpoint.get(mock_request)
         assert result.status_code == 500
-        assert result.body.decode('utf-8') == 'result query 🦄'
+        assert result.body.decode("utf-8") == "result query 🦄"
 
 
 class TestObjectSubmissionEndpoint:
     def test_endpoint(self, monkeypatch):
         mock_handler = Mock(spec=ChangeSubmissionHandler)
-        monkeypatch.setattr('irrd.server.http.endpoints.ChangeSubmissionHandler',
-                            lambda: mock_handler)
-        mock_handler.submitter_report_json = lambda: {'response': True}
+        monkeypatch.setattr("irrd.server.http.endpoints.ChangeSubmissionHandler", lambda: mock_handler)
+        mock_handler.submitter_report_json = lambda: {"response": True}
 
         client = TestClient(app)
         data = {
-            'objects': [
-                {'attributes': [
-                    {'name': 'person', 'value': 'Placeholder Person Object'},
-                    {'name': 'nic-hdl', 'value': 'PERSON-TEST'},
-                    {'name': 'changed', 'value': 'changed@example.com 20190701 # comment'},
-                    {'name': 'source', 'value': 'TEST'},
-                ]},
+            "objects": [
+                {
+                    "attributes": [
+                        {"name": "person", "value": "Placeholder Person Object"},
+                        {"name": "nic-hdl", "value": "PERSON-TEST"},
+                        {"name": "changed", "value": "changed@example.com 20190701 # comment"},
+                        {"name": "source", "value": "TEST"},
+                    ]
+                },
             ],
-            'passwords': ['invalid1', 'invalid2'],
+            "passwords": ["invalid1", "invalid2"],
         }
         expected_data = RPSLChangeSubmission.parse_obj(data)
 
-        response_post = client.post('/v1/submit/', data=ujson.dumps(data))
+        response_post = client.post(
+            "/v1/submit/", data=ujson.dumps(data), headers={"X-irrd-metadata": '{"meta": 2}'}
+        )
         assert response_post.status_code == 200
         assert response_post.text == '{"response":true}'
         mock_handler.load_change_submission.assert_called_once_with(
             data=expected_data,
             delete=False,
-            request_meta={'HTTP-client-IP': 'testclient', 'HTTP-User-Agent': 'testclient'},
+            request_meta={"HTTP-client-IP": "testclient", "HTTP-User-Agent": "testclient", "meta": 2},
         )
         mock_handler.send_notification_target_reports.assert_called_once()
         mock_handler.reset_mock()
 
-        response_delete = client.delete('/v1/submit/', data=ujson.dumps(data))
+        response_delete = client.delete("/v1/submit/", data=ujson.dumps(data))
         assert response_delete.status_code == 200
         assert response_delete.text == '{"response":true}'
         mock_handler.load_change_submission.assert_called_once_with(
             data=expected_data,
             delete=True,
-            request_meta={'HTTP-client-IP': 'testclient', 'HTTP-User-Agent': 'testclient'},
+            request_meta={"HTTP-client-IP": "testclient", "HTTP-User-Agent": "testclient"},
         )
         mock_handler.send_notification_target_reports.assert_called_once()
         mock_handler.reset_mock()
 
-        response_invalid_format = client.post('/v1/submit/', data='{"invalid": true}')
+        response_invalid_format = client.post("/v1/submit/", data='{"invalid": true}')
         assert response_invalid_format.status_code == 400
-        assert 'field required' in response_invalid_format.text
+        assert "field required" in response_invalid_format.text
         mock_handler.load_change_submission.assert_not_called()
         mock_handler.send_notification_target_reports.assert_not_called()
 
-        response_invalid_json = client.post('/v1/submit/', data='invalid')
+        response_invalid_json = client.post("/v1/submit/", data="invalid")
         assert response_invalid_json.status_code == 400
-        assert 'expect' in response_invalid_json.text.lower()
+        assert "expect" in response_invalid_json.text.lower()
         mock_handler.load_change_submission.assert_not_called()
         mock_handler.send_notification_target_reports.assert_not_called()
+
+
+class TestSuspensionSubmissionEndpoint:
+    def test_endpoint(self, monkeypatch):
+        mock_handler = Mock(spec=ChangeSubmissionHandler)
+        monkeypatch.setattr("irrd.server.http.endpoints.ChangeSubmissionHandler", lambda: mock_handler)
+        mock_handler.submitter_report_json = lambda: {"response": True}
+
+        client = TestClient(app)
+        data = {
+            "objects": [{"mntner": "DASHCARE-MNT", "source": "DASHCARE", "request_type": "reactivate"}],
+            "override": "<>",
+        }
+        expected_data = RPSLSuspensionSubmission.parse_obj(data)
+
+        response_post = client.post("/v1/suspension/", data=ujson.dumps(data))
+        assert response_post.status_code == 200
+        assert response_post.text == '{"response":true}'
+        mock_handler.load_suspension_submission.assert_called_once_with(
+            data=expected_data,
+            request_meta={"HTTP-client-IP": "testclient", "HTTP-User-Agent": "testclient"},
+        )
+        mock_handler.reset_mock()
+
+        response_invalid_format = client.post("/v1/suspension/", data='{"invalid": true}')
+        assert response_invalid_format.status_code == 400
+        assert "field required" in response_invalid_format.text
+
+        response_invalid_json = client.post("/v1/suspension/", data="invalid")
+        assert response_invalid_json.status_code == 400
+        assert "expect" in response_invalid_json.text.lower()
```

### Comparing `irrd-4.2.8/irrd/server/http/tests/test_status_generator.py` & `irrd-4.3.0/irrd/server/http/tests/test_status_generator.py`

 * *Files 19% similar despite different names*

```diff
@@ -3,133 +3,144 @@
 import socket
 import textwrap
 from datetime import datetime, timezone
 from unittest.mock import Mock
 
 from irrd import __version__
 from irrd.conf import get_setting
+
 from ..status_generator import StatusGenerator
 
 
 class TestStatusGenerator:
-
     def test_request(self, monkeypatch, config_override):
         mock_database_handler = Mock()
-        monkeypatch.setattr('irrd.server.http.status_generator.DatabaseHandler', lambda: mock_database_handler)
+        monkeypatch.setattr(
+            "irrd.server.http.status_generator.DatabaseHandler", lambda: mock_database_handler
+        )
         mock_status_query = Mock()
-        monkeypatch.setattr('irrd.server.http.status_generator.DatabaseStatusQuery', lambda: mock_status_query)
-        monkeypatch.setattr('irrd.server.http.status_generator.is_serial_synchronised',
-                            lambda dh, source: False)
+        monkeypatch.setattr(
+            "irrd.server.http.status_generator.DatabaseStatusQuery", lambda: mock_status_query
+        )
+        monkeypatch.setattr(
+            "irrd.server.http.status_generator.is_serial_synchronised", lambda dh, source: False
+        )
         mock_statistics_query = Mock()
-        monkeypatch.setattr('irrd.server.http.status_generator.RPSLDatabaseObjectStatisticsQuery',
-                            lambda: mock_statistics_query)
+        monkeypatch.setattr(
+            "irrd.server.http.status_generator.RPSLDatabaseObjectStatisticsQuery",
+            lambda: mock_statistics_query,
+        )
 
         def mock_whois_query(nrtm_host, nrtm_port, source):
-            assert source in ['TEST1', 'TEST2', 'TEST3']
-            if source == 'TEST1':
-                assert nrtm_host == 'nrtm1.example.com'
+            assert source in ["TEST1", "TEST2", "TEST3"]
+            if source == "TEST1":
+                assert nrtm_host == "nrtm1.example.com"
                 assert nrtm_port == 43
                 return True, 142, 143, 144
-            elif source == 'TEST2':
+            elif source == "TEST2":
                 raise ValueError()
-            elif source == 'TEST3':
+            elif source == "TEST3":
                 raise socket.timeout()
 
-        monkeypatch.setattr('irrd.server.http.status_generator.whois_query_source_status', mock_whois_query)
+        monkeypatch.setattr("irrd.server.http.status_generator.whois_query_source_status", mock_whois_query)
 
-        config_override({
-            'sources': {
-                'rpki': {
-                    'roa_source': 'roa source'
-                },
-                'TEST1': {
-                    'authoritative': False,
-                    'keep_journal': True,
-                    'nrtm_host': 'nrtm1.example.com',
-                    'nrtm_port': 43,
-                    'object_class_filter': 'object-class-filter',
-                    'rpki_excluded': True,
-                },
-                'TEST2': {
-                    'authoritative': True,
-                    'keep_journal': False,
-                    'nrtm_host': 'nrtm2.example.com',
-                    'nrtm_port': 44,
-                },
-                'TEST3': {
-                    'authoritative': True,
-                    'keep_journal': False,
-                    'nrtm_host': 'nrtm3.example.com',
-                    'nrtm_port': 45,
-                },
-                'TEST4': {
-                    'authoritative': False,
-                    'keep_journal': False,
-                },
+        config_override(
+            {
+                "sources": {
+                    "rpki": {"roa_source": "roa source"},
+                    "TEST1": {
+                        "authoritative": False,
+                        "keep_journal": True,
+                        "nrtm_host": "nrtm1.example.com",
+                        "nrtm_port": 43,
+                        "object_class_filter": "object-class-filter",
+                        "rpki_excluded": True,
+                        "route_object_preference": 200,
+                    },
+                    "TEST2": {
+                        "authoritative": True,
+                        "keep_journal": False,
+                        "nrtm_host": "nrtm2.example.com",
+                        "nrtm_port": 44,
+                    },
+                    "TEST3": {
+                        "authoritative": True,
+                        "keep_journal": False,
+                        "nrtm_host": "nrtm3.example.com",
+                        "nrtm_port": 45,
+                    },
+                    "TEST4": {
+                        "authoritative": False,
+                        "keep_journal": False,
+                    },
+                }
             }
-        })
+        )
 
-        mock_query_result = iter([
-            [
-                {'source': 'TEST1', 'object_class': 'route', 'count': 10},
-                {'source': 'TEST1', 'object_class': 'aut-num', 'count': 10},
-                {'source': 'TEST1', 'object_class': 'other', 'count': 5},
-                {'source': 'TEST2', 'object_class': 'route', 'count': 42},
-            ],
+        mock_query_result = iter(
             [
-                {
-                    'source': 'TEST1',
-                    'serial_oldest_seen': 10,
-                    'serial_newest_seen': 21,
-                    'serial_oldest_journal': 15,
-                    'serial_newest_journal': 20,
-                    'serial_last_export': 16,
-                    'serial_newest_mirror': 25,
-                    'last_error_timestamp': datetime(2018, 1, 1, tzinfo=timezone.utc),
-                    'updated': datetime(2018, 6, 1, tzinfo=timezone.utc),
-                },
-                {
-                    'source': 'TEST2',
-                    'serial_oldest_seen': 210,
-                    'serial_newest_seen': 221,
-                    'serial_oldest_journal': None,
-                    'serial_newest_journal': None,
-                    'serial_last_export': None,
-                    'serial_newest_mirror': None,
-                    'last_error_timestamp': datetime(2019, 1, 1, tzinfo=timezone.utc),
-                    'updated': datetime(2019, 6, 1, tzinfo=timezone.utc),
-                },
-                {
-                    'source': 'TEST3',
-                    'serial_oldest_seen': None,
-                    'serial_newest_seen': None,
-                    'serial_oldest_journal': None,
-                    'serial_newest_journal': None,
-                    'serial_last_export': None,
-                    'serial_newest_mirror': None,
-                    'last_error_timestamp': None,
-                    'updated': None,
-                },
-                {
-                    'source': 'TEST4',
-                    'serial_oldest_seen': None,
-                    'serial_newest_seen': None,
-                    'serial_oldest_journal': None,
-                    'serial_newest_journal': None,
-                    'serial_last_export': None,
-                    'serial_newest_mirror': None,
-                    'last_error_timestamp': None,
-                    'updated': None,
-                },
-            ],
-        ])
+                [
+                    {"source": "TEST1", "object_class": "route", "count": 10},
+                    {"source": "TEST1", "object_class": "aut-num", "count": 10},
+                    {"source": "TEST1", "object_class": "other", "count": 5},
+                    {"source": "TEST2", "object_class": "route", "count": 42},
+                ],
+                [
+                    {
+                        "source": "TEST1",
+                        "serial_oldest_seen": 10,
+                        "serial_newest_seen": 21,
+                        "serial_oldest_journal": 15,
+                        "serial_newest_journal": 20,
+                        "serial_last_export": 16,
+                        "serial_newest_mirror": 25,
+                        "last_error_timestamp": datetime(2018, 1, 1, tzinfo=timezone.utc),
+                        "updated": datetime(2018, 6, 1, tzinfo=timezone.utc),
+                    },
+                    {
+                        "source": "TEST2",
+                        "serial_oldest_seen": 210,
+                        "serial_newest_seen": 221,
+                        "serial_oldest_journal": None,
+                        "serial_newest_journal": None,
+                        "serial_last_export": None,
+                        "serial_newest_mirror": None,
+                        "last_error_timestamp": datetime(2019, 1, 1, tzinfo=timezone.utc),
+                        "updated": datetime(2019, 6, 1, tzinfo=timezone.utc),
+                    },
+                    {
+                        "source": "TEST3",
+                        "serial_oldest_seen": None,
+                        "serial_newest_seen": None,
+                        "serial_oldest_journal": None,
+                        "serial_newest_journal": None,
+                        "serial_last_export": None,
+                        "serial_newest_mirror": None,
+                        "last_error_timestamp": None,
+                        "updated": None,
+                    },
+                    {
+                        "source": "TEST4",
+                        "serial_oldest_seen": None,
+                        "serial_newest_seen": None,
+                        "serial_oldest_journal": None,
+                        "serial_newest_journal": None,
+                        "serial_last_export": None,
+                        "serial_newest_mirror": None,
+                        "last_error_timestamp": None,
+                        "updated": None,
+                    },
+                ],
+            ]
+        )
         mock_database_handler.execute_query = lambda query, flush_rpsl_buffer=True: next(mock_query_result)
 
         status_report = StatusGenerator().generate_status()
-        expected_report = textwrap.dedent(f"""
+        expected_report = textwrap.dedent(
+            f"""
             IRRD version {__version__}
             Listening on ::0 port {get_setting('server.whois.port')}
             
             
             -----------------------------------------------------------------------
              source    total obj    rt obj    aut-num obj    serial    last export 
             -----------------------------------------------------------------------
@@ -153,14 +164,15 @@
                 Newest serial number mirrored: 25
                 Synchronised NRTM serials: No
                 Last update: 2018-06-01 00:00:00+00:00
                 Local journal kept: Yes
                 Last import error occurred at: 2018-01-01 00:00:00+00:00
                 RPKI validation enabled: No
                 Scope filter enabled: No
+                Route object preference: 200
             
             Remote information:
                 NRTM host: nrtm1.example.com port 43
                 Mirrorable: Yes
                 Oldest journal serial number: 142
                 Newest journal serial number: 143
                 Last export at serial number: 144
@@ -179,15 +191,16 @@
                 Newest serial number mirrored: None
                 Synchronised NRTM serials: No
                 Last update: 2019-06-01 00:00:00+00:00
                 Local journal kept: No
                 Last import error occurred at: 2019-01-01 00:00:00+00:00
                 RPKI validation enabled: Yes
                 Scope filter enabled: No
-            
+                Route object preference: None
+
             Remote information:
                 NRTM host: nrtm2.example.com port 44
                 Remote status query unsupported or query failed
             
             
             Status for TEST3
             -------------------
@@ -202,14 +215,15 @@
                 Newest serial number mirrored: None
                 Synchronised NRTM serials: No
                 Last update: None
                 Local journal kept: No
                 Last import error occurred at: None
                 RPKI validation enabled: Yes
                 Scope filter enabled: No
+                Route object preference: None
             
             Remote information:
                 NRTM host: nrtm3.example.com port 45
                 Unable to reach remote server for status query
             
             
             Status for TEST4
@@ -225,12 +239,14 @@
                 Newest serial number mirrored: None
                 Synchronised NRTM serials: No
                 Last update: None
                 Local journal kept: No
                 Last import error occurred at: None
                 RPKI validation enabled: Yes
                 Scope filter enabled: No
+                Route object preference: None
             
             Remote information:
-                No NRTM host configured.\n\n""").lstrip()
+                No NRTM host configured.\n\n"""
+        ).lstrip()
 
         assert expected_report == status_report
```

### Comparing `irrd-4.2.8/irrd/server/query_resolver.py` & `irrd-4.3.0/irrd/server/query_resolver.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,80 +1,89 @@
 import logging
 from collections import OrderedDict
 from enum import Enum
-from typing import Optional, List, Set, Tuple, Any, Dict
+from typing import Any, Dict, List, Optional, Set, Tuple
 
 from IPy import IP
 from pytz import timezone
 
-from irrd.conf import get_setting, RPKI_IRR_PSEUDO_SOURCE
+from irrd.conf import RPKI_IRR_PSEUDO_SOURCE, get_setting
+from irrd.routepref.status import RoutePreferenceStatus
 from irrd.rpki.status import RPKIStatus
-from irrd.rpsl.rpsl_objects import (OBJECT_CLASS_MAPPING, lookup_field_names)
+from irrd.rpsl.rpsl_objects import OBJECT_CLASS_MAPPING, lookup_field_names
 from irrd.scopefilter.status import ScopeFilterStatus
-from irrd.storage.database_handler import DatabaseHandler, is_serial_synchronised, \
-    RPSLDatabaseResponse
+from irrd.storage.database_handler import (
+    DatabaseHandler,
+    RPSLDatabaseResponse,
+    is_serial_synchronised,
+)
 from irrd.storage.preload import Preloader
-from irrd.storage.queries import RPSLDatabaseQuery, DatabaseStatusQuery
+from irrd.storage.queries import DatabaseStatusQuery, RPSLDatabaseQuery
 from irrd.utils.validators import parse_as_number
 
 logger = logging.getLogger(__name__)
 
 
 class InvalidQueryException(ValueError):
     pass
 
 
 class RouteLookupType(Enum):
-    EXACT = 'EXACT'
-    LESS_SPECIFIC_ONE_LEVEL = 'LESS_SPECIFIC_ONE_LEVEL'
-    LESS_SPECIFIC_WITH_EXACT = 'LESS_SPECIFIC_WITH_EXACT'
-    MORE_SPECIFIC_WITHOUT_EXACT = 'MORE_SPECIFIC_WITHOUT_EXACT'
+    EXACT = "EXACT"
+    LESS_SPECIFIC_ONE_LEVEL = "LESS_SPECIFIC_ONE_LEVEL"
+    LESS_SPECIFIC_WITH_EXACT = "LESS_SPECIFIC_WITH_EXACT"
+    MORE_SPECIFIC_WITHOUT_EXACT = "MORE_SPECIFIC_WITHOUT_EXACT"
 
 
 class QueryResolver:
     """
     Resolver for all RPSL queries.
 
     Some aspects like setting sources retain state, so a single instance
     should not be shared across unrelated query sessions.
     """
+
     lookup_field_names = lookup_field_names()
     database_handler: DatabaseHandler
     _current_set_root_object_class: Optional[str]
 
     def __init__(self, preloader: Preloader, database_handler: DatabaseHandler) -> None:
-        self.all_valid_sources = list(get_setting('sources', {}).keys())
-        self.sources_default = list(get_setting('sources_default', []))
+        self.all_valid_sources = list(get_setting("sources", {}).keys())
+        self.sources_default = list(get_setting("sources_default", []))
         self.sources: List[str] = self.sources_default if self.sources_default else self.all_valid_sources
-        if get_setting('rpki.roa_source'):
+        if get_setting("rpki.roa_source"):
             self.all_valid_sources.append(RPKI_IRR_PSEUDO_SOURCE)
         self.object_class_filter: List[str] = []
-        self.rpki_aware = bool(get_setting('rpki.roa_source'))
+        self.rpki_aware = bool(get_setting("rpki.roa_source"))
         self.rpki_invalid_filter_enabled = self.rpki_aware
         self.out_scope_filter_enabled = True
+        self.route_preference_filter_enabled = True
         self.user_agent: Optional[str] = None
         self.preloader = preloader
         self.database_handler = database_handler
         self.sql_queries: List[str] = []
         self.sql_trace = False
 
     def set_query_sources(self, sources: Optional[List[str]]) -> None:
         """Set the sources for future queries. If sources is None, default source list is set."""
         if sources is None:
             sources = self.sources_default if self.sources_default else self.all_valid_sources
         elif not all([source in self.all_valid_sources for source in sources]):
-            raise InvalidQueryException('One or more selected sources are unavailable.')
+            raise InvalidQueryException("One or more selected sources are unavailable.")
         self.sources = sources
 
     def disable_rpki_filter(self) -> None:
         self.rpki_invalid_filter_enabled = False
 
     def disable_out_of_scope_filter(self) -> None:
         self.out_scope_filter_enabled = False
 
+    def disable_route_preference_filter(self) -> None:
+        self.route_preference_filter_enabled = False
+
     def set_object_class_filter_next_query(self, object_classes: List[str]) -> None:
         """Restrict object classes for the next query, comma-seperated"""
         self.object_class_filter = object_classes
 
     def key_lookup(self, object_class: str, rpsl_pk: str) -> RPSLDatabaseResponse:
         """RPSL exact key lookup."""
         query = self._prepare_query().object_classes([object_class]).rpsl_pk(rpsl_pk).first_only()
@@ -82,15 +91,15 @@
 
     def rpsl_text_search(self, value: str) -> RPSLDatabaseResponse:
         query = self._prepare_query(ordered_by_sources=False).text_search(value)
         return self._execute_query(query)
 
     def route_search(self, address: IP, lookup_type: RouteLookupType):
         """Route(6) object search for an address, supporting exact/less/more specific."""
-        query = self._prepare_query(ordered_by_sources=False).object_classes(['route', 'route6'])
+        query = self._prepare_query(ordered_by_sources=False).object_classes(["route", "route6"])
         lookup_queries = {
             RouteLookupType.EXACT: query.ip_exact,
             RouteLookupType.LESS_SPECIFIC_ONE_LEVEL: query.ip_less_specific_one_level,
             RouteLookupType.LESS_SPECIFIC_WITH_EXACT: query.ip_less_specific,
             RouteLookupType.MORE_SPECIFIC_WITHOUT_EXACT: query.ip_more_specific,
         }
         query = lookup_queries[lookup_type](address)
@@ -99,64 +108,77 @@
     def rpsl_attribute_search(self, attribute: str, value: str) -> RPSLDatabaseResponse:
         """
         -i/!o query - inverse search for attribute values
         e.g. `-i mnt-by FOO` finds all objects where (one of the) maintainer(s) is FOO,
         as does `!oFOO`. Restricted to designated lookup fields.
         """
         if attribute not in self.lookup_field_names:
-            readable_lookup_field_names = ', '.join(self.lookup_field_names)
-            msg = (f'Inverse attribute search not supported for {attribute},' +
-                   f'only supported for attributes: {readable_lookup_field_names}')
+            readable_lookup_field_names = ", ".join(self.lookup_field_names)
+            msg = (
+                f"Inverse attribute search not supported for {attribute},"
+                + f"only supported for attributes: {readable_lookup_field_names}"
+            )
             raise InvalidQueryException(msg)
         query = self._prepare_query(ordered_by_sources=False).lookup_attr(attribute, value)
         return self._execute_query(query)
 
-    def routes_for_origin(self, origin: str, ip_version: Optional[int]=None) -> Set[str]:
+    def routes_for_origin(self, origin: str, ip_version: Optional[int] = None) -> Set[str]:
         """
         Resolve all route(6)s prefixes for an origin, returning a set
         of all prefixes. Origin must be in 'ASxxx' format.
         """
         prefixes = self.preloader.routes_for_origins([origin], self.sources, ip_version=ip_version)
         return prefixes
 
-    def routes_for_as_set(self, set_name: str, ip_version: Optional[int]=None, exclude_sets: Set[str]=None) -> Set[str]:
+    def routes_for_as_set(
+        self, set_name: str, ip_version: Optional[int] = None, exclude_sets: Optional[Set[str]] = None
+    ) -> Set[str]:
         """
         Find all originating prefixes for all members of an AS-set. May be restricted
         to IPv4 or IPv6. Returns a set of all prefixes.
         """
-        self._current_set_root_object_class = 'as-set'
+        self._current_set_root_object_class = "as-set"
         self._current_excluded_sets = exclude_sets if exclude_sets else set()
         self._current_set_maximum_depth = 0
         members = self._recursive_set_resolve({set_name})
         return self.preloader.routes_for_origins(members, self.sources, ip_version=ip_version)
 
-    def members_for_set_per_source(self, parameter: str, exclude_sets: Set[str]=None, depth=0, recursive=False) -> Dict[str, List[str]]:
+    def members_for_set_per_source(
+        self, parameter: str, exclude_sets: Optional[Set[str]] = None, depth=0, recursive=False
+    ) -> Dict[str, List[str]]:
         """
         Find all members of an as-set or route-set, possibly recursively, distinguishing
         between multiple root objects in different sources with the same name.
         Returns a dict with sources as keys, list of all members, including leaf members,
         as values.
         """
-        query = self._prepare_query(column_names=['source'])
-        object_classes = ['as-set', 'route-set']
+        query = self._prepare_query(column_names=["source"])
+        object_classes = ["as-set", "route-set"]
         query = query.object_classes(object_classes).rpsl_pk(parameter)
-        set_sources = [row['source'] for row in self._execute_query(query)]
+        set_sources = [row["source"] for row in self._execute_query(query)]
 
         return {
             source: self.members_for_set(
                 parameter=parameter,
                 exclude_sets=exclude_sets,
                 depth=depth,
                 recursive=recursive,
                 root_source=source,
             )
             for source in set_sources
         }
 
-    def members_for_set(self, parameter: str, exclude_sets: Set[str]=None, depth=0, recursive=False, root_source: Optional[str]=None) -> List[str]:
+    def members_for_set(
+        self,
+        parameter: str,
+        exclude_sets: Optional[Set[str]] = None,
+        depth=0,
+        recursive=False,
+        root_source: Optional[str] = None,
+    ) -> List[str]:
         """
         Find all members of an as-set or route-set, possibly recursively.
         Returns a list of all members, including leaf members.
         If root_source is set, the root object is only looked for in that source -
         resolving is then continued using the currently set sources.
         """
         self._current_set_root_object_class = None
@@ -166,15 +188,15 @@
             members, leaf_members = self._find_set_members({parameter}, limit_source=root_source)
             members.update(leaf_members)
         else:
             members = self._recursive_set_resolve({parameter}, root_source=root_source)
         if parameter in members:
             members.remove(parameter)
 
-        if get_setting('compatibility.ipv4_only_route_set_members'):
+        if get_setting("compatibility.ipv4_only_route_set_members"):
             original_members = set(members)
             for member in original_members:
                 try:
                     IP(member)
                 except ValueError:
                     continue  # This is not a prefix, ignore.
                 try:
@@ -182,15 +204,17 @@
                 except ValueError:
                     # This was a valid prefix, but not a valid IPv4 prefix,
                     # and should be removed.
                     members.remove(member)
 
         return sorted(members)
 
-    def _recursive_set_resolve(self, members: Set[str], sets_seen=None, root_source: Optional[str]=None) -> Set[str]:
+    def _recursive_set_resolve(
+        self, members: Set[str], sets_seen=None, root_source: Optional[str] = None
+    ) -> Set[str]:
         """
         Resolve all members of a number of sets, recursively.
 
         For each set in members, determines whether it has been seen already (to prevent
         infinite recursion), ignores it if already seen, and then either adds
         it directly or adds it to a set that requires further resolving.
         If root_source is set, the root object is only looked for in that source -
@@ -205,47 +229,53 @@
 
         set_members = set()
 
         resolved_as_members = set()
         sub_members, leaf_members = self._find_set_members(members, limit_source=root_source)
 
         for sub_member in sub_members:
-            if self._current_set_root_object_class is None or self._current_set_root_object_class == 'route-set':
+            if (
+                self._current_set_root_object_class is None
+                or self._current_set_root_object_class == "route-set"
+            ):
                 try:
-                    IP(sub_member.split('^')[0])
+                    IP(sub_member.split("^")[0])
                     set_members.add(sub_member)
                     continue
                 except ValueError:
                     pass
             # AS numbers are permitted in route-sets and as-sets, per RFC 2622 5.3.
             # When an AS number is encountered as part of route-set resolving,
             # the prefixes originating from that AS should be added to the response.
             try:
                 as_number_formatted, _ = parse_as_number(sub_member)
-                if self._current_set_root_object_class == 'route-set':
-                    set_members.update(self.preloader.routes_for_origins(
-                        [as_number_formatted], self.sources))
+                if self._current_set_root_object_class == "route-set":
+                    set_members.update(self.preloader.routes_for_origins([as_number_formatted], self.sources))
                     resolved_as_members.add(sub_member)
                 else:
                     set_members.add(sub_member)
                 continue
             except ValueError:
                 pass
 
         self._current_set_maximum_depth -= 1
         if self._current_set_maximum_depth == 0:
             return set_members | sub_members | leaf_members
 
-        further_resolving_required = sub_members - set_members - sets_seen - resolved_as_members - self._current_excluded_sets
+        further_resolving_required = (
+            sub_members - set_members - sets_seen - resolved_as_members - self._current_excluded_sets
+        )
         new_members = self._recursive_set_resolve(further_resolving_required, sets_seen)
         set_members.update(new_members)
 
         return set_members
 
-    def _find_set_members(self, set_names: Set[str], limit_source: Optional[str]=None) -> Tuple[Set[str], Set[str]]:
+    def _find_set_members(
+        self, set_names: Set[str], limit_source: Optional[str] = None
+    ) -> Tuple[Set[str], Set[str]]:
         """
         Find all members of a number of route-sets or as-sets. Includes both
         direct members listed in members attribute, but also
         members included by mbrs-by-ref/member-of.
         If limit_source is set, the set_names are only looked for in that source.
 
         Returns a tuple of two sets:
@@ -254,21 +284,21 @@
         - leaf members that were included in set_names, i.e.
           names for which no further data could be found - for
           example references to non-existent other sets
         """
         members: Set[str] = set()
         sets_already_resolved: Set[str] = set()
 
-        columns = ['parsed_data', 'rpsl_pk', 'source', 'object_class']
+        columns = ["parsed_data", "rpsl_pk", "source", "object_class"]
         query = self._prepare_query(column_names=columns)
 
-        object_classes = ['as-set', 'route-set']
+        object_classes = ["as-set", "route-set"]
         # Per RFC 2622 5.3, route-sets can refer to as-sets,
         # but as-sets can only refer to other as-sets.
-        if self._current_set_root_object_class == 'as-set':
+        if self._current_set_root_object_class == "as-set":
             object_classes = [self._current_set_root_object_class]
 
         query = query.object_classes(object_classes).rpsl_pks(set_names)
         if limit_source:
             query = query.sources([limit_source])
         query_result = list(self._execute_query(query))
 
@@ -277,91 +307,100 @@
             return set(), set_names
 
         # Track the object class of the root object set.
         # In one case self._current_set_root_object_class may already be set
         # on the first run: when the set resolving should be fixed to one
         # type of set object.
         if not self._current_set_root_object_class:
-            self._current_set_root_object_class = query_result[0]['object_class']
+            self._current_set_root_object_class = query_result[0]["object_class"]
 
         for result in query_result:
-            rpsl_pk = result['rpsl_pk']
+            rpsl_pk = result["rpsl_pk"]
 
             # The same PK may occur in multiple sources, but we are
             # only interested in the first matching object, prioritised
             # according to the source order. This priority is part of the
             # query ORDER BY, so basically we only process an RPSL pk once.
             if rpsl_pk in sets_already_resolved:
                 continue
             sets_already_resolved.add(rpsl_pk)
 
-            object_class = result['object_class']
-            object_data = result['parsed_data']
-            mbrs_by_ref = object_data.get('mbrs-by-ref', None)
-            for members_attr in ['members', 'mp-members']:
+            object_class = result["object_class"]
+            object_data = result["parsed_data"]
+            mbrs_by_ref = object_data.get("mbrs-by-ref", None)
+            for members_attr in ["members", "mp-members"]:
                 if members_attr in object_data:
                     members.update(set(object_data[members_attr]))
 
             if not rpsl_pk or not object_class or not mbrs_by_ref:
                 continue
 
             # If mbrs-by-ref is set, find any objects with member-of pointing to the route/as-set
             # under query, and include a maintainer listed in mbrs-by-ref, unless mbrs-by-ref
             # is set to ANY.
-            query_object_class = ['route', 'route6'] if object_class == 'route-set' else ['aut-num']
+            query_object_class = ["route", "route6"] if object_class == "route-set" else ["aut-num"]
             query = self._prepare_query(column_names=columns).object_classes(query_object_class)
-            query = query.lookup_attrs_in(['member-of'], [rpsl_pk])
+            query = query.lookup_attrs_in(["member-of"], [rpsl_pk])
 
-            if 'ANY' not in [m.strip().upper() for m in mbrs_by_ref]:
-                query = query.lookup_attrs_in(['mnt-by'], mbrs_by_ref)
+            if "ANY" not in [m.strip().upper() for m in mbrs_by_ref]:
+                query = query.lookup_attrs_in(["mnt-by"], mbrs_by_ref)
 
             referring_objects = self._execute_query(query)
 
             for result in referring_objects:
-                member_object_class = result['object_class']
-                members.add(result['parsed_data'][member_object_class])
+                member_object_class = result["object_class"]
+                members.add(result["parsed_data"][member_object_class])
 
         leaf_members = set_names - sets_already_resolved
         return members, leaf_members
 
-    def database_status(self, sources: Optional[List[str]]=None) -> 'OrderedDict[str, OrderedDict[str, Any]]':
+    def database_status(
+        self, sources: Optional[List[str]] = None
+    ) -> "OrderedDict[str, OrderedDict[str, Any]]":
         """Database status. If sources is None, return all valid sources."""
         if sources is None:
             sources = self.sources_default if self.sources_default else self.all_valid_sources
         invalid_sources = [s for s in sources if s not in self.all_valid_sources]
         query = DatabaseStatusQuery().sources(sources)
         query_results = self._execute_query(query)
 
         results: OrderedDict[str, OrderedDict[str, Any]] = OrderedDict()
         for query_result in query_results:
-            source = query_result['source'].upper()
+            source = query_result["source"].upper()
             results[source] = OrderedDict()
-            results[source]['authoritative'] = get_setting(f'sources.{source}.authoritative', False)
-            object_class_filter = get_setting(f'sources.{source}.object_class_filter')
-            results[source]['object_class_filter'] = list(object_class_filter) if object_class_filter else None
-            results[source]['rpki_rov_filter'] = bool(get_setting('rpki.roa_source') and not get_setting(f'sources.{source}.rpki_excluded'))
-            results[source]['scopefilter_enabled'] = bool(get_setting('scopefilter')) and not get_setting(f'sources.{source}.scopefilter_excluded')
-            results[source]['local_journal_kept'] = get_setting(f'sources.{source}.keep_journal', False)
-            results[source]['serial_oldest_journal'] = query_result['serial_oldest_journal']
-            results[source]['serial_newest_journal'] = query_result['serial_newest_journal']
-            results[source]['serial_last_export'] = query_result['serial_last_export']
-            results[source]['serial_newest_mirror'] = query_result['serial_newest_mirror']
-            results[source]['last_update'] = query_result['updated'].astimezone(timezone('UTC')).isoformat()
-            results[source]['synchronised_serials'] = is_serial_synchronised(self.database_handler, source)
+            results[source]["authoritative"] = get_setting(f"sources.{source}.authoritative", False)
+            object_class_filter = get_setting(f"sources.{source}.object_class_filter")
+            results[source]["object_class_filter"] = (
+                list(object_class_filter) if object_class_filter else None
+            )
+            results[source]["rpki_rov_filter"] = bool(
+                get_setting("rpki.roa_source") and not get_setting(f"sources.{source}.rpki_excluded")
+            )
+            results[source]["scopefilter_enabled"] = bool(get_setting("scopefilter")) and not get_setting(
+                f"sources.{source}.scopefilter_excluded"
+            )
+            results[source]["route_preference"] = get_setting(f"sources.{source}.route_object_preference")
+            results[source]["local_journal_kept"] = get_setting(f"sources.{source}.keep_journal", False)
+            results[source]["serial_oldest_journal"] = query_result["serial_oldest_journal"]
+            results[source]["serial_newest_journal"] = query_result["serial_newest_journal"]
+            results[source]["serial_last_export"] = query_result["serial_last_export"]
+            results[source]["serial_newest_mirror"] = query_result["serial_newest_mirror"]
+            results[source]["last_update"] = query_result["updated"].astimezone(timezone("UTC")).isoformat()
+            results[source]["synchronised_serials"] = is_serial_synchronised(self.database_handler, source)
 
         for invalid_source in invalid_sources:
-            results[invalid_source.upper()] = OrderedDict({'error': 'Unknown source'})
+            results[invalid_source.upper()] = OrderedDict({"error": "Unknown source"})
         return results
 
     def rpsl_object_template(self, object_class) -> str:
         """Return the RPSL template for an object class"""
         try:
             return OBJECT_CLASS_MAPPING[object_class]().generate_template()
         except KeyError:
-            raise InvalidQueryException(f'Unknown object class: {object_class}')
+            raise InvalidQueryException(f"Unknown object class: {object_class}")
 
     def enable_sql_trace(self):
         self.sql_trace = True
 
     def retrieve_sql_trace(self) -> List[str]:
         trace = self.sql_queries
         self.sql_trace = False
@@ -375,14 +414,16 @@
             query.sources(self.sources)
         if self.object_class_filter:
             query.object_classes(self.object_class_filter)
         if self.rpki_invalid_filter_enabled:
             query.rpki_status([RPKIStatus.not_found, RPKIStatus.valid])
         if self.out_scope_filter_enabled:
             query.scopefilter_status([ScopeFilterStatus.in_scope])
+        if self.route_preference_filter_enabled:
+            query.route_preference_status([RoutePreferenceStatus.visible])
         self.object_class_filter = []
         return query
 
     def _execute_query(self, query) -> RPSLDatabaseResponse:
         if self.sql_trace:
             self.sql_queries.append(repr(query))
         return self.database_handler.execute_query(query, refresh_on_error=True)
```

### Comparing `irrd-4.2.8/irrd/server/test_access_check.py` & `irrd-4.3.0/irrd/server/test_access_check.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,46 +1,52 @@
 from .access_check import is_client_permitted
 
 
 class TestIsClientPermitted:
-    client_ip = '192.0.2.1'
+    client_ip = "192.0.2.1"
 
     def test_no_access_list(self):
-        assert is_client_permitted(self.client_ip, 'server.whois.access_list', default_deny=False)
-        assert not is_client_permitted(self.client_ip, 'server.whois.access_list', default_deny=True)
+        assert is_client_permitted(self.client_ip, "server.whois.access_list", default_deny=False)
+        assert not is_client_permitted(self.client_ip, "server.whois.access_list", default_deny=True)
 
     def test_access_list_permitted(self, config_override):
-        config_override({
-            'server': {
-                'whois': {
-                    'access_list': 'test-access-list',
+        config_override(
+            {
+                "server": {
+                    "whois": {
+                        "access_list": "test-access-list",
+                    },
                 },
-            },
-            'access_lists': {
-                'test-access-list': ['192.0.2.0/25', '2001:db8::/32'],
-            },
-        })
-
-        assert is_client_permitted(self.client_ip, 'server.whois.access_list', default_deny=False)
-        assert is_client_permitted(self.client_ip, 'server.whois.access_list', default_deny=True)
-        assert is_client_permitted(f'::ffff:{self.client_ip}', 'server.whois.access_list', default_deny=True)
-        assert is_client_permitted('2001:db8::1', 'server.whois.access_list', default_deny=True)
+                "access_lists": {
+                    "test-access-list": ["192.0.2.0/25", "2001:db8::/32"],
+                },
+            }
+        )
+
+        assert is_client_permitted(self.client_ip, "server.whois.access_list", default_deny=False)
+        assert is_client_permitted(self.client_ip, "server.whois.access_list", default_deny=True)
+        assert is_client_permitted(f"::ffff:{self.client_ip}", "server.whois.access_list", default_deny=True)
+        assert is_client_permitted("2001:db8::1", "server.whois.access_list", default_deny=True)
 
     def test_access_list_denied(self, config_override):
-        config_override({
-            'server': {
-                'whois': {
-                    'access_list': 'test-access-list',
+        config_override(
+            {
+                "server": {
+                    "whois": {
+                        "access_list": "test-access-list",
+                    },
                 },
-            },
-            'access_lists': {
-                'test-access-list': ['192.0.2.128/25', '2001:db8::/32'],
-            },
-        })
-
-        assert not is_client_permitted(self.client_ip, 'server.whois.access_list', default_deny=False)
-        assert not is_client_permitted(f'::ffff:{self.client_ip}', 'server.whois.access_list', default_deny=False)
-        assert not is_client_permitted(self.client_ip, 'server.whois.access_list', default_deny=True)
+                "access_lists": {
+                    "test-access-list": ["192.0.2.128/25", "2001:db8::/32"],
+                },
+            }
+        )
+
+        assert not is_client_permitted(self.client_ip, "server.whois.access_list", default_deny=False)
+        assert not is_client_permitted(
+            f"::ffff:{self.client_ip}", "server.whois.access_list", default_deny=False
+        )
+        assert not is_client_permitted(self.client_ip, "server.whois.access_list", default_deny=True)
 
     def test_access_list_denied_invalid_ip(self):
-        assert not is_client_permitted('invalid', 'server.whois.access_list', default_deny=False)
-        assert not is_client_permitted('invalid', 'server.whois.access_list', default_deny=True)
+        assert not is_client_permitted("invalid", "server.whois.access_list", default_deny=False)
+        assert not is_client_permitted("invalid", "server.whois.access_list", default_deny=True)
```

### Comparing `irrd-4.2.8/irrd/server/tests/test_query_resolver.py` & `irrd-4.3.0/irrd/server/tests/test_query_resolver.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,20 +1,23 @@
 import datetime
 import uuid
+from collections import OrderedDict
 from unittest.mock import Mock
 
 import pytest
 from IPy import IP
 from pytz import timezone
 
+from irrd.routepref.status import RoutePreferenceStatus
 from irrd.rpki.status import RPKIStatus
 from irrd.scopefilter.status import ScopeFilterStatus
 from irrd.storage.preload import Preloader
 from irrd.utils.test_utils import flatten_mock_calls
-from ..query_resolver import QueryResolver, RouteLookupType, InvalidQueryException
+
+from ..query_resolver import InvalidQueryException, QueryResolver, RouteLookupType
 
 # Note that these mock objects are not entirely valid RPSL objects,
 # as they are meant to test all the scenarios in the query resolver.
 MOCK_ROUTE1 = """route:          192.0.2.0/25
 descr:          description
 origin:         AS65547
 mnt-by:         MNT-TEST
@@ -32,755 +35,794 @@
 MOCK_ROUTE3 = """route:          192.0.2.128/25
 descr:          description
 origin:         AS65545
 mnt-by:         MNT-TEST
 source:         TEST2
 """
 
-MOCK_ROUTE_COMBINED = MOCK_ROUTE1 + '\n' + MOCK_ROUTE2 + '\n' + MOCK_ROUTE3.strip()
+MOCK_ROUTE_COMBINED = MOCK_ROUTE1 + "\n" + MOCK_ROUTE2 + "\n" + MOCK_ROUTE3.strip()
 
 
 @pytest.fixture()
 def prepare_resolver(monkeypatch, config_override):
-    config_override({
-        'rpki': {'roa_source': None},
-        'sources': {'TEST1': {}, 'TEST2': {}},
-        'sources_default': [],
-    })
+    config_override(
+        {
+            "rpki": {"roa_source": None},
+            "sources": {"TEST1": {}, "TEST2": {}},
+            "sources_default": [],
+        }
+    )
 
     mock_database_handler = Mock()
     mock_database_query = Mock()
-    monkeypatch.setattr('irrd.server.query_resolver.RPSLDatabaseQuery',
-                        lambda columns=None, ordered_by_sources=True: mock_database_query)
+    monkeypatch.setattr(
+        "irrd.server.query_resolver.RPSLDatabaseQuery",
+        lambda columns=None, ordered_by_sources=True: mock_database_query,
+    )
     mock_preloader = Mock(spec=Preloader)
 
     resolver = QueryResolver(mock_preloader, mock_database_handler)
     resolver.out_scope_filter_enabled = False
+    resolver.route_preference_filter_enabled = False
 
     mock_query_result = [
         {
-            'pk': uuid.uuid4(),
-            'rpsl_pk': '192.0.2.0/25,AS65547',
-            'object_class': 'route',
-            'parsed_data': {
-                'route': '192.0.2.0/25', 'origin': 'AS65547', 'mnt-by': 'MNT-TEST',
-                'source': 'TEST1',
-                'members': ['AS1, AS2']
-            },
-            'object_text': MOCK_ROUTE1,
-            'rpki_status': RPKIStatus.not_found,
-            'source': 'TEST1',
+            "pk": uuid.uuid4(),
+            "rpsl_pk": "192.0.2.0/25,AS65547",
+            "object_class": "route",
+            "parsed_data": {
+                "route": "192.0.2.0/25",
+                "origin": "AS65547",
+                "mnt-by": "MNT-TEST",
+                "source": "TEST1",
+                "members": ["AS1, AS2"],
+            },
+            "object_text": MOCK_ROUTE1,
+            "rpki_status": RPKIStatus.not_found,
+            "source": "TEST1",
         },
         {
-            'pk': uuid.uuid4(),
-
-            'rpsl_pk': '192.0.2.0/25,AS65544',
-            'object_class': 'route',
-            'parsed_data': {'route': '192.0.2.0/25', 'origin': 'AS65544', 'mnt-by': 'MNT-TEST',
-                            'source': 'TEST2'},
-            'object_text': MOCK_ROUTE2,
-            'rpki_status': RPKIStatus.valid,
-            'source': 'TEST2',
+            "pk": uuid.uuid4(),
+            "rpsl_pk": "192.0.2.0/25,AS65544",
+            "object_class": "route",
+            "parsed_data": {
+                "route": "192.0.2.0/25",
+                "origin": "AS65544",
+                "mnt-by": "MNT-TEST",
+                "source": "TEST2",
+            },
+            "object_text": MOCK_ROUTE2,
+            "rpki_status": RPKIStatus.valid,
+            "source": "TEST2",
         },
         {
-            'pk': uuid.uuid4(),
-            'rpsl_pk': '192.0.2.128/25,AS65545',
-            'object_class': 'route',
-            'parsed_data': {'route': '192.0.2.128/25', 'origin': 'AS65545', 'mnt-by': 'MNT-TEST',
-                            'source': 'TEST2'},
-            'object_text': MOCK_ROUTE3,
-            'rpki_status': RPKIStatus.valid,
-            'source': 'TEST2',
+            "pk": uuid.uuid4(),
+            "rpsl_pk": "192.0.2.128/25,AS65545",
+            "object_class": "route",
+            "parsed_data": {
+                "route": "192.0.2.128/25",
+                "origin": "AS65545",
+                "mnt-by": "MNT-TEST",
+                "source": "TEST2",
+            },
+            "object_text": MOCK_ROUTE3,
+            "rpki_status": RPKIStatus.valid,
+            "source": "TEST2",
         },
     ]
     mock_database_handler.execute_query = lambda query, refresh_on_error=False: mock_query_result
 
     yield mock_database_query, mock_database_handler, mock_preloader, mock_query_result, resolver
 
 
 class TestQueryResolver:
     def test_set_sources(self, prepare_resolver):
         mock_dq, mock_dh, mock_preloader, mock_query_result, resolver = prepare_resolver
 
         resolver.set_query_sources(None)
         assert resolver.sources == resolver.all_valid_sources
 
-        resolver.set_query_sources(['TEST1'])
-        assert resolver.sources == ['TEST1']
+        resolver.set_query_sources(["TEST1"])
+        assert resolver.sources == ["TEST1"]
 
         # With RPKI-aware mode disabled, RPKI is not a valid source
         with pytest.raises(InvalidQueryException):
-            resolver.set_query_sources(['RPKI'])
+            resolver.set_query_sources(["RPKI"])
 
     def test_default_sources(self, prepare_resolver, config_override):
         mock_dq, mock_dh, mock_preloader, mock_query_result, resolver = prepare_resolver
         mock_dh.reset_mock()
-        config_override({
-            'sources': {'TEST1': {}, 'TEST2': {}},
-            'sources_default': ['TEST2', 'TEST1'],
-        })
+        config_override(
+            {
+                "sources": {"TEST1": {}, "TEST2": {}},
+                "sources_default": ["TEST2", "TEST1"],
+            }
+        )
         resolver = QueryResolver(mock_preloader, mock_dh)
-        assert list(resolver.sources_default) == ['TEST2', 'TEST1']
+        assert list(resolver.sources_default) == ["TEST2", "TEST1"]
 
     def test_restrict_object_class(self, prepare_resolver):
         mock_dq, mock_dh, mock_preloader, mock_query_result, resolver = prepare_resolver
         mock_dh.reset_mock()
 
-        resolver.set_object_class_filter_next_query(['route'])
-        result = resolver.rpsl_attribute_search('mnt-by', 'MNT-TEST')
+        resolver.set_object_class_filter_next_query(["route"])
+        result = resolver.rpsl_attribute_search("mnt-by", "MNT-TEST")
         assert list(result) == mock_query_result
         assert flatten_mock_calls(mock_dq) == [
-            ['sources', (['TEST1', 'TEST2'],), {}],
-            ['object_classes', (['route'],), {}],
-            ['lookup_attr', ('mnt-by', 'MNT-TEST'), {}],
+            ["sources", (["TEST1", "TEST2"],), {}],
+            ["object_classes", (["route"],), {}],
+            ["lookup_attr", ("mnt-by", "MNT-TEST"), {}],
         ]
         mock_dq.reset_mock()
 
         # filter should not persist
-        result = resolver.rpsl_attribute_search('mnt-by', 'MNT-TEST')
+        result = resolver.rpsl_attribute_search("mnt-by", "MNT-TEST")
         assert list(result) == mock_query_result
         assert flatten_mock_calls(mock_dq) == [
-            ['sources', (['TEST1', 'TEST2'],), {}],
-            ['lookup_attr', ('mnt-by', 'MNT-TEST'), {}],
+            ["sources", (["TEST1", "TEST2"],), {}],
+            ["lookup_attr", ("mnt-by", "MNT-TEST"), {}],
         ]
 
     def test_key_lookup(self, prepare_resolver):
         mock_dq, mock_dh, mock_preloader, mock_query_result, resolver = prepare_resolver
 
-        result = resolver.key_lookup('route', '192.0.2.0/25')
+        result = resolver.key_lookup("route", "192.0.2.0/25")
         assert list(result) == mock_query_result
         assert flatten_mock_calls(mock_dq) == [
-            ['sources', (['TEST1', 'TEST2'],), {}],
-            ['object_classes', (['route'],), {}],
-            ['rpsl_pk', ('192.0.2.0/25',), {}],
-            ['first_only', (), {}],
+            ["sources", (["TEST1", "TEST2"],), {}],
+            ["object_classes", (["route"],), {}],
+            ["rpsl_pk", ("192.0.2.0/25",), {}],
+            ["first_only", (), {}],
         ]
 
     def test_key_lookup_with_sql_trace(self, prepare_resolver):
         mock_dq, mock_dh, mock_preloader, mock_query_result, resolver = prepare_resolver
         resolver.enable_sql_trace()
 
-        result = resolver.key_lookup('route', '192.0.2.0/25')
+        result = resolver.key_lookup("route", "192.0.2.0/25")
         assert list(result) == mock_query_result
         assert len(resolver.retrieve_sql_trace()) == 1
         assert len(resolver.retrieve_sql_trace()) == 0
 
     def test_limit_sources_key_lookup(self, prepare_resolver):
         mock_dq, mock_dh, mock_preloader, mock_query_result, resolver = prepare_resolver
 
-        resolver.set_query_sources(['TEST1'])
-        result = resolver.key_lookup('route', '192.0.2.0/25')
+        resolver.set_query_sources(["TEST1"])
+        result = resolver.key_lookup("route", "192.0.2.0/25")
         assert list(result) == mock_query_result
         assert flatten_mock_calls(mock_dq) == [
-            ['sources', (['TEST1'],), {}],
-            ['object_classes', (['route'],), {}],
-            ['rpsl_pk', ('192.0.2.0/25',), {}],
-            ['first_only', (), {}],
+            ["sources", (["TEST1"],), {}],
+            ["object_classes", (["route"],), {}],
+            ["rpsl_pk", ("192.0.2.0/25",), {}],
+            ["first_only", (), {}],
         ]
 
     def test_text_search(self, prepare_resolver):
         mock_dq, mock_dh, mock_preloader, mock_query_result, resolver = prepare_resolver
         mock_dh.reset_mock()
 
-        result = resolver.rpsl_text_search('query')
+        result = resolver.rpsl_text_search("query")
         assert list(result) == mock_query_result
         assert flatten_mock_calls(mock_dq) == [
-            ['sources', (['TEST1', 'TEST2'],), {}],
-            ['text_search', ('query',), {}],
+            ["sources", (["TEST1", "TEST2"],), {}],
+            ["text_search", ("query",), {}],
         ]
 
     def test_route_search_exact(self, prepare_resolver):
         mock_dq, mock_dh, mock_preloader, mock_query_result, resolver = prepare_resolver
 
-        result = resolver.route_search(IP('192.0.2.0/25'), RouteLookupType.EXACT)
+        result = resolver.route_search(IP("192.0.2.0/25"), RouteLookupType.EXACT)
         assert list(result) == mock_query_result
         assert flatten_mock_calls(mock_dq) == [
-            ['sources', (['TEST1', 'TEST2'],), {}],
-            ['object_classes', (['route', 'route6'],), {}],
-            ['ip_exact', (IP('192.0.2.0/25'),), {}]
+            ["sources", (["TEST1", "TEST2"],), {}],
+            ["object_classes", (["route", "route6"],), {}],
+            ["ip_exact", (IP("192.0.2.0/25"),), {}],
         ]
         mock_dq.reset_mock()
 
     def test_route_search_less_specific_one_level(self, prepare_resolver):
         mock_dq, mock_dh, mock_preloader, mock_query_result, resolver = prepare_resolver
 
-        result = resolver.route_search(IP('192.0.2.0/25'), RouteLookupType.LESS_SPECIFIC_ONE_LEVEL)
+        result = resolver.route_search(IP("192.0.2.0/25"), RouteLookupType.LESS_SPECIFIC_ONE_LEVEL)
         assert list(result) == mock_query_result
         assert flatten_mock_calls(mock_dq) == [
-            ['sources', (['TEST1', 'TEST2'],), {}],
-            ['object_classes', (['route', 'route6'],), {}],
-            ['ip_less_specific_one_level', (IP('192.0.2.0/25'),), {}]
+            ["sources", (["TEST1", "TEST2"],), {}],
+            ["object_classes", (["route", "route6"],), {}],
+            ["ip_less_specific_one_level", (IP("192.0.2.0/25"),), {}],
         ]
 
     def test_route_search_less_specific(self, prepare_resolver):
         mock_dq, mock_dh, mock_preloader, mock_query_result, resolver = prepare_resolver
 
-        result = resolver.route_search(IP('192.0.2.0/25'), RouteLookupType.LESS_SPECIFIC_WITH_EXACT)
+        result = resolver.route_search(IP("192.0.2.0/25"), RouteLookupType.LESS_SPECIFIC_WITH_EXACT)
         assert list(result) == mock_query_result
         assert flatten_mock_calls(mock_dq) == [
-            ['sources', (['TEST1', 'TEST2'],), {}],
-            ['object_classes', (['route', 'route6'],), {}],
-            ['ip_less_specific', (IP('192.0.2.0/25'),), {}]
+            ["sources", (["TEST1", "TEST2"],), {}],
+            ["object_classes", (["route", "route6"],), {}],
+            ["ip_less_specific", (IP("192.0.2.0/25"),), {}],
         ]
 
     def test_route_search_more_specific(self, prepare_resolver):
         mock_dq, mock_dh, mock_preloader, mock_query_result, resolver = prepare_resolver
 
-        result = resolver.route_search(IP('192.0.2.0/25'), RouteLookupType.MORE_SPECIFIC_WITHOUT_EXACT)
+        result = resolver.route_search(IP("192.0.2.0/25"), RouteLookupType.MORE_SPECIFIC_WITHOUT_EXACT)
         assert list(result) == mock_query_result
         assert flatten_mock_calls(mock_dq) == [
-            ['sources', (['TEST1', 'TEST2'],), {}],
-            ['object_classes', (['route', 'route6'],), {}],
-            ['ip_more_specific', (IP('192.0.2.0/25'),), {}]
+            ["sources", (["TEST1", "TEST2"],), {}],
+            ["object_classes", (["route", "route6"],), {}],
+            ["ip_more_specific", (IP("192.0.2.0/25"),), {}],
         ]
 
     def test_route_search_exact_rpki_aware(self, prepare_resolver, config_override):
         mock_dq, mock_dh, mock_preloader, mock_query_result, resolver = prepare_resolver
-        config_override({
-            'sources': {'TEST1': {}, 'TEST2': {}},
-            'sources_default': [],
-            'rpki': {'roa_source': 'https://example.com/roa.json'},
-        })
+        config_override(
+            {
+                "sources": {"TEST1": {}, "TEST2": {}},
+                "sources_default": [],
+                "rpki": {"roa_source": "https://example.com/roa.json"},
+            }
+        )
         resolver = QueryResolver(mock_preloader, mock_dh)
         resolver.out_scope_filter_enabled = False
+        resolver.route_preference_filter_enabled = False
 
-        result = resolver.route_search(IP('192.0.2.0/25'), RouteLookupType.EXACT)
+        result = resolver.route_search(IP("192.0.2.0/25"), RouteLookupType.EXACT)
         assert list(result) == mock_query_result
         assert flatten_mock_calls(mock_dq) == [
-            ['sources', (['TEST1', 'TEST2', 'RPKI'],), {}],
-            ['rpki_status', ([RPKIStatus.not_found, RPKIStatus.valid],), {}],
-            ['object_classes', (['route', 'route6'],), {}],
-            ['ip_exact', (IP('192.0.2.0/25'),), {}]
+            ["sources", (["TEST1", "TEST2", "RPKI"],), {}],
+            ["rpki_status", ([RPKIStatus.not_found, RPKIStatus.valid],), {}],
+            ["object_classes", (["route", "route6"],), {}],
+            ["ip_exact", (IP("192.0.2.0/25"),), {}],
         ]
         mock_dq.reset_mock()
 
         resolver.disable_rpki_filter()
-        result = resolver.route_search(IP('192.0.2.0/25'), RouteLookupType.EXACT)
+        result = resolver.route_search(IP("192.0.2.0/25"), RouteLookupType.EXACT)
         assert list(result) == mock_query_result
         assert flatten_mock_calls(mock_dq) == [
-            ['sources', (['TEST1', 'TEST2', 'RPKI'],), {}],
-            ['object_classes', (['route', 'route6'],), {}],
-            ['ip_exact', (IP('192.0.2.0/25'),), {}]
+            ["sources", (["TEST1", "TEST2", "RPKI"],), {}],
+            ["object_classes", (["route", "route6"],), {}],
+            ["ip_exact", (IP("192.0.2.0/25"),), {}],
         ]
         mock_dq.reset_mock()
 
-        resolver.set_query_sources(['RPKI'])
-        assert resolver.sources == ['RPKI']
+        resolver.set_query_sources(["RPKI"])
+        assert resolver.sources == ["RPKI"]
 
     def test_route_search_exact_with_scopefilter(self, prepare_resolver, config_override):
         mock_dq, mock_dh, mock_preloader, mock_query_result, resolver = prepare_resolver
         resolver.out_scope_filter_enabled = True
 
-        result = resolver.route_search(IP('192.0.2.0/25'), RouteLookupType.EXACT)
+        result = resolver.route_search(IP("192.0.2.0/25"), RouteLookupType.EXACT)
         assert list(result) == mock_query_result
         assert flatten_mock_calls(mock_dq) == [
-            ['sources', (['TEST1', 'TEST2'],), {}],
-            ['scopefilter_status', ([ScopeFilterStatus.in_scope],), {}],
-            ['object_classes', (['route', 'route6'],), {}],
-            ['ip_exact', (IP('192.0.2.0/25'),), {}]
+            ["sources", (["TEST1", "TEST2"],), {}],
+            ["scopefilter_status", ([ScopeFilterStatus.in_scope],), {}],
+            ["object_classes", (["route", "route6"],), {}],
+            ["ip_exact", (IP("192.0.2.0/25"),), {}],
         ]
         mock_dq.reset_mock()
 
         resolver.disable_out_of_scope_filter()
-        result = resolver.route_search(IP('192.0.2.0/25'), RouteLookupType.EXACT)
+        result = resolver.route_search(IP("192.0.2.0/25"), RouteLookupType.EXACT)
         assert list(result) == mock_query_result
         assert flatten_mock_calls(mock_dq) == [
-            ['sources', (['TEST1', 'TEST2'],), {}],
-            ['object_classes', (['route', 'route6'],), {}],
-            ['ip_exact', (IP('192.0.2.0/25'),), {}]
+            ["sources", (["TEST1", "TEST2"],), {}],
+            ["object_classes", (["route", "route6"],), {}],
+            ["ip_exact", (IP("192.0.2.0/25"),), {}],
+        ]
+        mock_dq.reset_mock()
+
+    def test_route_search_exact_with_route_preference_filter(self, prepare_resolver, config_override):
+        mock_dq, mock_dh, mock_preloader, mock_query_result, resolver = prepare_resolver
+        resolver.route_preference_filter_enabled = True
+
+        result = resolver.route_search(IP("192.0.2.0/25"), RouteLookupType.EXACT)
+        assert list(result) == mock_query_result
+        assert flatten_mock_calls(mock_dq) == [
+            ["sources", (["TEST1", "TEST2"],), {}],
+            ["route_preference_status", ([RoutePreferenceStatus.visible],), {}],
+            ["object_classes", (["route", "route6"],), {}],
+            ["ip_exact", (IP("192.0.2.0/25"),), {}],
+        ]
+        mock_dq.reset_mock()
+
+        resolver.disable_route_preference_filter()
+        result = resolver.route_search(IP("192.0.2.0/25"), RouteLookupType.EXACT)
+        assert list(result) == mock_query_result
+        assert flatten_mock_calls(mock_dq) == [
+            ["sources", (["TEST1", "TEST2"],), {}],
+            ["object_classes", (["route", "route6"],), {}],
+            ["ip_exact", (IP("192.0.2.0/25"),), {}],
         ]
         mock_dq.reset_mock()
 
     def test_rpsl_attribute_search(self, prepare_resolver):
         mock_dq, mock_dh, mock_preloader, mock_query_result, resolver = prepare_resolver
 
-        result = resolver.rpsl_attribute_search('mnt-by', 'MNT-TEST')
+        result = resolver.rpsl_attribute_search("mnt-by", "MNT-TEST")
         assert list(result) == mock_query_result
         assert flatten_mock_calls(mock_dq) == [
-            ['sources', (['TEST1', 'TEST2'],), {}],
-            ['lookup_attr', ('mnt-by', 'MNT-TEST'), {}],
+            ["sources", (["TEST1", "TEST2"],), {}],
+            ["lookup_attr", ("mnt-by", "MNT-TEST"), {}],
         ]
 
         mock_dh.execute_query = lambda query, refresh_on_error=False: []
         with pytest.raises(InvalidQueryException):
-            resolver.rpsl_attribute_search('invalid-attr', 'MNT-TEST')
+            resolver.rpsl_attribute_search("invalid-attr", "MNT-TEST")
 
     def test_routes_for_origin(self, prepare_resolver):
         mock_dq, mock_dh, mock_preloader, mock_query_result, resolver = prepare_resolver
 
-        mock_preloader.routes_for_origins = Mock(return_value={'192.0.2.0/25', '192.0.2.128/25'})
+        mock_preloader.routes_for_origins = Mock(return_value={"192.0.2.0/25", "192.0.2.128/25"})
 
-        result = resolver.routes_for_origin('AS65547', 4)
-        assert result == {'192.0.2.0/25', '192.0.2.128/25'}
+        result = resolver.routes_for_origin("AS65547", 4)
+        assert result == {"192.0.2.0/25", "192.0.2.128/25"}
         assert flatten_mock_calls(mock_preloader.routes_for_origins) == [
-            ['', (['AS65547'], ['TEST1', 'TEST2']), {'ip_version': 4}],
+            ["", (["AS65547"], ["TEST1", "TEST2"]), {"ip_version": 4}],
         ]
 
         mock_preloader.routes_for_origins = Mock(return_value={})
-        result = resolver.routes_for_origin('AS65547', 4)
+        result = resolver.routes_for_origin("AS65547", 4)
         assert result == {}
         assert not mock_dq.mock_calls
 
     def test_routes_for_as_set(self, prepare_resolver, monkeypatch):
         mock_dq, mock_dh, mock_preloader, mock_query_result, resolver = prepare_resolver
 
         monkeypatch.setattr(
-            'irrd.server.query_resolver.QueryResolver._recursive_set_resolve',
-            lambda self, set_name: {'AS65547', 'AS65548'}
+            "irrd.server.query_resolver.QueryResolver._recursive_set_resolve",
+            lambda self, set_name: {"AS65547", "AS65548"},
         )
 
         mock_preloader.routes_for_origins = Mock(return_value=[])
-        result = resolver.routes_for_as_set('AS65547', 4)
+        result = resolver.routes_for_as_set("AS65547", 4)
         assert flatten_mock_calls(mock_preloader.routes_for_origins) == [
-            ['', ({'AS65547', 'AS65548'}, resolver.all_valid_sources), {'ip_version': 4}],
+            ["", ({"AS65547", "AS65548"}, resolver.all_valid_sources), {"ip_version": 4}],
         ]
         assert not result
 
-        mock_preloader.routes_for_origins = Mock(return_value={'192.0.2.0/25', '192.0.2.128/25'})
+        mock_preloader.routes_for_origins = Mock(return_value={"192.0.2.0/25", "192.0.2.128/25"})
 
-        result = resolver.routes_for_as_set('AS65547')
-        assert resolver._current_set_root_object_class == 'as-set'
-        assert result == {'192.0.2.0/25', '192.0.2.128/25'}
+        result = resolver.routes_for_as_set("AS65547")
+        assert resolver._current_set_root_object_class == "as-set"
+        assert result == {"192.0.2.0/25", "192.0.2.128/25"}
         assert flatten_mock_calls(mock_preloader.routes_for_origins) == [
-            ['', ({'AS65547', 'AS65548'}, resolver.all_valid_sources), {'ip_version': None}],
+            ["", ({"AS65547", "AS65548"}, resolver.all_valid_sources), {"ip_version": None}],
         ]
 
         assert not mock_dq.mock_calls
 
     def test_as_set_members(self, prepare_resolver):
         mock_dq, mock_dh, mock_preloader, mock_query_result, resolver = prepare_resolver
 
         mock_query_result1 = [
             {
-                'pk': uuid.uuid4(),
-                'rpsl_pk': 'AS-FIRSTLEVEL',
-                'parsed_data': {'as-set': 'AS-FIRSTLEVEL',
-                                'members': [
-                                    'AS65547', 'AS-FIRSTLEVEL', 'AS-SECONDLEVEL', 'AS-2nd-UNKNOWN'
-                                ]},
-                'object_text': 'text',
-                'object_class': 'as-set',
-                'source': 'TEST1',
+                "pk": uuid.uuid4(),
+                "rpsl_pk": "AS-FIRSTLEVEL",
+                "parsed_data": {
+                    "as-set": "AS-FIRSTLEVEL",
+                    "members": ["AS65547", "AS-FIRSTLEVEL", "AS-SECONDLEVEL", "AS-2nd-UNKNOWN"],
+                },
+                "object_text": "text",
+                "object_class": "as-set",
+                "source": "TEST1",
             },
         ]
         mock_query_result2 = [
             {
-                'pk': uuid.uuid4(),
-                'rpsl_pk': 'AS-SECONDLEVEL',
-                'parsed_data': {'as-set': 'AS-SECONDLEVEL',
-                                'members': ['AS-THIRDLEVEL', 'AS65544']},
-                'object_text': 'text',
-                'object_class': 'as-set',
-                'source': 'TEST1',
+                "pk": uuid.uuid4(),
+                "rpsl_pk": "AS-SECONDLEVEL",
+                "parsed_data": {"as-set": "AS-SECONDLEVEL", "members": ["AS-THIRDLEVEL", "AS65544"]},
+                "object_text": "text",
+                "object_class": "as-set",
+                "source": "TEST1",
             },
             {  # Should be ignored - only the first result per PK is accepted.
-                'pk': uuid.uuid4(),
-                'rpsl_pk': 'AS-SECONDLEVEL',
-                'parsed_data': {'as-set': 'AS-SECONDLEVEL', 'members': ['AS-IGNOREME']},
-                'object_text': 'text',
-                'object_class': 'as-set',
-                'source': 'TEST2',
+                "pk": uuid.uuid4(),
+                "rpsl_pk": "AS-SECONDLEVEL",
+                "parsed_data": {"as-set": "AS-SECONDLEVEL", "members": ["AS-IGNOREME"]},
+                "object_text": "text",
+                "object_class": "as-set",
+                "source": "TEST2",
             },
         ]
         mock_query_result3 = [
             {
-                'pk': uuid.uuid4(),
-                'rpsl_pk': 'AS-THIRDLEVEL',
+                "pk": uuid.uuid4(),
+                "rpsl_pk": "AS-THIRDLEVEL",
                 # Refers back to the first as-set to test infinite recursion issues
-                'parsed_data': {'as-set': 'AS-THIRDLEVEL',
-                                'members': ['AS65545', 'AS-FIRSTLEVEL', 'AS-4th-UNKNOWN']},
-                'object_text': 'text',
-                'object_class': 'as-set',
-                'source': 'TEST2',
+                "parsed_data": {
+                    "as-set": "AS-THIRDLEVEL",
+                    "members": ["AS65545", "AS-FIRSTLEVEL", "AS-4th-UNKNOWN"],
+                },
+                "object_text": "text",
+                "object_class": "as-set",
+                "source": "TEST2",
             },
         ]
         mock_dh.execute_query = lambda query, refresh_on_error=False: iter(mock_query_result1)
 
-        result = resolver.members_for_set('AS-FIRSTLEVEL', recursive=False)
-        assert result == ['AS-2nd-UNKNOWN', 'AS-SECONDLEVEL', 'AS65547']
+        result = resolver.members_for_set("AS-FIRSTLEVEL", recursive=False)
+        assert result == ["AS-2nd-UNKNOWN", "AS-SECONDLEVEL", "AS65547"]
         assert flatten_mock_calls(mock_dq) == [
-            ['sources', (['TEST1', 'TEST2'],), {}],
-            ['object_classes', (['as-set', 'route-set'],), {}],
-            ['rpsl_pks', ({'AS-FIRSTLEVEL'},), {}],
+            ["sources", (["TEST1", "TEST2"],), {}],
+            ["object_classes", (["as-set", "route-set"],), {}],
+            ["rpsl_pks", ({"AS-FIRSTLEVEL"},), {}],
         ]
         mock_dq.reset_mock()
 
         mock_query_iterator = iter(
-            [mock_query_result1, mock_query_result2, mock_query_result3, [], mock_query_result1,
-             []])
+            [mock_query_result1, mock_query_result2, mock_query_result3, [], mock_query_result1, []]
+        )
         mock_dh.execute_query = lambda query, refresh_on_error=False: iter(next(mock_query_iterator))
 
-        result = resolver.members_for_set('AS-FIRSTLEVEL', recursive=True)
-        assert result == ['AS65544', 'AS65545', 'AS65547']
+        result = resolver.members_for_set("AS-FIRSTLEVEL", recursive=True)
+        assert result == ["AS65544", "AS65545", "AS65547"]
         assert flatten_mock_calls(mock_dq) == [
-            ['sources', (['TEST1', 'TEST2'],), {}],
-            ['object_classes', (['as-set', 'route-set'],), {}],
-            ['rpsl_pks', ({'AS-FIRSTLEVEL'},), {}],
-
-            ['sources', (['TEST1', 'TEST2'],), {}],
-            ['object_classes', (['as-set'],), {}],
-            ['rpsl_pks', ({'AS-2nd-UNKNOWN', 'AS-SECONDLEVEL'},), {}],
-
-            ['sources', (['TEST1', 'TEST2'],), {}],
-            ['object_classes', (['as-set'],), {}],
-            ['rpsl_pks', ({'AS-THIRDLEVEL'},), {}],
-
-            ['sources', (['TEST1', 'TEST2'],), {}],
-            ['object_classes', (['as-set'],), {}],
-            ['rpsl_pks', ({'AS-4th-UNKNOWN'},), {}],
+            ["sources", (["TEST1", "TEST2"],), {}],
+            ["object_classes", (["as-set", "route-set"],), {}],
+            ["rpsl_pks", ({"AS-FIRSTLEVEL"},), {}],
+            ["sources", (["TEST1", "TEST2"],), {}],
+            ["object_classes", (["as-set"],), {}],
+            ["rpsl_pks", ({"AS-2nd-UNKNOWN", "AS-SECONDLEVEL"},), {}],
+            ["sources", (["TEST1", "TEST2"],), {}],
+            ["object_classes", (["as-set"],), {}],
+            ["rpsl_pks", ({"AS-THIRDLEVEL"},), {}],
+            ["sources", (["TEST1", "TEST2"],), {}],
+            ["object_classes", (["as-set"],), {}],
+            ["rpsl_pks", ({"AS-4th-UNKNOWN"},), {}],
         ]
         mock_dq.reset_mock()
 
-        result = resolver.members_for_set('AS-FIRSTLEVEL', depth=1, recursive=True)
-        assert result == ['AS-2nd-UNKNOWN', 'AS-SECONDLEVEL', 'AS65547']
+        result = resolver.members_for_set("AS-FIRSTLEVEL", depth=1, recursive=True)
+        assert result == ["AS-2nd-UNKNOWN", "AS-SECONDLEVEL", "AS65547"]
         mock_dq.reset_mock()
 
         mock_dh.execute_query = lambda query, refresh_on_error=False: iter([])
-        result = resolver.members_for_set('AS-NOTEXIST', recursive=True)
+        result = resolver.members_for_set("AS-NOTEXIST", recursive=True)
         assert not result
         assert flatten_mock_calls(mock_dq) == [
-            ['sources', (['TEST1', 'TEST2'],), {}],
-            ['object_classes', (['as-set', 'route-set'],), {}],
-            ['rpsl_pks', ({'AS-NOTEXIST'},), {}]
+            ["sources", (["TEST1", "TEST2"],), {}],
+            ["object_classes", (["as-set", "route-set"],), {}],
+            ["rpsl_pks", ({"AS-NOTEXIST"},), {}],
         ]
         mock_dq.reset_mock()
 
         mock_dh.execute_query = lambda query, refresh_on_error=False: iter([])
-        result = resolver.members_for_set('AS-NOTEXIST', recursive=True, root_source='ROOT')
+        result = resolver.members_for_set("AS-NOTEXIST", recursive=True, root_source="ROOT")
         assert not result
         assert flatten_mock_calls(mock_dq) == [
-            ['sources', (['TEST1', 'TEST2'],), {}],
-            ['object_classes', (['as-set', 'route-set'],), {}],
-            ['rpsl_pks', ({'AS-NOTEXIST'},), {}],
-            ['sources', (['ROOT'],), {}],
+            ["sources", (["TEST1", "TEST2"],), {}],
+            ["object_classes", (["as-set", "route-set"],), {}],
+            ["rpsl_pks", ({"AS-NOTEXIST"},), {}],
+            ["sources", (["ROOT"],), {}],
         ]
 
     def test_route_set_members(self, prepare_resolver):
         mock_dq, mock_dh, mock_preloader, mock_query_result, resolver = prepare_resolver
 
         mock_query_result1 = [
             {
-                'pk': uuid.uuid4(),
-                'rpsl_pk': 'RS-FIRSTLEVEL',
-                'parsed_data': {'as-set': 'RS-FIRSTLEVEL',
-                                'members': ['RS-SECONDLEVEL', 'RS-2nd-UNKNOWN']},
-                'object_text': 'text',
-                'object_class': 'route-set',
-                'source': 'TEST1',
+                "pk": uuid.uuid4(),
+                "rpsl_pk": "RS-FIRSTLEVEL",
+                "parsed_data": {"as-set": "RS-FIRSTLEVEL", "members": ["RS-SECONDLEVEL", "RS-2nd-UNKNOWN"]},
+                "object_text": "text",
+                "object_class": "route-set",
+                "source": "TEST1",
             },
         ]
         mock_query_result2 = [
             {
-                'pk': uuid.uuid4(),
-                'rpsl_pk': 'RS-SECONDLEVEL',
-                'parsed_data': {'as-set': 'RS-SECONDLEVEL',
-                                'members': [
-                                    'AS-REFERRED', '192.0.2.0/25', '192.0.2.0/26^32'
-                                ]},
-                'object_text': 'text',
-                'object_class': 'route-set',
-                'source': 'TEST1',
+                "pk": uuid.uuid4(),
+                "rpsl_pk": "RS-SECONDLEVEL",
+                "parsed_data": {
+                    "as-set": "RS-SECONDLEVEL",
+                    "members": ["AS-REFERRED", "192.0.2.0/25", "192.0.2.0/26^32"],
+                },
+                "object_text": "text",
+                "object_class": "route-set",
+                "source": "TEST1",
             },
         ]
         mock_query_result3 = [
             {
-                'pk': uuid.uuid4(),
-                'rpsl_pk': 'AS-REFERRED',
-                'parsed_data': {'as-set': 'AS-REFERRED',
-                                'members': ['AS65545']},
-                'object_text': 'text',
-                'object_class': 'as-set',
-                'source': 'TEST2',
+                "pk": uuid.uuid4(),
+                "rpsl_pk": "AS-REFERRED",
+                "parsed_data": {"as-set": "AS-REFERRED", "members": ["AS65545"]},
+                "object_text": "text",
+                "object_class": "as-set",
+                "source": "TEST2",
             },
         ]
         mock_query_iterator = iter([mock_query_result1, mock_query_result2, mock_query_result3, []])
         mock_dh.execute_query = lambda query, refresh_on_error=False: iter(next(mock_query_iterator))
-        mock_preloader.routes_for_origins = Mock(return_value=['192.0.2.128/25'])
+        mock_preloader.routes_for_origins = Mock(return_value=["192.0.2.128/25"])
 
-        result = resolver.members_for_set('RS-FIRSTLEVEL', recursive=True)
-        assert set(result) == {'192.0.2.0/26^32', '192.0.2.0/25', '192.0.2.128/25'}
+        result = resolver.members_for_set("RS-FIRSTLEVEL", recursive=True)
+        assert set(result) == {"192.0.2.0/26^32", "192.0.2.0/25", "192.0.2.128/25"}
         assert flatten_mock_calls(mock_dq) == [
-            ['sources', (['TEST1', 'TEST2'],), {}],
-            ['object_classes', (['as-set', 'route-set'],), {}],
-            ['rpsl_pks', ({'RS-FIRSTLEVEL'},), {}],
-
-            ['sources', (['TEST1', 'TEST2'],), {}],
-            ['object_classes', (['as-set', 'route-set'],), {}],
-            ['rpsl_pks', ({'RS-SECONDLEVEL', 'RS-2nd-UNKNOWN'},), {}],
-
-            ['sources', (['TEST1', 'TEST2'],), {}],
-            ['object_classes', (['as-set', 'route-set'],), {}],
-            ['rpsl_pks', ({'AS-REFERRED'},), {}],
+            ["sources", (["TEST1", "TEST2"],), {}],
+            ["object_classes", (["as-set", "route-set"],), {}],
+            ["rpsl_pks", ({"RS-FIRSTLEVEL"},), {}],
+            ["sources", (["TEST1", "TEST2"],), {}],
+            ["object_classes", (["as-set", "route-set"],), {}],
+            ["rpsl_pks", ({"RS-SECONDLEVEL", "RS-2nd-UNKNOWN"},), {}],
+            ["sources", (["TEST1", "TEST2"],), {}],
+            ["object_classes", (["as-set", "route-set"],), {}],
+            ["rpsl_pks", ({"AS-REFERRED"},), {}],
         ]
 
     def test_as_route_set_mbrs_by_ref(self, prepare_resolver):
         mock_dq, mock_dh, mock_preloader, mock_query_result, resolver = prepare_resolver
 
         mock_query_result1 = [
             {
                 # This route-set is intentionally misnamed RRS, as invalid names occur in real life.
-                'pk': uuid.uuid4(),
-                'rpsl_pk': 'RRS-TEST',
-                'parsed_data': {'route-set': 'RRS-TEST', 'members': ['192.0.2.0/32'],
-                                'mp-members': ['2001:db8::/32'], 'mbrs-by-ref': ['MNT-TEST']},
-                'object_text': 'text',
-                'object_class': 'route-set',
-                'source': 'TEST1',
+                "pk": uuid.uuid4(),
+                "rpsl_pk": "RRS-TEST",
+                "parsed_data": {
+                    "route-set": "RRS-TEST",
+                    "members": ["192.0.2.0/32"],
+                    "mp-members": ["2001:db8::/32"],
+                    "mbrs-by-ref": ["MNT-TEST"],
+                },
+                "object_text": "text",
+                "object_class": "route-set",
+                "source": "TEST1",
             },
         ]
         mock_query_result2 = [
             {
-                'pk': uuid.uuid4(),
-                'rpsl_pk': '192.0.2.0/24,AS65544',
-                'parsed_data': {'route': '192.0.2.0/24', 'member-of': 'rrs-test',
-                                'mnt-by': ['FOO', 'MNT-TEST']},
-                'object_text': 'text',
-                'object_class': 'route',
-                'source': 'TEST1',
+                "pk": uuid.uuid4(),
+                "rpsl_pk": "192.0.2.0/24,AS65544",
+                "parsed_data": {
+                    "route": "192.0.2.0/24",
+                    "member-of": "rrs-test",
+                    "mnt-by": ["FOO", "MNT-TEST"],
+                },
+                "object_text": "text",
+                "object_class": "route",
+                "source": "TEST1",
             },
         ]
         mock_query_iterator = iter([mock_query_result1, mock_query_result2, [], [], []])
         mock_dh.execute_query = lambda query, refresh_on_error=False: iter(next(mock_query_iterator))
 
-        result = resolver.members_for_set('RRS-TEST', recursive=True)
-        assert result == ['192.0.2.0/24', '192.0.2.0/32', '2001:db8::/32']
+        result = resolver.members_for_set("RRS-TEST", recursive=True)
+        assert result == ["192.0.2.0/24", "192.0.2.0/32", "2001:db8::/32"]
         assert flatten_mock_calls(mock_dq) == [
-            ['sources', (['TEST1', 'TEST2'],), {}],
-            ['object_classes', (['as-set', 'route-set'],), {}],
-            ['rpsl_pks', ({'RRS-TEST'},), {}],
-
-            ['sources', (['TEST1', 'TEST2'],), {}],
-            ['object_classes', (['route', 'route6'],), {}],
-            ['lookup_attrs_in', (['member-of'], ['RRS-TEST']), {}],
-            ['lookup_attrs_in', (['mnt-by'], ['MNT-TEST']), {}],
+            ["sources", (["TEST1", "TEST2"],), {}],
+            ["object_classes", (["as-set", "route-set"],), {}],
+            ["rpsl_pks", ({"RRS-TEST"},), {}],
+            ["sources", (["TEST1", "TEST2"],), {}],
+            ["object_classes", (["route", "route6"],), {}],
+            ["lookup_attrs_in", (["member-of"], ["RRS-TEST"]), {}],
+            ["lookup_attrs_in", (["mnt-by"], ["MNT-TEST"]), {}],
         ]
         mock_dq.reset_mock()
 
         # Disable maintainer check
-        mock_query_result1[0]['parsed_data']['mbrs-by-ref'] = ['ANY']
+        mock_query_result1[0]["parsed_data"]["mbrs-by-ref"] = ["ANY"]
         mock_query_iterator = iter([mock_query_result1, mock_query_result2, [], [], []])
-        result = resolver.members_for_set('RRS-TEST', recursive=True)
-        assert result == ['192.0.2.0/24', '192.0.2.0/32', '2001:db8::/32']
+        result = resolver.members_for_set("RRS-TEST", recursive=True)
+        assert result == ["192.0.2.0/24", "192.0.2.0/32", "2001:db8::/32"]
         assert flatten_mock_calls(mock_dq) == [
-            ['sources', (['TEST1', 'TEST2'],), {}],
-            ['object_classes', (['as-set', 'route-set'],), {}],
-            ['rpsl_pks', ({'RRS-TEST'},), {}],
-
-            ['sources', (['TEST1', 'TEST2'],), {}],
-            ['object_classes', (['route', 'route6'],), {}],
-            ['lookup_attrs_in', (['member-of'], ['RRS-TEST']), {}],
+            ["sources", (["TEST1", "TEST2"],), {}],
+            ["object_classes", (["as-set", "route-set"],), {}],
+            ["rpsl_pks", ({"RRS-TEST"},), {}],
+            ["sources", (["TEST1", "TEST2"],), {}],
+            ["object_classes", (["route", "route6"],), {}],
+            ["lookup_attrs_in", (["member-of"], ["RRS-TEST"]), {}],
         ]
 
-    def test_route_set_compatibility_ipv4_only_route_set_members(self, prepare_resolver,
-                                                                 config_override):
+    def test_route_set_compatibility_ipv4_only_route_set_members(self, prepare_resolver, config_override):
         mock_dq, mock_dh, mock_preloader, mock_query_result, resolver = prepare_resolver
 
         mock_query_result = [
             {
-                'pk': uuid.uuid4(),
-                'rpsl_pk': 'RS-TEST',
-                'parsed_data': {
-                    'route-set': 'RS-TEST',
-                    'members': ['192.0.2.0/32'],
-                    'mp-members': ['192.0.2.1/32', '2001:db8::/32', 'RS-OTHER']
+                "pk": uuid.uuid4(),
+                "rpsl_pk": "RS-TEST",
+                "parsed_data": {
+                    "route-set": "RS-TEST",
+                    "members": ["192.0.2.0/32"],
+                    "mp-members": ["192.0.2.1/32", "2001:db8::/32", "RS-OTHER"],
                 },
-                'object_text': 'text',
-                'object_class': 'route-set',
-                'source': 'TEST1',
+                "object_text": "text",
+                "object_class": "route-set",
+                "source": "TEST1",
             },
         ]
         mock_dh.execute_query = lambda query, refresh_on_error=False: mock_query_result
 
-        result = resolver.members_for_set('RS-TEST', recursive=False)
-        assert result == ['192.0.2.0/32', '192.0.2.1/32', '2001:db8::/32', 'RS-OTHER']
+        result = resolver.members_for_set("RS-TEST", recursive=False)
+        assert result == ["192.0.2.0/32", "192.0.2.1/32", "2001:db8::/32", "RS-OTHER"]
 
-        config_override({
-            'compatibility': {'ipv4_only_route_set_members': True},
-        })
+        config_override(
+            {
+                "compatibility": {"ipv4_only_route_set_members": True},
+            }
+        )
 
-        result = resolver.members_for_set('RS-TEST', recursive=False)
-        assert result == ['192.0.2.0/32', '192.0.2.1/32', 'RS-OTHER']
+        result = resolver.members_for_set("RS-TEST", recursive=False)
+        assert result == ["192.0.2.0/32", "192.0.2.1/32", "RS-OTHER"]
 
     def test_members_for_set_per_source(self, prepare_resolver):
         mock_dq, mock_dh, mock_preloader, mock_query_result, resolver = prepare_resolver
 
-        mock_query_result = iter([
+        mock_query_result = iter(
             [
-                {
-                    'rpsl_pk': 'AS-TEST',
-                    'source': 'TEST1',
-                },
-                {
-                    'rpsl_pk': 'AS-TEST',
-                    'source': 'TEST2',
-                },
-            ], [
-                {
-                    'pk': uuid.uuid4(),
-                    'rpsl_pk': 'AS-TEST',
-                    'parsed_data': {'as-set': 'AS-TEST',
-                                    'members': ['AS65547', 'AS-SECONDLEVEL']},
-                    'object_text': 'text',
-                    'object_class': 'as-set',
-                    'source': 'TEST1',
-                },
-            ], [
-                {
-                    'pk': uuid.uuid4(),
-                    'rpsl_pk': 'AS-SECONDLEVEL',
-                    'parsed_data': {'as-set': 'AS-SECONDLEVEL',
-                                    'members': ['AS65548']},
-                    'object_text': 'text',
-                    'object_class': 'as-set',
-                    'source': 'TEST1',
-                },
-            ], [
-                {
-                    'pk': uuid.uuid4(),
-                    'rpsl_pk': 'AS-TEST',
-                    'parsed_data': {'as-set': 'AS-TEST',
-                                    'members': ['AS65549']},
-                    'object_text': 'text',
-                    'object_class': 'as-set',
-                    'source': 'TEST2',
-                },
-            ],
-            [],
-        ])
+                [
+                    {
+                        "rpsl_pk": "AS-TEST",
+                        "source": "TEST1",
+                    },
+                    {
+                        "rpsl_pk": "AS-TEST",
+                        "source": "TEST2",
+                    },
+                ],
+                [
+                    {
+                        "pk": uuid.uuid4(),
+                        "rpsl_pk": "AS-TEST",
+                        "parsed_data": {"as-set": "AS-TEST", "members": ["AS65547", "AS-SECONDLEVEL"]},
+                        "object_text": "text",
+                        "object_class": "as-set",
+                        "source": "TEST1",
+                    },
+                ],
+                [
+                    {
+                        "pk": uuid.uuid4(),
+                        "rpsl_pk": "AS-SECONDLEVEL",
+                        "parsed_data": {"as-set": "AS-SECONDLEVEL", "members": ["AS65548"]},
+                        "object_text": "text",
+                        "object_class": "as-set",
+                        "source": "TEST1",
+                    },
+                ],
+                [
+                    {
+                        "pk": uuid.uuid4(),
+                        "rpsl_pk": "AS-TEST",
+                        "parsed_data": {"as-set": "AS-TEST", "members": ["AS65549"]},
+                        "object_text": "text",
+                        "object_class": "as-set",
+                        "source": "TEST2",
+                    },
+                ],
+                [],
+            ]
+        )
 
         mock_dh.execute_query = lambda query, refresh_on_error=False: next(mock_query_result)
 
-        result = resolver.members_for_set_per_source('AS-TEST', recursive=True)
-        assert result == {'TEST1': ['AS65547', 'AS65548'], 'TEST2': ['AS65549']}
+        result = resolver.members_for_set_per_source("AS-TEST", recursive=True)
+        assert result == {"TEST1": ["AS65547", "AS65548"], "TEST2": ["AS65549"]}
         assert flatten_mock_calls(mock_dq) == [
-            ['sources', (['TEST1', 'TEST2'],), {}],
-            ['object_classes', (['as-set', 'route-set'],), {}],
-            ['rpsl_pk', ('AS-TEST',), {}],
-
-            ['sources', (['TEST1', 'TEST2'],), {}],
-            ['object_classes', (['as-set', 'route-set'],), {}],
-            ['rpsl_pks', ({'AS-TEST'},), {}],
-            ['sources', (['TEST1'],), {}],
-
-            ['sources', (['TEST1', 'TEST2'],), {}],
-            ['object_classes', (['as-set'],), {}],
-            ['rpsl_pks', ({'AS-SECONDLEVEL'},), {}],
-
-            ['sources', (['TEST1', 'TEST2'],), {}],
-            ['object_classes', (['as-set', 'route-set'],), {}],
-            ['rpsl_pks', ({'AS-TEST'},), {}],
-            ['sources', (['TEST2'],), {}]
+            ["sources", (["TEST1", "TEST2"],), {}],
+            ["object_classes", (["as-set", "route-set"],), {}],
+            ["rpsl_pk", ("AS-TEST",), {}],
+            ["sources", (["TEST1", "TEST2"],), {}],
+            ["object_classes", (["as-set", "route-set"],), {}],
+            ["rpsl_pks", ({"AS-TEST"},), {}],
+            ["sources", (["TEST1"],), {}],
+            ["sources", (["TEST1", "TEST2"],), {}],
+            ["object_classes", (["as-set"],), {}],
+            ["rpsl_pks", ({"AS-SECONDLEVEL"},), {}],
+            ["sources", (["TEST1", "TEST2"],), {}],
+            ["object_classes", (["as-set", "route-set"],), {}],
+            ["rpsl_pks", ({"AS-TEST"},), {}],
+            ["sources", (["TEST2"],), {}],
         ]
         mock_dq.reset_mock()
 
     def test_database_status(self, monkeypatch, prepare_resolver, config_override):
-        config_override({
-            'rpki': {'roa_source': 'http://example.com/'},
-            'scopefilter': {'prefixes': ['192.0.2.0/24']},
-            'sources': {
-                'TEST1': {
-                    'authoritative': True,
-                    'object_class_filter': ['route'],
-                    'scopefilter_excluded': True
+        config_override(
+            {
+                "rpki": {"roa_source": "http://example.com/"},
+                "scopefilter": {"prefixes": ["192.0.2.0/24"]},
+                "sources": {
+                    "TEST1": {
+                        "authoritative": True,
+                        "object_class_filter": ["route"],
+                        "scopefilter_excluded": True,
+                        "route_preference": 200,
+                    },
+                    "TEST2": {"rpki_excluded": True, "keep_journal": True},
                 },
-                'TEST2': {'rpki_excluded': True, 'keep_journal': True}
-            },
-            'sources_default': [],
-        })
+                "sources_default": [],
+            }
+        )
         mock_dq, mock_dh, mock_preloader, mock_query_result, resolver = prepare_resolver
         mock_dsq = Mock()
-        monkeypatch.setattr('irrd.server.query_resolver.DatabaseStatusQuery', lambda: mock_dsq)
-        monkeypatch.setattr('irrd.server.query_resolver.is_serial_synchronised',
-                            lambda dh, s: False)
+        monkeypatch.setattr("irrd.server.query_resolver.DatabaseStatusQuery", lambda: mock_dsq)
+        monkeypatch.setattr("irrd.server.query_resolver.is_serial_synchronised", lambda dh, s: False)
 
         mock_query_result = [
             {
-                'source': 'TEST1',
-                'serial_oldest_journal': 10,
-                'serial_newest_journal': 10,
-                'serial_last_export': 10,
-                'serial_newest_mirror': 500,
-                'updated': datetime.datetime(2020, 1, 1, tzinfo=timezone('UTC')),
-            },
-            {
-                'source': 'TEST2',
-                'serial_oldest_journal': None,
-                'serial_newest_journal': None,
-                'serial_last_export': None,
-                'serial_newest_mirror': 20,
-                'updated': datetime.datetime(2020, 1, 1, tzinfo=timezone('UTC')),
-            },
-        ]
-        mock_dh.execute_query = lambda query, refresh_on_error=False: mock_query_result
-
-        result = resolver.database_status()
-        assert result == {
-            "TEST1": {
-                "authoritative": True,
-                "object_class_filter": [
-                    "route"
-                ],
-                "rpki_rov_filter": True,
-                "scopefilter_enabled": False,
-                "local_journal_kept": False,
+                "source": "TEST1",
                 "serial_oldest_journal": 10,
                 "serial_newest_journal": 10,
                 "serial_last_export": 10,
                 "serial_newest_mirror": 500,
-                "last_update": "2020-01-01T00:00:00+00:00",
-                "synchronised_serials": False
+                "updated": datetime.datetime(2020, 1, 1, tzinfo=timezone("UTC")),
             },
-            "TEST2": {
-                "authoritative": False,
-                "object_class_filter": None,
-                "rpki_rov_filter": False,
-                "scopefilter_enabled": True,
-                "local_journal_kept": True,
+            {
+                "source": "TEST2",
                 "serial_oldest_journal": None,
                 "serial_newest_journal": None,
                 "serial_last_export": None,
                 "serial_newest_mirror": 20,
-                "last_update": "2020-01-01T00:00:00+00:00",
-                "synchronised_serials": False
-            }
-        }
-        assert flatten_mock_calls(mock_dsq) == [
-            ['sources', (['TEST1', 'TEST2'],), {}]
+                "updated": datetime.datetime(2020, 1, 1, tzinfo=timezone("UTC")),
+            },
         ]
+        mock_dh.execute_query = lambda query, refresh_on_error=False: mock_query_result
+
+        result = resolver.database_status()
+        expected_test1_result = (
+            "TEST1",
+            OrderedDict(
+                [
+                    ("authoritative", True),
+                    ("object_class_filter", ["route"]),
+                    ("rpki_rov_filter", True),
+                    ("scopefilter_enabled", False),
+                    ("route_preference", None),
+                    ("local_journal_kept", False),
+                    ("serial_oldest_journal", 10),
+                    ("serial_newest_journal", 10),
+                    ("serial_last_export", 10),
+                    ("serial_newest_mirror", 500),
+                    ("last_update", "2020-01-01T00:00:00+00:00"),
+                    ("synchronised_serials", False),
+                ]
+            ),
+        )
+        assert result == OrderedDict(
+            [
+                expected_test1_result,
+                (
+                    "TEST2",
+                    OrderedDict(
+                        [
+                            ("authoritative", False),
+                            ("object_class_filter", None),
+                            ("rpki_rov_filter", False),
+                            ("scopefilter_enabled", True),
+                            ("route_preference", None),
+                            ("local_journal_kept", True),
+                            ("serial_oldest_journal", None),
+                            ("serial_newest_journal", None),
+                            ("serial_last_export", None),
+                            ("serial_newest_mirror", 20),
+                            ("last_update", "2020-01-01T00:00:00+00:00"),
+                            ("synchronised_serials", False),
+                        ]
+                    ),
+                ),
+            ]
+        )
+        assert flatten_mock_calls(mock_dsq) == [["sources", (["TEST1", "TEST2"],), {}]]
         mock_dsq.reset_mock()
 
         mock_query_result = mock_query_result[:1]
-        result = resolver.database_status(['TEST1', 'TEST-INVALID'])
-        assert result == {
-            "TEST1": {
-                "authoritative": True,
-                "object_class_filter": [
-                    "route"
-                ],
-                "rpki_rov_filter": True,
-                "scopefilter_enabled": False,
-                "local_journal_kept": False,
-                "serial_oldest_journal": 10,
-                "serial_newest_journal": 10,
-                "serial_last_export": 10,
-                "serial_newest_mirror": 500,
-                "last_update": "2020-01-01T00:00:00+00:00",
-                "synchronised_serials": False
-            },
-            "TEST-INVALID": {
-                "error": "Unknown source"
-            }
-        }
-        assert flatten_mock_calls(mock_dsq) == [
-            ['sources', (['TEST1', 'TEST-INVALID'],), {}]
-        ]
+        result = resolver.database_status(["TEST1", "TEST-INVALID"])
+        assert result == OrderedDict(
+            [
+                expected_test1_result,
+                ("TEST-INVALID", OrderedDict([("error", "Unknown source")])),
+            ]
+        )
+        assert flatten_mock_calls(mock_dsq) == [["sources", (["TEST1", "TEST-INVALID"],), {}]]
 
     def test_object_template(self, prepare_resolver):
         mock_dq, mock_dh, mock_preloader, mock_query_result, resolver = prepare_resolver
         mock_dh.reset_mock()
 
-        result = resolver.rpsl_object_template('aut-num')
-        assert 'aut-num:[mandatory][single][primary/look-upkey]' in result.replace(' ', '')
+        result = resolver.rpsl_object_template("aut-num")
+        assert "aut-num:[mandatory][single][primary/look-upkey]" in result.replace(" ", "")
         mock_dh.reset_mock()
 
         with pytest.raises(InvalidQueryException):
-            resolver.rpsl_object_template('does-not-exist')
+            resolver.rpsl_object_template("does-not-exist")
```

### Comparing `irrd-4.2.8/irrd/server/whois/query_parser.py` & `irrd-4.3.0/irrd/server/whois/query_parser.py`

 * *Files 8% similar despite different names*

```diff
@@ -3,25 +3,34 @@
 from typing import Optional
 
 import ujson
 from IPy import IP
 from ordered_set import OrderedSet
 
 from irrd import __version__
-from irrd.conf import get_setting, RPKI_IRR_PSEUDO_SOURCE, SOCKET_DEFAULT_TIMEOUT
+from irrd.conf import RPKI_IRR_PSEUDO_SOURCE, SOCKET_DEFAULT_TIMEOUT, get_setting
 from irrd.mirroring.nrtm_generator import NRTMGenerator, NRTMGeneratorException
 from irrd.rpki.status import RPKIStatus
-from irrd.rpsl.rpsl_objects import (OBJECT_CLASS_MAPPING, RPKI_RELEVANT_OBJECT_CLASSES)
-from irrd.server.query_resolver import QueryResolver, RouteLookupType, InvalidQueryException
+from irrd.rpsl.rpsl_objects import OBJECT_CLASS_MAPPING, RPKI_RELEVANT_OBJECT_CLASSES
+from irrd.server.query_resolver import (
+    InvalidQueryException,
+    QueryResolver,
+    RouteLookupType,
+)
 from irrd.storage.database_handler import DatabaseHandler, RPSLDatabaseResponse
 from irrd.storage.preload import Preloader
 from irrd.storage.queries import DatabaseStatusQuery
-from irrd.utils.validators import parse_as_number, ValidationError
-from .query_response import WhoisQueryResponseType, WhoisQueryResponseMode, WhoisQueryResponse
+from irrd.utils.validators import ValidationError, parse_as_number
+
 from ..access_check import is_client_permitted
+from .query_response import (
+    WhoisQueryResponse,
+    WhoisQueryResponseMode,
+    WhoisQueryResponseType,
+)
 
 logger = logging.getLogger(__name__)
 
 
 class WhoisQueryParser:
     """
     Parser for all whois-style queries.
@@ -31,16 +40,17 @@
     QueryResolver, with a few exceptions that are whois-specific.
 
     Some query flags, particularly -k/!! and -s/!s retain state across queries,
     so a single instance of this object should be created per session, with
     handle_query() being called for each individual query.
     """
 
-    def __init__(self, client_ip: str, client_str: str, preloader: Preloader,
-                 database_handler: DatabaseHandler) -> None:
+    def __init__(
+        self, client_ip: str, client_str: str, preloader: Preloader, database_handler: DatabaseHandler
+    ) -> None:
         self.multiple_command_mode = False
         self.timeout = SOCKET_DEFAULT_TIMEOUT
         self.key_fields_only = False
         self.client_ip = client_ip
         self.client_str = client_str
         self.database_handler = database_handler
         self.query_resolver = QueryResolver(
@@ -51,345 +61,370 @@
     def handle_query(self, query: str) -> WhoisQueryResponse:
         """
         Process a single query. Always returns a WhoisQueryResponse object.
         Not thread safe - only one call must be made to this method at the same time.
         """
         self.key_fields_only = False
 
-        if query.startswith('!'):
+        if chr(0) in query:
+            return WhoisQueryResponse(
+                response_type=WhoisQueryResponseType.ERROR_USER,
+                mode=WhoisQueryResponseMode.IRRD,
+                result="Queries may not contain null bytes",
+            )
+
+        if query.startswith("!"):
             try:
                 return self.handle_irrd_command(query[1:])
             except InvalidQueryException as exc:
-                logger.info(f'{self.client_str}: encountered parsing error while parsing query "{query}": {exc}')
+                logger.info(
+                    f'{self.client_str}: encountered parsing error while parsing query "{query}": {exc}'
+                )
                 return WhoisQueryResponse(
                     response_type=WhoisQueryResponseType.ERROR_USER,
                     mode=WhoisQueryResponseMode.IRRD,
-                    result=str(exc)
+                    result=str(exc),
                 )
             except Exception as exc:
-                logger.error(f'An exception occurred while processing whois query "{query}": {exc}', exc_info=exc)
+                logger.error(
+                    f'An exception occurred while processing whois query "{query}": {exc}', exc_info=exc
+                )
                 return WhoisQueryResponse(
                     response_type=WhoisQueryResponseType.ERROR_INTERNAL,
                     mode=WhoisQueryResponseMode.IRRD,
-                    result='An internal error occurred while processing this query.'
+                    result="An internal error occurred while processing this query.",
                 )
 
         try:
             return self.handle_ripe_command(query)
         except InvalidQueryException as exc:
             logger.info(f'{self.client_str}: encountered parsing error while parsing query "{query}": {exc}')
             return WhoisQueryResponse(
                 response_type=WhoisQueryResponseType.ERROR_USER,
                 mode=WhoisQueryResponseMode.RIPE,
-                result=str(exc)
+                result=str(exc),
             )
         except Exception as exc:
             logger.error(f'An exception occurred while processing whois query "{query}": {exc}', exc_info=exc)
             return WhoisQueryResponse(
                 response_type=WhoisQueryResponseType.ERROR_INTERNAL,
                 mode=WhoisQueryResponseMode.RIPE,
-                result='An internal error occurred while processing this query.'
+                result="An internal error occurred while processing this query.",
             )
 
     def handle_irrd_command(self, full_command: str) -> WhoisQueryResponse:
-        """Handle an IRRD-style query. full_command should not include the first exclamation mark. """
+        """Handle an IRRD-style query. full_command should not include the first exclamation mark."""
         if not full_command:
-            raise InvalidQueryException('Missing IRRD command')
+            raise InvalidQueryException("Missing IRRD command")
         command = full_command[0]
         parameter = full_command[1:]
         response_type = WhoisQueryResponseType.SUCCESS
         result = None
 
         # A is not tested here because it is already handled in handle_irrd_routes_for_as_set
-        queries_with_parameter = list('tg6ijmnors')
+        queries_with_parameter = list("tg6ijmnors")
         if command in queries_with_parameter and not parameter:
-            raise InvalidQueryException(f'Missing parameter for {command} query')
+            raise InvalidQueryException(f"Missing parameter for {command} query")
 
-        if command == '!':
+        if command == "!":
             self.multiple_command_mode = True
             result = None
             response_type = WhoisQueryResponseType.NO_RESPONSE
-        elif full_command.upper() == 'FNO-RPKI-FILTER':
+        elif full_command.upper() == "FNO-RPKI-FILTER":
             self.query_resolver.disable_rpki_filter()
-            result = 'Filtering out RPKI invalids is disabled for !r and RIPE style ' \
-                     'queries for the rest of this connection.'
-        elif full_command.upper() == 'FNO-SCOPE-FILTER':
+            result = (
+                "Filtering out RPKI invalids is disabled for !r and RIPE style "
+                "queries for the rest of this connection."
+            )
+        elif full_command.upper() == "FNO-SCOPE-FILTER":
             self.query_resolver.disable_out_of_scope_filter()
-            result = 'Filtering out out-of-scope objects is disabled for !r and RIPE style ' \
-                     'queries for the rest of this connection.'
-        elif command == 'v':
+            result = (
+                "Filtering out out-of-scope objects is disabled for !r and RIPE style "
+                "queries for the rest of this connection."
+            )
+        elif full_command.upper() == "FNO-ROUTE-PREFERENCE-FILTER":
+            self.query_resolver.disable_route_preference_filter()
+            result = (
+                "Filtering out objects suppressed due to route preference is disabled for "
+                "!r and RIPE style queries for the rest of this connection."
+            )
+        elif command == "v":
             result = self.handle_irrd_version()
-        elif command == 't':
+        elif command == "t":
             self.handle_irrd_timeout_update(parameter)
-        elif command == 'g':
+        elif command == "g":
             result = self.handle_irrd_routes_for_origin_v4(parameter)
             if not result:
                 response_type = WhoisQueryResponseType.KEY_NOT_FOUND
-        elif command == '6':
+        elif command == "6":
             result = self.handle_irrd_routes_for_origin_v6(parameter)
             if not result:
                 response_type = WhoisQueryResponseType.KEY_NOT_FOUND
-        elif command == 'a':
+        elif command == "a":
             result = self.handle_irrd_routes_for_as_set(parameter)
             if not result:
                 response_type = WhoisQueryResponseType.KEY_NOT_FOUND
-        elif command == 'i':
+        elif command == "i":
             result = self.handle_irrd_set_members(parameter)
             if not result:
                 response_type = WhoisQueryResponseType.KEY_NOT_FOUND
-        elif command == 'j':
+        elif command == "j":
             result = self.handle_irrd_database_serial_range(parameter)
-        elif command == 'J':
+        elif command == "J":
             result = self.handle_irrd_database_status(parameter)
-        elif command == 'm':
+        elif command == "m":
             result = self.handle_irrd_exact_key(parameter)
             if not result:
                 response_type = WhoisQueryResponseType.KEY_NOT_FOUND
-        elif command == 'n':
+        elif command == "n":
             self.handle_user_agent(parameter)
-        elif command == 'o':
-            result = self.handle_inverse_attr_search('mnt-by', parameter)
+        elif command == "o":
+            result = self.handle_inverse_attr_search("mnt-by", parameter)
             if not result:
                 response_type = WhoisQueryResponseType.KEY_NOT_FOUND
-        elif command == 'r':
+        elif command == "r":
             result = self.handle_irrd_route_search(parameter)
             if not result:
                 response_type = WhoisQueryResponseType.KEY_NOT_FOUND
-        elif command == 's':
+        elif command == "s":
             result = self.handle_irrd_sources_list(parameter)
         else:
-            raise InvalidQueryException(f'Unrecognised command: {command}')
+            raise InvalidQueryException(f"Unrecognised command: {command}")
 
         return WhoisQueryResponse(
             response_type=response_type,
             mode=WhoisQueryResponseMode.IRRD,
             result=result,
         )
 
     def handle_irrd_timeout_update(self, timeout: str) -> None:
         """!timeout query - update timeout in connection"""
         try:
             timeout_value = int(timeout)
         except ValueError:
-            raise InvalidQueryException(f'Invalid value for timeout: {timeout}')
+            raise InvalidQueryException(f"Invalid value for timeout: {timeout}")
 
         if timeout_value > 0 and timeout_value <= 1000:
             self.timeout = timeout_value
         else:
-            raise InvalidQueryException(f'Invalid value for timeout: {timeout}')
+            raise InvalidQueryException(f"Invalid value for timeout: {timeout}")
 
     def handle_irrd_routes_for_origin_v4(self, origin: str) -> str:
         """!g query - find all originating IPv4 prefixes from an origin, e.g. !gAS65537"""
         return self._routes_for_origin(origin, 4)
 
     def handle_irrd_routes_for_origin_v6(self, origin: str) -> str:
         """!6 query - find all originating IPv6 prefixes from an origin, e.g. !6as65537"""
         return self._routes_for_origin(origin, 6)
 
-    def _routes_for_origin(self, origin: str, ip_version: Optional[int]=None) -> str:
+    def _routes_for_origin(self, origin: str, ip_version: Optional[int] = None) -> str:
         """
         Resolve all route(6)s prefixes for an origin, returning a space-separated list
         of all originating prefixes, not including duplicates.
         """
         try:
             origin_formatted, _ = parse_as_number(origin)
         except ValidationError as ve:
             raise InvalidQueryException(str(ve))
 
         prefixes = self.query_resolver.routes_for_origin(origin_formatted, ip_version)
-        return ' '.join(prefixes)
+        return " ".join(prefixes)
 
     def handle_irrd_routes_for_as_set(self, set_name: str) -> str:
         """
         !a query - find all originating prefixes for all members of an AS-set, e.g. !a4AS-FOO or !a6AS-FOO
         """
         ip_version: Optional[int] = None
-        if set_name.startswith('4'):
+        if set_name.startswith("4"):
             set_name = set_name[1:]
             ip_version = 4
-        elif set_name.startswith('6'):
+        elif set_name.startswith("6"):
             set_name = set_name[1:]
             ip_version = 6
 
         if not set_name:
-            raise InvalidQueryException('Missing required set name for A query')
+            raise InvalidQueryException("Missing required set name for A query")
 
         prefixes = self.query_resolver.routes_for_as_set(set_name, ip_version)
-        return ' '.join(prefixes)
+        return " ".join(prefixes)
 
     def handle_irrd_set_members(self, parameter: str) -> str:
         """
         !i query - find all members of an as-set or route-set, possibly recursively.
         e.g. !iAS-FOO for non-recursive, !iAS-FOO,1 for recursive
         """
         recursive = False
-        if parameter.endswith(',1'):
+        if parameter.endswith(",1"):
             recursive = True
             parameter = parameter[:-2]
 
         members = self.query_resolver.members_for_set(parameter, recursive=recursive)
-        return ' '.join(members)
+        return " ".join(members)
 
     def handle_irrd_database_serial_range(self, parameter: str) -> str:
         """
         !j query - database serial range
         This query is legacy and only available in whois, so resolved
         directly here instead of in the query resolver.
         """
-        if parameter == '-*':
-            sources = self.query_resolver.sources_default if self.query_resolver.sources_default else self.query_resolver.all_valid_sources
+        if parameter == "-*":
+            sources = (
+                self.query_resolver.sources_default
+                if self.query_resolver.sources_default
+                else self.query_resolver.all_valid_sources
+            )
         else:
-            sources = [s.upper() for s in parameter.split(',')]
+            sources = [s.upper() for s in parameter.split(",")]
         invalid_sources = [s for s in sources if s not in self.query_resolver.all_valid_sources]
         query = DatabaseStatusQuery().sources(sources)
         query_results = self.database_handler.execute_query(query, refresh_on_error=True)
 
-        result_txt = ''
+        result_txt = ""
         for query_result in query_results:
-            source = query_result['source'].upper()
-            keep_journal = 'Y' if get_setting(f'sources.{source}.keep_journal') else 'N'
-            serial_newest = query_result['serial_newest_mirror']
+            source = query_result["source"].upper()
+            keep_journal = "Y" if get_setting(f"sources.{source}.keep_journal") else "N"
+            serial_newest = query_result["serial_newest_mirror"]
             fields = [
                 source,
                 keep_journal,
-                f'0-{serial_newest}' if serial_newest else '-',
+                f"0-{serial_newest}" if serial_newest else "-",
             ]
-            if query_result['serial_last_export']:
-                fields.append(str(query_result['serial_last_export']))
-            result_txt += ':'.join(fields) + '\n'
+            if query_result["serial_last_export"]:
+                fields.append(str(query_result["serial_last_export"]))
+            result_txt += ":".join(fields) + "\n"
 
         for invalid_source in invalid_sources:
-            result_txt += f'{invalid_source.upper()}:X:Database unknown\n'
+            result_txt += f"{invalid_source.upper()}:X:Database unknown\n"
         return result_txt.strip()
 
     def handle_irrd_database_status(self, parameter: str) -> str:
         """!J query - database status"""
-        if parameter == '-*':
+        if parameter == "-*":
             sources = None
         else:
-            sources = [s.upper() for s in parameter.split(',')]
+            sources = [s.upper() for s in parameter.split(",")]
         results = self.query_resolver.database_status(sources)
         return ujson.dumps(results, indent=4)
 
     def handle_irrd_exact_key(self, parameter: str):
         """!m query - exact object key lookup, e.g. !maut-num,AS65537"""
         try:
-            object_class, rpsl_pk = parameter.split(',', maxsplit=1)
+            object_class, rpsl_pk = parameter.split(",", maxsplit=1)
         except ValueError:
-            raise InvalidQueryException(f'Invalid argument for object lookup: {parameter}')
+            raise InvalidQueryException(f"Invalid argument for object lookup: {parameter}")
 
-        if object_class in ['route', 'route6']:
-            rpsl_pk = rpsl_pk.upper().replace(' ', '').replace('-', '')
+        if object_class in ["route", "route6"]:
+            rpsl_pk = rpsl_pk.upper().replace(" ", "").replace("-", "")
         query = self.query_resolver.key_lookup(object_class, rpsl_pk)
         return self._flatten_query_output(query)
 
     def handle_irrd_route_search(self, parameter: str):
         """
         !r query - route search with various options:
            !r192.0.2.0/24 returns all exact matching objects
            !r192.0.2.0/24,o returns space-separated origins of all exact matching objects
            !r192.0.2.0/24,l returns all one-level less specific objects, not including exact
            !r192.0.2.0/24,L returns all less specific objects, including exact
            !r192.0.2.0/24,M returns all more specific objects, not including exact
         """
         option: Optional[str] = None
-        if ',' in parameter:
-            address, option = parameter.split(',')
+        if "," in parameter:
+            address, option = parameter.split(",")
         else:
             address = parameter
         try:
             address = IP(address)
         except ValueError:
-            raise InvalidQueryException(f'Invalid input for route search: {parameter}')
+            raise InvalidQueryException(f"Invalid input for route search: {parameter}")
 
         lookup_types = {
             None: RouteLookupType.EXACT,
-            'o': RouteLookupType.EXACT,
-            'l': RouteLookupType.LESS_SPECIFIC_ONE_LEVEL,
-            'L': RouteLookupType.LESS_SPECIFIC_WITH_EXACT,
-            'M': RouteLookupType.MORE_SPECIFIC_WITHOUT_EXACT,
+            "o": RouteLookupType.EXACT,
+            "l": RouteLookupType.LESS_SPECIFIC_ONE_LEVEL,
+            "L": RouteLookupType.LESS_SPECIFIC_WITH_EXACT,
+            "M": RouteLookupType.MORE_SPECIFIC_WITHOUT_EXACT,
         }
         try:
             lookup_type = lookup_types[option]
         except KeyError:
-            raise InvalidQueryException(f'Invalid route search option: {option}')
+            raise InvalidQueryException(f"Invalid route search option: {option}")
 
         result = self.query_resolver.route_search(address, lookup_type)
-        if option == 'o':
-            prefixes = [r['parsed_data']['origin'] for r in result]
-            return ' '.join(prefixes)
+        if option == "o":
+            prefixes = [r["parsed_data"]["origin"] for r in result]
+            return " ".join(prefixes)
         return self._flatten_query_output(result)
 
     def handle_irrd_sources_list(self, parameter: str) -> Optional[str]:
         """
         !s query - set used sources
            !s-lc returns all enabled sources, space separated
            !sripe,nttcom limits sources to ripe and nttcom
         """
-        if parameter == '-lc':
-            return ','.join(self.query_resolver.sources)
+        if parameter == "-lc":
+            return ",".join(self.query_resolver.sources)
 
-        sources = parameter.upper().split(',')
+        sources = parameter.upper().split(",")
         self.query_resolver.set_query_sources(sources)
         return None
 
     def handle_irrd_version(self):
         """!v query - return version"""
-        return f'IRRd -- version {__version__}'
+        return f"IRRd -- version {__version__}"
 
     def handle_ripe_command(self, full_query: str) -> WhoisQueryResponse:
         """
         Process RIPE-style queries. Any query that is not explicitly an IRRD-style
         query (i.e. starts with exclamation mark) is presumed to be a RIPE query.
         """
-        full_query = re.sub(' +', ' ', full_query)
-        components = full_query.strip().split(' ')
+        full_query = re.sub(" +", " ", full_query)
+        components = full_query.strip().split(" ")
         result = None
         response_type = WhoisQueryResponseType.SUCCESS
         remove_auth_hashes = True
 
         while len(components):
             component = components.pop(0)
-            if component.startswith('-'):
+            if component.startswith("-"):
                 command = component[1:]
                 try:
-                    if command == 'k':
+                    if command == "k":
                         self.multiple_command_mode = True
-                    elif command in ['l', 'L', 'M', 'x']:
+                    elif command in ["l", "L", "M", "x"]:
                         result = self.handle_ripe_route_search(command, components.pop(0))
                         if not result:
                             response_type = WhoisQueryResponseType.KEY_NOT_FOUND
                         break
-                    elif command == 'i':
+                    elif command == "i":
                         result = self.handle_inverse_attr_search(components.pop(0), components.pop(0))
                         if not result:
                             response_type = WhoisQueryResponseType.KEY_NOT_FOUND
                         break
-                    elif command == 's':
+                    elif command == "s":
                         self.handle_ripe_sources_list(components.pop(0))
-                    elif command == 'a':
+                    elif command == "a":
                         self.handle_ripe_sources_list(None)
-                    elif command == 'T':
+                    elif command == "T":
                         self.handle_ripe_restrict_object_class(components.pop(0))
-                    elif command == 't':
+                    elif command == "t":
                         result = self.handle_ripe_request_object_template(components.pop(0))
                         break
-                    elif command == 'K':
+                    elif command == "K":
                         self.handle_ripe_key_fields_only()
-                    elif command == 'V':
+                    elif command == "V":
                         self.handle_user_agent(components.pop(0))
-                    elif command == 'g':
+                    elif command == "g":
                         result = self.handle_nrtm_request(components.pop(0))
                         remove_auth_hashes = False
-                    elif command in ['F', 'r']:
+                    elif command in ["F", "r"]:
                         continue  # These flags disable recursion, but IRRd never performs recursion anyways
                     else:
-                        raise InvalidQueryException(f'Unrecognised flag/search: {command}')
+                        raise InvalidQueryException(f"Unrecognised flag/search: {command}")
                 except IndexError:
-                    raise InvalidQueryException(f'Missing argument for flag/search: {command}')
+                    raise InvalidQueryException(f"Missing argument for flag/search: {command}")
             else:  # assume query to be a free text search
                 result = self.handle_ripe_text_search(component)
 
         return WhoisQueryResponse(
             response_type=response_type,
             mode=WhoisQueryResponseMode.RIPE,
             result=result,
@@ -403,37 +438,37 @@
            -l 192.0.2.0/2 returns all one-level less specific objects, not including exact
            -L 192.0.2.0/2 returns all less specific objects, including exact
            -M 192.0.2.0/2 returns all more specific objects, not including exact
         """
         try:
             address = IP(parameter)
         except ValueError:
-            raise InvalidQueryException(f'Invalid input for route search: {parameter}')
+            raise InvalidQueryException(f"Invalid input for route search: {parameter}")
 
         lookup_types = {
-            'x': RouteLookupType.EXACT,
-            'l': RouteLookupType.LESS_SPECIFIC_ONE_LEVEL,
-            'L': RouteLookupType.LESS_SPECIFIC_WITH_EXACT,
-            'M': RouteLookupType.MORE_SPECIFIC_WITHOUT_EXACT,
+            "x": RouteLookupType.EXACT,
+            "l": RouteLookupType.LESS_SPECIFIC_ONE_LEVEL,
+            "L": RouteLookupType.LESS_SPECIFIC_WITH_EXACT,
+            "M": RouteLookupType.MORE_SPECIFIC_WITHOUT_EXACT,
         }
         lookup_type = lookup_types[command]
         result = self.query_resolver.route_search(address, lookup_type)
         return self._flatten_query_output(result)
 
     def handle_ripe_sources_list(self, sources_list: Optional[str]) -> None:
-        """-s/-a parameter - set sources list. Empty list enables all sources. """
+        """-s/-a parameter - set sources list. Empty list enables all sources."""
         if sources_list:
-            sources = sources_list.upper().split(',')
+            sources = sources_list.upper().split(",")
             self.query_resolver.set_query_sources(sources)
         else:
             self.query_resolver.set_query_sources(None)
 
     def handle_ripe_restrict_object_class(self, object_classes) -> None:
         """-T parameter - restrict object classes for this query, comma-seperated"""
-        self.query_resolver.set_object_class_filter_next_query(object_classes.split(','))
+        self.query_resolver.set_object_class_filter_next_query(object_classes.split(","))
 
     def handle_ripe_request_object_template(self, object_class) -> str:
         """-t query - return the RPSL template for an object class"""
         return self.query_resolver.rpsl_object_template(object_class)
 
     def handle_ripe_key_fields_only(self) -> None:
         """-K paramater - only return primary key and members fields"""
@@ -442,48 +477,55 @@
     def handle_ripe_text_search(self, value: str) -> str:
         result = self.query_resolver.rpsl_text_search(value)
         return self._flatten_query_output(result)
 
     def handle_user_agent(self, user_agent: str):
         """-V/!n parameter/query - set a user agent for the client"""
         self.query_resolver.user_agent = user_agent
-        logger.info(f'{self.client_str}: user agent set to: {user_agent}')
+        logger.info(f"{self.client_str}: user agent set to: {user_agent}")
 
     def handle_nrtm_request(self, param):
         try:
-            source, version, serial_range = param.split(':')
+            source, version, serial_range = param.split(":")
         except ValueError:
-            raise InvalidQueryException('Invalid parameter: must contain three elements')
+            raise InvalidQueryException("Invalid parameter: must contain three elements")
 
         try:
-            serial_start, serial_end = serial_range.split('-')
+            serial_start, serial_end = serial_range.split("-")
             serial_start = int(serial_start)
-            if serial_end == 'LAST':
+            if serial_end == "LAST":
                 serial_end = None
             else:
                 serial_end = int(serial_end)
         except ValueError:
-            raise InvalidQueryException(f'Invalid serial range: {serial_range}')
+            raise InvalidQueryException(f"Invalid serial range: {serial_range}")
 
-        if version not in ['1', '3']:
-            raise InvalidQueryException(f'Invalid NRTM version: {version}')
+        if version not in ["1", "3"]:
+            raise InvalidQueryException(f"Invalid NRTM version: {version}")
 
         source = source.upper()
         if source not in self.query_resolver.all_valid_sources:
-            raise InvalidQueryException(f'Unknown source: {source}')
+            raise InvalidQueryException(f"Unknown source: {source}")
 
-        in_access_list = is_client_permitted(self.client_ip, f'sources.{source}.nrtm_access_list', log=False)
-        in_unfiltered_access_list = is_client_permitted(self.client_ip, f'sources.{source}.nrtm_access_list_unfiltered', log=False)
+        in_access_list = is_client_permitted(self.client_ip, f"sources.{source}.nrtm_access_list", log=False)
+        in_unfiltered_access_list = is_client_permitted(
+            self.client_ip, f"sources.{source}.nrtm_access_list_unfiltered", log=False
+        )
         if not in_access_list and not in_unfiltered_access_list:
-            raise InvalidQueryException('Access denied')
+            raise InvalidQueryException("Access denied")
 
         try:
             return NRTMGenerator().generate(
-                source, version, serial_start, serial_end, self.database_handler,
-                remove_auth_hashes=not in_unfiltered_access_list)
+                source,
+                version,
+                serial_start,
+                serial_end,
+                self.database_handler,
+                remove_auth_hashes=not in_unfiltered_access_list,
+            )
         except NRTMGeneratorException as nge:
             raise InvalidQueryException(str(nge))
 
     def handle_inverse_attr_search(self, attribute: str, value: str) -> str:
         """
         -i/!o query - inverse search for attribute values
         e.g. `-i mnt-by FOO` finds all objects where (one of the) maintainer(s) is FOO,
@@ -496,39 +538,39 @@
         """
         Flatten an RPSL database response into a string with object text
         for easy passing to a WhoisQueryResponse.
         """
         if self.key_fields_only:
             result = self._filter_key_fields(query_response)
         else:
-            result = ''
+            result = ""
             for obj in query_response:
-                result += obj['object_text']
+                result += obj["object_text"]
                 if (
-                        self.query_resolver.rpki_aware and
-                        obj['source'] != RPKI_IRR_PSEUDO_SOURCE and
-                        obj['object_class'] in RPKI_RELEVANT_OBJECT_CLASSES
+                    self.query_resolver.rpki_aware
+                    and obj["source"] != RPKI_IRR_PSEUDO_SOURCE
+                    and obj["object_class"] in RPKI_RELEVANT_OBJECT_CLASSES
                 ):
-                    comment = ''
-                    if obj['rpki_status'] == RPKIStatus.not_found:
-                        comment = ' # No ROAs found, or RPKI validation not enabled for source'
+                    comment = ""
+                    if obj["rpki_status"] == RPKIStatus.not_found:
+                        comment = " # No ROAs found, or RPKI validation not enabled for source"
                     result += f'rpki-ov-state:  {obj["rpki_status"].name}{comment}\n'
-                result += '\n'
-        return result.strip('\n\r')
+                result += "\n"
+        return result.strip("\n\r")
 
     def _filter_key_fields(self, query_response) -> str:
         results: OrderedSet[str] = OrderedSet()
         for obj in query_response:
-            result = ''
-            rpsl_object_class = OBJECT_CLASS_MAPPING[obj['object_class']]
-            fields_included = rpsl_object_class.pk_fields + ['members', 'mp-members']
+            result = ""
+            rpsl_object_class = OBJECT_CLASS_MAPPING[obj["object_class"]]
+            fields_included = rpsl_object_class.pk_fields + ["members", "mp-members"]
 
             for field_name in fields_included:
-                field_data = obj['parsed_data'].get(field_name)
+                field_data = obj["parsed_data"].get(field_name)
                 if field_data:
                     if isinstance(field_data, list):
                         for item in field_data:
-                            result += f'{field_name}: {item}\n'
+                            result += f"{field_name}: {item}\n"
                     else:
-                        result += f'{field_name}: {field_data}\n'
+                        result += f"{field_name}: {field_data}\n"
             results.add(result)
-        return '\n'.join(results)
+        return "\n".join(results)
```

### Comparing `irrd-4.2.8/irrd/server/whois/query_response.py` & `irrd-4.3.0/irrd/server/whois/query_response.py`

 * *Files 11% similar despite different names*

```diff
@@ -6,47 +6,50 @@
 
 class WhoisQueryResponseType(Enum):
     """
     Types of responses to queries.
     KEY_NOT_FOUND is specific to IRRD-style.
     NO_RESPONSE means no response should be sent at all.
     """
-    SUCCESS = 'success'
-    ERROR_INTERNAL = 'error_internal'
-    ERROR_USER = 'error_user'
-    KEY_NOT_FOUND = 'key_not_found'
-    NO_RESPONSE = 'no_response'
+
+    SUCCESS = "success"
+    ERROR_INTERNAL = "error_internal"
+    ERROR_USER = "error_user"
+    KEY_NOT_FOUND = "key_not_found"
+    NO_RESPONSE = "no_response"
 
 
 ERROR_TYPES = [WhoisQueryResponseType.ERROR_INTERNAL, WhoisQueryResponseType.ERROR_USER]
 
 
 class WhoisQueryResponseMode(Enum):
     """Response mode for queries - IRRD and RIPE queries have different output."""
-    IRRD = 'irrd'
-    RIPE = 'ripe'
+
+    IRRD = "irrd"
+    RIPE = "ripe"
 
 
 class WhoisQueryResponse:
     """
     Container for all data for a response to a query.
 
     Based on the response_type and mode, can render a string of the complete
     response to send back to the user.
     """
+
     response_type: WhoisQueryResponseType = WhoisQueryResponseType.SUCCESS
     mode: WhoisQueryResponseMode = WhoisQueryResponseMode.RIPE
     result: Optional[str] = None
 
     def __init__(
-            self,
-            response_type: WhoisQueryResponseType,
-            mode: WhoisQueryResponseMode,
-            result: Optional[str],
-            remove_auth_hashes=True,
+        self,
+        response_type: WhoisQueryResponseType,
+        mode: WhoisQueryResponseMode,
+        result: Optional[str],
+        remove_auth_hashes=True,
     ) -> None:
         self.response_type = response_type
         self.mode = mode
         self.result = result
         self.remove_auth_hashes = remove_auth_hashes
 
     def generate_response(self) -> str:
@@ -58,41 +61,43 @@
                 return response
 
         elif self.mode == WhoisQueryResponseMode.RIPE:
             response = self._generate_response_ripe()
             if response is not None:
                 return response
 
-        raise RuntimeError(f'Unable to formulate response for {self.response_type} / {self.mode}: {self.result}')
+        raise RuntimeError(
+            f"Unable to formulate response for {self.response_type} / {self.mode}: {self.result}"
+        )
 
     def clean_response(self):
         if self.remove_auth_hashes:
             self.result = remove_auth_hashes(self.result)
 
     def _generate_response_irrd(self) -> Optional[str]:
         if self.response_type == WhoisQueryResponseType.SUCCESS:
             if self.result:
                 result_len = len(self.result) + 1
-                return f'A{result_len}\n{self.result}\nC\n'
+                return f"A{result_len}\n{self.result}\nC\n"
             else:
-                return 'C\n'
+                return "C\n"
         elif self.response_type == WhoisQueryResponseType.KEY_NOT_FOUND:
-            return 'D\n'
+            return "D\n"
         elif self.response_type in ERROR_TYPES:
-            return f'F {self.result}\n'
+            return f"F {self.result}\n"
         elif self.response_type == WhoisQueryResponseType.NO_RESPONSE:
-            return ''
+            return ""
         return None
 
     def _generate_response_ripe(self) -> Optional[str]:
         # RIPE-style responses need two empty lines at the end, hence
         # the multiple newlines for each response (#335)
         # # https://www.ripe.net/manage-ips-and-asns/db/support/documentation/ripe-database-query-reference-manual#2-0-querying-the-ripe-database
         if self.response_type == WhoisQueryResponseType.SUCCESS:
             if self.result:
-                return self.result + '\n\n\n'
-            return '%  No entries found for the selected source(s).\n\n\n'
+                return self.result + "\n\n\n"
+            return "%  No entries found for the selected source(s).\n\n\n"
         elif self.response_type == WhoisQueryResponseType.KEY_NOT_FOUND:
-            return '%  No entries found for the selected source(s).\n\n\n'
+            return "%  No entries found for the selected source(s).\n\n\n"
         elif self.response_type in ERROR_TYPES:
-            return f'%% ERROR: {self.result}\n\n\n'
+            return f"%% ERROR: {self.result}\n\n\n"
         return None
```

### Comparing `irrd-4.2.8/irrd/server/whois/server.py` & `irrd-4.3.0/irrd/server/whois/server.py`

 * *Files 6% similar despite different names*

```diff
@@ -4,19 +4,19 @@
 import signal
 import socket
 import socketserver
 import threading
 import time
 
 from IPy import IP
-from daemon.daemon import change_process_owner
 from setproctitle import setproctitle
 
+from daemon.daemon import change_process_owner
 from irrd import ENV_MAIN_PROCESS_PID
-from irrd.conf import SOCKET_DEFAULT_TIMEOUT, get_setting
+from irrd.conf import get_setting
 from irrd.server.access_check import is_client_permitted
 from irrd.server.whois.query_parser import WhoisQueryParser
 from irrd.storage.database_handler import DatabaseHandler
 from irrd.storage.preload import Preloader
 from irrd.utils.process_support import memory_trim
 
 logger = logging.getLogger(__name__)
@@ -25,69 +25,72 @@
 
 # Covered by integration tests
 def start_whois_server(uid, gid):  # pragma: no cover
     """
     Start the whois server, listening forever.
     This function does not return, except after SIGTERM is received.
     """
-    setproctitle('irrd-whois-server-listener')
-    address = (get_setting('server.whois.interface'), get_setting('server.whois.port'))
-    logger.info(f'Starting whois server on TCP {address}')
+    setproctitle("irrd-whois-server-listener")
+    address = (get_setting("server.whois.interface"), get_setting("server.whois.port"))
+    logger.info(f"Starting whois server on TCP {address}")
     server = WhoisTCPServer(
         server_address=address,
         uid=uid,
         gid=gid,
     )
 
     # When this process receives SIGTERM, shut down the server cleanly.
     def sigterm_handler(signum, frame):
         nonlocal server
 
         def shutdown(server):
-            logging.info('Whois server shutting down')
+            logging.info("Whois server shutting down")
             server.shutdown()
             server.server_close()
+
         # Shutdown must be called from a thread to prevent blocking.
         threading.Thread(target=shutdown, args=(server,)).start()
+
     signal.signal(signal.SIGTERM, sigterm_handler)
 
     server.serve_forever()
 
 
 class WhoisTCPServer(socketserver.TCPServer):  # pragma: no cover
     """
     Server for whois queries.
 
     Starts a number of worker processes that handle the client connections.
     Whenever a client is connected, the connection is pushed onto a queue,
     from which a worker picks it up. The workers are responsible for the
     connection from then on.
     """
+
     allow_reuse_address = True
     request_queue_size = 50
 
     def __init__(self, server_address, uid, gid, bind_and_activate=True):  # noqa: N803
         self.address_family = socket.AF_INET6 if IP(server_address[0]).version() == 6 else socket.AF_INET
         super().__init__(server_address, None, bind_and_activate)
         if uid and gid:
             change_process_owner(uid=uid, gid=gid, initgroups=True)
 
         self.connection_queue = mp.Queue()
         self.workers = []
-        for i in range(int(get_setting('server.whois.max_connections'))):
+        for i in range(int(get_setting("server.whois.max_connections"))):
             worker = WhoisWorker(self.connection_queue)
             worker.start()
             self.workers.append(worker)
 
     def process_request(self, request, client_address):
         """Push the client connection onto the queue for further handling."""
         self.connection_queue.put((request, client_address))
 
     def handle_error(self, request, client_address):
-        logger.error(f'Error while handling request from {client_address}', exc_info=True)
+        logger.error(f"Error while handling request from {client_address}", exc_info=True)
 
     def shutdown(self):
         """
         Shut down the server, by killing all child processes,
         and then deferring to built-in TCPServer shutdown.
         """
         for worker in self.workers:
@@ -101,14 +104,15 @@
 
 class WhoisWorker(mp.Process, socketserver.StreamRequestHandler):
     """
     A whois worker is a process that handles whois client connections,
     which are retrieved from a queue. After handling a connection,
     the process waits for the next connection from the queue.s
     """
+
     def __init__(self, connection_queue, *args, **kwargs):
         self.connection_queue = connection_queue
         # Note that StreamRequestHandler.__init__ is not called - the
         # input for that is not available, as it's retrieved from the queue.
         super().__init__(*args, **kwargs)
 
     def run(self, keep_running=True) -> None:
@@ -122,111 +126,121 @@
         # (signal handlers are inherited)
         signal.signal(signal.SIGTERM, signal.SIG_DFL)
 
         try:
             self.preloader = Preloader()
             self.database_handler = DatabaseHandler(readonly=True)
         except Exception as e:
-            logger.critical(f'Whois worker failed to initialise preloader or database, '
-                            f'unable to start, terminating IRRd, traceback follows: {e}',
-                            exc_info=e)
+            logger.critical(
+                (
+                    "Whois worker failed to initialise preloader or database, "
+                    f"unable to start, terminating IRRd, traceback follows: {e}"
+                ),
+                exc_info=e,
+            )
             main_pid = os.getenv(ENV_MAIN_PROCESS_PID)
             if main_pid:  # pragma: no cover
                 os.kill(int(main_pid), signal.SIGTERM)
             else:
-                logger.error('Failed to terminate IRRd, unable to find main process PID')
+                logger.error("Failed to terminate IRRd, unable to find main process PID")
             return
 
         while True:
             try:
-                setproctitle('irrd-whois-worker')
+                setproctitle("irrd-whois-worker")
                 self.request, self.client_address = self.connection_queue.get()
-                self.request.settimeout(SOCKET_DEFAULT_TIMEOUT)
                 self.setup()
                 self.handle_connection()
                 self.finish()
                 self.close_request()
                 memory_trim()
             except Exception as e:
                 try:
                     self.close_request()
                 except Exception:  # pragma: no cover
                     pass
-                logger.error(f'Failed to handle whois connection, traceback follows: {e}',
-                             exc_info=e)
+                logger.error(f"Failed to handle whois connection, traceback follows: {e}", exc_info=e)
             if not keep_running:
                 break
 
     def close_request(self):
-        # Close the connection in the same way normally done by TCPServer
+        # Close the connection in a similar way normally done by TCPServer
+        try:
+            # Try to set the timeout of the shutdown call (#607)
+            self.request.settimeout(5)
+        except OSError:  # pragma: no cover
+            pass
         try:
             # explicitly shutdown.  socket.close() merely releases
             # the socket and waits for GC to perform the actual close.
             self.request.shutdown(socket.SHUT_RDWR)
         except OSError:  # pragma: no cover
             pass  # some platforms may raise ENOTCONN here
         self.request.close()
 
     def handle_connection(self):
         """
         Handle an individual whois client connection.
         When this method returns, the connection is closed.
         """
         client_ip = self.client_address[0]
-        self.client_str = client_ip + ':' + str(self.client_address[1])
-        setproctitle(f'irrd-whois-worker-{self.client_str}')
+        self.client_str = client_ip + ":" + str(self.client_address[1])
+        setproctitle(f"irrd-whois-worker-{self.client_str}")
 
         if not self.is_client_permitted(client_ip):
-            self.wfile.write(b'%% Access denied')
+            self.wfile.write(b"%% Access denied")
             return
 
-        self.query_parser = WhoisQueryParser(client_ip, self.client_str, self.preloader,
-                                             self.database_handler)
+        self.query_parser = WhoisQueryParser(
+            client_ip, self.client_str, self.preloader, self.database_handler
+        )
 
         data = True
         while data:
             timer = threading.Timer(self.query_parser.timeout, self.close_request)
             timer.start()
             data = self.rfile.readline()
             timer.cancel()
 
-            query = data.decode('utf-8', errors='backslashreplace').strip()
+            query = data.decode("utf-8", errors="backslashreplace").strip()
             if not query:
                 continue
 
-            logger.debug(f'{self.client_str}: processing query: {query}')
+            logger.debug(f"{self.client_str}: processing query: {query}")
 
             if not self.handle_query(query):
                 return
 
     def handle_query(self, query: str) -> bool:
         """
         Handle an individual query.
         Returns False when the connection should be closed,
         True when more queries should be read.
         """
         start_time = time.perf_counter()
-        if query.upper() == '!Q':
-            logger.debug(f'{self.client_str}: closed connection per request')
+        if query.upper() == "!Q":
+            logger.debug(f"{self.client_str}: closed connection per request")
             return False
 
         response = self.query_parser.handle_query(query)
-        response_bytes = response.generate_response().encode('utf-8')
+        response_bytes = response.generate_response().encode("utf-8")
         try:
             self.wfile.write(response_bytes)
         except OSError:
             return False
 
         elapsed = time.perf_counter() - start_time
-        logger.info(f'{self.client_str}: sent answer to query, elapsed {elapsed:.9f}s, '
-                    f'{len(response_bytes)} bytes: {query}')
+        logger.info(
+            f"{self.client_str}: sent answer to query, elapsed {elapsed:.9f}s, "
+            f"{len(response_bytes)} bytes: {query}"
+        )
 
         if not self.query_parser.multiple_command_mode:
-            logger.debug(f'{self.client_str}: auto-closed connection')
+            logger.debug(f"{self.client_str}: auto-closed connection")
             return False
         return True
 
     def is_client_permitted(self, ip: str) -> bool:
         """
         Check whether a client is permitted.
         """
-        return is_client_permitted(ip, 'server.whois.access_list', default_deny=False)
+        return is_client_permitted(ip, "server.whois.access_list", default_deny=False)
```

### Comparing `irrd-4.2.8/irrd/server/whois/tests/test_query_parser.py` & `irrd-4.3.0/irrd/server/whois/tests/test_query_parser.py`

 * *Files 5% similar despite different names*

```diff
@@ -2,19 +2,24 @@
 from unittest.mock import Mock
 
 import pytest
 from IPy import IP
 
 from irrd.mirroring.nrtm_generator import NRTMGeneratorException
 from irrd.rpki.status import RPKIStatus
-from irrd.server.query_resolver import QueryResolver, RouteLookupType, InvalidQueryException
+from irrd.server.query_resolver import (
+    InvalidQueryException,
+    QueryResolver,
+    RouteLookupType,
+)
 from irrd.storage.database_handler import DatabaseHandler
 from irrd.utils.test_utils import flatten_mock_calls
+
 from ..query_parser import WhoisQueryParser
-from ..query_response import WhoisQueryResponseType, WhoisQueryResponseMode
+from ..query_response import WhoisQueryResponseMode, WhoisQueryResponseType
 
 # Note that these mock objects are not entirely valid RPSL objects,
 # as they are meant to test all the scenarios in the query parser.
 MOCK_ROUTE1 = """route:          192.0.2.0/25
 descr:          description
 origin:         AS65547
 mnt-by:         MNT-TEST
@@ -32,809 +37,864 @@
 MOCK_ROUTE3 = """route:          192.0.2.128/25
 descr:          description
 origin:         AS65545
 mnt-by:         MNT-TEST
 source:         TEST2
 """
 
-MOCK_ROUTE_COMBINED = MOCK_ROUTE1 + '\n' + MOCK_ROUTE2 + '\n' + MOCK_ROUTE3.strip()
-MOCK_ROUTE_COMBINED_WITH_RPKI = MOCK_ROUTE1 + 'rpki-ov-state:  not_found # No ROAs found, or RPKI validation not enabled for source\n\n' + MOCK_ROUTE2 + 'rpki-ov-state:  valid\n\n' + MOCK_ROUTE3 + 'rpki-ov-state:  valid'
+MOCK_ROUTE_COMBINED = MOCK_ROUTE1 + "\n" + MOCK_ROUTE2 + "\n" + MOCK_ROUTE3.strip()
+MOCK_ROUTE_COMBINED_WITH_RPKI = (
+    MOCK_ROUTE1
+    + "rpki-ov-state:  not_found # No ROAs found, or RPKI validation not enabled for source\n\n"
+    + MOCK_ROUTE2
+    + "rpki-ov-state:  valid\n\n"
+    + MOCK_ROUTE3
+    + "rpki-ov-state:  valid"
+)
 
 
 MOCK_ROUTE_COMBINED_KEY_FIELDS = """route: 192.0.2.0/25
 origin: AS65547
 members: AS1, AS2
 
 route: 192.0.2.0/25
 origin: AS65544
 
 route: 192.0.2.128/25
 origin: AS65545"""
 
 MOCK_DATABASE_RESPONSE = [
     {
-        'pk': uuid.uuid4(),
-        'rpsl_pk': '192.0.2.0/25,AS65547',
-        'object_class': 'route',
-        'parsed_data': {
-            'route': '192.0.2.0/25', 'origin': 'AS65547', 'mnt-by': 'MNT-TEST', 'source': 'TEST1',
-            'members': ['AS1, AS2']
+        "pk": uuid.uuid4(),
+        "rpsl_pk": "192.0.2.0/25,AS65547",
+        "object_class": "route",
+        "parsed_data": {
+            "route": "192.0.2.0/25",
+            "origin": "AS65547",
+            "mnt-by": "MNT-TEST",
+            "source": "TEST1",
+            "members": ["AS1, AS2"],
         },
-        'object_text': MOCK_ROUTE1,
-        'rpki_status': RPKIStatus.not_found,
-        'source': 'TEST1',
+        "object_text": MOCK_ROUTE1,
+        "rpki_status": RPKIStatus.not_found,
+        "source": "TEST1",
     },
     {
-        'pk': uuid.uuid4(),
-
-        'rpsl_pk': '192.0.2.0/25,AS65544',
-        'object_class': 'route',
-        'parsed_data': {'route': '192.0.2.0/25', 'origin': 'AS65544', 'mnt-by': 'MNT-TEST',
-                        'source': 'TEST2'},
-        'object_text': MOCK_ROUTE2,
-        'rpki_status': RPKIStatus.valid,
-        'source': 'TEST2',
+        "pk": uuid.uuid4(),
+        "rpsl_pk": "192.0.2.0/25,AS65544",
+        "object_class": "route",
+        "parsed_data": {
+            "route": "192.0.2.0/25",
+            "origin": "AS65544",
+            "mnt-by": "MNT-TEST",
+            "source": "TEST2",
+        },
+        "object_text": MOCK_ROUTE2,
+        "rpki_status": RPKIStatus.valid,
+        "source": "TEST2",
     },
     {
-        'pk': uuid.uuid4(),
-        'rpsl_pk': '192.0.2.128/25,AS65545',
-        'object_class': 'route',
-        'parsed_data': {'route': '192.0.2.128/25', 'origin': 'AS65545', 'mnt-by': 'MNT-TEST',
-                        'source': 'TEST2'},
-        'object_text': MOCK_ROUTE3,
-        'rpki_status': RPKIStatus.valid,
-        'source': 'TEST2',
+        "pk": uuid.uuid4(),
+        "rpsl_pk": "192.0.2.128/25,AS65545",
+        "object_class": "route",
+        "parsed_data": {
+            "route": "192.0.2.128/25",
+            "origin": "AS65545",
+            "mnt-by": "MNT-TEST",
+            "source": "TEST2",
+        },
+        "object_text": MOCK_ROUTE3,
+        "rpki_status": RPKIStatus.valid,
+        "source": "TEST2",
     },
 ]
 
 
 @pytest.fixture()
 def prepare_parser(monkeypatch, config_override):
     mock_query_resolver = Mock(spec=QueryResolver)
     mock_query_resolver.rpki_aware = False
-    monkeypatch.setattr('irrd.server.whois.query_parser.QueryResolver',
-                        lambda preloader, database_handler: mock_query_resolver)
+    monkeypatch.setattr(
+        "irrd.server.whois.query_parser.QueryResolver",
+        lambda preloader, database_handler: mock_query_resolver,
+    )
 
     mock_dh = Mock(spec=DatabaseHandler)
-    parser = WhoisQueryParser('127.0.0.1', '127.0.0.1:99999', None, mock_dh)
+    parser = WhoisQueryParser("127.0.0.1", "127.0.0.1:99999", None, mock_dh)
     yield mock_query_resolver, mock_dh, parser
 
 
 class TestWhoisQueryParserRIPE:
     """Test RIPE-style queries"""
 
     def test_invalid_flag(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
 
-        response = parser.handle_query('-e foo')
+        response = parser.handle_query("-e foo")
         assert response.response_type == WhoisQueryResponseType.ERROR_USER
         assert response.mode == WhoisQueryResponseMode.RIPE
-        assert response.result == 'Unrecognised flag/search: e'
+        assert response.result == "Unrecognised flag/search: e"
+
+    def test_null_bytes(self, prepare_parser):  # #581
+        mock_query_resolver, mock_dh, parser = prepare_parser
+
+        response = parser.handle_query("\x00 foo")
+        assert response.response_type == WhoisQueryResponseType.ERROR_USER
+        assert response.mode == WhoisQueryResponseMode.IRRD
+        assert response.result == "Queries may not contain null bytes"
 
     def test_keepalive(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
 
-        response = parser.handle_query('-k')
+        response = parser.handle_query("-k")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.RIPE
         assert not response.result
         assert parser.multiple_command_mode
 
     def test_route_search_exact(self, prepare_parser):
         # This also tests the recursion disabled flag, which should have no effect,
         # and the reset of the key fields only flag.
         # It also tests the handling of extra spaces.
         mock_query_resolver, mock_dh, parser = prepare_parser
         mock_query_resolver.route_search = Mock(return_value=MOCK_DATABASE_RESPONSE)
 
         parser.key_fields_only = True
-        response = parser.handle_query('-r  -x   192.0.2.0/25')
+        response = parser.handle_query("-r  -x   192.0.2.0/25")
         assert not parser.key_fields_only
 
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.RIPE
         assert response.result == MOCK_ROUTE_COMBINED
         mock_query_resolver.route_search.assert_called_once_with(
-            IP('192.0.2.0/25'), RouteLookupType.EXACT,
+            IP("192.0.2.0/25"),
+            RouteLookupType.EXACT,
         )
         mock_query_resolver.reset_mock()
 
         mock_query_resolver.route_search = Mock(return_value=[])
-        response = parser.handle_query('-x 192.0.2.0/32')
+        response = parser.handle_query("-x 192.0.2.0/32")
         assert response.response_type == WhoisQueryResponseType.KEY_NOT_FOUND
         assert response.mode == WhoisQueryResponseMode.RIPE
         assert not response.result
         assert response.remove_auth_hashes
 
     def test_route_search_less_specific_one_level(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
         mock_query_resolver.route_search = Mock(return_value=MOCK_DATABASE_RESPONSE)
 
-        response = parser.handle_query('-l 192.0.2.0/25')
+        response = parser.handle_query("-l 192.0.2.0/25")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.RIPE
         assert response.result == MOCK_ROUTE_COMBINED
         mock_query_resolver.route_search.assert_called_once_with(
-            IP('192.0.2.0/25'), RouteLookupType.LESS_SPECIFIC_ONE_LEVEL,
+            IP("192.0.2.0/25"),
+            RouteLookupType.LESS_SPECIFIC_ONE_LEVEL,
         )
 
     def test_route_search_less_specific(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
         mock_query_resolver.route_search = Mock(return_value=MOCK_DATABASE_RESPONSE)
 
-        response = parser.handle_query('-L 192.0.2.0/25')
+        response = parser.handle_query("-L 192.0.2.0/25")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.RIPE
         assert response.result == MOCK_ROUTE_COMBINED
         mock_query_resolver.route_search.assert_called_once_with(
-            IP('192.0.2.0/25'), RouteLookupType.LESS_SPECIFIC_WITH_EXACT,
+            IP("192.0.2.0/25"),
+            RouteLookupType.LESS_SPECIFIC_WITH_EXACT,
         )
 
     def test_route_search_more_specific(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
         mock_query_resolver.route_search = Mock(return_value=MOCK_DATABASE_RESPONSE)
 
-        response = parser.handle_query('-M 192.0.2.0/25')
+        response = parser.handle_query("-M 192.0.2.0/25")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.RIPE
         assert response.result == MOCK_ROUTE_COMBINED
         mock_query_resolver.route_search.assert_called_once_with(
-            IP('192.0.2.0/25'), RouteLookupType.MORE_SPECIFIC_WITHOUT_EXACT,
+            IP("192.0.2.0/25"),
+            RouteLookupType.MORE_SPECIFIC_WITHOUT_EXACT,
         )
 
     def test_route_search_invalid_parameter(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
 
-        response = parser.handle_query('-x not-a-prefix')
+        response = parser.handle_query("-x not-a-prefix")
         assert response.response_type == WhoisQueryResponseType.ERROR_USER
         assert response.mode == WhoisQueryResponseMode.RIPE
-        assert response.result == 'Invalid input for route search: not-a-prefix'
+        assert response.result == "Invalid input for route search: not-a-prefix"
 
     def test_inverse_attribute_search(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
         mock_query_resolver.rpsl_attribute_search = Mock(return_value=MOCK_DATABASE_RESPONSE)
 
-        response = parser.handle_query('-i mnt-by MNT-TEST')
+        response = parser.handle_query("-i mnt-by MNT-TEST")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.RIPE
         assert response.result == MOCK_ROUTE_COMBINED
-        mock_query_resolver.rpsl_attribute_search.assert_called_once_with('mnt-by', 'MNT-TEST')
+        mock_query_resolver.rpsl_attribute_search.assert_called_once_with("mnt-by", "MNT-TEST")
 
         mock_query_resolver.rpsl_attribute_search = Mock(return_value=[])
-        response = parser.handle_query('-i mnt-by MNT-NOT-EXISTING')
+        response = parser.handle_query("-i mnt-by MNT-NOT-EXISTING")
         assert response.response_type == WhoisQueryResponseType.KEY_NOT_FOUND
         assert response.mode == WhoisQueryResponseMode.RIPE
         assert not response.result
 
     def test_sources_list(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
         mock_query_resolver.set_query_sources = Mock()
 
-        response = parser.handle_query('-s test1')
+        response = parser.handle_query("-s test1")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.RIPE
         assert not response.result
-        mock_query_resolver.rpsl_attribute_search.set_query_sources(['TEST1'])
+        mock_query_resolver.rpsl_attribute_search.set_query_sources(["TEST1"])
 
     def test_sources_all(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
         mock_query_resolver.set_query_sources = Mock()
 
-        response = parser.handle_query('-a')
+        response = parser.handle_query("-a")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.RIPE
         assert not response.result
         mock_query_resolver.rpsl_attribute_search.set_query_sources(None)
 
     def test_restrict_object_class(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
         mock_query_resolver.set_object_class_filter_next_query = Mock()
         mock_query_resolver.rpsl_attribute_search = Mock(return_value=MOCK_DATABASE_RESPONSE)
 
-        response = parser.handle_query('-T route -i mnt-by MNT-TEST')
+        response = parser.handle_query("-T route -i mnt-by MNT-TEST")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.RIPE
         assert response.result == MOCK_ROUTE_COMBINED
         assert response.remove_auth_hashes
-        mock_query_resolver.rpsl_attribute_search.set_object_class_filter_next_query(['route'])
-        mock_query_resolver.rpsl_attribute_search.rpsl_attribute_search('mnt-by', 'MNT-TEST')
+        mock_query_resolver.rpsl_attribute_search.set_object_class_filter_next_query(["route"])
+        mock_query_resolver.rpsl_attribute_search.rpsl_attribute_search("mnt-by", "MNT-TEST")
 
     def test_object_template(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
-        mock_query_resolver.rpsl_object_template = Mock(return_value='<template>')
+        mock_query_resolver.rpsl_object_template = Mock(return_value="<template>")
 
-        response = parser.handle_query('-t aut-num')
+        response = parser.handle_query("-t aut-num")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.RIPE
-        assert response.result == '<template>'
-        mock_query_resolver.rpsl_attribute_search.rpsl_object_template('aut-num')
+        assert response.result == "<template>"
+        mock_query_resolver.rpsl_attribute_search.rpsl_object_template("aut-num")
 
     def test_key_fields_only(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
         mock_query_resolver.route_search = Mock(return_value=MOCK_DATABASE_RESPONSE)
 
-        response = parser.handle_query('-K -x 192.0.2.0/25')
+        response = parser.handle_query("-K -x 192.0.2.0/25")
         assert parser.key_fields_only
 
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.RIPE
         assert response.result == MOCK_ROUTE_COMBINED_KEY_FIELDS
         mock_query_resolver.route_search.assert_called_once_with(
-            IP('192.0.2.0/25'), RouteLookupType.EXACT,
+            IP("192.0.2.0/25"),
+            RouteLookupType.EXACT,
         )
 
         mock_query_resolver.route_search = Mock(return_value=[])
-        response = parser.handle_query('-x 192.0.2.0/32')
+        response = parser.handle_query("-x 192.0.2.0/32")
         assert response.response_type == WhoisQueryResponseType.KEY_NOT_FOUND
         assert response.mode == WhoisQueryResponseMode.RIPE
         assert not response.result
 
     def test_user_agent(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
 
-        response = parser.handle_query('-V user-agent')
+        response = parser.handle_query("-V user-agent")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.RIPE
         assert not response.result
 
     def test_nrtm_request(self, prepare_parser, monkeypatch, config_override):
         mock_query_resolver, mock_dh, parser = prepare_parser
-        mock_query_resolver.all_valid_sources = ['TEST1']
+        mock_query_resolver.all_valid_sources = ["TEST1"]
 
         mock_nrg = Mock()
-        monkeypatch.setattr('irrd.server.whois.query_parser.NRTMGenerator', lambda: mock_nrg)
-        mock_nrg.generate = lambda source, version, serial_start, serial_end, dh, remove_auth_hashes: f'{source}/{version}/{serial_start}/{serial_end}/{remove_auth_hashes}'
+        monkeypatch.setattr("irrd.server.whois.query_parser.NRTMGenerator", lambda: mock_nrg)
+        mock_nrg.generate = (
+            lambda source, version, serial_start, serial_end, dh, remove_auth_hashes: (
+                f"{source}/{version}/{serial_start}/{serial_end}/{remove_auth_hashes}"
+            )
+        )
 
-        response = parser.handle_query('-g TEST1:3:1-5')
+        response = parser.handle_query("-g TEST1:3:1-5")
         assert response.response_type == WhoisQueryResponseType.ERROR_USER
         assert response.mode == WhoisQueryResponseMode.RIPE
-        assert response.result == 'Access denied'
+        assert response.result == "Access denied"
 
-        config_override({
-            'sources': {
-                'TEST1': {'nrtm_access_list': 'nrtm_access'},
-            },
-            'access_lists': {
-                'nrtm_access': ['0/0', '0::/0'],
-            },
-            'sources_default': [],
-        })
+        config_override(
+            {
+                "sources": {
+                    "TEST1": {"nrtm_access_list": "nrtm_access"},
+                },
+                "access_lists": {
+                    "nrtm_access": ["0/0", "0::/0"],
+                },
+                "sources_default": [],
+            }
+        )
 
-        response = parser.handle_query('-g TEST1:3:1-5')
+        response = parser.handle_query("-g TEST1:3:1-5")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.RIPE
-        assert response.result == 'TEST1/3/1/5/True'
+        assert response.result == "TEST1/3/1/5/True"
         assert not response.remove_auth_hashes
 
-        response = parser.handle_query('-g TEST1:3:1-LAST')
+        response = parser.handle_query("-g TEST1:3:1-LAST")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.RIPE
-        assert response.result == 'TEST1/3/1/None/True'
+        assert response.result == "TEST1/3/1/None/True"
         assert not response.remove_auth_hashes
 
-        config_override({
-            'sources': {
-                'TEST1': {'nrtm_access_list_unfiltered': 'nrtm_access'},
-            },
-            'access_lists': {
-                'nrtm_access': ['0/0', '0::/0'],
-            },
-            'sources_default': [],
-        })
-        response = parser.handle_query('-g TEST1:3:1-LAST')
+        config_override(
+            {
+                "sources": {
+                    "TEST1": {"nrtm_access_list_unfiltered": "nrtm_access"},
+                },
+                "access_lists": {
+                    "nrtm_access": ["0/0", "0::/0"],
+                },
+                "sources_default": [],
+            }
+        )
+        response = parser.handle_query("-g TEST1:3:1-LAST")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.RIPE
-        assert response.result == 'TEST1/3/1/None/False'
+        assert response.result == "TEST1/3/1/None/False"
         assert not response.remove_auth_hashes
 
-        config_override({
-            'sources': {
-                'TEST1': {
-                    'nrtm_access_list': 'nrtm_access',
-                    'nrtm_access_list_unfiltered': 'nrtm_access',
+        config_override(
+            {
+                "sources": {
+                    "TEST1": {
+                        "nrtm_access_list": "nrtm_access",
+                        "nrtm_access_list_unfiltered": "nrtm_access",
+                    },
                 },
-            },
-            'access_lists': {
-                'nrtm_access': ['0/0', '0::/0'],
-            },
-            'sources_default': [],
-        })
-        response = parser.handle_query('-g TEST1:3:1-LAST')
+                "access_lists": {
+                    "nrtm_access": ["0/0", "0::/0"],
+                },
+                "sources_default": [],
+            }
+        )
+        response = parser.handle_query("-g TEST1:3:1-LAST")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.RIPE
-        assert response.result == 'TEST1/3/1/None/False'
+        assert response.result == "TEST1/3/1/None/False"
         assert not response.remove_auth_hashes
 
-        response = parser.handle_query('-g TEST1:9:1-LAST')
+        response = parser.handle_query("-g TEST1:9:1-LAST")
         assert response.response_type == WhoisQueryResponseType.ERROR_USER
         assert response.mode == WhoisQueryResponseMode.RIPE
-        assert response.result == 'Invalid NRTM version: 9'
+        assert response.result == "Invalid NRTM version: 9"
 
-        response = parser.handle_query('-g TEST1:1:1-LAST:foo')
+        response = parser.handle_query("-g TEST1:1:1-LAST:foo")
         assert response.response_type == WhoisQueryResponseType.ERROR_USER
         assert response.mode == WhoisQueryResponseMode.RIPE
-        assert response.result == 'Invalid parameter: must contain three elements'
+        assert response.result == "Invalid parameter: must contain three elements"
 
-        response = parser.handle_query('-g UNKNOWN:1:1-LAST')
+        response = parser.handle_query("-g UNKNOWN:1:1-LAST")
         assert response.response_type == WhoisQueryResponseType.ERROR_USER
         assert response.mode == WhoisQueryResponseMode.RIPE
-        assert response.result == 'Unknown source: UNKNOWN'
+        assert response.result == "Unknown source: UNKNOWN"
 
-        for invalid_range in ['1', 'LAST-1', 'LAST', '1-last']:
-            response = parser.handle_query(f'-g TEST1:3:{invalid_range}')
+        for invalid_range in ["1", "LAST-1", "LAST", "1-last"]:
+            response = parser.handle_query(f"-g TEST1:3:{invalid_range}")
             assert response.response_type == WhoisQueryResponseType.ERROR_USER
             assert response.mode == WhoisQueryResponseMode.RIPE
-            assert response.result == f'Invalid serial range: {invalid_range}'
+            assert response.result == f"Invalid serial range: {invalid_range}"
 
-        mock_nrg.generate = Mock(side_effect=NRTMGeneratorException('expected-test-error'))
-        response = parser.handle_query('-g TEST1:3:1-5')
+        mock_nrg.generate = Mock(side_effect=NRTMGeneratorException("expected-test-error"))
+        response = parser.handle_query("-g TEST1:3:1-5")
         assert response.response_type == WhoisQueryResponseType.ERROR_USER
         assert response.mode == WhoisQueryResponseMode.RIPE
-        assert response.result == 'expected-test-error'
+        assert response.result == "expected-test-error"
 
     def test_text_search(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
         mock_query_resolver.rpsl_text_search = Mock(return_value=MOCK_DATABASE_RESPONSE)
 
-        response = parser.handle_query('query')
+        response = parser.handle_query("query")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.RIPE
         assert response.result == MOCK_ROUTE_COMBINED
         assert response.remove_auth_hashes
-        mock_query_resolver.rpsl_text_search.assert_called_once_with('query')
+        mock_query_resolver.rpsl_text_search.assert_called_once_with("query")
 
     def test_missing_argument(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
 
-        missing_arg_queries = ['-i ', '-i mnt-by ', '-s', '-T', '-t', '-V', '-x   ']
+        missing_arg_queries = ["-i ", "-i mnt-by ", "-s", "-T", "-t", "-V", "-x   "]
         for query in missing_arg_queries:
             response = parser.handle_query(query)
             assert response.response_type == WhoisQueryResponseType.ERROR_USER
             assert response.mode == WhoisQueryResponseMode.RIPE
-            assert response.result == 'Missing argument for flag/search: ' + query[1]
+            assert response.result == "Missing argument for flag/search: " + query[1]
 
     def test_exception_handling(self, prepare_parser, caplog):
         mock_query_resolver, mock_dh, parser = prepare_parser
-        mock_query_resolver.rpsl_text_search = Mock(side_effect=Exception('test-error'))
+        mock_query_resolver.rpsl_text_search = Mock(side_effect=Exception("test-error"))
 
-        response = parser.handle_query('foo')
+        response = parser.handle_query("foo")
         assert response.response_type == WhoisQueryResponseType.ERROR_INTERNAL
         assert response.mode == WhoisQueryResponseMode.RIPE
-        assert response.result == 'An internal error occurred while processing this query.'
+        assert response.result == "An internal error occurred while processing this query."
 
-        assert 'An exception occurred while processing whois query' in caplog.text
-        assert 'test-error' in caplog.text
+        assert "An exception occurred while processing whois query" in caplog.text
+        assert "test-error" in caplog.text
 
-        mock_query_resolver.rpsl_text_search = Mock(side_effect=InvalidQueryException('user error'))
+        mock_query_resolver.rpsl_text_search = Mock(side_effect=InvalidQueryException("user error"))
 
-        response = parser.handle_query('foo')
+        response = parser.handle_query("foo")
         assert response.response_type == WhoisQueryResponseType.ERROR_USER
         assert response.mode == WhoisQueryResponseMode.RIPE
-        assert response.result == 'user error'
+        assert response.result == "user error"
 
 
 class TestWhoisQueryParserIRRD:
     """Test IRRD-style queries"""
 
     def test_invalid_command(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
 
-        response = parser.handle_query('!')
+        response = parser.handle_query("!")
         assert response.response_type == WhoisQueryResponseType.ERROR_USER
         assert response.mode == WhoisQueryResponseMode.IRRD
-        assert response.result == 'Missing IRRD command'
+        assert response.result == "Missing IRRD command"
 
-        response = parser.handle_query('!e')
+        response = parser.handle_query("!e")
         assert response.response_type == WhoisQueryResponseType.ERROR_USER
         assert response.mode == WhoisQueryResponseMode.IRRD
-        assert response.result == 'Unrecognised command: e'
+        assert response.result == "Unrecognised command: e"
 
     def test_parameter_required(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
 
-        queries_with_parameter = list('tg6ijmnors')
+        queries_with_parameter = list("tg6ijmnors")
         for query in queries_with_parameter:
-            response = parser.handle_query(f'!{query}')
+            response = parser.handle_query(f"!{query}")
             assert response.response_type == WhoisQueryResponseType.ERROR_USER
             assert response.mode == WhoisQueryResponseMode.IRRD
-            assert response.result == f'Missing parameter for {query} query'
+            assert response.result == f"Missing parameter for {query} query"
 
     def test_multiple_command_mode(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
 
-        response = parser.handle_query('!!')
+        response = parser.handle_query("!!")
         assert response.response_type == WhoisQueryResponseType.NO_RESPONSE
         assert response.mode == WhoisQueryResponseMode.IRRD
         assert not response.result
         assert parser.multiple_command_mode
 
     def test_update_timeout(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
 
-        response = parser.handle_query('!t300')
+        response = parser.handle_query("!t300")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.IRRD
         assert not response.result
         assert parser.timeout == 300
 
-        for invalid_value in ['foo', '-5', '1001']:
-            response = parser.handle_query(f'!t{invalid_value}')
+        for invalid_value in ["foo", "-5", "1001"]:
+            response = parser.handle_query(f"!t{invalid_value}")
             assert response.response_type == WhoisQueryResponseType.ERROR_USER
             assert response.mode == WhoisQueryResponseMode.IRRD
-            assert response.result == f'Invalid value for timeout: {invalid_value}'
+            assert response.result == f"Invalid value for timeout: {invalid_value}"
 
         assert parser.timeout == 300
 
     def test_routes_for_origin(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
-        mock_query_resolver.routes_for_origin = Mock(
-            return_value=['192.0.2.0/25', '192.0.2.128/25']
-        )
+        mock_query_resolver.routes_for_origin = Mock(return_value=["192.0.2.0/25", "192.0.2.128/25"])
 
-        response = parser.handle_query('!gas065547')
+        response = parser.handle_query("!gas065547")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.IRRD
-        assert response.result == '192.0.2.0/25 192.0.2.128/25'
-        mock_query_resolver.routes_for_origin.assert_called_once_with('AS65547', 4)
+        assert response.result == "192.0.2.0/25 192.0.2.128/25"
+        mock_query_resolver.routes_for_origin.assert_called_once_with("AS65547", 4)
 
-        mock_query_resolver.routes_for_origin = Mock(
-            return_value=['2001:db8::/32']
-        )
-        response = parser.handle_query('!6as065547')
+        mock_query_resolver.routes_for_origin = Mock(return_value=["2001:db8::/32"])
+        response = parser.handle_query("!6as065547")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.IRRD
-        assert response.result == '2001:db8::/32'
-        mock_query_resolver.routes_for_origin.assert_called_once_with('AS65547', 6)
+        assert response.result == "2001:db8::/32"
+        mock_query_resolver.routes_for_origin.assert_called_once_with("AS65547", 6)
 
         mock_query_resolver.routes_for_origin = Mock(return_value=[])
-        response = parser.handle_query('!gAS65547')
+        response = parser.handle_query("!gAS65547")
         assert response.response_type == WhoisQueryResponseType.KEY_NOT_FOUND
         assert response.mode == WhoisQueryResponseMode.IRRD
         assert not response.result
 
-        response = parser.handle_query('!6AS65547')
+        response = parser.handle_query("!6AS65547")
         assert response.response_type == WhoisQueryResponseType.KEY_NOT_FOUND
         assert response.mode == WhoisQueryResponseMode.IRRD
         assert not response.result
 
     def test_routes_for_origin_invalid(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
 
-        response = parser.handle_query('!gASfoobar')
+        response = parser.handle_query("!gASfoobar")
         assert response.response_type == WhoisQueryResponseType.ERROR_USER
         assert response.mode == WhoisQueryResponseMode.IRRD
-        assert response.result == 'Invalid AS number ASFOOBAR: number part is not numeric'
+        assert response.result == "Invalid AS number ASFOOBAR: number part is not numeric"
 
     def test_handle_irrd_routes_for_as_set(self, prepare_parser, monkeypatch):
         mock_query_resolver, mock_dh, parser = prepare_parser
 
-        for parameter in ['', '4', '6']:
-            response = parser.handle_query(f'!a{parameter}')
+        for parameter in ["", "4", "6"]:
+            response = parser.handle_query(f"!a{parameter}")
             assert response.response_type == WhoisQueryResponseType.ERROR_USER
             assert response.mode == WhoisQueryResponseMode.IRRD
-            assert response.result == 'Missing required set name for A query'
+            assert response.result == "Missing required set name for A query"
 
-        mock_query_resolver.routes_for_as_set = Mock(
-            return_value=['192.0.2.0/25', '192.0.2.128/25']
-        )
+        mock_query_resolver.routes_for_as_set = Mock(return_value=["192.0.2.0/25", "192.0.2.128/25"])
 
-        response = parser.handle_query('!aAS-FOO')
+        response = parser.handle_query("!aAS-FOO")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.IRRD
-        assert response.result == '192.0.2.0/25 192.0.2.128/25'
-        mock_query_resolver.routes_for_as_set.assert_called_once_with('AS-FOO', None)
+        assert response.result == "192.0.2.0/25 192.0.2.128/25"
+        mock_query_resolver.routes_for_as_set.assert_called_once_with("AS-FOO", None)
         mock_query_resolver.routes_for_as_set.reset_mock()
 
-        response = parser.handle_query('!a4AS-FOO')
+        response = parser.handle_query("!a4AS-FOO")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.IRRD
-        assert response.result == '192.0.2.0/25 192.0.2.128/25'
-        mock_query_resolver.routes_for_as_set.assert_called_once_with('AS-FOO', 4)
+        assert response.result == "192.0.2.0/25 192.0.2.128/25"
+        mock_query_resolver.routes_for_as_set.assert_called_once_with("AS-FOO", 4)
         mock_query_resolver.routes_for_as_set.reset_mock()
 
-        response = parser.handle_query('!a6AS-FOO')
+        response = parser.handle_query("!a6AS-FOO")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.IRRD
-        assert response.result == '192.0.2.0/25 192.0.2.128/25'
-        mock_query_resolver.routes_for_as_set.assert_called_once_with('AS-FOO', 6)
+        assert response.result == "192.0.2.0/25 192.0.2.128/25"
+        mock_query_resolver.routes_for_as_set.assert_called_once_with("AS-FOO", 6)
         mock_query_resolver.routes_for_as_set.reset_mock()
 
         mock_query_resolver.routes_for_as_set = Mock(return_value=[])
-        response = parser.handle_query('!a6AS-FOO')
+        response = parser.handle_query("!a6AS-FOO")
         assert response.response_type == WhoisQueryResponseType.KEY_NOT_FOUND
         assert response.mode == WhoisQueryResponseMode.IRRD
         assert not response.result
 
     def test_set_members(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
-        mock_query_resolver.members_for_set = Mock(return_value=['MEMBER1', 'MEMBER2'])
+        mock_query_resolver.members_for_set = Mock(return_value=["MEMBER1", "MEMBER2"])
 
-        response = parser.handle_query('!iAS-FOO')
+        response = parser.handle_query("!iAS-FOO")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.IRRD
-        assert response.result == 'MEMBER1 MEMBER2'
-        mock_query_resolver.members_for_set.assert_called_once_with('AS-FOO', recursive=False)
+        assert response.result == "MEMBER1 MEMBER2"
+        mock_query_resolver.members_for_set.assert_called_once_with("AS-FOO", recursive=False)
         mock_query_resolver.members_for_set.reset_mock()
 
-        response = parser.handle_query('!iAS-FOO,1')
+        response = parser.handle_query("!iAS-FOO,1")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.IRRD
-        assert response.result == 'MEMBER1 MEMBER2'
-        mock_query_resolver.members_for_set.assert_called_once_with('AS-FOO', recursive=True)
+        assert response.result == "MEMBER1 MEMBER2"
+        mock_query_resolver.members_for_set.assert_called_once_with("AS-FOO", recursive=True)
 
         mock_query_resolver.members_for_set = Mock(return_value=[])
-        response = parser.handle_query('!iAS-FOO')
+        response = parser.handle_query("!iAS-FOO")
         assert response.response_type == WhoisQueryResponseType.KEY_NOT_FOUND
         assert response.mode == WhoisQueryResponseMode.IRRD
         assert not response.result
 
     def test_database_serial_range(self, monkeypatch, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
-        mock_query_resolver.sources_default = ['TEST1', 'TEST2']
-        mock_query_resolver.all_valid_sources = ['TEST1', 'TEST2']
+        mock_query_resolver.sources_default = ["TEST1", "TEST2"]
+        mock_query_resolver.all_valid_sources = ["TEST1", "TEST2"]
 
         mock_dsq = Mock()
-        monkeypatch.setattr('irrd.server.whois.query_parser.DatabaseStatusQuery', lambda: mock_dsq)
+        monkeypatch.setattr("irrd.server.whois.query_parser.DatabaseStatusQuery", lambda: mock_dsq)
 
         mock_query_result = [
-            {'source': 'TEST1', 'serial_oldest_seen': 10, 'serial_newest_mirror': 20, 'serial_last_export': 10},
-            {'source': 'TEST2', 'serial_oldest_seen': None, 'serial_newest_mirror': None, 'serial_last_export': None},
+            {
+                "source": "TEST1",
+                "serial_oldest_seen": 10,
+                "serial_newest_mirror": 20,
+                "serial_last_export": 10,
+            },
+            {
+                "source": "TEST2",
+                "serial_oldest_seen": None,
+                "serial_newest_mirror": None,
+                "serial_last_export": None,
+            },
         ]
         mock_dh.execute_query = lambda query, refresh_on_error=False: mock_query_result
 
-        response = parser.handle_query('!j-*')
+        response = parser.handle_query("!j-*")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.IRRD
-        assert response.result == 'TEST1:N:0-20:10\nTEST2:N:-'
-        assert flatten_mock_calls(mock_dsq) == [
-            ['sources', (['TEST1', 'TEST2'],), {}]
-        ]
+        assert response.result == "TEST1:N:0-20:10\nTEST2:N:-"
+        assert flatten_mock_calls(mock_dsq) == [["sources", (["TEST1", "TEST2"],), {}]]
         mock_dsq.reset_mock()
 
-        response = parser.handle_query('!jtest1,test-invalid')
+        response = parser.handle_query("!jtest1,test-invalid")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.IRRD
-        assert response.result == 'TEST1:N:0-20:10\nTEST2:N:-\nTEST-INVALID:X:Database unknown'
-        assert flatten_mock_calls(mock_dsq) == [
-            ['sources', (['TEST1', 'TEST-INVALID'],), {}]
-        ]
+        assert response.result == "TEST1:N:0-20:10\nTEST2:N:-\nTEST-INVALID:X:Database unknown"
+        assert flatten_mock_calls(mock_dsq) == [["sources", (["TEST1", "TEST-INVALID"],), {}]]
 
     def test_database_status(self, monkeypatch, prepare_parser, config_override):
         mock_query_resolver, mock_dh, parser = prepare_parser
-        mock_query_resolver.database_status = Mock(return_value={'dict': True})
+        mock_query_resolver.database_status = Mock(return_value={"dict": True})
 
-        response = parser.handle_query('!J-*')
+        response = parser.handle_query("!J-*")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.IRRD
-        assert response.result.replace(' ', '') == '{\n"dict":true\n}'
+        assert response.result.replace(" ", "") == '{\n"dict":true\n}'
         mock_query_resolver.database_status.assert_called_once_with(None)
         mock_query_resolver.database_status.reset_mock()
 
-        response = parser.handle_query('!Jtest1,test-invalid')
+        response = parser.handle_query("!Jtest1,test-invalid")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.IRRD
-        assert response.result.replace(' ', '') == '{\n"dict":true\n}'
-        mock_query_resolver.database_status.assert_called_once_with(['TEST1', 'TEST-INVALID'])
+        assert response.result.replace(" ", "") == '{\n"dict":true\n}'
+        mock_query_resolver.database_status.assert_called_once_with(["TEST1", "TEST-INVALID"])
 
     def test_exact_key(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
         mock_query_resolver.key_lookup = Mock(return_value=MOCK_DATABASE_RESPONSE)
 
-        response = parser.handle_query('!mroute,192.0.2.0/25')
+        response = parser.handle_query("!mroute,192.0.2.0/25")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.IRRD
         assert response.result == MOCK_ROUTE_COMBINED
         assert response.remove_auth_hashes
-        mock_query_resolver.key_lookup.assert_called_once_with('route', '192.0.2.0/25')
+        mock_query_resolver.key_lookup.assert_called_once_with("route", "192.0.2.0/25")
         mock_query_resolver.key_lookup.reset_mock()
 
-        response = parser.handle_query('!mroute,192.0.2.0/25AS65530')
+        response = parser.handle_query("!mroute,192.0.2.0/25AS65530")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.IRRD
         assert response.result == MOCK_ROUTE_COMBINED
         assert response.remove_auth_hashes
-        mock_query_resolver.key_lookup.assert_called_once_with('route', '192.0.2.0/25AS65530')
+        mock_query_resolver.key_lookup.assert_called_once_with("route", "192.0.2.0/25AS65530")
         mock_query_resolver.key_lookup.reset_mock()
 
         # https://github.com/irrdnet/irrd/issues/551
-        response = parser.handle_query('!mroute,192.0.2.0/25 AS65530')
-        mock_query_resolver.key_lookup.assert_called_once_with('route', '192.0.2.0/25AS65530')
+        response = parser.handle_query("!mroute,192.0.2.0/25 AS65530")
+        mock_query_resolver.key_lookup.assert_called_once_with("route", "192.0.2.0/25AS65530")
         mock_query_resolver.key_lookup.reset_mock()
-        response = parser.handle_query('!mroute,192.0.2.0/25-As65530')
-        mock_query_resolver.key_lookup.assert_called_once_with('route', '192.0.2.0/25AS65530')
+        response = parser.handle_query("!mroute,192.0.2.0/25-As65530")
+        mock_query_resolver.key_lookup.assert_called_once_with("route", "192.0.2.0/25AS65530")
         mock_query_resolver.key_lookup.reset_mock()
-        response = parser.handle_query('!mas-set,AS-FOO')
-        mock_query_resolver.key_lookup.assert_called_once_with('as-set', 'AS-FOO')
+        response = parser.handle_query("!mas-set,AS-FOO")
+        mock_query_resolver.key_lookup.assert_called_once_with("as-set", "AS-FOO")
         mock_query_resolver.key_lookup.reset_mock()
 
         mock_query_resolver.key_lookup = Mock(return_value=[])
-        response = parser.handle_query('!mroute,192.0.2.0/25')
+        response = parser.handle_query("!mroute,192.0.2.0/25")
         assert response.response_type == WhoisQueryResponseType.KEY_NOT_FOUND
         assert response.mode == WhoisQueryResponseMode.IRRD
         assert response.remove_auth_hashes
         assert not response.result
 
-        response = parser.handle_query('!mfoo')
+        response = parser.handle_query("!mfoo")
         assert response.response_type == WhoisQueryResponseType.ERROR_USER
         assert response.mode == WhoisQueryResponseMode.IRRD
         assert response.remove_auth_hashes
-        assert response.result == 'Invalid argument for object lookup: foo'
+        assert response.result == "Invalid argument for object lookup: foo"
 
     def test_user_agent(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
 
-        response = parser.handle_query('!nuser-agent')
+        response = parser.handle_query("!nuser-agent")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.IRRD
         assert not response.result
 
     def test_objects_maintained_by(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
         mock_query_resolver.rpsl_attribute_search = Mock(return_value=MOCK_DATABASE_RESPONSE)
 
-        response = parser.handle_query('!oMNT-TEST')
+        response = parser.handle_query("!oMNT-TEST")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.IRRD
         assert response.result == MOCK_ROUTE_COMBINED
         assert response.remove_auth_hashes
-        mock_query_resolver.rpsl_attribute_search.assert_called_once_with('mnt-by', 'MNT-TEST')
+        mock_query_resolver.rpsl_attribute_search.assert_called_once_with("mnt-by", "MNT-TEST")
 
         mock_query_resolver.rpsl_attribute_search = Mock(return_value=[])
-        response = parser.handle_query('!oMNT-NOT-EXISTING')
+        response = parser.handle_query("!oMNT-NOT-EXISTING")
         assert response.response_type == WhoisQueryResponseType.KEY_NOT_FOUND
         assert response.mode == WhoisQueryResponseMode.IRRD
         assert not response.result
 
     def test_route_search_exact(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
         mock_query_resolver.route_search = Mock(return_value=MOCK_DATABASE_RESPONSE)
 
-        response = parser.handle_query('!r192.0.2.0/25')
+        response = parser.handle_query("!r192.0.2.0/25")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.IRRD
         assert response.result == MOCK_ROUTE_COMBINED
         mock_query_resolver.route_search.assert_called_once_with(
-            IP('192.0.2.0/25'), RouteLookupType.EXACT,
+            IP("192.0.2.0/25"),
+            RouteLookupType.EXACT,
         )
         mock_query_resolver.route_search.reset_mock()
 
-        response = parser.handle_query('!r192.0.2.0/25,o')
+        response = parser.handle_query("!r192.0.2.0/25,o")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.IRRD
-        assert response.result == 'AS65547 AS65544 AS65545'
+        assert response.result == "AS65547 AS65544 AS65545"
         mock_query_resolver.route_search.assert_called_once_with(
-            IP('192.0.2.0/25'), RouteLookupType.EXACT,
+            IP("192.0.2.0/25"),
+            RouteLookupType.EXACT,
         )
 
         mock_query_resolver.route_search = Mock(return_value=[])
-        response = parser.handle_query('!r192.0.2.0/32,o')
+        response = parser.handle_query("!r192.0.2.0/32,o")
         assert response.response_type == WhoisQueryResponseType.KEY_NOT_FOUND
         assert response.mode == WhoisQueryResponseMode.IRRD
         assert not response.result
 
     def test_route_search_exact_rpki_aware(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
         mock_query_resolver.route_search = Mock(return_value=MOCK_DATABASE_RESPONSE)
         mock_query_resolver.rpki_aware = True
 
-        response = parser.handle_query('!r192.0.2.0/25')
+        response = parser.handle_query("!r192.0.2.0/25")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.IRRD
         assert response.result == MOCK_ROUTE_COMBINED_WITH_RPKI
         mock_query_resolver.route_search.assert_called_once_with(
-            IP('192.0.2.0/25'), RouteLookupType.EXACT,
+            IP("192.0.2.0/25"),
+            RouteLookupType.EXACT,
         )
         mock_query_resolver.route_search.reset_mock()
 
     def test_route_search_less_specific_one_level(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
         mock_query_resolver.route_search = Mock(return_value=MOCK_DATABASE_RESPONSE)
 
-        response = parser.handle_query('!r192.0.2.0/25,l')
+        response = parser.handle_query("!r192.0.2.0/25,l")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.IRRD
         assert response.result == MOCK_ROUTE_COMBINED
         mock_query_resolver.route_search.assert_called_once_with(
-            IP('192.0.2.0/25'), RouteLookupType.LESS_SPECIFIC_ONE_LEVEL,
+            IP("192.0.2.0/25"),
+            RouteLookupType.LESS_SPECIFIC_ONE_LEVEL,
         )
 
     def test_route_search_less_specific(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
         mock_query_resolver.route_search = Mock(return_value=MOCK_DATABASE_RESPONSE)
 
-        response = parser.handle_query('!r192.0.2.0/25,L')
+        response = parser.handle_query("!r192.0.2.0/25,L")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.IRRD
         assert response.result == MOCK_ROUTE_COMBINED
         mock_query_resolver.route_search.assert_called_once_with(
-            IP('192.0.2.0/25'), RouteLookupType.LESS_SPECIFIC_WITH_EXACT,
+            IP("192.0.2.0/25"),
+            RouteLookupType.LESS_SPECIFIC_WITH_EXACT,
         )
 
     def test_route_search_more_specific(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
         mock_query_resolver.route_search = Mock(return_value=MOCK_DATABASE_RESPONSE)
 
-        response = parser.handle_query('!r192.0.2.0/25,M')
+        response = parser.handle_query("!r192.0.2.0/25,M")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.IRRD
         assert response.result == MOCK_ROUTE_COMBINED
         mock_query_resolver.route_search.assert_called_once_with(
-            IP('192.0.2.0/25'), RouteLookupType.MORE_SPECIFIC_WITHOUT_EXACT,
+            IP("192.0.2.0/25"),
+            RouteLookupType.MORE_SPECIFIC_WITHOUT_EXACT,
         )
 
     def test_route_search_invalid(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
 
-        response = parser.handle_query('!rz')
+        response = parser.handle_query("!rz")
         assert response.response_type == WhoisQueryResponseType.ERROR_USER
         assert response.mode == WhoisQueryResponseMode.IRRD
-        assert response.result == 'Invalid input for route search: z'
+        assert response.result == "Invalid input for route search: z"
 
-        response = parser.handle_query('!rz,o')
+        response = parser.handle_query("!rz,o")
         assert response.response_type == WhoisQueryResponseType.ERROR_USER
         assert response.mode == WhoisQueryResponseMode.IRRD
-        assert response.result == 'Invalid input for route search: z,o'
+        assert response.result == "Invalid input for route search: z,o"
 
-        response = parser.handle_query('!r192.0.2.0/25,z')
+        response = parser.handle_query("!r192.0.2.0/25,z")
         assert response.response_type == WhoisQueryResponseType.ERROR_USER
         assert response.mode == WhoisQueryResponseMode.IRRD
-        assert response.result == 'Invalid route search option: z'
+        assert response.result == "Invalid route search option: z"
 
     def test_sources_list(self, prepare_parser, config_override):
         mock_query_resolver, mock_dh, parser = prepare_parser
-        mock_query_resolver.sources = ['TEST1']
+        mock_query_resolver.sources = ["TEST1"]
         mock_query_resolver.set_query_sources = Mock()
 
-        response = parser.handle_query('!stest1')
+        response = parser.handle_query("!stest1")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.IRRD
         assert not response.result
-        mock_query_resolver.set_query_sources.assert_called_once_with(['TEST1'])
+        mock_query_resolver.set_query_sources.assert_called_once_with(["TEST1"])
 
-        response = parser.handle_query('!s-lc')
+        response = parser.handle_query("!s-lc")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.IRRD
-        assert response.result == 'TEST1'
+        assert response.result == "TEST1"
 
     def test_irrd_version(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
 
-        response = parser.handle_query('!v')
+        response = parser.handle_query("!v")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.IRRD
-        assert response.result.startswith('IRRd')
+        assert response.result.startswith("IRRd")
 
     def test_disable_filters(self, prepare_parser):
         mock_query_resolver, mock_dh, parser = prepare_parser
 
         mock_query_resolver.disable_rpki_filter = Mock()
-        response = parser.handle_query('!fno-rpki-filter')
+        response = parser.handle_query("!fno-rpki-filter")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.IRRD
-        assert response.result.startswith('Filtering out RPKI invalids')
+        assert response.result.startswith("Filtering out RPKI invalids")
         mock_query_resolver.disable_rpki_filter.assert_called_once_with()
 
         mock_query_resolver.disable_out_of_scope_filter = Mock()
-        response = parser.handle_query('!fno-scope-filter')
+        response = parser.handle_query("!fno-scope-filter")
         assert response.response_type == WhoisQueryResponseType.SUCCESS
         assert response.mode == WhoisQueryResponseMode.IRRD
-        assert response.result.startswith('Filtering out out-of-scope')
+        assert response.result.startswith("Filtering out out-of-scope")
         mock_query_resolver.disable_out_of_scope_filter.assert_called_once_with()
 
+        mock_query_resolver.disable_route_preference_filter = Mock()
+        response = parser.handle_query("!fno-route-preference-filter")
+        assert response.response_type == WhoisQueryResponseType.SUCCESS
+        assert response.mode == WhoisQueryResponseMode.IRRD
+        assert response.result.startswith("Filtering out objects suppressed due to route")
+        mock_query_resolver.disable_route_preference_filter.assert_called_once_with()
+
     def test_exception_handling(self, prepare_parser, caplog):
         mock_query_resolver, mock_dh, parser = prepare_parser
-        mock_query_resolver.members_for_set = Mock(side_effect=Exception('test-error'))
+        mock_query_resolver.members_for_set = Mock(side_effect=Exception("test-error"))
 
-        response = parser.handle_query('!i123')
+        response = parser.handle_query("!i123")
         assert response.response_type == WhoisQueryResponseType.ERROR_INTERNAL
         assert response.mode == WhoisQueryResponseMode.IRRD
-        assert response.result == 'An internal error occurred while processing this query.'
+        assert response.result == "An internal error occurred while processing this query."
 
-        assert 'An exception occurred while processing whois query' in caplog.text
-        assert 'test-error' in caplog.text
+        assert "An exception occurred while processing whois query" in caplog.text
+        assert "test-error" in caplog.text
 
-        mock_query_resolver.members_for_set = Mock(side_effect=InvalidQueryException('user error'))
+        mock_query_resolver.members_for_set = Mock(side_effect=InvalidQueryException("user error"))
 
-        response = parser.handle_query('!i123')
+        response = parser.handle_query("!i123")
         assert response.response_type == WhoisQueryResponseType.ERROR_USER
         assert response.mode == WhoisQueryResponseMode.IRRD
-        assert response.result == 'user error'
+        assert response.result == "user error"
```

### Comparing `irrd-4.2.8/irrd/server/whois/tests/test_server.py` & `irrd-4.3.0/irrd/server/whois/tests/test_server.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,32 +1,32 @@
 import socket
 import time
 from io import BytesIO
 from queue import Queue
 from unittest.mock import Mock
 
 import pytest
-from irrd.conf import SOCKET_DEFAULT_TIMEOUT
 
 from irrd.storage.preload import Preloader
+
 from ..server import WhoisWorker
 
 
 class MockSocket:
     def __init__(self):
         # rfile (for read) and wfile (for write) are from the perspective
         # of the WhoisWorker, opposite of the test's perspective
         self.rfile = BytesIO()
         self.wfile = BytesIO()
         self.shutdown_called = False
         self.close_called = False
         self.timeout_set = None
 
     def makefile(self, mode, bufsize):
-        return self.wfile if 'w' in mode else self.rfile
+        return self.wfile if "w" in mode else self.rfile
 
     def sendall(self, bytes):
         self.wfile.write(bytes)
 
     def shutdown(self, flags):
         self.shutdown_called = True
 
@@ -36,68 +36,70 @@
     def settimeout(self, timeout):
         self.timeout_set = timeout
 
 
 @pytest.fixture()
 def create_worker(config_override, monkeypatch):
     mock_preloader = Mock(spec=Preloader)
-    monkeypatch.setattr('irrd.server.whois.server.Preloader', lambda: mock_preloader)
+    monkeypatch.setattr("irrd.server.whois.server.Preloader", lambda: mock_preloader)
 
-    config_override({
-        'redis_url': 'redis://invalid-host.example.com',  # Not actually used
-    })
+    config_override(
+        {
+            "redis_url": "redis://invalid-host.example.com",  # Not actually used
+        }
+    )
     queue = Queue()
     worker = WhoisWorker(queue)
     request = MockSocket()
-    queue.put((request, ('192.0.2.1', 99999)))
+    queue.put((request, ("192.0.2.1", 99999)))
     yield worker, request
 
 
 class TestWhoisWorker:
     def test_whois_request_worker_no_access_list(self, create_worker):
         worker, request = create_worker
         # Empty query in first line should be ignored.
-        request.rfile.write(b' \n!v\r\n')
+        request.rfile.write(b" \n!v\r\n")
         request.rfile.seek(0)
         worker.run(keep_running=False)
 
-        assert worker.client_str == '192.0.2.1:99999'
+        assert worker.client_str == "192.0.2.1:99999"
         request.wfile.seek(0)
-        assert b'IRRd -- version' in request.wfile.read()
+        assert b"IRRd -- version" in request.wfile.read()
         assert request.shutdown_called
         assert request.close_called
-        assert request.timeout_set == SOCKET_DEFAULT_TIMEOUT
+        assert request.timeout_set == 5
 
     def test_whois_request_worker_exception(self, create_worker, monkeypatch, caplog):
-        monkeypatch.setattr('irrd.server.whois.server.WhoisQueryParser',
-                            Mock(side_effect=OSError('expected')))
+        monkeypatch.setattr(
+            "irrd.server.whois.server.WhoisQueryParser", Mock(side_effect=OSError("expected"))
+        )
 
         worker, request = create_worker
-        request.rfile.write(b'!v\r\n')
+        request.rfile.write(b"!v\r\n")
         request.rfile.seek(0)
         worker.run(keep_running=False)
 
         request.wfile.seek(0)
         assert not request.wfile.read()
         assert request.shutdown_called
         assert request.close_called
-        assert 'Failed to handle whois connection' in caplog.text
+        assert "Failed to handle whois connection" in caplog.text
 
     def test_whois_request_worker_preload_failed(self, create_worker, monkeypatch, caplog):
-        monkeypatch.setattr('irrd.server.whois.server.Preloader',
-                            Mock(side_effect=OSError('expected')))
+        monkeypatch.setattr("irrd.server.whois.server.Preloader", Mock(side_effect=OSError("expected")))
 
         worker, request = create_worker
-        request.rfile.write(b'!v\r\n')
+        request.rfile.write(b"!v\r\n")
         request.rfile.seek(0)
         worker.run(keep_running=False)
 
         request.wfile.seek(0)
         assert not request.wfile.read()
-        assert 'worker failed to initialise preloader' in caplog.text
+        assert "worker failed to initialise preloader" in caplog.text
 
     def test_whois_request_worker_timeout(self, create_worker):
         worker, request = create_worker
 
         request.rfile = Mock()
         readline_call_count = 0
 
@@ -107,73 +109,78 @@
         # Then, !t1 is used to set a very short timeout.
         # Third, readline() blocks for 2s, simulating a user not sending
         # any query to IRRd, triggering the shutdown call by the timeout.
         def mock_readline():
             nonlocal readline_call_count
             readline_call_count += 1
             if readline_call_count == 1:
-                return b'!!\n'
+                return b"!!\n"
             if readline_call_count == 2:
-                return b'!t1\n'
+                return b"!t1\n"
             if readline_call_count == 3:
                 time.sleep(2)
-                return b''
+                return b""
+
         request.rfile.readline = mock_readline
 
         request.rfile.seek(0)
         worker.run(keep_running=False)
 
         request.wfile.seek(0)
         assert request.shutdown_called
 
     def test_whois_request_worker_write_error(self, create_worker, caplog):
         worker, request = create_worker
-        request.rfile.write(b'!!\n!v\n')
+        request.rfile.write(b"!!\n!v\n")
         request.rfile.seek(0)
         # Write errors are usually due to the connection being
         # dropped, and should cause the connection to be closed
         # from our end.
-        request.sendall = Mock(side_effect=socket.error('expected'))
+        request.sendall = Mock(side_effect=socket.error("expected"))
         worker.run(keep_running=False)
 
     def test_whois_request_worker_access_list_permitted(self, config_override, create_worker):
-        config_override({
-            'redis_url': 'redis://invalid-host.example.com',  # Not actually used
-            'server': {
-                'whois': {
-                    'access_list': 'test-access-list',
+        config_override(
+            {
+                "redis_url": "redis://invalid-host.example.com",  # Not actually used
+                "server": {
+                    "whois": {
+                        "access_list": "test-access-list",
+                    },
                 },
-            },
-            'access_lists': {
-                'test-access-list': ['192.0.2.0/25'],
-            },
-        })
+                "access_lists": {
+                    "test-access-list": ["192.0.2.0/25"],
+                },
+            }
+        )
 
         worker, request = create_worker
-        request.rfile.write(b'!q\n')
+        request.rfile.write(b"!q\n")
         request.rfile.seek(0)
         worker.run(keep_running=False)
 
         request.wfile.seek(0)
         assert not request.wfile.read()
 
     def test_whois_request_worker_access_list_denied(self, config_override, create_worker):
-        config_override({
-            'redis_url': 'redis://invalid-host.example.com',  # Not actually used
-            'server': {
-                'whois': {
-                    'access_list': 'test-access-list',
+        config_override(
+            {
+                "redis_url": "redis://invalid-host.example.com",  # Not actually used
+                "server": {
+                    "whois": {
+                        "access_list": "test-access-list",
+                    },
+                },
+                "access_lists": {
+                    "test-access-list": ["192.0.2.128/25"],
                 },
-            },
-            'access_lists': {
-                'test-access-list': ['192.0.2.128/25'],
-            },
-        })
+            }
+        )
 
         worker, request = create_worker
-        request.rfile.write(b'!v\n')
+        request.rfile.write(b"!v\n")
         request.rfile.seek(0)
         worker.run(keep_running=False)
 
-        assert worker.client_str == '192.0.2.1:99999'
+        assert worker.client_str == "192.0.2.1:99999"
         request.wfile.seek(0)
-        assert request.wfile.read() == b'%% Access denied'
+        assert request.wfile.read() == b"%% Access denied"
```

### Comparing `irrd-4.2.8/irrd/storage/__init__.py` & `irrd-4.3.0/irrd/storage/__init__.py`

 * *Files 20% similar despite different names*

```diff
@@ -10,37 +10,36 @@
 
 
 def get_engine():
     global engine
     if engine:
         return engine
     engine = sa.create_engine(
-        translate_url(get_setting('database_url')),
-        pool_size=2,
+        translate_url(get_setting("database_url")),
+        pool_size=50,
         json_deserializer=ujson.loads,
     )
 
     # https://docs.sqlalchemy.org/en/13/core/pooling.html#using-connection-pools-with-multiprocessing
     @sa.event.listens_for(engine, "connect")
     def connect(dbapi_connection, connection_record):
-        connection_record.info['pid'] = os.getpid()
+        connection_record.info["pid"] = os.getpid()
 
     @sa.event.listens_for(engine, "checkout")
     def checkout(dbapi_connection, connection_record, connection_proxy):
         pid = os.getpid()
-        if connection_record.info['pid'] != pid:  # pragma: no cover
+        if connection_record.info["pid"] != pid:  # pragma: no cover
             connection_record.connection = connection_proxy.connection = None
             raise sa.exc.DisconnectionError(
-                "Connection record belongs to pid %s, "
-                "attempting to check out in pid %s" %
-                (connection_record.info['pid'], pid)
+                "Connection record belongs to pid %s, attempting to check out in pid %s"
+                % (connection_record.info["pid"], pid)
             )
 
     return engine
 
 
 def translate_url(url_str: str) -> sa.engine.url.URL:
     """Translate a url string to a SQLAlchemy URL object with the right driver"""
     url = sa.engine.url.make_url(url_str)
-    if url.drivername == 'postgresql' and platform.python_implementation() == 'PyPy':  # pragma: no cover
-        url.drivername = 'postgresql+psycopg2cffi'
+    if url.drivername == "postgresql" and platform.python_implementation() == "PyPy":  # pragma: no cover
+        url.drivername = "postgresql+psycopg2cffi"
     return url
```

### Comparing `irrd-4.2.8/irrd/storage/alembic/env.py` & `irrd-4.3.0/irrd/storage/alembic/env.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,20 +1,20 @@
 # flake8: noqa: E402
 import os
 import sys
+from pathlib import Path
 
 from alembic import context
-from pathlib import Path
 from sqlalchemy import create_engine
 
 from irrd.storage import translate_url
 
 sys.path.append(str(Path(__file__).resolve().parents[3]))
 
-from irrd.conf import get_setting, config_init, is_config_initialised
+from irrd.conf import config_init, get_setting, is_config_initialised
 from irrd.storage.models import Base
 
 target_metadata = Base.metadata
 
 
 def run_migrations_offline():
     """Run migrations in 'offline' mode.
@@ -25,18 +25,20 @@
     we don't even need a DBAPI to be available.
 
     Calls to context.execute() here emit the given string to the
     script output.
 
     """
     if not is_config_initialised():
-        config_init(os.environ['IRRD_CONFIG_FILE'])
-    url = translate_url(get_setting('database_url'))
+        config_init(os.environ["IRRD_CONFIG_FILE"])
+    url = translate_url(get_setting("database_url"))
     context.configure(
-        url=url, target_metadata=target_metadata, literal_binds=True,
+        url=url,
+        target_metadata=target_metadata,
+        literal_binds=True,
         transaction_per_migration=True,
     )
 
     with context.begin_transaction():
         context.run_migrations()
 
 
@@ -44,22 +46,19 @@
     """Run migrations in 'online' mode.
 
     In this scenario we need to create an Engine
     and associate a connection with the context.
 
     """
     if not is_config_initialised():
-        config_init(os.environ['IRRD_CONFIG_FILE'])
-    engine = create_engine(translate_url(get_setting('database_url')))
+        config_init(os.environ["IRRD_CONFIG_FILE"])
+    engine = create_engine(translate_url(get_setting("database_url")))
 
     with engine.connect() as connection:
-        context.configure(
-            connection=connection,
-            target_metadata=target_metadata
-        )
+        context.configure(connection=connection, target_metadata=target_metadata)
 
         with context.begin_transaction():
             context.run_migrations()
 
 
 if context.is_offline_mode():
     run_migrations_offline()
```

### Comparing `irrd-4.2.8/irrd/storage/alembic/versions/1743f98a456d_add_serial_newest_mirror.py` & `irrd-4.3.0/irrd/storage/alembic/versions/1743f98a456d_add_serial_newest_mirror.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,25 +1,24 @@
 """Add serial_newest_mirror
 
 Revision ID: 1743f98a456d
 Revises: 181670a62643
 Create Date: 2020-04-15 20:08:59.925809
 
 """
-from alembic import op
 import sqlalchemy as sa
-
+from alembic import op
 
 # revision identifiers, used by Alembic.
-revision = '1743f98a456d'
-down_revision = '181670a62643'
+revision = "1743f98a456d"
+down_revision = "181670a62643"
 branch_labels = None
 depends_on = None
 
 
 def upgrade():
-    op.add_column('database_status', sa.Column('serial_newest_mirror', sa.Integer(), nullable=True))
-    op.execute('UPDATE database_status SET serial_newest_mirror = serial_newest_seen')
+    op.add_column("database_status", sa.Column("serial_newest_mirror", sa.Integer(), nullable=True))
+    op.execute("UPDATE database_status SET serial_newest_mirror = serial_newest_seen")
 
 
 def downgrade():
-    op.drop_column('database_status', 'serial_newest_mirror')
+    op.drop_column("database_status", "serial_newest_mirror")
```

### Comparing `irrd-4.2.8/irrd/storage/alembic/versions/28dc1cd85bdc_initial_db.py` & `irrd-4.3.0/irrd/storage/alembic/versions/28dc1cd85bdc_initial_db.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,155 +1,220 @@
 """initial db
 
 Revision ID: 28dc1cd85bdc
 Revises:
 Create Date: 2018-06-11 14:37:13.472465
 
 """
-from alembic import op
 import sqlalchemy as sa
+from alembic import op
 from sqlalchemy.dialects import postgresql
 
 # revision identifiers, used by Alembic.
-revision = '28dc1cd85bdc'
+revision = "28dc1cd85bdc"
 down_revision = None
 branch_labels = None
 depends_on = None
 
 
 def upgrade():
     # Creating this extension requires extra permissions.
     # However, if it is already created, this command succeeds
     # even if this user does not have sufficient permissions.
     op.execute('CREATE EXTENSION IF NOT EXISTS "pgcrypto";')
 
-    op.create_table('database_status',
-                    sa.Column('pk', postgresql.UUID(as_uuid=True), server_default=sa.text('gen_random_uuid()'),
-                              nullable=False),
-                    sa.Column('source', sa.String(), nullable=False),
-                    sa.Column('serial_oldest_seen', sa.Integer(), nullable=True),
-                    sa.Column('serial_newest_seen', sa.Integer(), nullable=True),
-                    sa.Column('serial_oldest_journal', sa.Integer(), nullable=True),
-                    sa.Column('serial_newest_journal', sa.Integer(), nullable=True),
-                    sa.Column('serial_last_export', sa.Integer(), nullable=True),
-                    sa.Column('force_reload', sa.Boolean(), nullable=False),
-                    sa.Column('last_error', sa.Text(), nullable=True),
-                    sa.Column('last_error_timestamp', sa.DateTime(timezone=True), nullable=True),
-                    sa.Column('created', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
-                    sa.Column('updated', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
-                    sa.PrimaryKeyConstraint('pk')
-                    )
-    op.create_index(op.f('ix_database_status_source'), 'database_status', ['source'], unique=True)
-
-    op.create_table('rpsl_database_journal',
-                    sa.Column('pk', postgresql.UUID(as_uuid=True), server_default=sa.text('gen_random_uuid()'),
-                              nullable=False),
-                    sa.Column('rpsl_pk', sa.String(), nullable=False),
-                    sa.Column('source', sa.String(), nullable=False),
-                    sa.Column('serial_nrtm', sa.Integer(), nullable=False),
-                    sa.Column('operation', sa.Enum('add_or_update', 'delete', name='databaseoperation'),
-                              nullable=False),
-                    sa.Column('object_class', sa.String(), nullable=False),
-                    sa.Column('object_text', sa.Text(), nullable=False),
-                    sa.Column('timestamp', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
-                    sa.PrimaryKeyConstraint('pk'),
-                    sa.UniqueConstraint('serial_nrtm', 'source', name='rpsl_objects_history_serial_nrtm_source_unique')
-                    )
-    op.create_index(op.f('ix_rpsl_database_journal_object_class'), 'rpsl_database_journal', ['object_class'],
-                    unique=False)
-    op.create_index(op.f('ix_rpsl_database_journal_rpsl_pk'), 'rpsl_database_journal', ['rpsl_pk'], unique=False)
-    op.create_index(op.f('ix_rpsl_database_journal_serial_nrtm'), 'rpsl_database_journal', ['serial_nrtm'],
-                    unique=False)
-    op.create_index(op.f('ix_rpsl_database_journal_source'), 'rpsl_database_journal', ['source'], unique=False)
-
-    op.create_table('rpsl_objects',
-                    sa.Column('pk', postgresql.UUID(as_uuid=True), server_default=sa.text('gen_random_uuid()'),
-                              nullable=False),
-                    sa.Column('rpsl_pk', sa.String(), nullable=False),
-                    sa.Column('source', sa.String(), nullable=False),
-                    sa.Column('object_class', sa.String(), nullable=False),
-                    sa.Column('parsed_data', postgresql.JSONB(astext_type=sa.Text()), nullable=False),
-                    sa.Column('object_text', sa.Text(), nullable=False),
-                    sa.Column('ip_version', sa.Integer(), nullable=True),
-                    sa.Column('ip_first', postgresql.INET(), nullable=True),
-                    sa.Column('ip_last', postgresql.INET(), nullable=True),
-                    sa.Column('ip_size', sa.DECIMAL(scale=0), nullable=True),
-                    sa.Column('asn_first', sa.BigInteger(), nullable=True),
-                    sa.Column('asn_last', sa.BigInteger(), nullable=True),
-                    sa.Column('created', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
-                    sa.Column('updated', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
-                    sa.PrimaryKeyConstraint('pk'),
-                    sa.UniqueConstraint('rpsl_pk', 'source', name='rpsl_objects_rpsl_pk_source_unique')
-                    )
-    op.create_index(op.f('ix_rpsl_objects_ip_first'), 'rpsl_objects', ['ip_first'], unique=False)
-    op.create_index(op.f('ix_rpsl_objects_ip_last'), 'rpsl_objects', ['ip_last'], unique=False)
-    op.create_index(op.f('ix_rpsl_objects_asn_first'), 'rpsl_objects', ['asn_first'], unique=False)
-    op.create_index(op.f('ix_rpsl_objects_asn_last'), 'rpsl_objects', ['asn_last'], unique=False)
-    op.create_index(op.f('ix_rpsl_objects_ip_version'), 'rpsl_objects', ['ip_version'], unique=False)
-    op.create_index(op.f('ix_rpsl_objects_object_class'), 'rpsl_objects', ['object_class'], unique=False)
-    op.create_index(op.f('ix_rpsl_objects_rpsl_pk'), 'rpsl_objects', ['rpsl_pk'], unique=False)
-    op.create_index(op.f('ix_rpsl_objects_source'), 'rpsl_objects', ['source'], unique=False)
-    op.create_index('ix_rpsl_objects_asn_first_asn_last', 'rpsl_objects', ['asn_first', 'asn_last'], unique=False)
-    op.create_index('ix_rpsl_objects_ip_first_ip_last', 'rpsl_objects', ['ip_first', 'ip_last'], unique=False)
-    op.create_index('ix_rpsl_objects_ip_last_ip_first', 'rpsl_objects', ['ip_last', 'ip_first'], unique=False)
+    op.create_table(
+        "database_status",
+        sa.Column(
+            "pk", postgresql.UUID(as_uuid=True), server_default=sa.text("gen_random_uuid()"), nullable=False
+        ),
+        sa.Column("source", sa.String(), nullable=False),
+        sa.Column("serial_oldest_seen", sa.Integer(), nullable=True),
+        sa.Column("serial_newest_seen", sa.Integer(), nullable=True),
+        sa.Column("serial_oldest_journal", sa.Integer(), nullable=True),
+        sa.Column("serial_newest_journal", sa.Integer(), nullable=True),
+        sa.Column("serial_last_export", sa.Integer(), nullable=True),
+        sa.Column("force_reload", sa.Boolean(), nullable=False),
+        sa.Column("last_error", sa.Text(), nullable=True),
+        sa.Column("last_error_timestamp", sa.DateTime(timezone=True), nullable=True),
+        sa.Column("created", sa.DateTime(timezone=True), server_default=sa.text("now()"), nullable=False),
+        sa.Column("updated", sa.DateTime(timezone=True), server_default=sa.text("now()"), nullable=False),
+        sa.PrimaryKeyConstraint("pk"),
+    )
+    op.create_index(op.f("ix_database_status_source"), "database_status", ["source"], unique=True)
+
+    op.create_table(
+        "rpsl_database_journal",
+        sa.Column(
+            "pk", postgresql.UUID(as_uuid=True), server_default=sa.text("gen_random_uuid()"), nullable=False
+        ),
+        sa.Column("rpsl_pk", sa.String(), nullable=False),
+        sa.Column("source", sa.String(), nullable=False),
+        sa.Column("serial_nrtm", sa.Integer(), nullable=False),
+        sa.Column("operation", sa.Enum("add_or_update", "delete", name="databaseoperation"), nullable=False),
+        sa.Column("object_class", sa.String(), nullable=False),
+        sa.Column("object_text", sa.Text(), nullable=False),
+        sa.Column("timestamp", sa.DateTime(timezone=True), server_default=sa.text("now()"), nullable=False),
+        sa.PrimaryKeyConstraint("pk"),
+        sa.UniqueConstraint("serial_nrtm", "source", name="rpsl_objects_history_serial_nrtm_source_unique"),
+    )
+    op.create_index(
+        op.f("ix_rpsl_database_journal_object_class"), "rpsl_database_journal", ["object_class"], unique=False
+    )
+    op.create_index(
+        op.f("ix_rpsl_database_journal_rpsl_pk"), "rpsl_database_journal", ["rpsl_pk"], unique=False
+    )
+    op.create_index(
+        op.f("ix_rpsl_database_journal_serial_nrtm"), "rpsl_database_journal", ["serial_nrtm"], unique=False
+    )
+    op.create_index(
+        op.f("ix_rpsl_database_journal_source"), "rpsl_database_journal", ["source"], unique=False
+    )
+
+    op.create_table(
+        "rpsl_objects",
+        sa.Column(
+            "pk", postgresql.UUID(as_uuid=True), server_default=sa.text("gen_random_uuid()"), nullable=False
+        ),
+        sa.Column("rpsl_pk", sa.String(), nullable=False),
+        sa.Column("source", sa.String(), nullable=False),
+        sa.Column("object_class", sa.String(), nullable=False),
+        sa.Column("parsed_data", postgresql.JSONB(astext_type=sa.Text()), nullable=False),
+        sa.Column("object_text", sa.Text(), nullable=False),
+        sa.Column("ip_version", sa.Integer(), nullable=True),
+        sa.Column("ip_first", postgresql.INET(), nullable=True),
+        sa.Column("ip_last", postgresql.INET(), nullable=True),
+        sa.Column("ip_size", sa.DECIMAL(scale=0), nullable=True),
+        sa.Column("asn_first", sa.BigInteger(), nullable=True),
+        sa.Column("asn_last", sa.BigInteger(), nullable=True),
+        sa.Column("created", sa.DateTime(timezone=True), server_default=sa.text("now()"), nullable=False),
+        sa.Column("updated", sa.DateTime(timezone=True), server_default=sa.text("now()"), nullable=False),
+        sa.PrimaryKeyConstraint("pk"),
+        sa.UniqueConstraint("rpsl_pk", "source", name="rpsl_objects_rpsl_pk_source_unique"),
+    )
+    op.create_index(op.f("ix_rpsl_objects_ip_first"), "rpsl_objects", ["ip_first"], unique=False)
+    op.create_index(op.f("ix_rpsl_objects_ip_last"), "rpsl_objects", ["ip_last"], unique=False)
+    op.create_index(op.f("ix_rpsl_objects_asn_first"), "rpsl_objects", ["asn_first"], unique=False)
+    op.create_index(op.f("ix_rpsl_objects_asn_last"), "rpsl_objects", ["asn_last"], unique=False)
+    op.create_index(op.f("ix_rpsl_objects_ip_version"), "rpsl_objects", ["ip_version"], unique=False)
+    op.create_index(op.f("ix_rpsl_objects_object_class"), "rpsl_objects", ["object_class"], unique=False)
+    op.create_index(op.f("ix_rpsl_objects_rpsl_pk"), "rpsl_objects", ["rpsl_pk"], unique=False)
+    op.create_index(op.f("ix_rpsl_objects_source"), "rpsl_objects", ["source"], unique=False)
+    op.create_index(
+        "ix_rpsl_objects_asn_first_asn_last", "rpsl_objects", ["asn_first", "asn_last"], unique=False
+    )
+    op.create_index("ix_rpsl_objects_ip_first_ip_last", "rpsl_objects", ["ip_first", "ip_last"], unique=False)
+    op.create_index("ix_rpsl_objects_ip_last_ip_first", "rpsl_objects", ["ip_last", "ip_first"], unique=False)
 
     # Manually added
-    op.create_index(op.f('ix_rpsl_objects_parsed_data_admin_c'), 'rpsl_objects',
-                    [sa.text("((parsed_data->'admin-c'))")],
-                    unique=False, postgresql_using='gin')
-    op.create_index(op.f('ix_rpsl_objects_parsed_data_tech_c'), 'rpsl_objects', [sa.text("((parsed_data->'tech-c'))")],
-                    unique=False, postgresql_using='gin')
-    op.create_index(op.f('ix_rpsl_objects_parsed_data_zone_c'), 'rpsl_objects', [sa.text("((parsed_data->'zone-c'))")],
-                    unique=False, postgresql_using='gin')
-    op.create_index(op.f('ix_rpsl_objects_parsed_data_role'), 'rpsl_objects', [sa.text("((parsed_data->'role'))")],
-                    unique=False, postgresql_using='gin')
-    op.create_index(op.f('ix_rpsl_objects_parsed_data_members'), 'rpsl_objects',
-                    [sa.text("((parsed_data->'members'))")], unique=False, postgresql_using='gin')
-    op.create_index(op.f('ix_rpsl_objects_parsed_data_person'), 'rpsl_objects', [sa.text("((parsed_data->'person'))")],
-                    unique=False, postgresql_using='gin')
-    op.create_index(op.f('ix_rpsl_objects_parsed_data_mnt_by'), 'rpsl_objects', [sa.text("((parsed_data->'mnt-by'))")],
-                    unique=False, postgresql_using='gin')
-    op.create_index(op.f('ix_rpsl_objects_parsed_data_member_of'), 'rpsl_objects',
-                    [sa.text("((parsed_data->'member-of'))")], unique=False, postgresql_using='gin')
-    op.create_index(op.f('ix_rpsl_objects_parsed_data_mp_members'), 'rpsl_objects',
-                    [sa.text("((parsed_data->'mp-members'))")], unique=False,
-                    postgresql_using='gin')
-    op.create_index(op.f('ix_rpsl_objects_parsed_data_origin'), 'rpsl_objects', [sa.text("((parsed_data->'origin'))")],
-                    unique=False, postgresql_using='gin')
-    op.create_index(op.f('ix_rpsl_objects_parsed_data_mbrs_by_ref'), 'rpsl_objects',
-                    [sa.text("((parsed_data->'mbrs-by-ref'))")],
-                    unique=False, postgresql_using='gin')
+    op.create_index(
+        op.f("ix_rpsl_objects_parsed_data_admin_c"),
+        "rpsl_objects",
+        [sa.text("((parsed_data->'admin-c'))")],
+        unique=False,
+        postgresql_using="gin",
+    )
+    op.create_index(
+        op.f("ix_rpsl_objects_parsed_data_tech_c"),
+        "rpsl_objects",
+        [sa.text("((parsed_data->'tech-c'))")],
+        unique=False,
+        postgresql_using="gin",
+    )
+    op.create_index(
+        op.f("ix_rpsl_objects_parsed_data_zone_c"),
+        "rpsl_objects",
+        [sa.text("((parsed_data->'zone-c'))")],
+        unique=False,
+        postgresql_using="gin",
+    )
+    op.create_index(
+        op.f("ix_rpsl_objects_parsed_data_role"),
+        "rpsl_objects",
+        [sa.text("((parsed_data->'role'))")],
+        unique=False,
+        postgresql_using="gin",
+    )
+    op.create_index(
+        op.f("ix_rpsl_objects_parsed_data_members"),
+        "rpsl_objects",
+        [sa.text("((parsed_data->'members'))")],
+        unique=False,
+        postgresql_using="gin",
+    )
+    op.create_index(
+        op.f("ix_rpsl_objects_parsed_data_person"),
+        "rpsl_objects",
+        [sa.text("((parsed_data->'person'))")],
+        unique=False,
+        postgresql_using="gin",
+    )
+    op.create_index(
+        op.f("ix_rpsl_objects_parsed_data_mnt_by"),
+        "rpsl_objects",
+        [sa.text("((parsed_data->'mnt-by'))")],
+        unique=False,
+        postgresql_using="gin",
+    )
+    op.create_index(
+        op.f("ix_rpsl_objects_parsed_data_member_of"),
+        "rpsl_objects",
+        [sa.text("((parsed_data->'member-of'))")],
+        unique=False,
+        postgresql_using="gin",
+    )
+    op.create_index(
+        op.f("ix_rpsl_objects_parsed_data_mp_members"),
+        "rpsl_objects",
+        [sa.text("((parsed_data->'mp-members'))")],
+        unique=False,
+        postgresql_using="gin",
+    )
+    op.create_index(
+        op.f("ix_rpsl_objects_parsed_data_origin"),
+        "rpsl_objects",
+        [sa.text("((parsed_data->'origin'))")],
+        unique=False,
+        postgresql_using="gin",
+    )
+    op.create_index(
+        op.f("ix_rpsl_objects_parsed_data_mbrs_by_ref"),
+        "rpsl_objects",
+        [sa.text("((parsed_data->'mbrs-by-ref'))")],
+        unique=False,
+        postgresql_using="gin",
+    )
 
 
 def downgrade():
     # Manually added
-    op.drop_index(op.f('ix_rpsl_objects_parsed_data_mbrs_by_ref'), table_name='rpsl_objects')
-    op.drop_index(op.f('ix_rpsl_objects_ip_first'), table_name='rpsl_objects')
-    op.drop_index(op.f('ix_rpsl_objects_ip_last'), table_name='rpsl_objects')
-    op.drop_index(op.f('ix_rpsl_objects_parsed_data_zone_c'), table_name='rpsl_objects')
-    op.drop_index(op.f('ix_rpsl_objects_parsed_data_role'), table_name='rpsl_objects')
-    op.drop_index(op.f('ix_rpsl_objects_parsed_data_members'), table_name='rpsl_objects')
-    op.drop_index(op.f('ix_rpsl_objects_parsed_data_person'), table_name='rpsl_objects')
-    op.drop_index(op.f('ix_rpsl_objects_parsed_data_mnt_by'), table_name='rpsl_objects')
-    op.drop_index(op.f('ix_rpsl_objects_parsed_data_member_of'), table_name='rpsl_objects')
-    op.drop_index(op.f('ix_rpsl_objects_parsed_data_mp_members'), table_name='rpsl_objects')
+    op.drop_index(op.f("ix_rpsl_objects_parsed_data_mbrs_by_ref"), table_name="rpsl_objects")
+    op.drop_index(op.f("ix_rpsl_objects_ip_first"), table_name="rpsl_objects")
+    op.drop_index(op.f("ix_rpsl_objects_ip_last"), table_name="rpsl_objects")
+    op.drop_index(op.f("ix_rpsl_objects_parsed_data_zone_c"), table_name="rpsl_objects")
+    op.drop_index(op.f("ix_rpsl_objects_parsed_data_role"), table_name="rpsl_objects")
+    op.drop_index(op.f("ix_rpsl_objects_parsed_data_members"), table_name="rpsl_objects")
+    op.drop_index(op.f("ix_rpsl_objects_parsed_data_person"), table_name="rpsl_objects")
+    op.drop_index(op.f("ix_rpsl_objects_parsed_data_mnt_by"), table_name="rpsl_objects")
+    op.drop_index(op.f("ix_rpsl_objects_parsed_data_member_of"), table_name="rpsl_objects")
+    op.drop_index(op.f("ix_rpsl_objects_parsed_data_mp_members"), table_name="rpsl_objects")
 
     # Autogenerated
-    op.drop_index('ix_rpsl_objects_ip_last_ip_first', table_name='rpsl_objects')
-    op.drop_index('ix_rpsl_objects_ip_first_ip_last', table_name='rpsl_objects')
-    op.drop_index('ix_rpsl_objects_asn_first_asn_last', table_name='rpsl_objects')
-    op.drop_index(op.f('ix_rpsl_objects_source'), table_name='rpsl_objects')
-    op.drop_index(op.f('ix_rpsl_objects_rpsl_pk'), table_name='rpsl_objects')
-    op.drop_index(op.f('ix_rpsl_objects_object_class'), table_name='rpsl_objects')
-    op.drop_index(op.f('ix_rpsl_objects_ip_version'), table_name='rpsl_objects')
-    op.drop_index(op.f('ix_rpsl_objects_asn_last'), table_name='rpsl_objects')
-    op.drop_index(op.f('ix_rpsl_objects_asn_first'), table_name='rpsl_objects')
-    op.drop_table('rpsl_objects')
-
-    op.drop_index(op.f('ix_rpsl_database_journal_source'), table_name='rpsl_database_journal')
-    op.drop_index(op.f('ix_rpsl_database_journal_serial_nrtm'), table_name='rpsl_database_journal')
-    op.drop_index(op.f('ix_rpsl_database_journal_rpsl_pk'), table_name='rpsl_database_journal')
-    op.drop_index(op.f('ix_rpsl_database_journal_object_class'), table_name='rpsl_database_journal')
-    op.drop_table('rpsl_database_journal')
+    op.drop_index("ix_rpsl_objects_ip_last_ip_first", table_name="rpsl_objects")
+    op.drop_index("ix_rpsl_objects_ip_first_ip_last", table_name="rpsl_objects")
+    op.drop_index("ix_rpsl_objects_asn_first_asn_last", table_name="rpsl_objects")
+    op.drop_index(op.f("ix_rpsl_objects_source"), table_name="rpsl_objects")
+    op.drop_index(op.f("ix_rpsl_objects_rpsl_pk"), table_name="rpsl_objects")
+    op.drop_index(op.f("ix_rpsl_objects_object_class"), table_name="rpsl_objects")
+    op.drop_index(op.f("ix_rpsl_objects_ip_version"), table_name="rpsl_objects")
+    op.drop_index(op.f("ix_rpsl_objects_asn_last"), table_name="rpsl_objects")
+    op.drop_index(op.f("ix_rpsl_objects_asn_first"), table_name="rpsl_objects")
+    op.drop_table("rpsl_objects")
+
+    op.drop_index(op.f("ix_rpsl_database_journal_source"), table_name="rpsl_database_journal")
+    op.drop_index(op.f("ix_rpsl_database_journal_serial_nrtm"), table_name="rpsl_database_journal")
+    op.drop_index(op.f("ix_rpsl_database_journal_rpsl_pk"), table_name="rpsl_database_journal")
+    op.drop_index(op.f("ix_rpsl_database_journal_object_class"), table_name="rpsl_database_journal")
+    op.drop_table("rpsl_database_journal")
 
-    op.drop_index(op.f('ix_database_status_source'), table_name='database_status')
-    op.drop_table('database_status')
+    op.drop_index(op.f("ix_database_status_source"), table_name="database_status")
+    op.drop_table("database_status")
```

### Comparing `irrd-4.2.8/irrd/storage/alembic/versions/39e4f15ed80c_add_bogon_status.py` & `irrd-4.3.0/irrd/storage/alembic/versions/39e4f15ed80c_add_bogon_status.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,23 +1,22 @@
 """Add bogon_status
 
 Revision ID: 39e4f15ed80c
 Revises: 1743f98a456d
 Create Date: 2020-04-22 14:43:57.985437
 
 """
-from alembic import op
 import sqlalchemy as sa
+from alembic import op
 from sqlalchemy.engine.reflection import Inspector
 
-
 # revision identifiers, used by Alembic.
 
-revision = '39e4f15ed80c'
-down_revision = '1743f98a456d'
+revision = "39e4f15ed80c"
+down_revision = "1743f98a456d"
 branch_labels = None
 depends_on = None
 
 
 # This is a very strange migration.
 # The bogon_status column was added at some point, but was never used.
 # Some people's deployments have this migration already deployed, so
@@ -26,21 +25,22 @@
 # because it takes a lot of time.
 #
 # Therefore, this migration does nothing to upgrade, and on downgrade
 # removes the column, but only if it exists.
 # It assumes the existence of the bogonstatus enum matches that of
 # the bogon_status column.
 
+
 def upgrade():
     pass
 
 
 def downgrade():
     conn = op.get_bind()
     inspector = Inspector.from_engine(conn)
-    columns = inspector.get_columns('rpsl_objects')
+    columns = inspector.get_columns("rpsl_objects")
 
-    if 'bogon_status' in columns:
-        op.drop_index(op.f('ix_rpsl_objects_bogon_status'), table_name='rpsl_objects')
-        op.drop_column('rpsl_objects', 'bogon_status')
-        bogon_status = sa.Enum('unknown', 'not_bogon', 'bogon_as', 'bogon_prefix', name='bogonstatus')
+    if "bogon_status" in columns:
+        op.drop_index(op.f("ix_rpsl_objects_bogon_status"), table_name="rpsl_objects")
+        op.drop_column("rpsl_objects", "bogon_status")
+        bogon_status = sa.Enum("unknown", "not_bogon", "bogon_as", "bogon_prefix", name="bogonstatus")
         bogon_status.drop(op.get_bind())
```

### Comparing `irrd-4.2.8/irrd/storage/alembic/versions/4a514ead8fc2_bogon_to_scope_filter.py` & `irrd-4.3.0/irrd/storage/alembic/versions/4a514ead8fc2_bogon_to_scope_filter.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,55 +1,65 @@
 """bogon_to_scopefilter
 
 Revision ID: 4a514ead8fc2
 Revises: 39e4f15ed80c
 Create Date: 2020-07-09 20:11:45.873381
 
 """
-from alembic import op
 import sqlalchemy as sa
+from alembic import op
 from sqlalchemy.engine.reflection import Inspector
 from sqlalchemy.exc import ProgrammingError
 
-revision = '4a514ead8fc2'
-down_revision = '39e4f15ed80c'
+revision = "4a514ead8fc2"
+down_revision = "39e4f15ed80c"
 branch_labels = None
 depends_on = None
 
 
 def upgrade():
-    scopefilter_status = sa.Enum('in_scope', 'out_scope_as', 'out_scope_prefix', name='scopefilterstatus')
+    scopefilter_status = sa.Enum("in_scope", "out_scope_as", "out_scope_prefix", name="scopefilterstatus")
     scopefilter_status.create(op.get_bind())
 
-    op.add_column('rpsl_objects', sa.Column('scopefilter_status', sa.Enum('in_scope', 'out_scope_as', 'out_scope_prefix', name='scopefilterstatus'), server_default='in_scope', nullable=False))
-    op.create_index(op.f('ix_rpsl_objects_scopefilter_status'), 'rpsl_objects', ['scopefilter_status'], unique=False)
+    op.add_column(
+        "rpsl_objects",
+        sa.Column(
+            "scopefilter_status",
+            sa.Enum("in_scope", "out_scope_as", "out_scope_prefix", name="scopefilterstatus"),
+            server_default="in_scope",
+            nullable=False,
+        ),
+    )
+    op.create_index(
+        op.f("ix_rpsl_objects_scopefilter_status"), "rpsl_objects", ["scopefilter_status"], unique=False
+    )
 
     conn = op.get_bind()
     inspector = Inspector.from_engine(conn)
-    columns = inspector.get_columns('rpsl_objects')
+    columns = inspector.get_columns("rpsl_objects")
 
     # This is a somewhat strange migration: the bogon_status column may or may
     # not exist, depending on which version of 39e4f15ed80c people have run.
     # If it does exist, it is deleted.
     # This is also why downgrade() does not recreate this column.
-    if 'bogon_status' in columns:
-        op.drop_index('ix_rpsl_objects_bogon_status', table_name='rpsl_objects')
-        op.drop_column('rpsl_objects', 'bogon_status')
-        bogon_status = sa.Enum('unknown', 'not_bogon', 'bogon_as', 'bogon_prefix', name='bogonstatus')
+    if "bogon_status" in columns:
+        op.drop_index("ix_rpsl_objects_bogon_status", table_name="rpsl_objects")
+        op.drop_column("rpsl_objects", "bogon_status")
+        bogon_status = sa.Enum("unknown", "not_bogon", "bogon_as", "bogon_prefix", name="bogonstatus")
         bogon_status.drop(op.get_bind())
 
     # downgrade() can't remove this entry from the enum, so if this migration
     # is reverted and then re-applied, altering the enum will fail
     with op.get_context().autocommit_block():
         try:
             op.execute("ALTER TYPE journalentryorigin ADD VALUE 'scope_filter'")
         except ProgrammingError as pe:
-            if 'DuplicateObject' not in str(pe):
+            if "DuplicateObject" not in str(pe):
                 raise pe
 
 
 def downgrade():
-    op.drop_index(op.f('ix_rpsl_objects_scopefilter_status'), table_name='rpsl_objects')
-    op.drop_column('rpsl_objects', 'scopefilter_status')
+    op.drop_index(op.f("ix_rpsl_objects_scopefilter_status"), table_name="rpsl_objects")
+    op.drop_column("rpsl_objects", "scopefilter_status")
 
-    scopefilter_status = sa.Enum('in_scope', 'out_scope_as', 'out_scope_prefix', name='scopefilterstatus')
+    scopefilter_status = sa.Enum("in_scope", "out_scope_as", "out_scope_prefix", name="scopefilterstatus")
     scopefilter_status.drop(op.get_bind())
```

### Comparing `irrd-4.2.8/irrd/storage/alembic/versions/8744b4b906bb_fix_rpsl_unique_key.py` & `irrd-4.3.0/irrd/storage/alembic/versions/8744b4b906bb_fix_rpsl_unique_key.py`

 * *Files 25% similar despite different names*

```diff
@@ -6,37 +6,36 @@
 Revision ID: 8744b4b906bb
 Revises: 893d0d5363b3
 Create Date: 2021-11-30 19:46:01.195353
 
 """
 from alembic import op
 
-
 # revision identifiers, used by Alembic.
-revision = '8744b4b906bb'
-down_revision = '893d0d5363b3'
+revision = "8744b4b906bb"
+down_revision = "893d0d5363b3"
 branch_labels = None
 depends_on = None
 
 
 def upgrade():
     op.create_unique_constraint(
-        constraint_name='rpsl_objects_rpsl_pk_source_class_unique',
-        table_name='rpsl_objects',
-        columns=['rpsl_pk', 'source', 'object_class'],
+        constraint_name="rpsl_objects_rpsl_pk_source_class_unique",
+        table_name="rpsl_objects",
+        columns=["rpsl_pk", "source", "object_class"],
     )
     op.drop_constraint(
-        constraint_name='rpsl_objects_rpsl_pk_source_unique',
-        table_name='rpsl_objects',
+        constraint_name="rpsl_objects_rpsl_pk_source_unique",
+        table_name="rpsl_objects",
     )
 
 
 def downgrade():
     op.create_unique_constraint(
-        name='rpsl_objects_rpsl_pk_source_unique',
-        table_name='rpsl_objects',
-        columns=['rpsl_pk', 'source'],
+        name="rpsl_objects_rpsl_pk_source_unique",
+        table_name="rpsl_objects",
+        columns=["rpsl_pk", "source"],
     )
     op.drop_constraint(
-        constraint_name='rpsl_objects_rpsl_pk_source_class_unique',
-        table_name='rpsl_objects',
+        constraint_name="rpsl_objects_rpsl_pk_source_class_unique",
+        table_name="rpsl_objects",
     )
```

### Comparing `irrd-4.2.8/irrd/storage/alembic/versions/893d0d5363b3_add_rpsl_prefix_idx.py` & `irrd-4.3.0/irrd/storage/alembic/versions/893d0d5363b3_add_rpsl_prefix_idx.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,23 +1,29 @@
 """add_rpsl_prefix_idx
 
 Revision ID: 893d0d5363b3
 Revises: b175c262448f
 Create Date: 2021-03-01 16:11:28.275554
 
 """
-from alembic import op
 import sqlalchemy as sa
+from alembic import op
 
 # revision identifiers, used by Alembic.
-revision = '893d0d5363b3'
-down_revision = 'b175c262448f'
+revision = "893d0d5363b3"
+down_revision = "b175c262448f"
 branch_labels = None
 depends_on = None
 
 
 def upgrade():
-    op.create_index('ix_rpsl_objects_prefix_gist', 'rpsl_objects', [sa.text('prefix inet_ops')], unique=False, postgresql_using='gist')
+    op.create_index(
+        "ix_rpsl_objects_prefix_gist",
+        "rpsl_objects",
+        [sa.text("prefix inet_ops")],
+        unique=False,
+        postgresql_using="gist",
+    )
 
 
 def downgrade():
-    op.drop_index('ix_rpsl_objects_prefix_gist', table_name='rpsl_objects')
+    op.drop_index("ix_rpsl_objects_prefix_gist", table_name="rpsl_objects")
```

### Comparing `irrd-4.2.8/irrd/storage/alembic/versions/a8609af97aa3_set_prefix_length_in_existing_rpsl_.py` & `irrd-4.3.0/irrd/storage/alembic/versions/a8609af97aa3_set_prefix_length_in_existing_rpsl_.py`

 * *Files 11% similar despite different names*

```diff
@@ -6,48 +6,51 @@
 
 """
 import sqlalchemy as sa
 from alembic import op
 from sqlalchemy.dialects import postgresql as pg
 from sqlalchemy.ext.declarative import declarative_base
 
-
 # revision identifiers, used by Alembic.
-revision = 'a8609af97aa3'
-down_revision = '64a3d6faf6d4'
+revision = "a8609af97aa3"
+down_revision = "64a3d6faf6d4"
 branch_labels = None
 depends_on = None
 Base = declarative_base()
 
 
 class RPSLDatabaseObject(Base):  # type:ignore
-    __tablename__ = 'rpsl_objects'
+    __tablename__ = "rpsl_objects"
 
-    pk = sa.Column(pg.UUID(as_uuid=True), server_default=sa.text('gen_random_uuid()'), primary_key=True)
+    pk = sa.Column(pg.UUID(as_uuid=True), server_default=sa.text("gen_random_uuid()"), primary_key=True)
     object_class = sa.Column(sa.String, nullable=False, index=True)
     ip_size = sa.Column(sa.DECIMAL(scale=0))
     prefix_length = sa.Column(sa.Integer, nullable=True)
 
 
 def upgrade():
     connection = op.get_bind()
     t_rpsl_objects = RPSLDatabaseObject.__table__
 
     for length in range(33):
-        ip_size = pow(2, 32-length)
-        connection.execute(t_rpsl_objects.update().where(
-            sa.and_(t_rpsl_objects.c.ip_size == ip_size, t_rpsl_objects.c.object_class == 'route')
-        ).values(
-            prefix_length=length,
-        ))
+        ip_size = pow(2, 32 - length)
+        connection.execute(
+            t_rpsl_objects.update()
+            .where(sa.and_(t_rpsl_objects.c.ip_size == ip_size, t_rpsl_objects.c.object_class == "route"))
+            .values(
+                prefix_length=length,
+            )
+        )
 
     for length in range(129):
-        ip_size = pow(2, 128-length)
-        connection.execute(t_rpsl_objects.update().where(
-            sa.and_(t_rpsl_objects.c.ip_size == ip_size, t_rpsl_objects.c.object_class == 'route6')
-        ).values(
-            prefix_length=length,
-        ))
+        ip_size = pow(2, 128 - length)
+        connection.execute(
+            t_rpsl_objects.update()
+            .where(sa.and_(t_rpsl_objects.c.ip_size == ip_size, t_rpsl_objects.c.object_class == "route6"))
+            .values(
+                prefix_length=length,
+            )
+        )
 
 
 def downgrade():
     pass
```

### Comparing `irrd-4.2.8/irrd/storage/alembic/versions/e07863eac52f_add_roa_object_table.py` & `irrd-4.3.0/irrd/storage/alembic/versions/e07863eac52f_add_roa_object_table.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,40 +1,49 @@
 """Add roa_object table.
 
 Revision ID: e07863eac52f
 Revises: 28dc1cd85bdc
 Create Date: 2019-02-28 16:03:57.797697
 
 """
-from alembic import op
 import sqlalchemy as sa
+from alembic import op
 from sqlalchemy.dialects import postgresql
 
 # revision identifiers, used by Alembic.
-revision = 'e07863eac52f'
-down_revision = '28dc1cd85bdc'
+revision = "e07863eac52f"
+down_revision = "28dc1cd85bdc"
 branch_labels = None
 depends_on = None
 
 
 def upgrade():
-    op.create_table('roa_object',
-                    sa.Column('pk', postgresql.UUID(as_uuid=True), server_default=sa.text('gen_random_uuid()'),
-                              nullable=False),
-                    sa.Column('prefix', postgresql.CIDR(), nullable=False),
-                    sa.Column('asn', sa.BigInteger(), nullable=False),
-                    sa.Column('max_length', sa.Integer(), nullable=False),
-                    sa.Column('trust_anchor', sa.String(), nullable=True),
-                    sa.Column('ip_version', sa.Integer(), nullable=False),
-                    sa.Column('created', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
-                    sa.PrimaryKeyConstraint('pk'),
-                    sa.UniqueConstraint('prefix', 'asn', 'max_length', 'trust_anchor', name='roa_object_prefix_asn_maxlength_unique')
-                    )
-    op.create_index(op.f('ix_roa_object_ip_version'), 'roa_object', ['ip_version'], unique=False)
-    op.create_index('ix_roa_objects_prefix_gist', 'roa_object', [sa.text('prefix inet_ops')], unique=False,
-                    postgresql_using='gist')
+    op.create_table(
+        "roa_object",
+        sa.Column(
+            "pk", postgresql.UUID(as_uuid=True), server_default=sa.text("gen_random_uuid()"), nullable=False
+        ),
+        sa.Column("prefix", postgresql.CIDR(), nullable=False),
+        sa.Column("asn", sa.BigInteger(), nullable=False),
+        sa.Column("max_length", sa.Integer(), nullable=False),
+        sa.Column("trust_anchor", sa.String(), nullable=True),
+        sa.Column("ip_version", sa.Integer(), nullable=False),
+        sa.Column("created", sa.DateTime(timezone=True), server_default=sa.text("now()"), nullable=False),
+        sa.PrimaryKeyConstraint("pk"),
+        sa.UniqueConstraint(
+            "prefix", "asn", "max_length", "trust_anchor", name="roa_object_prefix_asn_maxlength_unique"
+        ),
+    )
+    op.create_index(op.f("ix_roa_object_ip_version"), "roa_object", ["ip_version"], unique=False)
+    op.create_index(
+        "ix_roa_objects_prefix_gist",
+        "roa_object",
+        [sa.text("prefix inet_ops")],
+        unique=False,
+        postgresql_using="gist",
+    )
 
 
 def downgrade():
-    op.drop_index(op.f('ix_roa_objects_prefix_gist'), table_name='roa_object')
-    op.drop_index(op.f('ix_roa_object_ip_version'), table_name='roa_object')
-    op.drop_table('roa_object')
+    op.drop_index(op.f("ix_roa_objects_prefix_gist"), table_name="roa_object")
+    op.drop_index(op.f("ix_roa_object_ip_version"), table_name="roa_object")
+    op.drop_table("roa_object")
```

### Comparing `irrd-4.2.8/irrd/storage/models.py` & `irrd-4.3.0/irrd/storage/models.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,55 +1,61 @@
 import enum
 
 import sqlalchemy as sa
 from sqlalchemy.dialects import postgresql as pg
 from sqlalchemy.ext.declarative import declarative_base, declared_attr
 
+from irrd.routepref.status import RoutePreferenceStatus
 from irrd.rpki.status import RPKIStatus
 from irrd.rpsl.rpsl_objects import lookup_field_names
 from irrd.scopefilter.status import ScopeFilterStatus
 
 
 class DatabaseOperation(enum.Enum):
-    add_or_update = 'ADD'
-    delete = 'DEL'
+    add_or_update = "ADD"
+    delete = "DEL"
 
 
 class JournalEntryOrigin(enum.Enum):
     # Legacy journal entries for which the origin is unknown, can be auth_change or mirror
-    unknown = 'UNKNOWN'
+    unknown = "UNKNOWN"
     # Journal entry received from a mirror by NRTM or importing from a file
-    mirror = 'MIRROR'
+    mirror = "MIRROR"
     # Journal entry generated from synthesized NRTM
-    synthetic_nrtm = 'SYNTHETIC_NRTM'
+    synthetic_nrtm = "SYNTHETIC_NRTM"
     # Journal entry generated from pseudo IRR
-    pseudo_irr = 'PSEUDO_IRR'
+    pseudo_irr = "PSEUDO_IRR"
     # Journal entry caused by a user-submitted change in an authoritative database
-    auth_change = 'AUTH_CHANGE'
+    auth_change = "AUTH_CHANGE"
     # Journal entry caused by a change in in the RPKI status of an object in an authoritative db
-    rpki_status = 'RPKI_STATUS'
+    rpki_status = "RPKI_STATUS"
     # Journal entry caused by a change in the scope filter status
-    scope_filter = 'SCOPE_FILTER'
+    scope_filter = "SCOPE_FILTER"
+    # Journal entry caused by an object being suspended or reactivated
+    suspension = "SUSPENSION"
+    # Journal entry caused by an object's route preference changing between suppressed and visible
+    route_preference = "ROUTE_PREFERENCE"
 
 
 Base = declarative_base()
 
 
 class RPSLDatabaseObject(Base):  # type: ignore
     """
     SQLAlchemy ORM object for RPSL database objects.
 
     Note that SQLAlchemy does not require you to use the ORM for ORM
     objects - as that can be slower with large queries.
     """
-    __tablename__ = 'rpsl_objects'
+
+    __tablename__ = "rpsl_objects"
 
     # Requires extension pgcrypto
     # in alembic: op.execute('create EXTENSION if not EXISTS 'pgcrypto';')
-    pk = sa.Column(pg.UUID(as_uuid=True), server_default=sa.text('gen_random_uuid()'), primary_key=True)
+    pk = sa.Column(pg.UUID(as_uuid=True), server_default=sa.text("gen_random_uuid()"), primary_key=True)
     rpsl_pk = sa.Column(sa.String, index=True, nullable=False)
     source = sa.Column(sa.String, index=True, nullable=False)
 
     object_class = sa.Column(sa.String, nullable=False, index=True)
     parsed_data = sa.Column(pg.JSONB, nullable=False)
     object_text = sa.Column(sa.Text, nullable=False)
 
@@ -60,85 +66,148 @@
     # Only filled for route/route6
     prefix_length = sa.Column(sa.Integer, nullable=True)
     prefix = sa.Column(pg.CIDR, nullable=True)
 
     asn_first = sa.Column(sa.BigInteger, index=True)
     asn_last = sa.Column(sa.BigInteger, index=True)
 
-    rpki_status = sa.Column(sa.Enum(RPKIStatus), nullable=False, index=True, server_default=RPKIStatus.not_found.name)
-    scopefilter_status = sa.Column(sa.Enum(ScopeFilterStatus), nullable=False, index=True, server_default=ScopeFilterStatus.in_scope.name)
+    rpki_status = sa.Column(
+        sa.Enum(RPKIStatus), nullable=False, index=True, server_default=RPKIStatus.not_found.name
+    )
+    scopefilter_status = sa.Column(
+        sa.Enum(ScopeFilterStatus), nullable=False, index=True, server_default=ScopeFilterStatus.in_scope.name
+    )
+    route_preference_status = sa.Column(
+        sa.Enum(RoutePreferenceStatus),
+        nullable=False,
+        index=True,
+        server_default=RoutePreferenceStatus.visible.name,
+    )
 
     created = sa.Column(sa.DateTime(timezone=True), server_default=sa.func.now(), nullable=False)
     updated = sa.Column(sa.DateTime(timezone=True), server_default=sa.func.now(), nullable=False)
 
     @declared_attr
     def __table_args__(cls):  # noqa
         args = [
-            sa.UniqueConstraint('rpsl_pk', 'source', 'object_class', name='rpsl_objects_rpsl_pk_source_class_unique'),
-            sa.Index('ix_rpsl_objects_ip_first_ip_last', 'ip_first', 'ip_last', ),
-            sa.Index('ix_rpsl_objects_ip_last_ip_first', 'ip_last', 'ip_first'),
-            sa.Index('ix_rpsl_objects_asn_first_asn_last', 'asn_first', 'asn_last'),
-            sa.Index('ix_rpsl_objects_prefix_gist', sa.text('prefix inet_ops'),
-                     postgresql_using='gist')
+            sa.UniqueConstraint(
+                "rpsl_pk", "source", "object_class", name="rpsl_objects_rpsl_pk_source_class_unique"
+            ),
+            sa.Index(
+                "ix_rpsl_objects_ip_first_ip_last",
+                "ip_first",
+                "ip_last",
+            ),
+            sa.Index("ix_rpsl_objects_ip_last_ip_first", "ip_last", "ip_first"),
+            sa.Index("ix_rpsl_objects_asn_first_asn_last", "asn_first", "asn_last"),
+            sa.Index("ix_rpsl_objects_prefix_gist", sa.text("prefix inet_ops"), postgresql_using="gist"),
         ]
         for name in lookup_field_names():
-            index_name = 'ix_rpsl_objects_parsed_data_' + name.replace('-', '_')
+            index_name = "ix_rpsl_objects_parsed_data_" + name.replace("-", "_")
             index_on = sa.text(f"(parsed_data->'{name}')")
-            args.append(sa.Index(index_name, index_on, postgresql_using='gin'))
+            args.append(sa.Index(index_name, index_on, postgresql_using="gin"))
         return tuple(args)
 
     def __repr__(self):
-        return f'<{self.rpsl_pk}/{self.source}/{self.pk}>'
+        return f"<{self.rpsl_pk}/{self.source}/{self.pk}>"
 
 
 class RPSLDatabaseJournal(Base):  # type: ignore
     """
     SQLAlchemy ORM object for change history of RPSL database objects.
     """
-    __tablename__ = 'rpsl_database_journal'
+
+    __tablename__ = "rpsl_database_journal"
 
     # Requires extension pgcrypto
     # in alembic: op.execute('create EXTENSION if not EXISTS 'pgcrypto';')
-    pk = sa.Column(pg.UUID(as_uuid=True), server_default=sa.text('gen_random_uuid()'), primary_key=True)
+    pk = sa.Column(pg.UUID(as_uuid=True), server_default=sa.text("gen_random_uuid()"), primary_key=True)
+
+    # Serial_journal is intended to allow querying by insertion order.
+    # This could almost be met by the timestamp, except that fails in case
+    # of clock changes. Unique and in insertion order, but may have gaps.
+    serial_global_seq = sa.Sequence("rpsl_database_journal_serial_global_seq")
+    serial_global = sa.Column(
+        sa.BigInteger,
+        serial_global_seq,
+        server_default=serial_global_seq.next_value(),
+        nullable=False,
+        index=True,
+        unique=True,
+    )
+
     rpsl_pk = sa.Column(sa.String, index=True, nullable=False)
     source = sa.Column(sa.String, index=True, nullable=False)
-    origin = sa.Column(sa.Enum(JournalEntryOrigin), nullable=False, index=True, server_default=JournalEntryOrigin.unknown.name)
+    origin = sa.Column(
+        sa.Enum(JournalEntryOrigin),
+        nullable=False,
+        index=True,
+        server_default=JournalEntryOrigin.unknown.name,
+    )
 
     serial_nrtm = sa.Column(sa.Integer, index=True, nullable=False)
     operation = sa.Column(sa.Enum(DatabaseOperation), nullable=False)
 
     object_class = sa.Column(sa.String, nullable=False, index=True)
     object_text = sa.Column(sa.Text, nullable=False)
 
     # These objects are not mutable, so creation time is sufficient.
-    timestamp = sa.Column(sa.DateTime(timezone=True), server_default=sa.func.now(), nullable=False)
+    timestamp = sa.Column(
+        sa.DateTime(timezone=True), server_default=sa.func.now(), index=True, nullable=False
+    )
 
     @declared_attr
     def __table_args__(cls):  # noqa
         return (
-            sa.UniqueConstraint('serial_nrtm', 'source', name='rpsl_objects_history_serial_nrtm_source_unique'),
+            sa.UniqueConstraint(
+                "serial_nrtm", "source", name="rpsl_objects_history_serial_nrtm_source_unique"
+            ),
         )
 
     def __repr__(self):
-        return f'<{self.source}/{self.serial}/{self.operation}/{self.rpsl_pk}>'
+        return f"<{self.source}/{self.serial}/{self.operation}/{self.rpsl_pk}>"
+
+
+class RPSLDatabaseObjectSuspended(Base):  # type: ignore
+    """
+    SQLAlchemy ORM object for suspended RPSL objects (#577)
+    """
+
+    __tablename__ = "rpsl_objects_suspended"
+
+    pk = sa.Column(pg.UUID(as_uuid=True), server_default=sa.text("gen_random_uuid()"), primary_key=True)
+    rpsl_pk = sa.Column(sa.String, index=True, nullable=False)
+    source = sa.Column(sa.String, index=True, nullable=False)
+
+    object_class = sa.Column(sa.String, nullable=False, index=True)
+    object_text = sa.Column(sa.Text, nullable=False)
+    mntners = sa.Column(pg.ARRAY(sa.Text), nullable=False, index=True)
+
+    timestamp = sa.Column(sa.DateTime(timezone=True), server_default=sa.func.now(), nullable=False)
+    original_created = sa.Column(sa.DateTime(timezone=True), nullable=False)
+    original_updated = sa.Column(sa.DateTime(timezone=True), nullable=False)
+
+    def __repr__(self):
+        return f"<{self.rpsl_pk}/{self.source}/{self.pk}>"
 
 
 class RPSLDatabaseStatus(Base):  # type: ignore
     """
     SQLAlchemy ORM object for the status of authoritative and mirrored DBs.
 
     Note that this database is for keeping status, and is not the source
     of configuration parameters.
     """
-    __tablename__ = 'database_status'
 
-    pk = sa.Column(pg.UUID(as_uuid=True), server_default=sa.text('gen_random_uuid()'), primary_key=True)
+    __tablename__ = "database_status"
+
+    pk = sa.Column(pg.UUID(as_uuid=True), server_default=sa.text("gen_random_uuid()"), primary_key=True)
     source = sa.Column(sa.String, index=True, nullable=False, unique=True)
 
-    # The oldest and newest serials seen, for any reason since the last import
+    # The oldest and newest serial_nrtm's seen, for any reason since the last import
     serial_oldest_seen = sa.Column(sa.Integer)
     serial_newest_seen = sa.Column(sa.Integer)
     # The oldest and newest serials in the current local journal
     serial_oldest_journal = sa.Column(sa.Integer)
     serial_newest_journal = sa.Column(sa.Integer)
     # The serial at which this IRRd last exported
     serial_last_export = sa.Column(sa.Integer)
@@ -148,49 +217,65 @@
     force_reload = sa.Column(sa.Boolean(), default=False, nullable=False)
     synchronised_serials = sa.Column(sa.Boolean(), default=True, nullable=False)
 
     last_error = sa.Column(sa.Text)
     last_error_timestamp = sa.Column(sa.DateTime(timezone=True))
 
     created = sa.Column(sa.DateTime(timezone=True), server_default=sa.func.now(), nullable=False)
-    updated = sa.Column(sa.DateTime(timezone=True), server_default=sa.func.now(), onupdate=sa.func.now(), nullable=False)
+    updated = sa.Column(
+        sa.DateTime(timezone=True), server_default=sa.func.now(), onupdate=sa.func.now(), nullable=False
+    )
 
     def __repr__(self):
         return self.source
 
 
 class ROADatabaseObject(Base):  # type: ignore
     """
     SQLAlchemy ORM object for ROA objects.
     """
-    __tablename__ = 'roa_object'
 
-    pk = sa.Column(pg.UUID(as_uuid=True), server_default=sa.text('gen_random_uuid()'), primary_key=True)
+    __tablename__ = "roa_object"
+
+    pk = sa.Column(pg.UUID(as_uuid=True), server_default=sa.text("gen_random_uuid()"), primary_key=True)
 
     prefix = sa.Column(pg.CIDR, nullable=False)
     asn = sa.Column(sa.BigInteger, nullable=False)
     max_length = sa.Column(sa.Integer, nullable=False)
     trust_anchor = sa.Column(sa.String)
     ip_version = sa.Column(sa.Integer, nullable=False, index=True)
 
     created = sa.Column(sa.DateTime(timezone=True), server_default=sa.func.now(), nullable=False)
 
     @declared_attr
     def __table_args__(cls):  # noqa
         args = [
-            sa.UniqueConstraint('prefix', 'asn', 'max_length', 'trust_anchor', name='roa_object_prefix_asn_maxlength_unique'),
-            sa.Index('ix_roa_objects_prefix_gist', sa.text('prefix inet_ops'), postgresql_using='gist')
+            sa.UniqueConstraint(
+                "prefix", "asn", "max_length", "trust_anchor", name="roa_object_prefix_asn_maxlength_unique"
+            ),
+            sa.Index("ix_roa_objects_prefix_gist", sa.text("prefix inet_ops"), postgresql_using="gist"),
         ]
         return tuple(args)
 
     def __repr__(self):
-        return f'<{self.prefix}/{self.asn}>'
+        return f"<{self.prefix}/{self.asn}>"
 
 
 # Before you update this, please check the storage documentation for changing lookup fields.
 expected_lookup_field_names = {
-    'admin-c', 'tech-c', 'zone-c', 'member-of', 'mnt-by', 'role', 'members', 'person',
-    'mp-members', 'origin', 'mbrs-by-ref',
+    "admin-c",
+    "tech-c",
+    "zone-c",
+    "member-of",
+    "mnt-by",
+    "role",
+    "members",
+    "person",
+    "mp-members",
+    "origin",
+    "mbrs-by-ref",
 }
 if sorted(lookup_field_names()) != sorted(expected_lookup_field_names):  # pragma: no cover
-    raise RuntimeError(f'Field names of lookup fields do not match expected set. Indexes may be missing. '
-                       f'Expected: {expected_lookup_field_names}, actual: {lookup_field_names()}')
+    raise RuntimeError(
+        "Field names of lookup fields do not match expected set. Indexes may be missing. "
+        f"Expected: {expected_lookup_field_names}, actual: {lookup_field_names()}"
+    )
```

### Comparing `irrd-4.2.8/irrd/storage/preload.py` & `irrd-4.3.0/irrd/storage/preload.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,146 +1,168 @@
 import logging
 import random
 import signal
 import threading
 import time
 from collections import defaultdict
-from typing import Optional, List, Set, Dict, Union
+from typing import Dict, List, Optional, Set, Union
 
 import redis
 from setproctitle import setproctitle
 
 from irrd.conf import get_setting
+from irrd.routepref.status import RoutePreferenceStatus
 from irrd.rpki.status import RPKIStatus
 from irrd.scopefilter.status import ScopeFilterStatus
 from irrd.utils.process_support import ExceptionLoggingProcess
+
 from .queries import RPSLDatabaseQuery
 
-SENTINEL_HASH_CREATED = b'SENTINEL_HASH_CREATED'
-REDIS_ORIGIN_ROUTE4_STORE_KEY = b'irrd-preload-origin-route4'
-REDIS_ORIGIN_ROUTE6_STORE_KEY = b'irrd-preload-origin-route6'
-REDIS_PRELOAD_RELOAD_CHANNEL = 'irrd-preload-reload-channel'
-REDIS_PRELOAD_COMPLETE_CHANNEL = 'irrd-preload-complete-channel'
-REDIS_ORIGIN_LIST_SEPARATOR = ','
-REDIS_KEY_ORIGIN_SOURCE_SEPARATOR = '_'
+SENTINEL_HASH_CREATED = b"SENTINEL_HASH_CREATED"
+REDIS_ORIGIN_ROUTE4_STORE_KEY = b"irrd-preload-origin-route4"
+REDIS_ORIGIN_ROUTE6_STORE_KEY = b"irrd-preload-origin-route6"
+REDIS_PRELOAD_RELOAD_CHANNEL = "irrd-preload-reload-channel"
+REDIS_PRELOAD_COMPLETE_CHANNEL = "irrd-preload-complete-channel"
+REDIS_ORIGIN_LIST_SEPARATOR = ","
+REDIS_KEY_ORIGIN_SOURCE_SEPARATOR = "_"
 MAX_MEMORY_LIFETIME = 60
 
 logger = logging.getLogger(__name__)
 
 """
 The preloader allows information to be preloaded into memory.
 
 For queries that repeat often, or repeatedly retrieve nearly the same,
 large data sets, this can improve performance.
 """
 
 
 class PersistentPubSubWorkerThread(redis.client.PubSubWorkerThread):  # type: ignore
+    """
+    This is a variation of PubSubWorkerThread which persists after an error.
+    Rather than terminate, the thread will attempt to reconnect periodically
+    until the connection is re-established.
+    """
+
     def __init__(self, callback, *args, **kwargs):
         self.callback = callback
         self.should_resubscribe = True
         super().__init__(*args, **kwargs)
 
     def run(self):
         self._running.set()
         while self._running.is_set():
             try:
                 if self.should_resubscribe:
                     self.pubsub.subscribe(**{REDIS_PRELOAD_COMPLETE_CHANNEL: self.callback})
                     self.should_resubscribe = False
                 self.pubsub.get_message(ignore_subscribe_messages=True, timeout=self.sleep_time)
             except redis.ConnectionError as rce:  # pragma: no cover
-                logger.error(f'Failed redis pubsub connection, '
-                             f'attempting reconnect and reload in 5s: {rce}')
+                logger.error(f"Failed redis pubsub connection, attempting reconnect and reload in 5s: {rce}")
                 time.sleep(5)
                 self.should_resubscribe = True
             except Exception as exc:  # pragma: no cover
                 logger.error(
-                    f'Error while loading in-memory preload, attempting reconnect and reload in 5s,'
-                    f'traceback follows: {exc}', exc_info=exc)
+                    (
+                        "Error while loading in-memory preload, attempting reconnect and reload in 5s,"
+                        f"traceback follows: {exc}"
+                    ),
+                    exc_info=exc,
+                )
                 time.sleep(5)
                 self.should_resubscribe = True
         self.pubsub.close()  # pragma: no cover
 
 
 class Preloader:
     """
     A preloader object provides access to the preload store.
     It can be used to query the store, or signal that the store
     needs to be updated. This interface can be used from any thread
     or process.
     """
+
     _memory_loaded = False
 
     def __init__(self, enable_queries=True):
         """
         Initialise the preloader.
         If this instance is only used for signalling that the store needs to be
         updated, set enable_queries=False.
         Otherwise, this method starts a background thread that keeps an in-memory store,
         which is automatically updated.
         """
-        self._redis_conn = redis.Redis.from_url(get_setting('redis_url'))
+        self._redis_conn = redis.Redis.from_url(get_setting("redis_url"))
         if enable_queries:
             self._pubsub = self._redis_conn.pubsub()
             self._pubsub_thread = PersistentPubSubWorkerThread(
-                callback=self._load_routes_into_memory,
-                pubsub=self._pubsub,
-                sleep_time=5,
-                daemon=True
+                callback=self._load_routes_into_memory, pubsub=self._pubsub, sleep_time=5, daemon=True
             )
             self._pubsub_thread.start()
-            if get_setting('database_readonly'):  # pragma: no cover
+            if get_setting("database_readonly"):  # pragma: no cover
                 # If this instance is readonly, another IRRd process will be updating
                 # the store, and likely has already done so, meaning we can try to load
                 # from Redis right away instead of waiting for a signal.
                 self._load_routes_into_memory()
 
-    def signal_reload(self, object_classes_changed: Optional[Set[str]]=None) -> None:
+    def signal_reload(self, object_classes_changed: Optional[Set[str]] = None) -> None:
         """
         Perform a (re)load.
         Should be called after changes to the DB have been committed.
 
         This will signal the process running PreloadStoreManager to reload
         the store.
 
         If object_classes_changed is provided, a reload is only performed
         if those classes are relevant to the data in the preload store.
         """
-        relevant_object_classes = {'route', 'route6'}
+        relevant_object_classes = {"route", "route6"}
         if object_classes_changed is None or object_classes_changed.intersection(relevant_object_classes):
-            self._redis_conn.publish(REDIS_PRELOAD_RELOAD_CHANNEL, 'reload')
+            self._redis_conn.publish(REDIS_PRELOAD_RELOAD_CHANNEL, "reload")
 
-    def routes_for_origins(self, origins: Union[List[str], Set[str]], sources: List[str],
-                           ip_version: Optional[int] = None) -> Set[str]:
+    def routes_for_origins(
+        self, origins: Union[List[str], Set[str]], sources: List[str], ip_version: Optional[int] = None
+    ) -> Set[str]:
         """
         Retrieve all prefixes (in str format) originating from the provided origins,
         from the given sources.
 
         Prefixes are guaranteed to be unique. ip_version can be set to 4 or 6
         to restrict responses to IPv4 or IPv6 prefixes. Blocks until the first
         store has been built.
         Origins must be strings in a cleaned format, e.g. AS65537, but not
         AS065537 or as65537.
         This call will block until the preload store is loaded.
         """
         while not self._memory_loaded:
             time.sleep(1)  # pragma: no cover
         if ip_version and ip_version not in [4, 6]:
-            raise ValueError(f'Invalid IP version: {ip_version}')
+            raise ValueError(f"Invalid IP version: {ip_version}")
         if not origins or not sources:
             return set()
 
         prefix_sets: Set[str] = set()
         for source in sources:
             for origin in origins:
-                if (not ip_version or ip_version == 4) and source in self._origin_route4_store and origin in self._origin_route4_store[source]:
-                    prefix_sets.update(self._origin_route4_store[source][origin].split(REDIS_ORIGIN_LIST_SEPARATOR))
-                if (not ip_version or ip_version == 6) and source in self._origin_route6_store and origin in self._origin_route6_store[source]:
-                    prefix_sets.update(self._origin_route6_store[source][origin].split(REDIS_ORIGIN_LIST_SEPARATOR))
+                if (
+                    (not ip_version or ip_version == 4)
+                    and source in self._origin_route4_store
+                    and origin in self._origin_route4_store[source]
+                ):
+                    prefix_sets.update(
+                        self._origin_route4_store[source][origin].split(REDIS_ORIGIN_LIST_SEPARATOR)
+                    )
+                if (
+                    (not ip_version or ip_version == 6)
+                    and source in self._origin_route6_store
+                    and origin in self._origin_route6_store[source]
+                ):
+                    prefix_sets.update(
+                        self._origin_route6_store[source][origin].split(REDIS_ORIGIN_LIST_SEPARATOR)
+                    )
 
         return prefix_sets
 
     def _load_routes_into_memory(self, redis_message=None):
         """
         Update the in-memory store. This is called whenever a
         message is sent to REDIS_PRELOAD_COMPLETE_CHANNEL.
@@ -154,18 +176,18 @@
         new_origin_route4_store = dict()
         new_origin_route6_store = dict()
 
         def _load(redis_key, target):
             for key, routes in self._redis_conn.hgetall(redis_key).items():
                 if key == SENTINEL_HASH_CREATED:
                     continue
-                source, origin = key.decode('ascii').split(REDIS_KEY_ORIGIN_SOURCE_SEPARATOR)
+                source, origin = key.decode("ascii").split(REDIS_KEY_ORIGIN_SOURCE_SEPARATOR)
                 if source not in target:
                     target[source] = dict()
-                target[source][origin] = routes.decode('ascii')
+                target[source][origin] = routes.decode("ascii")
 
         _load(REDIS_ORIGIN_ROUTE4_STORE_KEY, new_origin_route4_store)
         _load(REDIS_ORIGIN_ROUTE6_STORE_KEY, new_origin_route6_store)
 
         self._origin_route4_store = new_origin_route4_store
         self._origin_route6_store = new_origin_route6_store
 
@@ -178,63 +200,65 @@
     it is created and updated.
     There should only be one of these per IRRd instance.
     """
 
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
         self._target = self.main
-        self._redis_conn = redis.Redis.from_url(get_setting('redis_url'))
+        self._redis_conn = redis.Redis.from_url(get_setting("redis_url"))
 
     def main(self):
         """
         Main function for the preload manager.
 
         Monitors a Redis pubsub channel, and triggers a reload when
         a message is received.
         """
-        setproctitle('irrd-preload-store-manager')
+        setproctitle("irrd-preload-store-manager")
         try:
             signal.signal(signal.SIGTERM, signal.SIG_DFL)
         except ValueError:
             # During tests, this is run from a thread,
             # which does not allow setting signal handlers.
             pass
-        logging.info('Starting preload store manager')
+        logging.info("Starting preload store manager")
 
         self._clear_existing_data()
         self._pubsub = self._redis_conn.pubsub()
 
         self._reload_lock = threading.Lock()
         self._threads = []
         self.terminate = False  # Used to exit main() in tests
 
         while not self.terminate:
             self.perform_reload()
             try:
                 self._pubsub.subscribe(REDIS_PRELOAD_RELOAD_CHANNEL)
                 for item in self._pubsub.listen():
-                    if item['type'] == 'message':
-                        logger.debug('Reload requested through redis channel')
+                    if item["type"] == "message":
+                        logger.debug("Reload requested through redis channel")
                         self.perform_reload()
                         if self.terminate:
                             return
             except redis.ConnectionError as rce:  # pragma: no cover
-                logger.error(f'Failed redis pubsub connection, attempting reconnect and reload in 5s: {rce}')
+                logger.error(f"Failed redis pubsub connection, attempting reconnect and reload in 5s: {rce}")
                 time.sleep(5)
 
     def _clear_existing_data(self) -> None:
         """
         Clear the existing data. This is done on startup, to ensure no
         queries are being answered with outdated data.
         """
         try:
             self._redis_conn.delete(REDIS_ORIGIN_ROUTE4_STORE_KEY, REDIS_ORIGIN_ROUTE6_STORE_KEY)
         except redis.ConnectionError as rce:  # pragma: no cover
-            logger.error(f'Failed to empty preload store due to redis connection error, '
-                         f'queries may have outdated results until full reload is completed (max 30s): {rce}')
+            logger.error(
+                "Failed to empty preload store due to redis connection error, "
+                f"queries may have outdated results until full reload is completed (max 30s): {rce}"
+            )
 
     def perform_reload(self) -> None:
         """
         Perform a (re)load.
         Should be called after changes to the DB have been committed.
 
         This will start a new thread to reload the store. If a thread is
@@ -258,31 +282,36 @@
         """
         Store the new route information in redis. Returns True on success, False on failure.
         """
         try:
             pipeline = self._redis_conn.pipeline(transaction=True)
             pipeline.delete(REDIS_ORIGIN_ROUTE4_STORE_KEY, REDIS_ORIGIN_ROUTE6_STORE_KEY)
             # The redis store can't store sets, only strings
-            origin_route4_str_dict = {k: REDIS_ORIGIN_LIST_SEPARATOR.join(v) for k, v in new_origin_route4_store.items()}
-            origin_route6_str_dict = {k: REDIS_ORIGIN_LIST_SEPARATOR.join(v) for k, v in new_origin_route6_store.items()}
+            origin_route4_str_dict = {
+                k: REDIS_ORIGIN_LIST_SEPARATOR.join(v) for k, v in new_origin_route4_store.items()
+            }
+            origin_route6_str_dict = {
+                k: REDIS_ORIGIN_LIST_SEPARATOR.join(v) for k, v in new_origin_route6_store.items()
+            }
             # Redis can't handle empty dicts, but the dict needs to be present
             # in order not to block queries.
-            origin_route4_str_dict[SENTINEL_HASH_CREATED] = '1'
-            origin_route6_str_dict[SENTINEL_HASH_CREATED] = '1'
-            # hmset causes a deprecation warning, but is required for Redis 3 compatibility
-            pipeline.hmset(REDIS_ORIGIN_ROUTE4_STORE_KEY, origin_route4_str_dict)
-            pipeline.hmset(REDIS_ORIGIN_ROUTE6_STORE_KEY, origin_route6_str_dict)
+            origin_route4_str_dict[SENTINEL_HASH_CREATED] = "1"
+            origin_route6_str_dict[SENTINEL_HASH_CREATED] = "1"
+            pipeline.hset(REDIS_ORIGIN_ROUTE4_STORE_KEY, mapping=origin_route4_str_dict)
+            pipeline.hset(REDIS_ORIGIN_ROUTE6_STORE_KEY, mapping=origin_route6_str_dict)
             pipeline.execute()
 
-            self._redis_conn.publish(REDIS_PRELOAD_COMPLETE_CHANNEL, 'complete')
+            self._redis_conn.publish(REDIS_PRELOAD_COMPLETE_CHANNEL, "complete")
             return True
 
         except redis.ConnectionError as rce:  # pragma: no cover
-            logger.error(f'Failed to update preload store due to redis connection error, '
-                         f'attempting new reload in 5s: {rce}')
+            logger.error(
+                "Failed to update preload store due to redis connection error, "
+                f"attempting new reload in 5s: {rce}"
+            )
             time.sleep(5)
             self.perform_reload()
             return False
 
     def _remove_dead_threads(self) -> None:
         """
         Remove dead threads from self.threads(),
@@ -315,16 +344,17 @@
 
         For tests, mock_database_handler can be used to provide a mock.
         """
         self.reload_lock.acquire()
         try:
             self.update(mock_database_handler)
         except Exception as exc:
-            logger.critical(f'Updating preload store failed, retrying in 5s, '
-                            f'traceback follows: {exc}', exc_info=exc)
+            logger.critical(
+                f"Updating preload store failed, retrying in 5s, traceback follows: {exc}", exc_info=exc
+            )
             time.sleep(5)
             self.preloader.perform_reload()
         finally:
             self.reload_lock.release()
 
     def update(self, mock_database_handler=None) -> None:
         """
@@ -332,36 +362,41 @@
 
         After loading the data from the database, sets the two new stores
         on the provided preloader object.
         The lock is then released to allow another thread to start, and
         the store_ready_event set to indicate that the store has been
         loaded at least once, and answers can be provided based on it.
         """
-        logger.debug(f'Starting preload store update from thread {self}')
+        logger.debug(f"Starting preload store update from thread {self}")
 
         new_origin_route4_store: Dict[str, set] = defaultdict(set)
         new_origin_route6_store: Dict[str, set] = defaultdict(set)
 
         if not mock_database_handler:  # pragma: no cover
             from .database_handler import DatabaseHandler
+
             dh = DatabaseHandler(readonly=True)
         else:
             dh = mock_database_handler
 
-        q = RPSLDatabaseQuery(column_names=['ip_version', 'ip_first', 'prefix_length', 'asn_first', 'source'], enable_ordering=False)
-        q = q.object_classes(['route', 'route6']).rpki_status([RPKIStatus.not_found, RPKIStatus.valid])
+        q = RPSLDatabaseQuery(
+            column_names=["ip_version", "ip_first", "prefix_length", "asn_first", "source"],
+            enable_ordering=False,
+        )
+        q = q.object_classes(["route", "route6"]).rpki_status([RPKIStatus.not_found, RPKIStatus.valid])
         q = q.scopefilter_status([ScopeFilterStatus.in_scope])
+        q = q.route_preference_status([RoutePreferenceStatus.visible])
 
         for result in dh.execute_query(q):
-            prefix = result['ip_first']
-            key = result['source'] + REDIS_KEY_ORIGIN_SOURCE_SEPARATOR + 'AS' + str(result['asn_first'])
-            length = result['prefix_length']
-
-            if result['ip_version'] == 4:
-                new_origin_route4_store[key].add(f'{prefix}/{length}')
-            if result['ip_version'] == 6:
-                new_origin_route6_store[key].add(f'{prefix}/{length}')
+            prefix = result["ip_first"]
+            key = result["source"] + REDIS_KEY_ORIGIN_SOURCE_SEPARATOR + "AS" + str(result["asn_first"])
+            length = result["prefix_length"]
+
+            if result["ip_version"] == 4:
+                new_origin_route4_store[key].add(f"{prefix}/{length}")
+            if result["ip_version"] == 6:
+                new_origin_route6_store[key].add(f"{prefix}/{length}")
 
         dh.close()
 
         if self.preloader.update_route_store(new_origin_route4_store, new_origin_route6_store):
-            logger.info(f'Completed updating preload store from thread {self}')
+            logger.info(f"Completed updating preload store from thread {self}")
```

### Comparing `irrd-4.2.8/irrd/storage/queries.py` & `irrd-4.3.0/irrd/storage/queries.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,27 +1,54 @@
 import logging
+from datetime import datetime
 from typing import List, Optional
 
 import sqlalchemy as sa
-from IPy import IP
-from sqlalchemy.sql import Select, ColumnCollection
 import sqlalchemy.dialects.postgresql as pg
+from IPy import IP
+from sqlalchemy.sql import ColumnCollection, Select
 
 from irrd.conf import get_setting
+from irrd.routepref.status import RoutePreferenceStatus
 from irrd.rpki.status import RPKIStatus
 from irrd.rpsl.rpsl_objects import lookup_field_names
 from irrd.scopefilter.status import ScopeFilterStatus
-from irrd.storage.models import (RPSLDatabaseObject, RPSLDatabaseJournal, RPSLDatabaseStatus,
-                                 ROADatabaseObject)
-from irrd.utils.validators import parse_as_number, ValidationError
+from irrd.storage.models import (
+    ROADatabaseObject,
+    RPSLDatabaseJournal,
+    RPSLDatabaseObject,
+    RPSLDatabaseObjectSuspended,
+    RPSLDatabaseStatus,
+)
+from irrd.utils.validators import ValidationError, parse_as_number
 
 logger = logging.getLogger(__name__)
 
 
-class BaseRPSLObjectDatabaseQuery:
+class BaseDatabaseQuery:
+    def __init__(self):  # pragma: no cover
+        self.statement = sa.select([1])
+
+    def __eq__(self, other):
+        return all(
+            [
+                str(self.statement) == str(other.statement),
+                self.statement.compile().params == other.statement.compile().params,
+                getattr(self, "_query_frozen", False) is getattr(other, "_query_frozen", False),
+            ]
+        )
+
+    def finalise_statement(self):
+        return self.statement
+
+    def __repr__(self):
+        return f"{self.__class__.__name__}: {self.statement}\nPARAMS: {self.statement.compile().params}"
+
+
+class BaseRPSLObjectDatabaseQuery(BaseDatabaseQuery):
     statement: Select
     table: sa.Table
     columns: ColumnCollection
 
     def __init__(self, ordered_by_sources=True, enable_ordering=True):
         self._query_frozen = False
         self._sources_list = []
@@ -86,19 +113,19 @@
         each other - particularly statements that determine the sort order of
         the query, which depends on sources_list() and prioritise_source().
         """
         self._query_frozen = True
 
         if self._enable_ordering:
             order_by = []
-            if 'ip_first' in self.columns:
+            if "ip_first" in self.columns:
                 order_by.append(self.columns.ip_first.asc())
-            if 'asn_first' in self.columns:
+            if "asn_first" in self.columns:
                 order_by.append(self.columns.asn_first.asc())
-            if 'rpsl_pk' in self.columns:
+            if "rpsl_pk" in self.columns:
                 order_by.append(self.columns.rpsl_pk.asc())
 
             if self._ordered_by_sources and self._sources_list:
                 case_elements = []
                 for idx, source in enumerate(self._sources_list):
                     case_elements.append((self.columns.source == source, idx + 1))
 
@@ -111,29 +138,30 @@
     def _filter(self, fltr):
         self._check_query_frozen()
         self.statement = self.statement.where(fltr)
         return self
 
     def _check_query_frozen(self) -> None:
         if self._query_frozen:
-            raise ValueError('This query was frozen - no more filters can be applied.')
+            raise ValueError("This query was frozen - no more filters can be applied.")
 
 
 class RPSLDatabaseQuery(BaseRPSLObjectDatabaseQuery):
     """
     RPSL data query builder for retrieving RPSL objects.
 
     Offers various ways to filter, which are always constructed in an AND query.
     For example:
         q = RPSLDatabaseQuery().sources(['NTTCOM']).asn_less_specific(65537)
     would match all objects that refer or include AS65537 (i.e. aut-num, route,
     as-block, route6) from the NTTCOM source.
 
     For methods taking a prefix or IP address, this should be an IPy.IP object.
     """
+
     table = RPSLDatabaseObject.__table__
     columns = RPSLDatabaseObject.__table__.c
     lookup_field_names = lookup_field_names()
 
     def __init__(self, column_names=None, *args, **kwargs):
         super().__init__(*args, **kwargs)
         if column_names is None:
@@ -170,26 +198,28 @@
         Filter on one or more lookup attributes, e.g. mnt-by, or ['admin-c', 'tech-c']
         At least one of the values for at least one of the lookup attributes must
         match one of the items in attr_values. Matching is case-insensitive.
         """
         attr_names = [attr_name.lower() for attr_name in attr_names]
         for attr_name in attr_names:
             if attr_name not in self.lookup_field_names:
-                raise ValueError(f'Invalid lookup attribute: {attr_name}')
+                raise ValueError(f"Invalid lookup attribute: {attr_name}")
         self._check_query_frozen()
 
         value_filters = []
         statement_params = {}
         for attr_name in attr_names:
             for attr_value in attr_values:
                 counter = self._lookup_attr_counter
                 self._lookup_attr_counter += 1
-                value_filters.append(sa.text(f'parsed_data->:lookup_attr_name{counter} ? :lookup_attr_value{counter}'))
-                statement_params[f'lookup_attr_name{counter}'] = attr_name
-                statement_params[f'lookup_attr_value{counter}'] = attr_value.upper()
+                value_filters.append(
+                    sa.text(f"parsed_data->:lookup_attr_name{counter} ? :lookup_attr_value{counter}")
+                )
+                statement_params[f"lookup_attr_name{counter}"] = attr_name
+                statement_params[f"lookup_attr_value{counter}"] = attr_value.upper()
         fltr = sa.or_(*value_filters)
         self.statement = self.statement.where(fltr).params(**statement_params)
 
         return self
 
     def ip_exact(self, ip: IP):
         """
@@ -197,28 +227,28 @@
 
         The provided ip should be an IPy.IP class, and can be a prefix or
         an address.
         """
         fltr = sa.and_(
             self.columns.ip_first == str(ip.net()),
             self.columns.ip_last == str(ip.broadcast()),
-            self.columns.ip_version == ip.version()
+            self.columns.ip_version == ip.version(),
         )
         return self._filter(fltr)
 
     def ip_less_specific(self, ip: IP):
         """Filter any less specifics or exact matches of a prefix."""
         if self._prefix_query_permitted():
             pg_prefix = sa.cast(str(ip), pg.CIDR)
             fltr = self.columns.prefix.op(">>=")(pg_prefix)
         else:
             fltr = sa.and_(
                 self.columns.ip_first <= str(ip.net()),
                 self.columns.ip_last >= str(ip.broadcast()),
-                self.columns.ip_version == ip.version()
+                self.columns.ip_version == ip.version(),
             )
         return self._filter(fltr)
 
     def ip_less_specific_one_level(self, ip: IP):
         """
         Filter one level less specific of a prefix.
 
@@ -230,15 +260,17 @@
         # One level less specific could still have multiple objects.
         # A subquery determines the smallest possible size less specific object,
         # and this is then used to filter for any objects with that size.
         fltr = sa.and_(
             self.columns.ip_first <= str(ip.net()),
             self.columns.ip_last >= str(ip.broadcast()),
             self.columns.ip_version == ip.version(),
-            sa.not_(sa.and_(self.columns.ip_first == str(ip.net()), self.columns.ip_last == str(ip.broadcast()))),
+            sa.not_(
+                sa.and_(self.columns.ip_first == str(ip.net()), self.columns.ip_last == str(ip.broadcast()))
+            ),
         )
         self.statement = self.statement.where(fltr)
 
         size_subquery = self.statement.with_only_columns([self.columns.ip_size])
         size_subquery = size_subquery.order_by(self.columns.ip_size.asc())
         size_subquery = size_subquery.limit(1)
 
@@ -258,15 +290,19 @@
         else:
             fltr = sa.and_(
                 self.columns.ip_first >= str(ip.net()),
                 self.columns.ip_first <= str(ip.broadcast()),
                 self.columns.ip_last <= str(ip.broadcast()),
                 self.columns.ip_last >= str(ip.net()),
                 self.columns.ip_version == ip.version(),
-                sa.not_(sa.and_(self.columns.ip_first == str(ip.net()), self.columns.ip_last == str(ip.broadcast()))),
+                sa.not_(
+                    sa.and_(
+                        self.columns.ip_first == str(ip.net()), self.columns.ip_last == str(ip.broadcast())
+                    )
+                ),
             )
         return self._filter(fltr)
 
     def ip_any(self, ip: IP):
         """
         Filter any less specifics, more specifics or exact matches of a prefix.
 
@@ -289,15 +325,15 @@
                     sa.and_(
                         self.columns.ip_first >= str(ip.net()),
                         self.columns.ip_first <= str(ip.broadcast()),
                         self.columns.ip_last <= str(ip.broadcast()),
                         self.columns.ip_last >= str(ip.net()),
                     ),
                 ),
-                self.columns.ip_version == ip.version()
+                self.columns.ip_version == ip.version(),
             )
         return self._filter(fltr)
 
     def asn(self, asn: int):
         """
         Filter for exact matches on an ASN.
         """
@@ -332,14 +368,21 @@
     def scopefilter_status(self, status: List[ScopeFilterStatus]):
         """
         Filter for RPSL objects with a specific scope filter status.
         """
         fltr = self.columns.scopefilter_status.in_(status)
         return self._filter(fltr)
 
+    def route_preference_status(self, status: List[RoutePreferenceStatus]):
+        """
+        Filter for RPSL objects with a specific route preference filter status.
+        """
+        fltr = self.columns.route_preference_status.in_(status)
+        return self._filter(fltr)
+
     def text_search(self, value: str, extract_asn_ip=True):
         """
         Search the database for a specific free text.
 
         In order, this attempts:
         - If the value is a valid AS number, return all as-block, as-set, aut-num objects
           relating or including that AS number.
@@ -350,110 +393,166 @@
           actual person/role attribute value).
          If extract_asn_ip is False, the first two steps are skipped.
         """
         self._check_query_frozen()
         if extract_asn_ip:
             try:
                 _, asn = parse_as_number(value)
-                return self.object_classes(['as-block', 'as-set', 'aut-num']).asn_less_specific(asn)
+                return self.object_classes(["as-block", "as-set", "aut-num"]).asn_less_specific(asn)
             except ValidationError:
                 pass
 
             try:
                 ip = IP(value)
                 return self.ip_less_specific(ip)
             except ValueError:
                 pass
 
         counter = self._lookup_attr_counter
         self._lookup_attr_counter += 1
         fltr = sa.or_(
             self.columns.rpsl_pk == value.upper(),
             sa.and_(
-                self.columns.object_class == 'person',
-                sa.text(f"parsed_data->>'person' ILIKE :lookup_attr_text_search{counter}")
+                self.columns.object_class == "person",
+                sa.text(f"parsed_data->>'person' ILIKE :lookup_attr_text_search{counter}"),
             ),
             sa.and_(
-                self.columns.object_class == 'role',
-                sa.text(f"parsed_data->>'role' ILIKE :lookup_attr_text_search{counter}")
+                self.columns.object_class == "role",
+                sa.text(f"parsed_data->>'role' ILIKE :lookup_attr_text_search{counter}"),
             ),
         )
         self.statement = self.statement.where(fltr).params(
-            **{f'lookup_attr_text_search{counter}': '%' + value + '%'}
+            **{f"lookup_attr_text_search{counter}": "%" + value + "%"}
         )
         return self
 
     def _prefix_query_permitted(self):
-        return (
-            get_setting('compatibility.inetnum_search_disabled')
-            or (self._set_object_classes and 'inetnum' not in self._set_object_classes)
-        ) and not get_setting('compatibility.irrd42_migration_in_progress')
-
-    def __repr__(self):
-        return f'{self.statement}\nPARAMS: {self.statement.compile().params}'
+        return get_setting("compatibility.inetnum_search_disabled") or (
+            self._set_object_classes and "inetnum" not in self._set_object_classes
+        )
 
 
 class RPSLDatabaseJournalQuery(BaseRPSLObjectDatabaseQuery):
     """
     RPSL data query builder for retrieving the journal,
     analogous to RPSLDatabaseQuery.
     """
+
     table = RPSLDatabaseJournal.__table__
     columns = RPSLDatabaseJournal.__table__.c
 
-    def __init__(self, *args, **kwargs):
+    def __init__(self, column_names=None, *args, **kwargs):
         super().__init__(*args, **kwargs)
-        self.statement = sa.select([
-            self.columns.pk,
-            self.columns.rpsl_pk,
-            self.columns.source,
-            self.columns.serial_nrtm,
-            self.columns.operation,
-            self.columns.object_class,
-            self.columns.object_text,
-            self.columns.origin,
-            self.columns.timestamp,
-        ]).order_by(self.columns.source.asc(), self.columns.serial_nrtm.asc())
+        if column_names is None:
+            columns = [
+                self.columns.pk,
+                self.columns.rpsl_pk,
+                self.columns.source,
+                self.columns.serial_nrtm,
+                self.columns.serial_global,
+                self.columns.operation,
+                self.columns.object_class,
+                self.columns.object_text,
+                self.columns.origin,
+                self.columns.timestamp,
+            ]
+        else:
+            columns = [self.columns.get(name) for name in column_names]
+        self.statement = sa.select(columns).order_by(
+            self.columns.source.asc(), self.columns.serial_nrtm.asc()
+        )
+
+    def entries_before_date(self, timestamp: datetime):
+        """
+        Filter for journal entries before a given date. Used in expiry.
+        """
+        return self._filter(self.columns.timestamp < timestamp)
+
+    def serial_nrtm_range(self, start: int, end: Optional[int] = None):
+        """
+        Filter for NRTM serials within a specific range, inclusive.
+        """
+        return self._filter_range(self.columns.serial_nrtm, start, end)
 
-    def serial_range(self, start: int, end: Optional[int]=None):
+    def serial_global_range(self, start: int, end: Optional[int] = None):
         """
-        Filter for a serials within a specific range, inclusive.
+        Filter for journal-wide serials within a specific range, inclusive.
         """
+        return self._filter_range(self.columns.serial_global, start, end)
+
+    def _filter_range(self, target: sa.Column, start: int, end: Optional[int] = None):
         if end is not None:
-            fltr = sa.and_(self.columns.serial_nrtm >= start, self.columns.serial_nrtm <= end)
+            fltr = sa.and_(target >= start, target <= end)
         else:
-            fltr = self.columns.serial_nrtm >= start
+            fltr = target >= start
         return self._filter(fltr)
 
-    def __repr__(self):
-        return f'RPSLDatabaseJournalQuery: {self.statement}\nPARAMS: {self.statement.compile().params}'
 
+class RPSLDatabaseJournalStatisticsQuery(BaseDatabaseQuery):
+    """
+    Special journal statistics query.
+    """
+
+    table = RPSLDatabaseJournal.__table__
+    columns = RPSLDatabaseJournal.__table__.c
+
+    def __init__(self):
+        self.statement = sa.select(
+            [
+                sa.func.max(self.columns.serial_global).label("max_serial_global"),
+                sa.func.max(self.columns.timestamp).label("max_timestamp"),
+            ]
+        )
+
+
+class RPSLDatabaseSuspendedQuery(BaseRPSLObjectDatabaseQuery):
+    """
+    RPSL data query builder for retrieving suspended objects,
+    analogous to RPSLDatabaseQuery.
+    """
+
+    table = RPSLDatabaseObjectSuspended.__table__
+    columns = RPSLDatabaseObjectSuspended.__table__.c
 
-class DatabaseStatusQuery:
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self.statement = sa.select(self.columns).order_by(self.columns.timestamp.asc())
+
+    def mntner(self, mntner_rpsl_pk: str):
+        """
+        Filter for objects with a specific mntner.
+        """
+        fltr = self.columns.mntners.any(mntner_rpsl_pk)
+        return self._filter(fltr)
+
+
+class DatabaseStatusQuery(BaseDatabaseQuery):
     table = RPSLDatabaseStatus.__table__
     columns = RPSLDatabaseStatus.__table__.c
 
     def __init__(self):
         self._sources_list: List[str] = []
-        self.statement = sa.select([
-            self.columns.pk,
-            self.columns.source,
-            self.columns.serial_oldest_seen,
-            self.columns.serial_newest_seen,
-            self.columns.serial_oldest_journal,
-            self.columns.serial_newest_journal,
-            self.columns.serial_last_export,
-            self.columns.serial_newest_mirror,
-            self.columns.force_reload,
-            self.columns.synchronised_serials,
-            self.columns.last_error,
-            self.columns.last_error_timestamp,
-            self.columns.created,
-            self.columns.updated,
-        ])
+        self.statement = sa.select(
+            [
+                self.columns.pk,
+                self.columns.source,
+                self.columns.serial_oldest_seen,
+                self.columns.serial_newest_seen,
+                self.columns.serial_oldest_journal,
+                self.columns.serial_newest_journal,
+                self.columns.serial_last_export,
+                self.columns.serial_newest_mirror,
+                self.columns.force_reload,
+                self.columns.synchronised_serials,
+                self.columns.last_error,
+                self.columns.last_error_timestamp,
+                self.columns.created,
+                self.columns.updated,
+            ]
+        )
 
     def source(self, source: str):
         """Filter on a source."""
         return self.sources([source])
 
     def sources(self, sources: List[str]):
         """Filter on one or more sources."""
@@ -477,64 +576,52 @@
         self.statement = self.statement.order_by(*order_by)
         return self.statement
 
     def _filter(self, fltr):
         self.statement = self.statement.where(fltr)
         return self
 
-    def __repr__(self):
-        return f'DatabaseStatusQuery: {self.statement}\nPARAMS: {self.statement.compile().params}'
-
 
-class RPSLDatabaseObjectStatisticsQuery:
+class RPSLDatabaseObjectStatisticsQuery(BaseDatabaseQuery):
     """
     Special statistics query, calculating the number of
     objects per object class per source.
     """
+
     table = RPSLDatabaseObject.__table__
     columns = RPSLDatabaseObject.__table__.c
 
     def __init__(self):
-        self.statement = sa.select([
-            self.columns.source,
-            self.columns.object_class,
-            sa.func.count(self.columns.pk).label('count'),
-        ]).group_by(self.columns.source, self.columns.object_class)
-
-    def finalise_statement(self):
-        return self.statement
-
-    def __repr__(self):
-        return f'RPSLDatabaseObjectStatisticsQuery: {self.statement}\nPARAMS: {self.statement.compile().params}'
+        self.statement = sa.select(
+            [
+                self.columns.source,
+                self.columns.object_class,
+                sa.func.count(self.columns.pk).label("count"),
+            ]
+        ).group_by(self.columns.source, self.columns.object_class)
 
 
-class ROADatabaseObjectQuery:
+class ROADatabaseObjectQuery(BaseDatabaseQuery):
     """
     Query builder for ROA objects.
     """
+
     table = ROADatabaseObject.__table__
     columns = ROADatabaseObject.__table__.c
 
     def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        self.statement = sa.select([
-            self.columns.pk,
-            self.columns.prefix,
-            self.columns.asn,
-            self.columns.max_length,
-            self.columns.trust_anchor,
-            self.columns.ip_version,
-        ])
+        self.statement = sa.select(
+            [
+                self.columns.pk,
+                self.columns.prefix,
+                self.columns.asn,
+                self.columns.max_length,
+                self.columns.trust_anchor,
+                self.columns.ip_version,
+            ]
+        )
 
     def ip_less_specific_or_exact(self, ip: IP):
         """Filter any less specifics or exact matches of a prefix."""
-        fltr = sa.and_(
-            self.columns.prefix.op('>>=')(str(ip))
-        )
+        fltr = sa.and_(self.columns.prefix.op(">>=")(str(ip)))
         self.statement = self.statement.where(fltr)
         return self
-
-    def finalise_statement(self):
-        return self.statement
-
-    def __repr__(self):
-        return f'ROADatabaseObjectQuery: {self.statement}\nPARAMS: {self.statement.compile().params}'
```

### Comparing `irrd-4.2.8/irrd/storage/tests/test_preload.py` & `irrd-4.3.0/irrd/storage/tests/test_preload.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,198 +1,230 @@
 import threading
 import time
 from unittest.mock import Mock
 
 import pytest
 
+from irrd.routepref.status import RoutePreferenceStatus
 from irrd.rpki.status import RPKIStatus
 from irrd.scopefilter.status import ScopeFilterStatus
 from irrd.utils.test_utils import flatten_mock_calls
+
 from ..database_handler import DatabaseHandler
-from ..preload import (Preloader, PreloadStoreManager, PreloadUpdater,
-                       REDIS_KEY_ORIGIN_SOURCE_SEPARATOR)
+from ..preload import (
+    REDIS_KEY_ORIGIN_SOURCE_SEPARATOR,
+    Preloader,
+    PreloadStoreManager,
+    PreloadUpdater,
+)
 from ..queries import RPSLDatabaseQuery
 
 # Use different stores in tests
-TEST_REDIS_ORIGIN_ROUTE4_STORE_KEY = 'TEST-irrd-preload-origin-route4'
-TEST_REDIS_ORIGIN_ROUTE6_STORE_KEY = 'TEST-irrd-preload-origin-route6'
-TEST_REDIS_PRELOAD_RELOAD_CHANNEL = 'TEST-irrd-preload-reload-channel'
-TEST_REDIS_PRELOAD_COMPLETE_CHANNEL = 'TEST-irrd-preload-complete-channel'
+TEST_REDIS_ORIGIN_ROUTE4_STORE_KEY = "TEST-irrd-preload-origin-route4"
+TEST_REDIS_ORIGIN_ROUTE6_STORE_KEY = "TEST-irrd-preload-origin-route6"
+TEST_REDIS_PRELOAD_RELOAD_CHANNEL = "TEST-irrd-preload-reload-channel"
+TEST_REDIS_PRELOAD_COMPLETE_CHANNEL = "TEST-irrd-preload-complete-channel"
 
 
 @pytest.fixture()
 def mock_preload_updater(monkeypatch, config_override):
     mock_preload_updater = Mock(spec=PreloadUpdater)
-    monkeypatch.setattr('irrd.storage.preload.PreloadUpdater', mock_preload_updater)
+    monkeypatch.setattr("irrd.storage.preload.PreloadUpdater", mock_preload_updater)
     yield mock_preload_updater
 
 
 @pytest.fixture()
 def mock_redis_keys(monkeypatch, config_override):
-    monkeypatch.setattr('irrd.storage.preload.REDIS_ORIGIN_ROUTE4_STORE_KEY', TEST_REDIS_ORIGIN_ROUTE4_STORE_KEY)
-    monkeypatch.setattr('irrd.storage.preload.REDIS_ORIGIN_ROUTE6_STORE_KEY', TEST_REDIS_ORIGIN_ROUTE6_STORE_KEY)
-    monkeypatch.setattr('irrd.storage.preload.REDIS_PRELOAD_RELOAD_CHANNEL', TEST_REDIS_PRELOAD_RELOAD_CHANNEL)
-    monkeypatch.setattr('irrd.storage.preload.REDIS_PRELOAD_COMPLETE_CHANNEL', TEST_REDIS_PRELOAD_COMPLETE_CHANNEL)
+    monkeypatch.setattr(
+        "irrd.storage.preload.REDIS_ORIGIN_ROUTE4_STORE_KEY", TEST_REDIS_ORIGIN_ROUTE4_STORE_KEY
+    )
+    monkeypatch.setattr(
+        "irrd.storage.preload.REDIS_ORIGIN_ROUTE6_STORE_KEY", TEST_REDIS_ORIGIN_ROUTE6_STORE_KEY
+    )
+    monkeypatch.setattr(
+        "irrd.storage.preload.REDIS_PRELOAD_RELOAD_CHANNEL", TEST_REDIS_PRELOAD_RELOAD_CHANNEL
+    )
+    monkeypatch.setattr(
+        "irrd.storage.preload.REDIS_PRELOAD_COMPLETE_CHANNEL", TEST_REDIS_PRELOAD_COMPLETE_CHANNEL
+    )
 
 
 class TestPreloading:
     def test_load_reload_thread_management(self, mock_preload_updater, mock_redis_keys):
         preload_manager = PreloadStoreManager()
 
         preload_manager_thread = threading.Thread(target=preload_manager.main, daemon=True)
         preload_manager_thread.start()
         time.sleep(1)
-        assert mock_preload_updater.mock_calls[0][0] == ''
+        assert mock_preload_updater.mock_calls[0][0] == ""
         assert mock_preload_updater.mock_calls[0][1][0] == preload_manager
         assert mock_preload_updater.mock_calls[0][1][1] == preload_manager._reload_lock
-        assert mock_preload_updater.mock_calls[1][0] == '().start'
+        assert mock_preload_updater.mock_calls[1][0] == "().start"
         assert len(mock_preload_updater.mock_calls) == 2
         assert len(preload_manager._threads) == 1
         mock_preload_updater.reset_mock()
 
         preload_manager.perform_reload()
 
-        assert mock_preload_updater.mock_calls[0][0] == '().is_alive'
-        assert mock_preload_updater.mock_calls[1][0] == ''
+        assert mock_preload_updater.mock_calls[0][0] == "().is_alive"
+        assert mock_preload_updater.mock_calls[1][0] == ""
         assert mock_preload_updater.mock_calls[1][1][0] == preload_manager
-        assert mock_preload_updater.mock_calls[2][0] == '().start'
+        assert mock_preload_updater.mock_calls[2][0] == "().start"
         assert len(mock_preload_updater.mock_calls) == 3
         assert len(preload_manager._threads) == 2
         mock_preload_updater.reset_mock()
 
         # Two threads already running, do nothing
         preload_manager.perform_reload()
 
-        assert mock_preload_updater.mock_calls[0][0] == '().is_alive'
-        assert mock_preload_updater.mock_calls[1][0] == '().is_alive'
+        assert mock_preload_updater.mock_calls[0][0] == "().is_alive"
+        assert mock_preload_updater.mock_calls[1][0] == "().is_alive"
         assert len(mock_preload_updater.mock_calls) == 2
         assert len(preload_manager._threads) == 2
         mock_preload_updater.reset_mock()
 
         # Assume all threads are dead
         for thread in preload_manager._threads:
             thread.is_alive = lambda: False
 
         # Reload through the redis channel. First call is ignored, inetnums are not relevant.
-        Preloader().signal_reload({'inetnum'})
+        Preloader().signal_reload({"inetnum"})
         Preloader().signal_reload()
         Preloader().signal_reload()
         time.sleep(0.5)
 
         # As all threads are considered dead, a new thread should be started
-        assert mock_preload_updater.mock_calls[0][0] == ''
-        assert mock_preload_updater.mock_calls[1][0] == '().start'
+        assert mock_preload_updater.mock_calls[0][0] == ""
+        assert mock_preload_updater.mock_calls[1][0] == "().start"
 
         # Listen() on redis is blocking, unblock it after setting terminate
         preload_manager.terminate = True
         Preloader().signal_reload()
 
     def test_routes_for_origins(self, mock_redis_keys):
         preloader = Preloader()
         preload_manager = PreloadStoreManager()
 
         # Wait for the preloader instance to start listening on pubsub
         time.sleep(1)
 
         preload_manager.update_route_store(
             {
-                f'TEST2{REDIS_KEY_ORIGIN_SOURCE_SEPARATOR}AS65546': {'192.0.2.0/25'},
-                f'TEST1{REDIS_KEY_ORIGIN_SOURCE_SEPARATOR}AS65547': {'192.0.2.128/25', '198.51.100.0/25'},
+                f"TEST2{REDIS_KEY_ORIGIN_SOURCE_SEPARATOR}AS65546": {"192.0.2.0/25"},
+                f"TEST1{REDIS_KEY_ORIGIN_SOURCE_SEPARATOR}AS65547": {"192.0.2.128/25", "198.51.100.0/25"},
             },
             {
-                f'TEST2{REDIS_KEY_ORIGIN_SOURCE_SEPARATOR}AS65547': {'2001:db8::/32'},
+                f"TEST2{REDIS_KEY_ORIGIN_SOURCE_SEPARATOR}AS65547": {"2001:db8::/32"},
             },
         )
 
-        sources = ['TEST1', 'TEST2']
+        sources = ["TEST1", "TEST2"]
         assert preloader.routes_for_origins([], sources) == set()
-        assert preloader.routes_for_origins(['AS65545'], sources) == set()
-        assert preloader.routes_for_origins(['AS65546'], []) == set()
-        assert preloader.routes_for_origins(['AS65546'], sources, 4) == {'192.0.2.0/25'}
-        assert preloader.routes_for_origins(['AS65547'], sources, 4) == {'192.0.2.128/25', '198.51.100.0/25'}
-        assert preloader.routes_for_origins(['AS65546'], sources, 6) == set()
-        assert preloader.routes_for_origins(['AS65547'], sources, 6) == {'2001:db8::/32'}
-        assert preloader.routes_for_origins(['AS65546'], sources) == {'192.0.2.0/25'}
-        assert preloader.routes_for_origins(['AS65547'], sources) == {'192.0.2.128/25', '198.51.100.0/25', '2001:db8::/32'}
-        assert preloader.routes_for_origins(['AS65547', 'AS65546'], sources, 4) == {'192.0.2.0/25', '192.0.2.128/25', '198.51.100.0/25'}
-
-        assert preloader.routes_for_origins(['AS65547', 'AS65546'], ['TEST1']) == {'192.0.2.128/25', '198.51.100.0/25'}
-        assert preloader.routes_for_origins(['AS65547', 'AS65546'], ['TEST2']) == {'192.0.2.0/25', '2001:db8::/32'}
+        assert preloader.routes_for_origins(["AS65545"], sources) == set()
+        assert preloader.routes_for_origins(["AS65546"], []) == set()
+        assert preloader.routes_for_origins(["AS65546"], sources, 4) == {"192.0.2.0/25"}
+        assert preloader.routes_for_origins(["AS65547"], sources, 4) == {"192.0.2.128/25", "198.51.100.0/25"}
+        assert preloader.routes_for_origins(["AS65546"], sources, 6) == set()
+        assert preloader.routes_for_origins(["AS65547"], sources, 6) == {"2001:db8::/32"}
+        assert preloader.routes_for_origins(["AS65546"], sources) == {"192.0.2.0/25"}
+        assert preloader.routes_for_origins(["AS65547"], sources) == {
+            "192.0.2.128/25",
+            "198.51.100.0/25",
+            "2001:db8::/32",
+        }
+        assert preloader.routes_for_origins(["AS65547", "AS65546"], sources, 4) == {
+            "192.0.2.0/25",
+            "192.0.2.128/25",
+            "198.51.100.0/25",
+        }
+
+        assert preloader.routes_for_origins(["AS65547", "AS65546"], ["TEST1"]) == {
+            "192.0.2.128/25",
+            "198.51.100.0/25",
+        }
+        assert preloader.routes_for_origins(["AS65547", "AS65546"], ["TEST2"]) == {
+            "192.0.2.0/25",
+            "2001:db8::/32",
+        }
 
         with pytest.raises(ValueError) as ve:
-            preloader.routes_for_origins(['AS65547'], [], 2)
-        assert 'Invalid IP version: 2' in str(ve.value)
+            preloader.routes_for_origins(["AS65547"], [], 2)
+        assert "Invalid IP version: 2" in str(ve.value)
 
 
 class TestPreloadUpdater:
     def test_preload_updater(self, monkeypatch):
         mock_database_handler = Mock(spec=DatabaseHandler)
         mock_database_query = Mock(spec=RPSLDatabaseQuery)
-        monkeypatch.setattr('irrd.storage.preload.RPSLDatabaseQuery',
-                            lambda column_names, enable_ordering: mock_database_query)
+        monkeypatch.setattr(
+            "irrd.storage.preload.RPSLDatabaseQuery",
+            lambda column_names, enable_ordering: mock_database_query,
+        )
         mock_reload_lock = Mock()
         mock_preload_obj = Mock()
 
         mock_query_result = [
             {
-                'ip_version': 4,
-                'ip_first': '192.0.2.0',
-                'prefix_length': 25,
-                'asn_first': 65546,
-                'source': 'TEST1',
-            },
-            {
-                'ip_version': 4,
-                'ip_first': '192.0.2.128',
-                'prefix_length': 25,
-                'asn_first': 65547,
-                'source': 'TEST1',
-            },
-            {
-                'ip_version': 4,
-                'ip_first': '198.51.100.0',
-                'prefix_length': 25,
-                'asn_first': 65547,
-                'source': 'TEST1',
-            },
-            {
-                'ip_version': 6,
-                'ip_first': '2001:db8::',
-                'prefix_length': 32,
-                'asn_first': 65547,
-                'source': 'TEST2',
+                "ip_version": 4,
+                "ip_first": "192.0.2.0",
+                "prefix_length": 25,
+                "asn_first": 65546,
+                "source": "TEST1",
+            },
+            {
+                "ip_version": 4,
+                "ip_first": "192.0.2.128",
+                "prefix_length": 25,
+                "asn_first": 65547,
+                "source": "TEST1",
+            },
+            {
+                "ip_version": 4,
+                "ip_first": "198.51.100.0",
+                "prefix_length": 25,
+                "asn_first": 65547,
+                "source": "TEST1",
+            },
+            {
+                "ip_version": 6,
+                "ip_first": "2001:db8::",
+                "prefix_length": 32,
+                "asn_first": 65547,
+                "source": "TEST2",
             },
         ]
         mock_database_handler.execute_query = lambda query: mock_query_result
         PreloadUpdater(mock_preload_obj, mock_reload_lock).run(mock_database_handler)
 
-        assert flatten_mock_calls(mock_reload_lock) == [['acquire', (), {}], ['release', (), {}]]
+        assert flatten_mock_calls(mock_reload_lock) == [["acquire", (), {}], ["release", (), {}]]
         assert flatten_mock_calls(mock_database_query) == [
-            ['object_classes', (['route', 'route6'],), {}],
-            ['rpki_status', ([RPKIStatus.not_found, RPKIStatus.valid],), {}],
-            ['scopefilter_status', ([ScopeFilterStatus.in_scope],), {}],
+            ["object_classes", (["route", "route6"],), {}],
+            ["rpki_status", ([RPKIStatus.not_found, RPKIStatus.valid],), {}],
+            ["scopefilter_status", ([ScopeFilterStatus.in_scope],), {}],
+            ["route_preference_status", ([RoutePreferenceStatus.visible],), {}],
         ]
 
         assert flatten_mock_calls(mock_preload_obj) == [
             [
-                'update_route_store',
+                "update_route_store",
                 (
                     {
-                        f'TEST1{REDIS_KEY_ORIGIN_SOURCE_SEPARATOR}AS65546': {'192.0.2.0/25'},
-                        f'TEST1{REDIS_KEY_ORIGIN_SOURCE_SEPARATOR}AS65547': {'192.0.2.128/25', '198.51.100.0/25'},
-                    },
-                    {
-                        f'TEST2{REDIS_KEY_ORIGIN_SOURCE_SEPARATOR}AS65547': {'2001:db8::/32'}
+                        f"TEST1{REDIS_KEY_ORIGIN_SOURCE_SEPARATOR}AS65546": {"192.0.2.0/25"},
+                        f"TEST1{REDIS_KEY_ORIGIN_SOURCE_SEPARATOR}AS65547": {
+                            "192.0.2.128/25",
+                            "198.51.100.0/25",
+                        },
                     },
+                    {f"TEST2{REDIS_KEY_ORIGIN_SOURCE_SEPARATOR}AS65547": {"2001:db8::/32"}},
                 ),
-                {}
+                {},
             ]
         ]
 
     def test_preload_updater_failure(self, caplog):
         mock_database_handler = Mock()
         mock_reload_lock = Mock()
         mock_preload_obj = Mock()
         PreloadUpdater(mock_preload_obj, mock_reload_lock).run(mock_database_handler)
 
-        assert 'Updating preload store failed' in caplog.text
-        assert flatten_mock_calls(mock_reload_lock) == [['acquire', (), {}], ['release', (), {}]]
+        assert "Updating preload store failed" in caplog.text
+        assert flatten_mock_calls(mock_reload_lock) == [["acquire", (), {}], ["release", (), {}]]
```

### Comparing `irrd-4.2.8/irrd/updates/email.py` & `irrd-4.3.0/irrd/updates/email.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,69 +1,94 @@
 # flake8: noqa: W293
 import logging
 import textwrap
 from typing import Optional
 
 from irrd.utils import email
+
 from .handler import ChangeSubmissionHandler
 
 logger = logging.getLogger(__name__)
 
 
 def handle_email_submission(email_txt: str) -> Optional[ChangeSubmissionHandler]:
     handler = None
     try:
         msg = email.EmailParser(email_txt)
         request_meta = {
-            'Message-ID': msg.message_id,
-            'From': msg.message_from,
-            'Date': msg.message_date,
-            'Subject': msg.message_subject,
+            "Message-ID": msg.message_id,
+            "From": msg.message_from,
+            "Date": msg.message_date,
+            "Subject": msg.message_subject,
         }
     except Exception as exc:
-        logger.critical(f'An exception occurred while attempting to parse the following update e-mail: {email_txt}\n'
-                        f'--- traceback for {exc} follows:', exc_info=exc)
+        logger.critical(
+            (
+                f"An exception occurred while attempting to parse the following update e-mail: {email_txt}\n"
+                f"--- traceback for {exc} follows:"
+            ),
+            exc_info=exc,
+        )
         return None
 
     if not msg.message_from:
-        logger.critical(f'No from address was found while attempting to parse the following update e-mail - '
-                        f'update not processed: {email_txt}\n')
+        logger.critical(
+            "No from address was found while attempting to parse the following update e-mail - "
+            f"update not processed: {email_txt}\n"
+        )
         return None
 
     try:
         if not msg.body:
-            logger.warning(f'Unable to extract message body from e-mail {msg.message_id} from {msg.message_from}')
-            subject = f'FAILED: {msg.message_subject}'
-            reply_content = textwrap.dedent(f"""
+            logger.warning(
+                f"Unable to extract message body from e-mail {msg.message_id} from {msg.message_from}"
+            )
+            subject = f"FAILED: {msg.message_subject}"
+            reply_content = textwrap.dedent(
+                f"""
             Unfortunately, your message with ID {msg.message_id}
             could not be processed, as no text/plain part could be found.
             
             Please try to resend your message as plain text email.
-            """)
+            """
+            )
         else:
             handler = ChangeSubmissionHandler().load_text_blob(msg.body, msg.pgp_fingerprint, request_meta)
-            logger.info(f'Processed e-mail {msg.message_id} from {msg.message_from}: {handler.status()}')
-            logger.debug(f'Report for e-mail {msg.message_id} from {msg.message_from}: {handler.submitter_report_human()}')
+            logger.info(f"Processed e-mail {msg.message_id} from {msg.message_from}: {handler.status()}")
+            logger.debug(
+                f"Report for e-mail {msg.message_id} from {msg.message_from}:"
+                f" {handler.submitter_report_human()}"
+            )
 
-            subject = f'{handler.status()}: {msg.message_subject}'
+            subject = f"{handler.status()}: {msg.message_subject}"
             reply_content = handler.submitter_report_human()
 
     except Exception as exc:
-        logger.critical(f'An exception occurred while attempting to process the following update: {email_txt}\n'
-                        f'--- traceback for {exc} follows:', exc_info=exc)
-        subject = f'ERROR: {msg.message_subject}'
-        reply_content = textwrap.dedent(f"""
+        logger.critical(
+            (
+                f"An exception occurred while attempting to process the following update: {email_txt}\n"
+                f"--- traceback for {exc} follows:"
+            ),
+            exc_info=exc,
+        )
+        subject = f"ERROR: {msg.message_subject}"
+        reply_content = textwrap.dedent(
+            f"""
         Unfortunately, your message with ID {msg.message_id}
         could not be processed, due to an internal error.
-        """)
+        """
+        )
 
     try:
         email.send_email(msg.message_from, subject, reply_content)
         if handler:
             handler.send_notification_target_reports()
     except Exception as exc:
-        logger.critical(f'An exception occurred while attempting to send a reply to an update: '
-                        f'{subject}\n{reply_content}\n --- traceback for {exc} follows:', exc_info=exc)
+        logger.critical(
+            (
+                "An exception occurred while attempting to send a reply to an update: "
+                f"{subject}\n{reply_content}\n --- traceback for {exc} follows:"
+            ),
+            exc_info=exc,
+        )
 
     return handler
-
-
```

### Comparing `irrd-4.2.8/irrd/updates/handler.py` & `irrd-4.3.0/irrd/updates/handler.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,56 +1,69 @@
 import logging
 import textwrap
 from collections import defaultdict
-from typing import List, Optional, Dict, Any, Union
+from typing import Any, Dict, List, Optional, Union
 
 from ordered_set import OrderedSet
 
 from irrd.conf import get_setting
+from irrd.rpsl.rpsl_objects import RPSLMntner
 from irrd.storage.database_handler import DatabaseHandler
 from irrd.storage.queries import RPSLDatabaseQuery
 from irrd.utils import email
-from .parser import parse_change_requests, ChangeRequest
-from .parser_state import UpdateRequestStatus, UpdateRequestType
-from .validators import ReferenceValidator, AuthValidator
-from ..utils.validators import RPSLChangeSubmission
+
+from ..utils.validators import RPSLChangeSubmission, RPSLSuspensionSubmission
+from .parser import ChangeRequest, SuspensionRequest, parse_change_requests
+from .parser_state import SuspensionRequestType, UpdateRequestStatus, UpdateRequestType
+from .validators import AuthValidator, ReferenceValidator
 
 logger = logging.getLogger(__name__)
 
 
 class ChangeSubmissionHandler:
     """
     The ChangeSubmissionHandler handles the text of one or more requested RPSL changes
     (create, modify or delete), parses, validates and eventually saves
     them. This includes validating references between objects, including
     those part of the same message, and checking authentication.
     """
 
-    def load_text_blob(self, object_texts_blob: str, pgp_fingerprint: str=None, request_meta: Dict[str, Optional[str]]=None):
+    def load_text_blob(
+        self,
+        object_texts_blob: str,
+        pgp_fingerprint: Optional[str] = None,
+        request_meta: Optional[Dict[str, Optional[str]]] = None,
+    ):
         self.database_handler = DatabaseHandler()
         self.request_meta = request_meta if request_meta else {}
         self._pgp_key_id = self._resolve_pgp_key_id(pgp_fingerprint) if pgp_fingerprint else None
 
         reference_validator = ReferenceValidator(self.database_handler)
         auth_validator = AuthValidator(self.database_handler, self._pgp_key_id)
-        change_requests = parse_change_requests(object_texts_blob, self.database_handler,
-                                                auth_validator, reference_validator)
+        change_requests = parse_change_requests(
+            object_texts_blob, self.database_handler, auth_validator, reference_validator
+        )
 
         self._handle_change_requests(change_requests, reference_validator, auth_validator)
         self.database_handler.commit()
         self.database_handler.close()
         return self
 
-    def load_change_submission(self, data: RPSLChangeSubmission, delete=False, request_meta: Dict[str, Optional[str]]=None):
+    def load_change_submission(
+        self,
+        data: RPSLChangeSubmission,
+        delete=False,
+        request_meta: Optional[Dict[str, Optional[str]]] = None,
+    ):
         self.database_handler = DatabaseHandler()
         self.request_meta = request_meta if request_meta else {}
 
         reference_validator = ReferenceValidator(self.database_handler)
         auth_validator = AuthValidator(self.database_handler)
-        change_requests = []
+        change_requests: List[Union[ChangeRequest, SuspensionRequest]] = []
 
         delete_reason = None
         if delete:
             delete_reason = data.delete_reason
 
         auth_validator.passwords = data.passwords
         auth_validator.overrides = [data.override] if data.override else []
@@ -58,109 +71,153 @@
         for rpsl_obj in data.objects:
             object_text = rpsl_obj.object_text
             if rpsl_obj.attributes:
                 # We don't have a neat way to process individual attribute pairs,
                 # so construct a pseudo-object by appending the text.
                 composite_object = []
                 for attribute in rpsl_obj.attributes:
-                    composite_object.append(attribute.name + ': ' + attribute.value)  # type: ignore
-                object_text = '\n'.join(composite_object) + '\n'
+                    composite_object.append(attribute.name + ": " + attribute.value)  # type: ignore
+                object_text = "\n".join(composite_object) + "\n"
 
             assert object_text  # enforced by pydantic
-            change_requests.append(ChangeRequest(
-                object_text,
-                self.database_handler,
-                auth_validator,
-                reference_validator,
-                delete_reason
-            ))
+            change_requests.append(
+                ChangeRequest(
+                    object_text, self.database_handler, auth_validator, reference_validator, delete_reason
+                )
+            )
 
         self._handle_change_requests(change_requests, reference_validator, auth_validator)
         self.database_handler.commit()
         self.database_handler.close()
         return self
 
-    def _handle_change_requests(self, change_requests: List[ChangeRequest],
-                             reference_validator: ReferenceValidator,
-                             auth_validator: AuthValidator) -> None:
+    def load_suspension_submission(
+        self, data: RPSLSuspensionSubmission, request_meta: Optional[Dict[str, Optional[str]]] = None
+    ):
+        self.database_handler = DatabaseHandler()
+        self.request_meta = request_meta if request_meta else {}
+
+        reference_validator = ReferenceValidator(self.database_handler)
+        auth_validator = AuthValidator(self.database_handler)
+        change_requests: List[Union[ChangeRequest, SuspensionRequest]] = []
+
+        auth_validator.overrides = [data.override] if data.override else []
+
+        for rpsl_obj in data.objects:
+            # We don't have a neat way to process individual attribute pairs,
+            # so construct a pseudo-object by appending the text.
+            object_text = f"mntner: {rpsl_obj.mntner}\nsource: {rpsl_obj.source}\n"
+            change_requests.append(
+                SuspensionRequest(
+                    object_text,
+                    self.database_handler,
+                    auth_validator,
+                    rpsl_obj.request_type.value,
+                )
+            )
 
+        self._handle_change_requests(change_requests, reference_validator, auth_validator)
+        self.database_handler.commit()
+        self.database_handler.close()
+        return self
+
+    def _handle_change_requests(
+        self,
+        change_requests: List[Union[ChangeRequest, SuspensionRequest]],
+        reference_validator: ReferenceValidator,
+        auth_validator: AuthValidator,
+    ) -> None:
+        objects = ", ".join(
+            [f"{request.rpsl_obj_new} (request {id(request)})" for request in change_requests]
+        )
+        logger.info(f"Processing change requests for {objects}, metadata is {self.request_meta}")
         # When an object references another object, e.g. tech-c referring a person or mntner,
         # an add/update is only valid if those referred objects exist. To complicate matters,
         # the object referred to may be part of this very same submission. For this reason, the
         # reference validator can be provided with all new objects to be added in this submission.
         # However, a possible scenario is that A, B and C are submitted. Object A refers to B,
         # B refers to C, C refers to D and D does not exist - or C fails authentication.
         # At a first scan, A is valid because B exists, B is valid because C exists. C
         # becomes invalid on the first scan, which is why another scan is performed, which
         # will mark B invalid due to the reference to an invalid C, etc. This continues until
         # all references are resolved and repeated scans lead to the same conclusions.
         valid_changes = [r for r in change_requests if r.is_valid()]
-        previous_valid_changes: List[ChangeRequest] = []
+        previous_valid_changes: List[Union[ChangeRequest, SuspensionRequest]] = []
         loop_count = 0
         loop_max = len(change_requests) + 10
 
         while valid_changes != previous_valid_changes:
             previous_valid_changes = valid_changes
             reference_validator.preload(valid_changes)
-            auth_validator.pre_approve(valid_changes)
+            valid_potential_new_mntners = [
+                r.rpsl_obj_new
+                for r in valid_changes
+                if r.request_type == UpdateRequestType.CREATE and isinstance(r.rpsl_obj_new, RPSLMntner)
+            ]
+            auth_validator.pre_approve(valid_potential_new_mntners)
 
             for result in valid_changes:
                 result.validate()
             valid_changes = [r for r in change_requests if r.is_valid()]
 
             loop_count += 1
             if loop_count > loop_max:  # pragma: no cover
-                msg = f'Update validity resolver ran an excessive amount of loops, may be stuck, aborting ' \
-                      f'processing. Message metadata: {self.request_meta}'
+                msg = (
+                    "Update validity resolver ran an excessive amount of loops, may be stuck, aborting "
+                    f"processing. Message metadata: {self.request_meta}"
+                )
                 logger.error(msg)
                 raise ValueError(msg)
 
         for result in change_requests:
             if result.is_valid():
-                result.save(self.database_handler)
+                result.save()
 
         self.results = change_requests
 
     def _resolve_pgp_key_id(self, pgp_fingerprint: str) -> Optional[str]:
         """
         Find a PGP key ID for a given fingerprint.
         This method looks for an actual matching object in the database,
         and then returns the object's PK.
         """
-        clean_fingerprint = pgp_fingerprint.replace(' ', '')
-        key_id = 'PGPKEY-' + clean_fingerprint[-8:]
-        query = RPSLDatabaseQuery().object_classes(['key-cert']).rpsl_pk(key_id)
+        clean_fingerprint = pgp_fingerprint.replace(" ", "")
+        key_id = "PGPKEY-" + clean_fingerprint[-8:]
+        query = RPSLDatabaseQuery().object_classes(["key-cert"]).rpsl_pk(key_id)
         results = list(self.database_handler.execute_query(query))
 
         for result in results:
-            if result['parsed_data'].get('fingerpr', '').replace(' ', '') == clean_fingerprint:
+            if result["parsed_data"].get("fingerpr", "").replace(" ", "") == clean_fingerprint:
                 return key_id
-        logger.info(f'Message was signed with key {key_id}, but key was not found in the database. Treating message '
-                    f'as unsigned. Message metadata: {self.request_meta}')
+        logger.info(
+            f"Message was signed with key {key_id}, but key was not found in the database. Treating message "
+            f"as unsigned. Message metadata: {self.request_meta}"
+        )
         return None
 
     def status(self) -> str:
         """Provide a simple SUCCESS/FAILED string based - former used if all objects were saved."""
         if all([result.status == UpdateRequestStatus.SAVED for result in self.results]):
-            return 'SUCCESS'
-        return 'FAILED'
+            return "SUCCESS"
+        return "FAILED"
 
     def submitter_report_human(self) -> str:
         """Produce a human-readable report for the submitter."""
         # flake8: noqa: W293
         successful = [r for r in self.results if r.status == UpdateRequestStatus.SAVED]
         failed = [r for r in self.results if r.status != UpdateRequestStatus.SAVED]
         number_successful_create = len([r for r in successful if r.request_type == UpdateRequestType.CREATE])
         number_successful_modify = len([r for r in successful if r.request_type == UpdateRequestType.MODIFY])
         number_successful_delete = len([r for r in successful if r.request_type == UpdateRequestType.DELETE])
         number_failed_create = len([r for r in failed if r.request_type == UpdateRequestType.CREATE])
         number_failed_modify = len([r for r in failed if r.request_type == UpdateRequestType.MODIFY])
         number_failed_delete = len([r for r in failed if r.request_type == UpdateRequestType.DELETE])
 
-        user_report = self._request_meta_str() + textwrap.dedent(f"""
+        user_report = self._request_meta_str() + textwrap.dedent(
+            f"""
         SUMMARY OF UPDATE:
 
         Number of objects found:                  {len(self.results):3}
         Number of objects processed successfully: {len(successful):3}
             Create:      {number_successful_create:3}
             Modify:      {number_successful_modify:3}
             Delete:      {number_successful_delete:3}
@@ -168,47 +225,48 @@
             Create:      {number_failed_create:3}
             Modify:      {number_failed_modify:3}
             Delete:      {number_failed_delete:3}
             
         DETAILED EXPLANATION:
         
         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-        """)
+        """
+        )
         for result in self.results:
-            user_report += '---\n'
+            user_report += "---\n"
             user_report += result.submitter_report_human()
-            user_report += '\n'
-        user_report += '~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n'
+            user_report += "\n"
+        user_report += "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
         return user_report
 
     def submitter_report_json(self):
         """Produce a JSON-ready report for the submitter."""
         successful = [r for r in self.results if r.status == UpdateRequestStatus.SAVED]
         failed = [r for r in self.results if r.status != UpdateRequestStatus.SAVED]
         number_successful_create = len([r for r in successful if r.request_type == UpdateRequestType.CREATE])
         number_successful_modify = len([r for r in successful if r.request_type == UpdateRequestType.MODIFY])
         number_successful_delete = len([r for r in successful if r.request_type == UpdateRequestType.DELETE])
         number_failed_create = len([r for r in failed if r.request_type == UpdateRequestType.CREATE])
         number_failed_modify = len([r for r in failed if r.request_type == UpdateRequestType.MODIFY])
         number_failed_delete = len([r for r in failed if r.request_type == UpdateRequestType.DELETE])
 
         return {
-            'request_meta': self.request_meta,
-            'summary': {
-                'objects_found': len(self.results),
-                'successful': len(successful),
-                'successful_create': number_successful_create,
-                'successful_modify': number_successful_modify,
-                'successful_delete': number_successful_delete,
-                'failed': len(failed),
-                'failed_create': number_failed_create,
-                'failed_modify': number_failed_modify,
-                'failed_delete': number_failed_delete,
+            "request_meta": self.request_meta,
+            "summary": {
+                "objects_found": len(self.results),
+                "successful": len(successful),
+                "successful_create": number_successful_create,
+                "successful_modify": number_successful_modify,
+                "successful_delete": number_successful_delete,
+                "failed": len(failed),
+                "failed_create": number_failed_create,
+                "failed_modify": number_failed_modify,
+                "failed_delete": number_failed_delete,
             },
-            'objects': [result.submitter_report_json() for result in self.results],
+            "objects": [result.submitter_report_json() for result in self.results],
         }
 
     def send_notification_target_reports(self):
         # First key is e-mail address of recipient, second is UpdateRequestStatus.SAVED
         # or UpdateRequestStatus.ERROR_AUTH
         reports_per_recipient: Dict[str, Dict[UpdateRequestStatus, OrderedSet]] = defaultdict(dict)
         sources: OrderedSet[str] = OrderedSet()
@@ -217,48 +275,51 @@
             for target in result.notification_targets():
                 if result.status in [UpdateRequestStatus.SAVED, UpdateRequestStatus.ERROR_AUTH]:
                     if result.status not in reports_per_recipient[target]:
                         reports_per_recipient[target][result.status] = OrderedSet()
                     reports_per_recipient[target][result.status].add(result.notification_target_report())
                     sources.add(result.rpsl_obj_new.source())
 
-        sources_str = '/'.join(sources)
-        subject = f'Notification of {sources_str} database changes'
-        header = get_setting('email.notification_header', '').format(sources_str=sources_str)
-        header += '\nThis message is auto-generated.\n'
-        header += 'The request was made by email, with the following details:\n'
-        header_saved = textwrap.dedent("""
+        sources_str = "/".join(sources)
+        subject = f"Notification of {sources_str} database changes"
+        header = get_setting("email.notification_header", "").format(sources_str=sources_str)
+        header += "\nThis message is auto-generated.\n"
+        header += "The request was made with the following details:\n"
+        header_saved = textwrap.dedent(
+            """
             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
             Some objects in which you are referenced have been created,
             deleted or changed.
             
-        """)
+        """
+        )
 
-        header_failed = textwrap.dedent("""
+        header_failed = textwrap.dedent(
+            """
             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
             Some objects in which you are referenced were requested
             to be created, deleted or changed, but *failed* the 
             proper authorisation for any of the referenced maintainers.
             
-        """)
+        """
+        )
 
         for recipient, reports_per_status in reports_per_recipient.items():
             user_report = header + self._request_meta_str()
             if UpdateRequestStatus.ERROR_AUTH in reports_per_status:
                 user_report += header_failed
                 for report in reports_per_status[UpdateRequestStatus.ERROR_AUTH]:
-                    user_report += f'---\n{report}\n'
-                user_report += '~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n'
+                    user_report += f"---\n{report}\n"
+                user_report += "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n"
             if UpdateRequestStatus.SAVED in reports_per_status:
                 user_report += header_saved
                 for report in reports_per_status[UpdateRequestStatus.SAVED]:
-                    user_report += f'---\n{report}\n'
-                user_report += '~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n'
+                    user_report += f"---\n{report}\n"
+                user_report += "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n"
 
             email.send_email(recipient, subject, user_report)
 
     def _request_meta_str(self):
-        request_meta_str = '\n'.join([f'> {k}: {v}' for k, v in self.request_meta.items() if v])
+        request_meta_str = "\n".join([f"> {k}: {v}" for k, v in self.request_meta.items() if v])
         if request_meta_str:
-            request_meta_str = '\n' + request_meta_str + '\n\n'
+            request_meta_str = "\n" + request_meta_str + "\n\n"
         return request_meta_str
-
```

### Comparing `irrd-4.2.8/irrd/updates/parser.py` & `irrd-4.3.0/irrd/updates/validators.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,396 +1,462 @@
-import difflib
+import functools
 import logging
-from typing import List, Optional, Set, Dict, Union
+from dataclasses import dataclass, field
+from typing import TYPE_CHECKING, List, Optional, Set, Tuple, Union
+
+from ordered_set import OrderedSet
+from passlib.hash import md5_crypt
 
 from irrd.conf import get_setting
-from irrd.rpki.status import RPKIStatus
-from irrd.rpki.validators import SingleRouteROAValidator
-from irrd.rpsl.parser import UnknownRPSLObjectClassException, RPSLObject
-from irrd.rpsl.rpsl_objects import rpsl_object_from_text, RPSLMntner
-from irrd.scopefilter.status import ScopeFilterStatus
-from irrd.scopefilter.validators import ScopeFilterValidator
+from irrd.rpsl.parser import RPSLObject
+from irrd.rpsl.rpsl_objects import RPSLMntner, RPSLSet, rpsl_object_from_text
 from irrd.storage.database_handler import DatabaseHandler
-from irrd.storage.models import JournalEntryOrigin
-from irrd.storage.queries import RPSLDatabaseQuery
-from irrd.utils.text import splitline_unicodesafe, remove_auth_hashes
-from .parser_state import UpdateRequestType, UpdateRequestStatus
-from .validators import ReferenceValidator, AuthValidator
+from irrd.storage.queries import RPSLDatabaseQuery, RPSLDatabaseSuspendedQuery
+
+from .parser_state import RPSLSetAutnumAuthenticationMode, UpdateRequestType
+
+if TYPE_CHECKING:  # pragma: no cover
+    # http://mypy.readthedocs.io/en/latest/common_issues.html#import-cycles
+    from .parser import ChangeRequest, SuspensionRequest  # noqa: F401
 
 logger = logging.getLogger(__name__)
 
 
-class ChangeRequest:
+@dataclass
+class ValidatorResult:
+    # OrderedSet has some obscure issues with mypy
+    error_messages: Set[str] = field(default_factory=OrderedSet)  # type: ignore
+    info_messages: Set[str] = field(default_factory=OrderedSet)  # type: ignore
+    # mntners that may need to be notified
+    mntners_notify: List[RPSLMntner] = field(default_factory=list)
+    # whether the authentication succeeded due to use of an override password
+    used_override: bool = field(default=False)
+
+    def is_valid(self):
+        return len(self.error_messages) == 0
+
+
+class ReferenceValidator:
     """
-    A ChangeRequest tracks and processes a request for a single change.
-    In this context, a change can be creating, modifying or deleting an
-    RPSL object.
+    The ReferenceValidator validates references to other objects, given
+    their expected object_class, source and PK.
+
+    Sometimes updates are made to objects, referencing objects newly created
+    in the same update message. To handle this, the validator can be preloaded
+    with objects that should be considered valid.
     """
-    rpsl_text_submitted: str
-    rpsl_obj_new: Optional[RPSLObject]
-    rpsl_obj_current: Optional[RPSLObject] = None
-    status = UpdateRequestStatus.PROCESSING
-    request_type: Optional[UpdateRequestType] = None
-    mntners_notify: List[RPSLMntner]
-
-    error_messages: List[str]
-    info_messages: List[str]
-
-    def __init__(self, rpsl_text_submitted: str, database_handler: DatabaseHandler, auth_validator: AuthValidator,
-                 reference_validator: ReferenceValidator, delete_reason=Optional[str]) -> None:
-        """
-        Initialise a new change request for a single RPSL object.
-
-        :param rpsl_text_submitted: the object text
-        :param database_handler: a DatabaseHandler instance
-        :param auth_validator: a AuthValidator instance, to resolve authentication requirements
-        :param reference_validator: a ReferenceValidator instance, to resolve references between objects
-        :param delete_reason: a string with the deletion reason, if this was a deletion request
-
-        The rpsl_text passed into this function should be cleaned from any
-        meta attributes like delete/override/password. Those should be passed
-        into this method as delete_reason, or provided to the AuthValidator.
-
-        The auth_validator and reference_validator must be shared between
-        different instances, to benefit from caching, and to resolve references
-        between different objects that are part of the same submission with
-        possibly multiple changes.
-        """
+
+    def __init__(self, database_handler: DatabaseHandler) -> None:
         self.database_handler = database_handler
-        self.auth_validator = auth_validator
-        self.reference_validator = reference_validator
-        self.rpsl_text_submitted = rpsl_text_submitted
-        self.mntners_notify = []
-        self.used_override = False
-        self._cached_roa_validity: Optional[bool] = None
-        self.roa_validator = SingleRouteROAValidator(database_handler)
-        self.scopefilter_validator = ScopeFilterValidator()
-
-        try:
-            self.rpsl_obj_new = rpsl_object_from_text(rpsl_text_submitted, strict_validation=True)
-            if self.rpsl_obj_new.messages.errors():
-                self.status = UpdateRequestStatus.ERROR_PARSING
-            self.error_messages = self.rpsl_obj_new.messages.errors()
-            self.info_messages = self.rpsl_obj_new.messages.infos()
-            logger.debug(f'{id(self)}: Processing new ChangeRequest for object {self.rpsl_obj_new}: request {id(self)}')
-
-        except UnknownRPSLObjectClassException as exc:
-            self.rpsl_obj_new = None
-            self.request_type = None
-            self.status = UpdateRequestStatus.ERROR_UNKNOWN_CLASS
-            self.info_messages = []
-            self.error_messages = [str(exc)]
-
-        if self.is_valid() and self.rpsl_obj_new:
-            source = self.rpsl_obj_new.source()
-            if not get_setting(f'sources.{source}.authoritative'):
-                logger.debug(f'{id(self)}: change is for non-authoritative source {source}, rejected')
-                self.error_messages.append(f'This instance is not authoritative for source {source}')
-                self.status = UpdateRequestStatus.ERROR_NON_AUTHORITIVE
-                return
-
-            self._retrieve_existing_version()
-
-        if delete_reason:
-            self.request_type = UpdateRequestType.DELETE
-            if not self.rpsl_obj_current:
-                self.status = UpdateRequestStatus.ERROR_PARSING
-                self.error_messages.append('Can not delete object: no object found for this key in this database.')
-                logger.debug(f'{id(self)}: Request attempts to delete object {self.rpsl_obj_new}, '
-                             f'but no existing object found.')
+        self._cache: Set[Tuple[str, str, str]] = set()
+        self._preloaded_new: Set[Tuple[str, str, str]] = set()
+        self._preloaded_deleted: Set[Tuple[str, str, str]] = set()
+
+    def preload(self, results: List[Union["ChangeRequest", "SuspensionRequest"]]) -> None:
+        """Preload an iterable of ChangeRequest objects to be considered valid, or to be considered deleted."""
+        self._preloaded_new = set()
+        self._preloaded_deleted = set()
+        for request in results:
+            assert request.rpsl_obj_new
+            if request.request_type == UpdateRequestType.DELETE:
+                self._preloaded_deleted.add(
+                    (
+                        request.rpsl_obj_new.rpsl_object_class,
+                        request.rpsl_obj_new.pk(),
+                        request.rpsl_obj_new.source(),
+                    )
+                )
+            elif request.request_type in [UpdateRequestType.CREATE, UpdateRequestType.MODIFY]:
+                self._preloaded_new.add(
+                    (
+                        request.rpsl_obj_new.rpsl_object_class,
+                        request.rpsl_obj_new.pk(),
+                        request.rpsl_obj_new.source(),
+                    )
+                )
+
+    def check_references_to_others(self, rpsl_obj: RPSLObject) -> ValidatorResult:
+        """
+        Check the validity of references of a particular object, i.e. whether
+        all references to other objects actually exist in the database.
+        """
+        result = ValidatorResult()
+        references = rpsl_obj.referred_strong_objects()
+        source = rpsl_obj.source()
+
+        for field_name, objects_referred, object_pks in references:
+            for object_pk in object_pks:
+                if not self._check_reference_to_others(objects_referred, object_pk, source):
+                    if len(objects_referred) > 1:
+                        objects_referred_str = "one of " + ", ".join(objects_referred)
+                    else:
+                        objects_referred_str = objects_referred[0]
+                    result.error_messages.add(
+                        f"Object {object_pk} referenced in field {field_name} not found in "
+                        f"database {source} - must reference {objects_referred_str}."
+                    )
+        return result
+
+    def _check_reference_to_others(self, object_classes: List[str], object_pk: str, source: str) -> bool:
+        """
+        Check whether one reference to a particular object class/source/PK is valid,
+        i.e. such an object exists in the database.
+
+        Object classes is a list of classes to which the reference may point, e.g.
+        person/role for admin-c, route for route-set members, or route/route6 for mp-members.
+        """
+        for object_class in object_classes:
+            if (object_class, object_pk, source) in self._cache:
+                return True
+            if (object_class, object_pk, source) in self._preloaded_new:
+                return True
+            if (object_class, object_pk, source) in self._preloaded_deleted:
+                return False
 
-    def _retrieve_existing_version(self):
-        """
-        Retrieve the current version of this object, if any, and store it in rpsl_obj_current.
-        Update self.status appropriately.
-        """
-        query = RPSLDatabaseQuery().sources([self.rpsl_obj_new.source()])
-        query = query.object_classes([self.rpsl_obj_new.rpsl_object_class]).rpsl_pk(self.rpsl_obj_new.pk())
+        query = RPSLDatabaseQuery().sources([source]).object_classes(object_classes).rpsl_pk(object_pk)
         results = list(self.database_handler.execute_query(query))
+        for result in results:
+            self._cache.add((result["object_class"], object_pk, source))
+        if len(results):
+            return True
 
-        if not results:
-            self.request_type = UpdateRequestType.CREATE
-            logger.debug(f'{id(self)}: Did not find existing version for object {self.rpsl_obj_new}, request is CREATE')
-        elif len(results) == 1:
-            self.request_type = UpdateRequestType.MODIFY
-            self.rpsl_obj_current = rpsl_object_from_text(results[0]['object_text'], strict_validation=False)
-            logger.debug(f'{id(self)}: Retrieved existing version for object '
-                         f'{self.rpsl_obj_current}, request is MODIFY/DELETE')
-        else:  # pragma: no cover
-            # This should not be possible, as rpsl_pk/source are a composite unique value in the database scheme.
-            # Therefore, a query should not be able to affect more than one row.
-            affected_pks = ', '.join([r['pk'] for r in results])
-            msg = f'{id(self)}: Attempted to retrieve current version of object {self.rpsl_obj_new.pk()}/'
-            msg += f'{self.rpsl_obj_new.source()}, but multiple '
-            msg += f'objects were found, internal pks found: {affected_pks}'
-            logger.error(msg)
-            raise ValueError(msg)
-
-    def save(self, database_handler: DatabaseHandler) -> None:
-        """Save the change to the database."""
-        if self.status != UpdateRequestStatus.PROCESSING or not self.rpsl_obj_new:
-            raise ValueError('ChangeRequest can only be saved in status PROCESSING')
-        if self.request_type == UpdateRequestType.DELETE and self.rpsl_obj_current is not None:
-            logger.info(f'{id(self)}: Saving change for {self.rpsl_obj_new}: deleting current object')
-            database_handler.delete_rpsl_object(rpsl_object=self.rpsl_obj_current, origin=JournalEntryOrigin.auth_change)
-        else:
-            logger.info(f'{id(self)}: Saving change for {self.rpsl_obj_new}: inserting/updating current object')
-            database_handler.upsert_rpsl_object(self.rpsl_obj_new, JournalEntryOrigin.auth_change)
-        self.status = UpdateRequestStatus.SAVED
-
-    def is_valid(self) -> bool:
-        return self.status in [UpdateRequestStatus.SAVED, UpdateRequestStatus.PROCESSING]
-
-    def submitter_report_human(self) -> str:
-        """Produce a string suitable for reporting back status and messages to the human submitter."""
-        status = 'succeeded' if self.is_valid() else 'FAILED'
-
-        report = f'{self.request_type_str().title()} {status}: [{self.object_class_str()}] {self.object_pk_str()}\n'
-        if self.info_messages or self.error_messages:
-            if not self.rpsl_obj_new or self.error_messages:
-                report += '\n' + remove_auth_hashes(self.rpsl_text_submitted) + '\n'
-            else:
-                report += '\n' + remove_auth_hashes(self.rpsl_obj_new.render_rpsl_text()) + '\n'
-            report += ''.join([f'ERROR: {e}\n' for e in self.error_messages])
-            report += ''.join([f'INFO: {e}\n' for e in self.info_messages])
-        return report
-
-    def submitter_report_json(self) -> Dict[str, Union[None, bool, str, List[str]]]:
-        """Produce a dict suitable for reporting back status and messages in JSON."""
-        new_object_text = None
-        if self.rpsl_obj_new and not self.error_messages:
-            new_object_text = self.rpsl_obj_new.render_rpsl_text()
-        return {
-            'successful': self.is_valid(),
-            'type': str(self.request_type.value) if self.request_type else None,
-            'object_class': self.object_class_str(),
-            'rpsl_pk': self.object_pk_str(),
-            'info_messages': self.info_messages,
-            'error_messages': self.error_messages,
-            'new_object_text': remove_auth_hashes(new_object_text),
-            'submitted_object_text': remove_auth_hashes(self.rpsl_text_submitted),
-        }
+        return False
 
-    def notification_target_report(self):
+    def check_references_from_others(self, rpsl_obj: RPSLObject) -> ValidatorResult:
         """
-        Produce a string suitable for reporting back status and messages
-        to a human notification target, i.e. someone listed
-        in notify/upd-to/mnt-nfy.
-        """
-        if not self.is_valid() and self.status != UpdateRequestStatus.ERROR_AUTH:
-            raise ValueError('Notification reports can only be made for changes that are valid '
-                             'or have failed authorisation.')
-
-        status = 'succeeded' if self.is_valid() else 'FAILED AUTHORISATION'
-        report = f'{self.request_type_str().title()} {status} for object below: '
-        report += f'[{self.object_class_str()}] {self.object_pk_str()}:\n\n'
-
-        if self.request_type == UpdateRequestType.MODIFY:
-            current_text = list(splitline_unicodesafe(self.rpsl_obj_current.render_rpsl_text()))
-            new_text = list(splitline_unicodesafe(self.rpsl_obj_new.render_rpsl_text()))
-            diff = list(difflib.unified_diff(current_text, new_text, lineterm=''))
-
-            report += '\n'.join(diff[2:])  # skip the lines from the diff which would have filenames
-            if self.status == UpdateRequestStatus.ERROR_AUTH:
-                report += '\n\n*Rejected* new version of this object:\n\n'
-            else:
-                report += '\n\nNew version of this object:\n\n'
+        Check for any references to this object in the DB.
+        Used for validating deletions.
 
-        if self.request_type == UpdateRequestType.DELETE:
-            report += self.rpsl_obj_current.render_rpsl_text()
-        else:
-            report += self.rpsl_obj_new.render_rpsl_text()
-        return remove_auth_hashes(report)
+        Checks self._preloaded_deleted, because a reference from an object
+        that is also about to be deleted, is acceptable.
+        """
+        result = ValidatorResult()
+        if not rpsl_obj.references_strong_inbound():
+            return result
+
+        query = RPSLDatabaseQuery().sources([rpsl_obj.source()])
+        query = query.lookup_attrs_in(rpsl_obj.references_strong_inbound(), [rpsl_obj.pk()])
+        query_results = self.database_handler.execute_query(query)
+        for query_result in query_results:
+            reference_to_be_deleted = (
+                query_result["object_class"],
+                query_result["rpsl_pk"],
+                query_result["source"],
+            ) in self._preloaded_deleted
+            if not reference_to_be_deleted:
+                result.error_messages.add(
+                    f"Object {rpsl_obj.pk()} to be deleted, but still referenced "
+                    f"by {query_result['object_class']} {query_result['rpsl_pk']}"
+                )
+        return result
 
-    def request_type_str(self) -> str:
-        return self.request_type.value if self.request_type else 'request'
 
-    def object_pk_str(self) -> str:
-        return self.rpsl_obj_new.pk() if self.rpsl_obj_new else '(unreadable object key)'
+class AuthValidator:
+    """
+    The AuthValidator validates authentication. It looks for relevant mntner
+    objects and checks whether any of their auth methods pass.
 
-    def object_class_str(self) -> str:
-        return self.rpsl_obj_new.rpsl_object_class if self.rpsl_obj_new else '(unreadable object class)'
+    When adding a mntner in an update, a check for that mntner in the DB will
+    fail, as it does not exist yet. To prevent this failure, call pre_approve()
+    with a list of UpdateRequests.
+    """
 
-    def notification_targets(self) -> Set[str]:
+    passwords: List[str]
+    overrides: List[str]
+    keycert_obj_pk: Optional[str] = None
+
+    def __init__(self, database_handler: DatabaseHandler, keycert_obj_pk=None) -> None:
+        self.database_handler = database_handler
+        self.passwords = []
+        self.overrides = []
+        self._mntner_db_cache: Set[RPSLMntner] = set()
+        self._pre_approved: Set[str] = set()
+        self.keycert_obj_pk = keycert_obj_pk
+
+    def pre_approve(self, presumed_valid_new_mntners: List[RPSLMntner]) -> None:
+        """
+        Pre-approve certain maintainers that are part of this batch of updates.
+        This is required for creating new maintainers along with other objects.
+
+        All new maintainer PKs are added to self._pre_approved. When they are
+        encountered as mnt-by, the authentication is immediately approved,
+        as a check in the database would fail.
+        When the new mntner object's mnt-by is checked, there is an additional
+        check to verify that it passes the newly submitted authentication.
+        """
+        self._pre_approved = {obj.pk() for obj in presumed_valid_new_mntners}
+
+    def process_auth(
+        self, rpsl_obj_new: RPSLObject, rpsl_obj_current: Optional[RPSLObject]
+    ) -> ValidatorResult:
+        """
+        Check whether authentication passes for all required objects.
+        Returns a ValidatorResult object with error/info messages, and fills
+        result.mntners_notify with the RPSLMntner objects that may have
+        to be notified.
+
+        If a valid override password is provided, changes are immediately approved.
+        On the result object, used_override is set to True, but mntners_notify is
+        not filled, as mntner resolving does not take place.
+        """
+        source = rpsl_obj_new.source()
+        result = ValidatorResult()
+
+        if self.check_override():
+            result.used_override = True
+            logger.info("Found valid override password.")
+            return result
+
+        mntners_new = rpsl_obj_new.parsed_data["mnt-by"]
+        logger.debug(f"Checking auth for new object {rpsl_obj_new}, mntners in new object: {mntners_new}")
+        valid, mntner_objs_new = self._check_mntners(mntners_new, source)
+        if not valid:
+            self._generate_failure_message(result, mntners_new, rpsl_obj_new)
+
+        if rpsl_obj_current:
+            mntners_current = rpsl_obj_current.parsed_data["mnt-by"]
+            logger.debug(
+                f"Checking auth for current object {rpsl_obj_current}, "
+                f"mntners in current object: {mntners_current}"
+            )
+            valid, mntner_objs_current = self._check_mntners(mntners_current, source)
+            if not valid:
+                self._generate_failure_message(result, mntners_current, rpsl_obj_new)
+
+            result.mntners_notify = mntner_objs_current
+        else:
+            result.mntners_notify = mntner_objs_new
+            mntners_related = self._find_related_mntners(rpsl_obj_new, result)
+            if mntners_related:
+                related_object_class, related_pk, related_mntner_list = mntners_related
+                logger.debug(
+                    f"Checking auth for related object {related_object_class} / "
+                    f"{related_pk} with mntners {related_mntner_list}"
+                )
+                valid, mntner_objs_related = self._check_mntners(related_mntner_list, source)
+                if not valid:
+                    self._generate_failure_message(
+                        result, related_mntner_list, rpsl_obj_new, related_object_class, related_pk
+                    )
+                    result.mntners_notify = mntner_objs_related
+
+        if isinstance(rpsl_obj_new, RPSLMntner):
+            if not rpsl_obj_current:
+                result.error_messages.add("New mntner objects must be added by an administrator.")
+                return result
+            # Dummy auth values are only permitted in existing objects
+            if rpsl_obj_new.has_dummy_auth_value():
+                if len(self.passwords) == 1:
+                    logger.debug(
+                        f"Object {rpsl_obj_new} submitted with dummy hash values and single password, "
+                        "replacing all hashes with currently supplied password."
+                    )
+                    rpsl_obj_new.force_single_new_password(self.passwords[0])
+                    result.info_messages.add(
+                        "As you submitted dummy hash values, all password hashes on this object "
+                        "were replaced with a new BCRYPT-PW hash of the password you provided for "
+                        "authentication."
+                    )
+                else:
+                    result.error_messages.add(
+                        "Object submitted with dummy hash values, but multiple or no passwords "
+                        "submitted. Either submit only full hashes, or a single password."
+                    )
+            elif not rpsl_obj_new.verify_auth(self.passwords, self.keycert_obj_pk):
+                result.error_messages.add("Authorisation failed for the auth methods on this mntner object.")
+
+        return result
+
+    def check_override(self) -> bool:
+        override_hash = get_setting("auth.override_password")
+        if override_hash:
+            for override in self.overrides:
+                try:
+                    if md5_crypt.verify(override, override_hash):
+                        return True
+                    else:
+                        logger.info("Found invalid override password, ignoring.")
+                except ValueError as ve:
+                    logger.error(
+                        f"Exception occurred while checking override password: {ve} (possible misconfigured"
+                        " hash?)"
+                    )
+        elif self.overrides:
+            logger.info("Ignoring override password, auth.override_password not set.")
+        return False
+
+    def _check_mntners(self, mntner_pk_list: List[str], source: str) -> Tuple[bool, List[RPSLMntner]]:
+        """
+        Check whether authentication passes for a list of maintainers.
+
+        Returns True if at least one of the mntners in mntner_list
+        passes authentication, given self.passwords and
+        self.keycert_obj_pk. Updates and checks self._mntner_db_cache
+        to prevent double retrieval of maintainers.
+        """
+        mntner_pk_set = set(mntner_pk_list)
+        mntner_objs: List[RPSLMntner] = [
+            m for m in self._mntner_db_cache if m.pk() in mntner_pk_set and m.source() == source
+        ]
+        mntner_pks_to_resolve: Set[str] = mntner_pk_set - {m.pk() for m in mntner_objs}
+
+        if mntner_pks_to_resolve:
+            query = RPSLDatabaseQuery().sources([source])
+            query = query.object_classes(["mntner"]).rpsl_pks(mntner_pks_to_resolve)
+            results = self.database_handler.execute_query(query)
+
+            retrieved_mntner_objs: List[RPSLMntner] = [rpsl_object_from_text(r["object_text"]) for r in results]  # type: ignore
+            self._mntner_db_cache.update(retrieved_mntner_objs)
+            mntner_objs += retrieved_mntner_objs
+
+        for mntner_name in mntner_pk_list:
+            if mntner_name in self._pre_approved:
+                return True, mntner_objs
+
+        for mntner_obj in mntner_objs:
+            if mntner_obj.verify_auth(self.passwords, self.keycert_obj_pk):
+                return True, mntner_objs
+
+        return False, mntner_objs
+
+    def _generate_failure_message(
+        self,
+        result: ValidatorResult,
+        failed_mntner_list: List[str],
+        rpsl_obj,
+        related_object_class: Optional[str] = None,
+        related_pk: Optional[str] = None,
+    ) -> None:
+        mntner_str = ", ".join(failed_mntner_list)
+        msg = f"Authorisation for {rpsl_obj.rpsl_object_class} {rpsl_obj.pk()} failed: "
+        msg += f"must be authenticated by one of: {mntner_str}"
+        if related_object_class and related_pk:
+            msg += f" - from parent {related_object_class} {related_pk}"
+        result.error_messages.add(msg)
+
+    def _find_related_mntners(
+        self, rpsl_obj_new: RPSLObject, result: ValidatorResult
+    ) -> Optional[Tuple[str, str, List[str]]]:
+        """
+        Find the maintainers of the related object to rpsl_obj_new, if any.
+        This is used to authorise creating objects - authentication may be
+        required to pass for the related object as well.
+
+        Returns a tuple of:
+        - object class of the related object
+        - PK of the related object
+        - List of maintainers for the related object (at least one must pass)
+        Returns None if no related objects were found that should be authenticated.
+
+        Custom error messages may be added directly to the passed ValidatorResult.
+        """
+        related_object = None
+        if rpsl_obj_new.rpsl_object_class in ["route", "route6"]:
+            related_object = self._find_related_object_route(rpsl_obj_new)
+        if issubclass(rpsl_obj_new.__class__, RPSLSet):
+            related_object = self._find_related_object_set(rpsl_obj_new, result)
+
+        if related_object:
+            mntners = related_object.get("parsed_data", {}).get("mnt-by", [])
+            return related_object["object_class"], related_object["rpsl_pk"], mntners
+
+        return None
+
+    @functools.lru_cache(maxsize=50)
+    def _find_related_object_route(self, rpsl_obj_new: RPSLObject):
+        """
+        Find the related inetnum/route object to rpsl_obj_new, which must be a route(6).
+        Returns a dict as returned by the database handler.
+        """
+        if not get_setting("auth.authenticate_parents_route_creation"):
+            return None
+
+        inetnum_class = {
+            "route": "inetnum",
+            "route6": "inet6num",
+        }
+
+        object_class = inetnum_class[rpsl_obj_new.rpsl_object_class]
+        query = _init_related_object_query(object_class, rpsl_obj_new).ip_exact(rpsl_obj_new.prefix)
+        inetnums = list(self.database_handler.execute_query(query))
+
+        if not inetnums:
+            query = _init_related_object_query(object_class, rpsl_obj_new).ip_less_specific_one_level(
+                rpsl_obj_new.prefix
+            )
+            inetnums = list(self.database_handler.execute_query(query))
+
+        if inetnums:
+            return inetnums[0]
+
+        object_class = rpsl_obj_new.rpsl_object_class
+        query = _init_related_object_query(object_class, rpsl_obj_new).ip_less_specific_one_level(
+            rpsl_obj_new.prefix
+        )
+        routes = list(self.database_handler.execute_query(query))
+        if routes:
+            return routes[0]
+
+        return None
+
+    def _find_related_object_set(self, rpsl_obj_new: RPSLObject, result: ValidatorResult):
         """
-        Produce a set of e-mail addresses that should be notified
-        about the change to this object.
-        May include mntner upd-to or mnt-nfy, and notify of existing object.
+        Find the related aut-num object to rpsl_obj_new, which must be a set object,
+        depending on settings.
+        Returns a dict as returned by the database handler.
         """
-        targets: Set[str] = set()
-        status_qualifies_notification = self.is_valid() or self.status == UpdateRequestStatus.ERROR_AUTH
-        if self.used_override or not status_qualifies_notification:
-            return targets
 
-        mntner_attr = 'upd-to' if self.status == UpdateRequestStatus.ERROR_AUTH else 'mnt-nfy'
-        for mntner in self.mntners_notify:
-            for email in mntner.parsed_data.get(mntner_attr, []):
-                targets.add(email)
+        @functools.lru_cache(maxsize=50)
+        def _find_in_db():
+            query = _init_related_object_query("aut-num", rpsl_obj_new).rpsl_pk(rpsl_obj_new.pk_asn_segment)
+            aut_nums = list(self.database_handler.execute_query(query))
+            if aut_nums:
+                return aut_nums[0]
 
-        if self.rpsl_obj_current:
-            for email in self.rpsl_obj_current.parsed_data.get('notify', []):
-                targets.add(email)
+        if not rpsl_obj_new.pk_asn_segment:
+            return None
 
-        return targets
+        mode = RPSLSetAutnumAuthenticationMode.for_set_name(rpsl_obj_new.rpsl_object_class)
+        if mode == RPSLSetAutnumAuthenticationMode.DISABLED:
+            return None
 
-    def validate(self) -> bool:
-        if self.rpsl_obj_new and self.request_type == UpdateRequestType.CREATE:
-            if not self.rpsl_obj_new.clean_for_create():
-                self.error_messages += self.rpsl_obj_new.messages.errors()
-                self.status = UpdateRequestStatus.ERROR_PARSING
-                return False
-        auth_valid = self._check_auth()
-        if not auth_valid:
-            return False
-        references_valid = self._check_references()
-        rpki_valid = self._check_conflicting_roa()
-        scopefilter_valid = self._check_scopefilter()
-        return references_valid and rpki_valid and scopefilter_valid
-
-    def _check_auth(self) -> bool:
-        assert self.rpsl_obj_new
-        auth_result = self.auth_validator.process_auth(self.rpsl_obj_new, self.rpsl_obj_current)
-        self.info_messages += auth_result.info_messages
-        self.mntners_notify = auth_result.mntners_notify
-
-        if not auth_result.is_valid():
-            self.status = UpdateRequestStatus.ERROR_AUTH
-            self.error_messages += auth_result.error_messages
-            logger.debug(f'{id(self)}: Authentication check failed: {list(auth_result.error_messages)}')
-            return False
-
-        self.used_override = auth_result.used_override
-
-        logger.debug(f'{id(self)}: Authentication check succeeded')
-        return True
-
-    def _check_references(self) -> bool:
-        """
-        Check all references from this object to or from other objects.
-
-        For deletions, only references to the deleted object matter, as
-        they now become invalid. For other operations, only the validity
-        of references from the new object to others matter.
-        """
-        if self.request_type == UpdateRequestType.DELETE and self.rpsl_obj_current is not None:
-            assert self.rpsl_obj_new
-            references_result = self.reference_validator.check_references_from_others(self.rpsl_obj_current)
-        else:
-            assert self.rpsl_obj_new
-            references_result = self.reference_validator.check_references_to_others(self.rpsl_obj_new)
-        self.info_messages += references_result.info_messages
-
-        if not references_result.is_valid():
-            self.error_messages += references_result.error_messages
-            logger.debug(f'{id(self)}: Reference check failed: {list(references_result.error_messages)}')
-            if self.is_valid():  # Only change the status if this object was valid prior, so this is the first failure
-                self.status = UpdateRequestStatus.ERROR_REFERENCE
-            return False
-
-        logger.debug(f'{id(self)}: Reference check succeeded')
-        return True
-
-    def _check_conflicting_roa(self) -> bool:
-        """
-        Check whether there are any conflicting ROAs with the new object.
-        Result is cached, as validate() may be called multiple times,
-        but the result of this check will not change. Always returns
-        True when not in RPKI-aware mode.
-        """
-        assert self.rpsl_obj_new
-        if self._cached_roa_validity is not None:
-            return self._cached_roa_validity
-        if not get_setting('rpki.roa_source') or not self.rpsl_obj_new.rpki_relevant:
-            return True
-        # Deletes are permitted for RPKI-invalids, other operations are not
-        if self.request_type == UpdateRequestType.DELETE:
-            return True
+        aut_num = _find_in_db()
+        if aut_num:
+            return aut_num
+        elif mode == RPSLSetAutnumAuthenticationMode.REQUIRED:
+            result.error_messages.add(
+                f"Creating this object requires an aut-num for {rpsl_obj_new.pk_asn_segment} to exist."
+            )
+        return None
 
-        assert self.rpsl_obj_new.asn_first
-        validation_result = self.roa_validator.validate_route(
-            self.rpsl_obj_new.prefix, self.rpsl_obj_new.asn_first, self.rpsl_obj_new.source()
-        )
-        if validation_result == RPKIStatus.invalid:
-            import_timer = get_setting('rpki.roa_import_timer')
-            user_message = 'RPKI ROAs were found that conflict with this object. '
-            user_message += f'(This IRRd refreshes ROAs every {import_timer} seconds.)'
-            logger.debug(f'{id(self)}: Conflicting ROAs found')
-            if self.is_valid():  # Only change the status if this object was valid prior, so this is first failure
-                self.status = UpdateRequestStatus.ERROR_ROA
-            self.error_messages.append(user_message)
-            self._cached_roa_validity = False
-            return False
-        else:
-            logger.debug(f'{id(self)}: No conflicting ROAs found')
-        self._cached_roa_validity = True
-        return True
 
-    def _check_scopefilter(self) -> bool:
-        if self.request_type == UpdateRequestType.DELETE or not self.rpsl_obj_new:
-            return True
-        result, comment = self.scopefilter_validator.validate_rpsl_object(self.rpsl_obj_new)
-        if result in [ScopeFilterStatus.out_scope_prefix, ScopeFilterStatus.out_scope_as]:
-            user_message = 'Contains out of scope information: ' + comment
-            if self.request_type == UpdateRequestType.CREATE:
-                logger.debug(f'{id(self)}: object out of scope: ' + comment)
-                if self.is_valid():  # Only change the status if this object was valid prior, so this is first failure
-                    self.status = UpdateRequestStatus.ERROR_SCOPEFILTER
-                self.error_messages.append(user_message)
-                return False
-            elif self.request_type == UpdateRequestType.MODIFY:
-                self.info_messages.append(user_message)
-        return True
+def _init_related_object_query(rpsl_object_class: str, rpsl_obj_new: RPSLObject) -> RPSLDatabaseQuery:
+    query = RPSLDatabaseQuery().sources([rpsl_obj_new.source()])
+    query = query.object_classes([rpsl_object_class])
+    return query.first_only()
 
 
-def parse_change_requests(requests_text: str,
-                          database_handler: DatabaseHandler,
-                          auth_validator: AuthValidator,
-                          reference_validator: ReferenceValidator,
-                          ) -> List[ChangeRequest]:
+class RulesValidator:
     """
-    Parse change requests, a text of RPSL objects along with metadata like
-    passwords or deletion requests.
-
-    :param requests_text: a string containing all change requests
-    :param database_handler: a DatabaseHandler instance
-        :param auth_validator: a AuthValidator instance, to resolve authentication requirements
-    :param reference_validator: a ReferenceValidator instance
-    :return: a list of ChangeRequest instances
+    The RulesValidator validates any other rules for RPSL object changes.
+    This means: anything that is not authentication, references, RPKI or scope filter.
     """
-    results = []
-    passwords = []
-    overrides = []
-
-    requests_text = requests_text.replace('\r', '')
-    for object_text in requests_text.split('\n\n'):
-        object_text = object_text.strip()
-        if not object_text:
-            continue
-
-        rpsl_text = ''
-        delete_reason = None
-
-        # The attributes password/override/delete are meta attributes
-        # and need to be extracted before parsing. Delete refers to a specific
-        # object, password/override apply to all included objects.
-        for line in splitline_unicodesafe(object_text):
-            if line.startswith('password:'):
-                password = line.split(':', maxsplit=1)[1].strip()
-                passwords.append(password)
-            elif line.startswith('override:'):
-                override = line.split(':', maxsplit=1)[1].strip()
-                overrides.append(override)
-            elif line.startswith('delete:'):
-                delete_reason = line.split(':', maxsplit=1)[1].strip()
-            else:
-                rpsl_text += line + '\n'
-
-        if not rpsl_text:
-            continue
-
-        results.append(ChangeRequest(rpsl_text, database_handler, auth_validator, reference_validator,
-                                     delete_reason=delete_reason))
-
-    if auth_validator:
-        auth_validator.passwords = passwords
-        auth_validator.overrides = overrides
-    return results
+
+    def __init__(self, database_handler: DatabaseHandler) -> None:
+        self.database_handler = database_handler
+
+    def validate(self, rpsl_obj: RPSLObject, request_type: UpdateRequestType) -> ValidatorResult:
+        result = ValidatorResult()
+        if (
+            request_type == UpdateRequestType.CREATE
+            and rpsl_obj.rpsl_object_class == "mntner"
+            and self._check_suspended_mntner_with_same_pk(rpsl_obj.pk(), rpsl_obj.source())
+        ):
+            result.error_messages.add(
+                f"A suspended mntner with primary key {rpsl_obj.pk()} already exists for {rpsl_obj.source()}"
+            )
+        return result
+
+    @functools.lru_cache(maxsize=50)
+    def _check_suspended_mntner_with_same_pk(self, pk: str, source: str) -> bool:
+        q = RPSLDatabaseSuspendedQuery().object_classes(["mntner"]).rpsl_pk(pk).sources([source]).first_only()
+        return bool(list(self.database_handler.execute_query(q)))
```

### Comparing `irrd-4.2.8/irrd/updates/tests/test_email.py` & `irrd-4.3.0/irrd/updates/tests/test_email.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,29 +1,31 @@
 # flake8: noqa: W293
-import pytest
 import textwrap
 from unittest.mock import Mock
 
+import pytest
+
 from irrd.storage.database_handler import DatabaseHandler
 from irrd.updates.email import handle_email_submission
 
 
 @pytest.fixture()
 def mock_email_dh(monkeypatch):
     mock_email = Mock()
-    monkeypatch.setattr('irrd.utils.email.send_email', mock_email)
+    monkeypatch.setattr("irrd.utils.email.send_email", mock_email)
 
     mock_dh = Mock(spec=DatabaseHandler)
-    monkeypatch.setattr('irrd.updates.handler.DatabaseHandler', mock_dh)
+    monkeypatch.setattr("irrd.updates.handler.DatabaseHandler", mock_dh)
 
     yield mock_email, mock_dh
 
 
 class TestHandleEmailSubmission:
-    default_email = textwrap.dedent("""
+    default_email = textwrap.dedent(
+        """
         From sasha@localhost  Thu Jan  5 10:04:48 2018
         Received: from [127.0.0.1] (localhost.localdomain [127.0.0.1])
           by hostname (Postfix) with ESMTPS id 740AD310597
           for <sasha@localhost>; Thu,  5 Jan 2018 10:04:48 +0100 (CET)
         Message-ID: <1325754288.4989.6.camel@hostname>
         Subject: my subject
         Subject: not my subject
@@ -32,31 +34,33 @@
         Date: Thu, 05 Jan 2018 10:04:48 +0100
         Content-Type: text/plain; charset=us-ascii
         X-Mailer: Python 3.7
         Content-Transfer-Encoding: 7bit
         Mime-Version: 1.0
 
         aut-num: AS12345
-        """).strip()
+        """
+    ).strip()
 
     def test_valid_plain(self, mock_email_dh, tmp_gpg_dir):
         mock_email, mock_dh = mock_email_dh
         handler = handle_email_submission(self.default_email)
-        assert handler.request_meta['Message-ID'] == '<1325754288.4989.6.camel@hostname>'
+        assert handler.request_meta["Message-ID"] == "<1325754288.4989.6.camel@hostname>"
         assert len(handler.results) == 1
         assert len(handler.results[0].error_messages)
-        assert mock_email.mock_calls[0][0] == ''
-        assert mock_email.mock_calls[0][1][0] == 'Sasha <sasha@example.com>'
-        assert mock_email.mock_calls[0][1][1] == 'FAILED: my subject'
-        assert 'DETAILED EXPLANATION' in mock_email.mock_calls[0][1][2]
+        assert mock_email.mock_calls[0][0] == ""
+        assert mock_email.mock_calls[0][1][0] == "Sasha <sasha@example.com>"
+        assert mock_email.mock_calls[0][1][1] == "FAILED: my subject"
+        assert "DETAILED EXPLANATION" in mock_email.mock_calls[0][1][2]
 
     def test_invalid_no_text_plain(self, mock_email_dh, tmp_gpg_dir):
         mock_email, mock_dh = mock_email_dh
 
-        email = textwrap.dedent("""
+        email = textwrap.dedent(
+            """
         From sasha@localhost  Thu Jan  5 10:04:48 2018
         Received: from [127.0.0.1] (localhost.localdomain [127.0.0.1])
           by hostname (Postfix) with ESMTPS id 740AD310597
           for <sasha@localhost>; Thu,  5 Jan 2018 10:04:48 +0100 (CET)
         Message-ID: <1325754288.4989.6.camel@hostname>
         Subject: my subject
         From: Sasha <sasha@example.com>
@@ -71,67 +75,68 @@
         --Apple-Mail=_01FE5B2D-C7F3-4DDD-AB42-B92C88CFBF0F
         Content-Transfer-Encoding: 7bit
         Content-Type: text/html;
             charset=us-ascii
 
         <html><head><meta http-equiv="Content-Type" content="text/html charset=us-ascii"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;" class=""><b class="">test 1 2 3</b><div class=""><br class=""></div></body></html>
         --Apple-Mail=_01FE5B2D-C7F3-4DDD-AB42-B92C88CFBF0F--
-        """).strip()
+        """
+        ).strip()
         assert handle_email_submission(email) is None
 
-        assert mock_email.mock_calls[0][0] == ''
-        assert mock_email.mock_calls[0][1][0] == 'Sasha <sasha@example.com>'
-        assert mock_email.mock_calls[0][1][1] == 'FAILED: my subject'
-        assert 'no text/plain' in mock_email.mock_calls[0][1][2]
+        assert mock_email.mock_calls[0][0] == ""
+        assert mock_email.mock_calls[0][1][0] == "Sasha <sasha@example.com>"
+        assert mock_email.mock_calls[0][1][1] == "FAILED: my subject"
+        assert "no text/plain" in mock_email.mock_calls[0][1][2]
 
     def test_handles_exception_email_parser(self, monkeypatch, caplog, tmp_gpg_dir):
         mock_email = Mock()
-        monkeypatch.setattr('irrd.utils.email.send_email', mock_email)
+        monkeypatch.setattr("irrd.utils.email.send_email", mock_email)
 
-        mock_parser = Mock(side_effect=Exception('test-error'))
-        monkeypatch.setattr('irrd.utils.email.EmailParser', mock_parser)
+        mock_parser = Mock(side_effect=Exception("test-error"))
+        monkeypatch.setattr("irrd.utils.email.EmailParser", mock_parser)
 
         handle_email_submission(self.default_email)
         assert not mock_email.mock_calls
-        assert 'An exception occurred while attempting to send a reply to an submission: FAILED'
-        assert 'traceback for test-error follows' in caplog.text
-        assert 'test-error' in caplog.text
+        assert "An exception occurred while attempting to send a reply to an submission: FAILED"
+        assert "traceback for test-error follows" in caplog.text
+        assert "test-error" in caplog.text
 
     def test_handles_exception_submission_request_handler(self, monkeypatch, caplog, tmp_gpg_dir):
         mock_email = Mock()
-        monkeypatch.setattr('irrd.utils.email.send_email', mock_email)
+        monkeypatch.setattr("irrd.utils.email.send_email", mock_email)
 
-        mock_handler = Mock(side_effect=Exception('test-error'))
-        monkeypatch.setattr('irrd.updates.email.ChangeSubmissionHandler', mock_handler)
+        mock_handler = Mock(side_effect=Exception("test-error"))
+        monkeypatch.setattr("irrd.updates.email.ChangeSubmissionHandler", mock_handler)
 
         handle_email_submission(self.default_email)
 
-        assert mock_email.mock_calls[0][0] == ''
-        assert mock_email.mock_calls[0][1][0] == 'Sasha <sasha@example.com>'
-        assert mock_email.mock_calls[0][1][1] == 'ERROR: my subject'
-        assert 'internal error' in mock_email.mock_calls[0][1][2]
-
-        assert 'An exception occurred while attempting to send a reply to an submission: FAILED'
-        assert 'traceback for test-error follows' in caplog.text
-        assert 'test-error' in caplog.text
+        assert mock_email.mock_calls[0][0] == ""
+        assert mock_email.mock_calls[0][1][0] == "Sasha <sasha@example.com>"
+        assert mock_email.mock_calls[0][1][1] == "ERROR: my subject"
+        assert "internal error" in mock_email.mock_calls[0][1][2]
+
+        assert "An exception occurred while attempting to send a reply to an submission: FAILED"
+        assert "traceback for test-error follows" in caplog.text
+        assert "test-error" in caplog.text
 
     def test_handles_exception_smtp(self, mock_email_dh, caplog, tmp_gpg_dir):
         mock_email, mock_dh = mock_email_dh
-        mock_email.side_effect = Exception('test-error')
+        mock_email.side_effect = Exception("test-error")
 
         handle_email_submission(self.default_email)
 
-        assert mock_email.mock_calls[0][0] == ''
-        assert mock_email.mock_calls[0][1][0] == 'Sasha <sasha@example.com>'
-        assert mock_email.mock_calls[0][1][1] == 'FAILED: my subject'
-        assert 'DETAILED EXPLANATION' in mock_email.mock_calls[0][1][2]
-
-        assert 'An exception occurred while attempting to send a reply to an submission: FAILED'
-        assert 'traceback for test-error follows' in caplog.text
-        assert 'test-error' in caplog.text
+        assert mock_email.mock_calls[0][0] == ""
+        assert mock_email.mock_calls[0][1][0] == "Sasha <sasha@example.com>"
+        assert mock_email.mock_calls[0][1][1] == "FAILED: my subject"
+        assert "DETAILED EXPLANATION" in mock_email.mock_calls[0][1][2]
+
+        assert "An exception occurred while attempting to send a reply to an submission: FAILED"
+        assert "traceback for test-error follows" in caplog.text
+        assert "test-error" in caplog.text
 
     def test_invalid_no_from(self, mock_email_dh, caplog, tmp_gpg_dir):
         mock_email, mock_dh = mock_email_dh
 
-        assert handle_email_submission('') is None
+        assert handle_email_submission("") is None
         assert not len(mock_email.mock_calls)
-        assert 'No from address was found' in caplog.text
+        assert "No from address was found" in caplog.text
```

### Comparing `irrd-4.2.8/irrd/utils/email.py` & `irrd-4.3.0/irrd/utils/email.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,34 +1,33 @@
+import email
 import logging
 import socket
-
-import email
 from email.mime.text import MIMEText
 from smtplib import SMTP
 from typing import Optional
 
 from irrd import __version__
 from irrd.conf import get_setting
 from irrd.utils.pgp import validate_pgp_signature
 
-
 logger = logging.getLogger(__name__)
 
 
 class EmailParser:
     """
     Parse a raw email.
     """
+
     body: Optional[str] = None
     pgp_fingerprint: Optional[str] = None
     message_id: Optional[str] = None
     message_from: Optional[str] = None
     message_date: Optional[str] = None
     message_subject: Optional[str] = None
-    _default_encoding = 'ascii'
+    _default_encoding = "ascii"
     _raw_body: Optional[str] = None
     _pgp_signature: Optional[str] = None
 
     def __init__(self, email_txt: str) -> None:
         """
         Extract data from an MIME email message.
 
@@ -41,47 +40,47 @@
 
         # self._raw_body will include headers like Content-Type, self.body contains the decoded contents
         if message.is_multipart():
             for part in message.walk():
                 charset = part.get_content_charset()
                 if not charset:
                     charset = self._default_encoding
-                if part.get_content_type() == 'text/plain':
-                    self.body = part.get_payload(decode=True).decode(str(charset), 'ignore')  # type: ignore
+                if part.get_content_type() == "text/plain":
+                    self.body = part.get_payload(decode=True).decode(str(charset), "ignore")  # type: ignore
                     self._raw_body = part.as_string()
-                if part.get_content_type() == 'application/pgp-signature':
-                    self._pgp_signature = part.get_payload(decode=True).decode(str(charset), 'backslashreplace').strip()  # type: ignore
+                if part.get_content_type() == "application/pgp-signature":
+                    self._pgp_signature = part.get_payload(decode=True).decode(str(charset), "backslashreplace").strip()  # type: ignore
         else:
             charset = message.get_content_charset(failobj=self._default_encoding)
-            self.body = message.get_payload(decode=True).decode(charset, 'backslashreplace')  # type: ignore
+            self.body = message.get_payload(decode=True).decode(charset, "backslashreplace")  # type: ignore
             self._raw_body = self.body
 
-        self.message_id = message['Message-ID']  # type: ignore
-        self.message_from = message['From']  # type: ignore
-        self.message_date = message['Date']  # type: ignore
-        self.message_subject = message['Subject']  # type: ignore
+        self.message_id = message["Message-ID"]  # type: ignore
+        self.message_from = message["From"]  # type: ignore
+        self.message_date = message["Date"]  # type: ignore
+        self.message_subject = message["Subject"]  # type: ignore
 
         if not (self.body and self._raw_body):
             return
 
         new_body, self.pgp_fingerprint = validate_pgp_signature(self._raw_body, self._pgp_signature)
         if new_body:
             self.body = new_body
 
 
 def send_email(recipient, subject, body) -> None:
-    if get_setting('email.recipient_override'):
-        recipient = get_setting('email.recipient_override')
+    if get_setting("email.recipient_override"):
+        recipient = get_setting("email.recipient_override")
 
-    logger.debug(f'Sending email to {recipient}, subject {subject}')
-    body += get_setting('email.footer')
+    logger.debug(f"Sending email to {recipient}, subject {subject}")
+    body += get_setting("email.footer")
     hostname = socket.gethostname()
-    body += f'\n\nGenerated by IRRd version {__version__} on {hostname}'
+    body += f"\n\nGenerated by IRRd version {__version__} on {hostname}"
 
     msg = MIMEText(body)
-    msg['Subject'] = subject
-    msg['From'] = get_setting('email.from')
-    msg['To'] = recipient
+    msg["Subject"] = subject
+    msg["From"] = get_setting("email.from")
+    msg["To"] = recipient
 
-    s = SMTP(get_setting('email.smtp'))
+    s = SMTP(get_setting("email.smtp"))
     s.send_message(msg)
     s.quit()
```

### Comparing `irrd-4.2.8/irrd/utils/pgp.py` & `irrd-4.3.0/irrd/utils/pgp.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,30 +1,35 @@
 import logging
 import os
 import re
 from tempfile import NamedTemporaryFile
 from typing import Optional, Tuple
 
-import gnupg
+# Mypy struggles to see gnupg.GPG
+import gnupg  # type: ignore
 
 from irrd.conf import get_setting
 
 logger = logging.getLogger(__name__)
-pgp_inline_re = re.compile(r'-----BEGIN PGP SIGNED MESSAGE-----(\n.+)?\n\n((?s:.+))\n-----BEGIN PGP SIGNATURE-----\n',
-                           flags=re.MULTILINE)
+pgp_inline_re = re.compile(
+    r"-----BEGIN PGP SIGNED MESSAGE-----(\n.+)?\n\n((?s:.+))\n-----BEGIN PGP SIGNATURE-----\n",
+    flags=re.MULTILINE,
+)
 
 
-def get_gpg_instance() -> gnupg.GPG:
-    keyring = get_setting('auth.gnupg_keyring')
+def get_gpg_instance() -> gnupg.GPG:  # type: ignore
+    keyring = get_setting("auth.gnupg_keyring")
     if not os.path.exists(keyring):
         os.mkdir(keyring)
-    return gnupg.GPG(gnupghome=keyring)
+    return gnupg.GPG(gnupghome=keyring)  # type: ignore
 
 
-def validate_pgp_signature(message: str, detached_signature: Optional[str]=None) -> Tuple[Optional[str], Optional[str]]:
+def validate_pgp_signature(
+    message: str, detached_signature: Optional[str] = None
+) -> Tuple[Optional[str], Optional[str]]:
     """
     Verify a PGP signature in a message.
 
     If there is a valid signature, returns a tuple of an optional signed message
     part, and the PGP fingerprint, or None,None if there was no (valid) signature.
     The signed message part is relevant for inline signing, where only part of
     the message may be signed. If it is None, the entire message was signed.
@@ -49,25 +54,28 @@
     new_message = None
     if detached_signature:
         with NamedTemporaryFile() as data_file:
             data_file.write(message.encode(gpg.encoding))
             data_file.flush()
             result = gpg.verify(detached_signature, data_filename=data_file.name)
 
-    elif message.count('BEGIN PGP SIGNED MESSAGE') == 1:
+    elif message.count("BEGIN PGP SIGNED MESSAGE") == 1:
         result = gpg.verify(message)
-        match = pgp_inline_re.search(message.replace('\r\n', '\n').replace('\r', '\n'))
+        match = pgp_inline_re.search(message.replace("\r\n", "\n").replace("\r", "\n"))
         if not match:  # pragma: no cover
-            msg = f'message contained an inline PGP signature, but regular expression failed to extract body: {message}'
+            msg = (
+                "message contained an inline PGP signature, but regular expression failed to extract body:"
+                f" {message}"
+            )
             logger.info(msg)
             return None, None
-        new_message = match.group(2) + '\n'
+        new_message = match.group(2) + "\n"
 
     else:
         return None, None
 
-    log_message = result.stderr.replace('\n', ' -- ').replace('gpg:                ', '')
-    logger.info(f'checked PGP signature, response: {log_message}')
+    log_message = result.stderr.replace("\n", " -- ").replace("gpg:                ", "")
+    logger.info(f"checked PGP signature, response: {log_message}")
     if result.valid and result.key_status is None:
-        logger.info(f'Found valid PGP signature, fingerprint {result.fingerprint}')
+        logger.info(f"Found valid PGP signature, fingerprint {result.fingerprint}")
         return new_message, result.fingerprint
     return None, None
```

### Comparing `irrd-4.2.8/irrd/utils/process_support.py` & `irrd-4.3.0/irrd/utils/process_support.py`

 * *Files 13% similar despite different names*

```diff
@@ -16,16 +16,18 @@
 
 
 class ExceptionLoggingProcess(Process):  # pragma: no cover
     def run(self) -> None:
         try:
             super().run()
         except Exception as e:
-            logger.critical(f'Essential IRRd subprocess encountered a fatal error, '
-                            f'traceback follows, shutting down: {e}', exc_info=e)
+            logger.critical(
+                f"Essential IRRd subprocess encountered a fatal error, traceback follows, shutting down: {e}",
+                exc_info=e,
+            )
             os.kill(os.getppid(), signal.SIGTERM)
 
 
 def memory_trim():  # pragma: no cover
     # See https://github.com/irrdnet/irrd/issues/571
     try:
         ctypes.CDLL(None).malloc_trim(0)
@@ -35,14 +37,15 @@
 
 def set_traceback_handler():  # pragma: no cover
     """
     Log a traceback of all threads when receiving SIGUSR1.
     This is inherited by child processes, so only set twice:
     in the main process, and in the uvicorn app startup.
     """
+
     def sigusr1_handler(signal, frame):
         thread_names = {th.ident: th.name for th in threading.enumerate()}
         code = [f"Traceback follows for all threads of process {os.getpid()} ({getproctitle()}):"]
         for thread_id, stack in sys._current_frames().items():
             thread_name = thread_names.get(thread_id, "")
             code.append(f"\n## Thread: {thread_name}({thread_id}) ##\n")
             code += traceback.format_list(traceback.extract_stack(stack))
```

### Comparing `irrd-4.2.8/irrd/utils/rpsl_samples.py` & `irrd-4.3.0/irrd/utils/rpsl_samples.py`

 * *Files 2% similar despite different names*

```diff
@@ -490,23 +490,27 @@
 admin-c:        PERSON-TEST
 tech-c:         PERSON-TEST
 mnt-by:         TEST-MNT
 changed:        changed@example.com 20190701 # comment
 source:         TEST
 """
 
+SAMPLE_MNTNER_MD5 = "md5-password"
+SAMPLE_MNTNER_CRYPT = "crypt-password"
+SAMPLE_MNTNER_BCRYPT = "bcrypt-password"
 SAMPLE_MNTNER = """mntner:         TEST-MNT
 admin-c:        PERSON-TEST
 notify:         notify@example.net
 upd-to:         upd-to@example.net
 mnt-nfy:        mnt-nfy@example.net
 mnt-nfy:        mnt-nfy2@example.net
 auth:           PGPKey-80F238C6
 auth:           CRYPT-Pw LEuuhsBJNFV0Q  # crypt-password
 auth:           MD5-pw $1$fgW84Y9r$kKEn9MUq8PChNKpQhO6BM.  # md5-password
+auth:           bcrypt-pw $2b$12$RMrlONJ0tasnpo.zHDF.yuYm/Gb1ARmIjP097ZoIWBn9YLIM2ao5W # bcrypt-password
 mnt-by:         TEST-MNT
 mnt-by:         OTHER1-MNT,OTHER2-MNT
 changed:        changed@example.com 20190701 # comment
 remarks:        unįcöde tæst 🌈🦄
 source:         TEST
 remarks:        remark
 """
@@ -785,31 +789,31 @@
 zwBzW+p+qqN0rNRMFTNy3WnVVzZY5UWljU83jMBQkXiOSxo72/yIpG89xzi24Bqp
 +pewy9PIcK1JBKvGyeO2Gh1K2tsrVYzs7aP5/RmkmUyrQeXa3l4=
 =FLEo
 -----END PGP SIGNATURE-----
 """
 
 object_sample_mapping = {
-    'as-block': SAMPLE_AS_BLOCK,
-    'as-set': SAMPLE_AS_SET,
-    'aut-num': SAMPLE_AUT_NUM,
-    'domain': SAMPLE_DOMAIN,
-    'filter-set': SAMPLE_FILTER_SET,
-    'inet-rtr': SAMPLE_INET_RTR,
-    'inet6num': SAMPLE_INET6NUM,
-    'inetnum': SAMPLE_INETNUM,
-    'key-cert': SAMPLE_KEY_CERT,
-    'mntner': SAMPLE_MNTNER,
-    'peering-set': SAMPLE_PEERING_SET,
-    'person': SAMPLE_PERSON,
-    'role': SAMPLE_ROLE,
-    'route': SAMPLE_ROUTE,
-    'route-set': SAMPLE_ROUTE_SET,
-    'route6': SAMPLE_ROUTE6,
-    'rtr-set': SAMPLE_RTR_SET,
+    "as-block": SAMPLE_AS_BLOCK,
+    "as-set": SAMPLE_AS_SET,
+    "aut-num": SAMPLE_AUT_NUM,
+    "domain": SAMPLE_DOMAIN,
+    "filter-set": SAMPLE_FILTER_SET,
+    "inet-rtr": SAMPLE_INET_RTR,
+    "inet6num": SAMPLE_INET6NUM,
+    "inetnum": SAMPLE_INETNUM,
+    "key-cert": SAMPLE_KEY_CERT,
+    "mntner": SAMPLE_MNTNER,
+    "peering-set": SAMPLE_PEERING_SET,
+    "person": SAMPLE_PERSON,
+    "role": SAMPLE_ROLE,
+    "route": SAMPLE_ROUTE,
+    "route-set": SAMPLE_ROUTE_SET,
+    "route6": SAMPLE_ROUTE6,
+    "rtr-set": SAMPLE_RTR_SET,
 }
 
 TEMPLATE_ROUTE_OBJECT = """route:          [mandatory]  [single]    [primary/look-up key]
 descr:          [optional]   [multiple]  []
 origin:         [mandatory]  [single]    [primary key]
 holes:          [optional]   [multiple]  []
 member-of:      [optional]   [multiple]  [look-up key, weak references route-set]
```

### Comparing `irrd-4.2.8/irrd/utils/tests/test_email.py` & `irrd-4.3.0/irrd/utils/tests/test_email.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,25 +1,28 @@
 # flake8: noqa: W291, W293
 
-import pytest
 import textwrap
 from unittest.mock import Mock
 
+import pytest
+
 from irrd.conf import get_setting
+
 from ..email import EmailParser, send_email
 
 
 class TestEmailParser:
     # These tests do not mock utils.pgp.validate_pgp_signature, and have quite
     # some overlap with the tests for that part. This is on purpose, as PGP
     # validation and MIME parsing can be tricky, and errors in either part or
     # their coupling easily cause security issues.
 
     def test_parse_valid_plain_with_charset(self):
-        email = textwrap.dedent("""
+        email = textwrap.dedent(
+            """
         From sasha@localhost  Thu Jan  5 10:04:48 2018
         Received: from [127.0.0.1] (localhost.localdomain [127.0.0.1])
           by hostname (Postfix) with ESMTPS id 740AD310597
           for <sasha@localhost>; Thu,  5 Jan 2018 10:04:48 +0100 (CET)
         Message-ID: <1325754288.4989.6.camel@hostname>
         Subject: my subject
         Subject: not my subject
@@ -28,25 +31,27 @@
         Date: Thu, 05 Jan 2018 10:04:48 +0100
         Content-Type: text/plain; charset=us-ascii
         X-Mailer: Python 3.7
         Content-Transfer-Encoding: 7bit
         Mime-Version: 1.0
 
         message content
-        """).strip()
+        """
+        ).strip()
         parser = EmailParser(email)
-        assert parser.body == 'message content'
-        assert parser.message_id == '<1325754288.4989.6.camel@hostname>'
-        assert parser.message_from == 'Sasha <sasha@example.com>'
-        assert parser.message_date == 'Thu, 05 Jan 2018 10:04:48 +0100'
-        assert parser.message_subject == 'my subject'
+        assert parser.body == "message content"
+        assert parser.message_id == "<1325754288.4989.6.camel@hostname>"
+        assert parser.message_from == "Sasha <sasha@example.com>"
+        assert parser.message_date == "Thu, 05 Jan 2018 10:04:48 +0100"
+        assert parser.message_subject == "my subject"
         assert parser.pgp_fingerprint is None
 
     def test_parse_valid_plain_without_charset(self):
-        email = textwrap.dedent("""
+        email = textwrap.dedent(
+            """
         From sasha@localhost  Thu Jan  5 10:04:48 2018
         Received: from [127.0.0.1] (localhost.localdomain [127.0.0.1])
           by hostname (Postfix) with ESMTPS id 740AD310597
           for <sasha@localhost>; Thu,  5 Jan 2018 10:04:48 +0100 (CET)
         Message-ID: <1325754288.4989.6.camel@hostname>
         Subject: my subject
         From: Sasha <sasha@example.com>
@@ -54,21 +59,23 @@
         Date: Thu, 05 Jan 2018 10:04:48 +0100
         Content-Type: text/plain
         X-Mailer: Python 3.7
         Content-Transfer-Encoding: 7bit
         Mime-Version: 1.0
 
         message content
-        """).strip()
+        """
+        ).strip()
         parser = EmailParser(email)
-        assert parser.body == 'message content'
+        assert parser.body == "message content"
         assert parser.pgp_fingerprint is None
 
     def test_parse_valid_multipart_text_plain_with_charset(self):
-        email = textwrap.dedent("""
+        email = textwrap.dedent(
+            """
         From sasha@localhost  Thu Jan  5 10:04:48 2018
         Received: from [127.0.0.1] (localhost.localdomain [127.0.0.1])
           by hostname (Postfix) with ESMTPS id 740AD310597
           for <sasha@localhost>; Thu,  5 Jan 2018 10:04:48 +0100 (CET)
         Message-ID: <1325754288.4989.6.camel@hostname>
         Subject: my subject
         From: Sasha <sasha@example.com>
@@ -91,21 +98,23 @@
         --Apple-Mail=_01FE5B2D-C7F3-4DDD-AB42-B92C88CFBF0F
         Content-Transfer-Encoding: 7bit
         Content-Type: text/html;
             charset=us-ascii
         
         <html><head><meta http-equiv="Content-Type" content="text/html charset=us-ascii"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;" class=""><b class="">test 1 2 3</b><div class=""><br class=""></div></body></html>
         --Apple-Mail=_01FE5B2D-C7F3-4DDD-AB42-B92C88CFBF0F--
-        """).strip()
+        """
+        ).strip()
         parser = EmailParser(email)
-        assert parser.body.strip() == 'test 1 2 3'
+        assert parser.body.strip() == "test 1 2 3"
         assert parser.pgp_fingerprint is None
 
     def test_parse_valid_multipart_quoted_printable_with_charset(self):
-        email = textwrap.dedent("""
+        email = textwrap.dedent(
+            """
         From sasha@localhost  Thu Jan  5 10:04:48 2018
         Received: from [127.0.0.1] (localhost.localdomain [127.0.0.1])
           by hostname (Postfix) with ESMTPS id 740AD310597
           for <sasha@localhost>; Thu,  5 Jan 2018 10:04:48 +0100 (CET)
         Message-ID: <1325754288.4989.6.camel@hostname>
         Subject: my subject
         From: Sasha <sasha@example.com>
@@ -129,22 +138,24 @@
         --Apple-Mail=_01FE5B2D-C7F3-4DDD-AB42-B92C88CFBF0F
         Content-Transfer-Encoding: 7bit
         Content-Type: text/html;
             charset=us-ascii
 
         <html><head><meta http-equiv="Content-Type" content="text/html charset=us-ascii"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;" class=""><b class="">test 1 2 3</b><div class=""><br class=""></div></body></html>
         --Apple-Mail=_01FE5B2D-C7F3-4DDD-AB42-B92C88CFBF0F--
-        """).strip()
+        """
+        ).strip()
         parser = EmailParser(email)
-        assert parser.body.strip() == 'se font vite pédagogues'
+        assert parser.body.strip() == "se font vite pédagogues"
         assert parser.pgp_fingerprint is None
 
     def test_parse_valid_multipart_quoted_printable_without_charset(self):
         # latin-1 will be assumed
-        email = textwrap.dedent("""
+        email = textwrap.dedent(
+            """
         From sasha@localhost  Thu Jan  5 10:04:48 2018
         Received: from [127.0.0.1] (localhost.localdomain [127.0.0.1])
           by hostname (Postfix) with ESMTPS id 740AD310597
           for <sasha@localhost>; Thu,  5 Jan 2018 10:04:48 +0100 (CET)
         Message-ID: <1325754288.4989.6.camel@hostname>
         Subject: my subject
         From: Sasha <sasha@example.com>
@@ -169,22 +180,24 @@
         --Apple-Mail=_01FE5B2D-C7F3-4DDD-AB42-B92C88CFBF0F
         Content-Transfer-Encoding: 7bit
         Content-Type: text/html;
             charset=us-ascii
 
         <html><head><meta http-equiv='Content-Type' content='text/html charset=us-ascii'></head><body style='word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;' class=""><b class="">test 1 2 3</b><div class=""><br class=""></div></body></html>
         --Apple-Mail=_01FE5B2D-C7F3-4DDD-AB42-B92C88CFBF0F--
-        """).strip()
+        """
+        ).strip()
         parser = EmailParser(email)
-        assert parser.body.strip() == 'se font vite pdagogues'
+        assert parser.body.strip() == "se font vite pdagogues"
         assert parser.pgp_fingerprint is None
 
-    @pytest.mark.usefixtures('tmp_gpg_dir')
+    @pytest.mark.usefixtures("tmp_gpg_dir")
     def test_parse_valid_multipart_signed_ascii(self, tmp_gpg_dir, preload_gpg_key):
-        email = textwrap.dedent("""
+        email = textwrap.dedent(
+            """
         From sasha@localhost  Thu Jan  5 10:04:48 2018
         Received: from [127.0.0.1] (localhost.localdomain [127.0.0.1])
           by hostname (Postfix) with ESMTPS id 740AD310597
           for <sasha@localhost>; Thu,  5 Jan 2018 10:04:48 +0100 (CET)
         Message-ID: <1325754288.4989.6.camel@hostname>
         Subject: my subject
         From: Sasha <sasha@example.com>
@@ -224,18 +237,22 @@
         QAHrNeb/kRMcw7l6I3eL94W7ndfCZK7/XhHqYB4m88Jnbaklxih2gjJGWu50eQc7
         EXt0dl6BQeKlMtLWfgtBY4RzEglr1u99DSEqotJTlpSqUQ79rYwzKNvjI1Xc7yJc
         lLNwRJTtoWd8sUc0njlemxtVELNHUj0ahpQgMTqw1WbJu+FJxaTcRdbu6fYwl7hc
         k1Bt6Qyyn4qWD19aV6yClqyhJwZB2uoSKHvBmPIu31nHRYNr9SWD75dht8YODsmF
         QxtFWD7kfutDc40U0GjukbcPsfni1BH9AZZbUsm6YS7JMxoh1Rk=
         =92HM
         -----END PGP SIGNATURE-----
-        """).strip()
+        """
+        ).strip()
         parser = EmailParser(email)
-        assert parser.body.strip() == 'test 1 2 3'
-        assert parser._pgp_signature == textwrap.dedent("""
+        assert parser.body.strip() == "test 1 2 3"
+        assert (
+            parser._pgp_signature
+            == textwrap.dedent(
+                """
         -----BEGIN PGP SIGNATURE-----
 
         iQIzBAEBCAAdFiEEhiYdjb69pPVGktZNqDg7p4DyOMYFAlthsw0ACgkQqDg7p4Dy
         OMbLdBAAnJ93CZYz6VNhHiSFhj2EtU6yG70chyAiCfrZXsXGWQXNBbkQBCpk3Cy0
         HD2cIBxh1612bDkct8ezWaS4uOEvp5gyEJg0/VrAMCvWIEXEFizE75kOgj3ay+hs
         Dj4C7TRlRvQdRbaNdCb3R27WMK9GPwDJgqjifZQu13pWkkAxQLpZBs+wbnWvh01X
         QWQdf7xr3PQUkm+uVE2fU7j+c1vQs5oJLMdvSuzSN59IvLiEmRFrkQF7yj3WCesh
@@ -243,20 +260,25 @@
         puk83Xr+WDgVo6w6KT7n5ZFg7XRH/WV0hhdy6i+wuyXnwdTP5JQbJn66xZV4iYZh
         QAHrNeb/kRMcw7l6I3eL94W7ndfCZK7/XhHqYB4m88Jnbaklxih2gjJGWu50eQc7
         EXt0dl6BQeKlMtLWfgtBY4RzEglr1u99DSEqotJTlpSqUQ79rYwzKNvjI1Xc7yJc
         lLNwRJTtoWd8sUc0njlemxtVELNHUj0ahpQgMTqw1WbJu+FJxaTcRdbu6fYwl7hc
         k1Bt6Qyyn4qWD19aV6yClqyhJwZB2uoSKHvBmPIu31nHRYNr9SWD75dht8YODsmF
         QxtFWD7kfutDc40U0GjukbcPsfni1BH9AZZbUsm6YS7JMxoh1Rk=
         =92HM
-        -----END PGP SIGNATURE-----""").strip()
-        assert parser.pgp_fingerprint == '86261D8DBEBDA4F54692D64DA8383BA780F238C6'
-
-    @pytest.mark.usefixtures('tmp_gpg_dir')
-    def test_parse_invalid_multipart_signed_ascii_with_additional_text_part(self, tmp_gpg_dir, preload_gpg_key):
-        email = textwrap.dedent("""
+        -----END PGP SIGNATURE-----"""
+            ).strip()
+        )
+        assert parser.pgp_fingerprint == "86261D8DBEBDA4F54692D64DA8383BA780F238C6"
+
+    @pytest.mark.usefixtures("tmp_gpg_dir")
+    def test_parse_invalid_multipart_signed_ascii_with_additional_text_part(
+        self, tmp_gpg_dir, preload_gpg_key
+    ):
+        email = textwrap.dedent(
+            """
         From sasha@localhost  Thu Jan  5 10:04:48 2018
         Received: from [127.0.0.1] (localhost.localdomain [127.0.0.1])
           by hostname (Postfix) with ESMTPS id 740AD310597
           for <sasha@localhost>; Thu,  5 Jan 2018 10:04:48 +0100 (CET)
         Message-ID: <1325754288.4989.6.camel@hostname>
         Subject: my subject
         From: Sasha <sasha@example.com>
@@ -304,22 +326,24 @@
         --Apple-Mail=_368A6867-FE85-4AFB-AACA-CDBA53C7DB25
         Content-Transfer-Encoding: 7bit
         Content-Type: text/plain;
         \tcharset=us-ascii
 
         additional text/plain part - not signed
 
-        """).strip()
+        """
+        ).strip()
         parser = EmailParser(email)
-        assert parser.body.strip() == 'additional text/plain part - not signed'
+        assert parser.body.strip() == "additional text/plain part - not signed"
         assert parser.pgp_fingerprint is None
 
-    @pytest.mark.usefixtures('tmp_gpg_dir')
+    @pytest.mark.usefixtures("tmp_gpg_dir")
     def test_parse_valid_inline_signed_ascii(self, tmp_gpg_dir, preload_gpg_key):
-        email = textwrap.dedent("""
+        email = textwrap.dedent(
+            """
         From sasha@localhost  Thu Jan  5 10:04:48 2018
         Received: from [127.0.0.1] (localhost.localdomain [127.0.0.1])
           by hostname (Postfix) with ESMTPS id 740AD310597
           for <sasha@localhost>; Thu,  5 Jan 2018 10:04:48 +0100 (CET)
         Message-ID: <1325754288.4989.6.camel@hostname>
         Subject: my subject
         From: Sasha <sasha@example.com>
@@ -348,22 +372,24 @@
         bgZ95P4z2i7XuCOJmI5LNOaqlZmt3RebLd2QIAfKONi2GfwLl4ibk6VlCdMIQoJS
         paQ7V2Dq9r6Pure2wn+xIPOuySQ+zVf7emY9GFxFim87Eu2aaV4E/S52EpOFqWCU
         TgTCi98X5RJyBdvii7XBl+VcM041hww221nw2WvRECaImgwx1eNx2UjXpsLN5VrT
         oxZBbe2zPrzKJSG3WmBRi8bekarDrCPSd1uWXRoCqUmIPLTTID52ikkPKEBTeNyv
         4Ni0aIkkZY3cM0QR9EEHSCJgS2RVQujw/KZTeTQTLAJLtGtLbq8=
         =Zn24
         -----END PGP SIGNATURE-----
-        """).strip()
+        """
+        ).strip()
         parser = EmailParser(email)
-        assert parser.body.strip() == 'test 1 2 3'
-        assert parser.pgp_fingerprint == '86261D8DBEBDA4F54692D64DA8383BA780F238C6'
+        assert parser.body.strip() == "test 1 2 3"
+        assert parser.pgp_fingerprint == "86261D8DBEBDA4F54692D64DA8383BA780F238C6"
 
-    @pytest.mark.usefixtures('tmp_gpg_dir')
+    @pytest.mark.usefixtures("tmp_gpg_dir")
     def test_parse_invalid_inline_signed_ascii_multiple_messages(self, tmp_gpg_dir, preload_gpg_key):
-        email = textwrap.dedent("""
+        email = textwrap.dedent(
+            """
         From sasha@localhost  Thu Jan  5 10:04:48 2018
         Received: from [127.0.0.1] (localhost.localdomain [127.0.0.1])
           by hostname (Postfix) with ESMTPS id 740AD310597
           for <sasha@localhost>; Thu,  5 Jan 2018 10:04:48 +0100 (CET)
         Message-ID: <1325754288.4989.6.camel@hostname>
         Subject: my subject
         From: Sasha <sasha@example.com>
@@ -411,21 +437,23 @@
         bgZ95P4z2i7XuCOJmI5LNOaqlZmt3RebLd2QIAfKONi2GfwLl4ibk6VlCdMIQoJS
         paQ7V2Dq9r6Pure2wn+xIPOuySQ+zVf7emY9GFxFim87Eu2aaV4E/S52EpOFqWCU
         TgTCi98X5RJyBdvii7XBl+VcM041hww221nw2WvRECaImgwx1eNx2UjXpsLN5VrT
         oxZBbe2zPrzKJSG3WmBRi8bekarDrCPSd1uWXRoCqUmIPLTTID52ikkPKEBTeNyv
         4Ni0aIkkZY3cM0QR9EEHSCJgS2RVQujw/KZTeTQTLAJLtGtLbq8=
         =Zn24
         -----END PGP SIGNATURE-----
-        """).strip()
+        """
+        ).strip()
         parser = EmailParser(email)
         assert parser.pgp_fingerprint is None
 
-    @pytest.mark.usefixtures('tmp_gpg_dir')
+    @pytest.mark.usefixtures("tmp_gpg_dir")
     def test_parse_valid_multipart_signed_unicode(self, tmp_gpg_dir, preload_gpg_key):
-        email = textwrap.dedent("""
+        email = textwrap.dedent(
+            """
         From sasha@localhost  Thu Jan  5 10:04:48 2018
         Received: from [127.0.0.1] (localhost.localdomain [127.0.0.1])
           by hostname (Postfix) with ESMTPS id 740AD310597
           for <sasha@localhost>; Thu,  5 Jan 2018 10:04:48 +0100 (CET)
         Message-ID: <1325754288.4989.6.camel@hostname>
         Subject: my subject
         From: Sasha <sasha@example.com>
@@ -466,22 +494,24 @@
         bOyKoVDa7K3DHydgQG3ntrqfMwlVEW+Wmw7G8qodH4fzhWJYXYGYbrTdVvTFUGs4
         zM+omk0t4azQf7UeJjMcf4anfn7qAHOJ1j13VIFF9EEmVOOe7CIUEHmwuvfml6B6
         rvBy4+gNANbnAJ61u8QZG5qVGT9YF/nlUIuoivm0TENdlwOv4Gw=
         =KDl+
         -----END PGP SIGNATURE-----
         
         --Apple-Mail=_18B291D9-548C-4458-8F17-B76537227FDF--
-        """).strip()
+        """
+        ).strip()
         parser = EmailParser(email)
-        assert parser.body.strip() == 'test 💩 é æ'
-        assert parser.pgp_fingerprint == '86261D8DBEBDA4F54692D64DA8383BA780F238C6'
+        assert parser.body.strip() == "test 💩 é æ"
+        assert parser.pgp_fingerprint == "86261D8DBEBDA4F54692D64DA8383BA780F238C6"
 
-    @pytest.mark.usefixtures('tmp_gpg_dir')
+    @pytest.mark.usefixtures("tmp_gpg_dir")
     def test_parse_invalid_signature_multipart_signed_ascii_bad_signature(self, tmp_gpg_dir, preload_gpg_key):
-        email = textwrap.dedent("""
+        email = textwrap.dedent(
+            """
         From sasha@localhost  Thu Jan  5 10:04:48 2018
         Received: from [127.0.0.1] (localhost.localdomain [127.0.0.1])
           by hostname (Postfix) with ESMTPS id 740AD310597
           for <sasha@localhost>; Thu,  5 Jan 2018 10:04:48 +0100 (CET)
         Message-ID: <1325754288.4989.6.camel@hostname>
         Subject: my subject
         From: Sasha <sasha@example.com>
@@ -521,58 +551,61 @@
         QAHrNeb/kRMcw7l6I3eL94W7ndfCZK7/XhHqYB4m88Jnbaklxih2gjJGWu50eQc7
         EXt0dl6BQeKlMtLWfgtBY4RzEglr1u99DSEqotJTlpSqUQ79rYwzKNvjI1Xc7yJc
         lLNwRJTtoWd8sUc0njlemxtVELNHUj0ahpQgMTqw1WbJu+FJxaTcRdbu6fYwl7hc
         k1Bt6Qyyn4qWD19aV6yClqyhJwZB2uoSKHvBmPIu31nHRYNr9SWD75dht8YODsmF
         QxtFWD7kfutDc40U0GjukbcPsfni1BH9AZZbUsm6YS7JMxoh1Rk=
         =92HM
         -----END PGP SIGNATURE-----
-        """).strip()
+        """
+        ).strip()
         parser = EmailParser(email)
-        assert parser.body.strip() == 'test 1 2 INVALID'
+        assert parser.body.strip() == "test 1 2 INVALID"
         assert parser.pgp_fingerprint is None
 
     def test_invalid_blank_body(self):
-        email = textwrap.dedent("""
+        email = textwrap.dedent(
+            """
         From sasha@localhost  Thu Jan  5 10:04:48 2018
         Received: from [127.0.0.1] (localhost.localdomain [127.0.0.1])
           by hostname (Postfix) with ESMTPS id 740AD310597
           for <sasha@localhost>; Thu,  5 Jan 2018 10:04:48 +0100 (CET)
         Message-ID: <1325754288.4989.6.camel@hostname>
         Subject: my subject
         From: Sasha <sasha@example.com>
         To: sasha@localhost
         Date: Thu, 05 Jan 2018 10:04:48 +0100
         Content-Type: multipart/signed;
          boundary="Apple-Mail=_368A6867-FE85-4AFB-AACA-CDBA53C7DB25"
         Mime-Version: 1.0 (Mac OS X Mail 10.3 
         To: sasha@localhost
         X-Mailer: Apple Mail (2.3273)
-        """).strip()
+        """
+        ).strip()
         parser = EmailParser(email)
         assert not parser.body.strip()
         assert parser.pgp_fingerprint is None
 
 
 class TestSendEmail:
     def test_send_email(self, monkeypatch):
         mock_smtp = Mock()
-        monkeypatch.setattr('irrd.utils.email.SMTP', lambda server: mock_smtp)
-        send_email('Sasha <sasha@example.com>', 'subject', 'body')
-        assert mock_smtp.mock_calls[0][0] == 'send_message'
-        assert mock_smtp.mock_calls[0][1][0]['From'] == get_setting('email.from')
-        assert mock_smtp.mock_calls[0][1][0]['To'] == 'Sasha <sasha@example.com>'
-        assert mock_smtp.mock_calls[0][1][0]['Subject'] == 'subject'
+        monkeypatch.setattr("irrd.utils.email.SMTP", lambda server: mock_smtp)
+        send_email("Sasha <sasha@example.com>", "subject", "body")
+        assert mock_smtp.mock_calls[0][0] == "send_message"
+        assert mock_smtp.mock_calls[0][1][0]["From"] == get_setting("email.from")
+        assert mock_smtp.mock_calls[0][1][0]["To"] == "Sasha <sasha@example.com>"
+        assert mock_smtp.mock_calls[0][1][0]["Subject"] == "subject"
         payload = mock_smtp.mock_calls[0][1][0].get_payload()
-        assert 'body' in payload
-        assert 'IRRd version' in payload
-        assert get_setting('email.footer') in payload
-        assert mock_smtp.mock_calls[1][0] == 'quit'
+        assert "body" in payload
+        assert "IRRd version" in payload
+        assert get_setting("email.footer") in payload
+        assert mock_smtp.mock_calls[1][0] == "quit"
 
     def test_send_email_with_recipient_override(self, monkeypatch, config_override):
-        config_override({'email': {'recipient_override': 'override@example.com'}})
+        config_override({"email": {"recipient_override": "override@example.com"}})
         mock_smtp = Mock()
-        monkeypatch.setattr('irrd.utils.email.SMTP', lambda server: mock_smtp)
-        send_email('Sasha <sasha@example.com>', 'subject', 'body')
-        assert mock_smtp.mock_calls[0][0] == 'send_message'
-        assert mock_smtp.mock_calls[0][1][0]['From'] == get_setting('email.from')
-        assert mock_smtp.mock_calls[0][1][0]['To'] == 'override@example.com'
-        assert mock_smtp.mock_calls[0][1][0]['Subject'] == 'subject'
+        monkeypatch.setattr("irrd.utils.email.SMTP", lambda server: mock_smtp)
+        send_email("Sasha <sasha@example.com>", "subject", "body")
+        assert mock_smtp.mock_calls[0][0] == "send_message"
+        assert mock_smtp.mock_calls[0][1][0]["From"] == get_setting("email.from")
+        assert mock_smtp.mock_calls[0][1][0]["To"] == "override@example.com"
+        assert mock_smtp.mock_calls[0][1][0]["Subject"] == "subject"
```

### Comparing `irrd-4.2.8/irrd/utils/tests/test_pgp.py` & `irrd-4.3.0/irrd/utils/tests/test_pgp.py`

 * *Files 10% similar despite different names*

```diff
@@ -4,26 +4,31 @@
 
 import pytest
 
 from ..pgp import validate_pgp_signature
 
 
 class TestValidatePGPSignature:
-
-    @pytest.mark.usefixtures('tmp_gpg_dir')
+    @pytest.mark.usefixtures("tmp_gpg_dir")
     def test_valid_detached_signed_ascii(self, tmp_gpg_dir, preload_gpg_key):
-        message = textwrap.dedent("""
+        message = (
+            textwrap.dedent(
+                """
         Content-Transfer-Encoding: 7bit
         Content-Type: text/plain;
         \tcharset=us-ascii
 
         test 1 2 3
-        """).strip() + '\n'
+        """
+            ).strip()
+            + "\n"
+        )
 
-        signature = textwrap.dedent("""
+        signature = textwrap.dedent(
+            """
         -----BEGIN PGP SIGNATURE-----
 
         iQIzBAEBCAAdFiEEhiYdjb69pPVGktZNqDg7p4DyOMYFAlthsw0ACgkQqDg7p4Dy
         OMbLdBAAnJ93CZYz6VNhHiSFhj2EtU6yG70chyAiCfrZXsXGWQXNBbkQBCpk3Cy0
         HD2cIBxh1612bDkct8ezWaS4uOEvp5gyEJg0/VrAMCvWIEXEFizE75kOgj3ay+hs
         Dj4C7TRlRvQdRbaNdCb3R27WMK9GPwDJgqjifZQu13pWkkAxQLpZBs+wbnWvh01X
         QWQdf7xr3PQUkm+uVE2fU7j+c1vQs5oJLMdvSuzSN59IvLiEmRFrkQF7yj3WCesh
@@ -32,22 +37,24 @@
         QAHrNeb/kRMcw7l6I3eL94W7ndfCZK7/XhHqYB4m88Jnbaklxih2gjJGWu50eQc7
         EXt0dl6BQeKlMtLWfgtBY4RzEglr1u99DSEqotJTlpSqUQ79rYwzKNvjI1Xc7yJc
         lLNwRJTtoWd8sUc0njlemxtVELNHUj0ahpQgMTqw1WbJu+FJxaTcRdbu6fYwl7hc
         k1Bt6Qyyn4qWD19aV6yClqyhJwZB2uoSKHvBmPIu31nHRYNr9SWD75dht8YODsmF
         QxtFWD7kfutDc40U0GjukbcPsfni1BH9AZZbUsm6YS7JMxoh1Rk=
         =92HM
         -----END PGP SIGNATURE-----
-        """).strip()
+        """
+        ).strip()
         new_message, fingerprint = validate_pgp_signature(message, signature)
         assert new_message is None
-        assert fingerprint == '86261D8DBEBDA4F54692D64DA8383BA780F238C6'
+        assert fingerprint == "86261D8DBEBDA4F54692D64DA8383BA780F238C6"
 
-    @pytest.mark.usefixtures('tmp_gpg_dir')
+    @pytest.mark.usefixtures("tmp_gpg_dir")
     def test_valid_inline_signed_ascii(self, tmp_gpg_dir, preload_gpg_key):
-        message = textwrap.dedent("""
+        message = textwrap.dedent(
+            """
         UNSIGNED TEXT TO BE IGNORED
 
         -----BEGIN PGP SIGNED MESSAGE-----
         Hash: SHA256
         
         test
         1
@@ -69,22 +76,24 @@
         GGqBgzVjHX2RTc24e9OrKSoN8dOs3jsVj0Ucnxuh2nX0y/RaBOM95hUwhokgxGoQ
         XZzQn3mPAPE3sZ05YyYIa1eWYxwwwI1xDxW/sC8VMYCDl1sc1w+g7riOm2eam6eg
         YBfZyLf62EwyIj+y9TGAyfGe41cDtNzcNB2wUdoW3TEN8u3jS/euvtjFOIQ6aofs
         JWTW+eqHBNWWC7Rfi1B7Pqh1bkk1FWPaxCJij73ekV8ZNiBBmwA=
         =iWS2
         -----END PGP SIGNATURE-----
 
-        """).strip()
+        """
+        ).strip()
         new_message, fingerprint = validate_pgp_signature(message)
-        assert new_message.strip() == 'test\n1\n\n2\n\n3'
-        assert fingerprint == '86261D8DBEBDA4F54692D64DA8383BA780F238C6'
+        assert new_message.strip() == "test\n1\n\n2\n\n3"
+        assert fingerprint == "86261D8DBEBDA4F54692D64DA8383BA780F238C6"
 
-    @pytest.mark.usefixtures('tmp_gpg_dir')
+    @pytest.mark.usefixtures("tmp_gpg_dir")
     def test_invalid_inline_signed_ascii_multiple_messages(self, tmp_gpg_dir, preload_gpg_key):
-        message = textwrap.dedent("""
+        message = textwrap.dedent(
+            """
         -----BEGIN PGP SIGNED MESSAGE-----
         Hash: SHA256
 
         test 1 2 3
         -----BEGIN PGP SIGNATURE-----
 
         iQIzBAEBCAAdFiEEhiYdjb69pPVGktZNqDg7p4DyOMYFAltix/QACgkQqDg7p4Dy
@@ -118,29 +127,36 @@
         bgZ95P4z2i7XuCOJmI5LNOaqlZmt3RebLd2QIAfKONi2GfwLl4ibk6VlCdMIQoJS
         paQ7V2Dq9r6Pure2wn+xIPOuySQ+zVf7emY9GFxFim87Eu2aaV4E/S52EpOFqWCU
         TgTCi98X5RJyBdvii7XBl+VcM041hww221nw2WvRECaImgwx1eNx2UjXpsLN5VrT
         oxZBbe2zPrzKJSG3WmBRi8bekarDrCPSd1uWXRoCqUmIPLTTID52ikkPKEBTeNyv
         4Ni0aIkkZY3cM0QR9EEHSCJgS2RVQujw/KZTeTQTLAJLtGtLbq8=
         =Zn24
         -----END PGP SIGNATURE-----
-        """).strip()
+        """
+        ).strip()
         new_message, fingerprint = validate_pgp_signature(message)
         assert new_message is None
         assert fingerprint is None
 
-    @pytest.mark.usefixtures('tmp_gpg_dir')
+    @pytest.mark.usefixtures("tmp_gpg_dir")
     def test_invalid_signature_detached_signed_ascii(self, tmp_gpg_dir, preload_gpg_key):
-        message = textwrap.dedent("""
+        message = (
+            textwrap.dedent(
+                """
         Content-Transfer-Encoding: 7bit
         Content-Type: text/plain;
         \tcharset=us-ascii
 
         test 1 2 INVALID
-        """).strip() + '\n'
-        signature = textwrap.dedent("""
+        """
+            ).strip()
+            + "\n"
+        )
+        signature = textwrap.dedent(
+            """
         -----BEGIN PGP SIGNATURE-----
 
         iQIzBAEBCAAdFiEEhiYdjb69pPVGktZNqDg7p4DyOMYFAlthsw0ACgkQqDg7p4Dy
         OMbLdBAAnJ93CZYz6VNhHiSFhj2EtU6yG70chyAiCfrZXsXGWQXNBbkQBCpk3Cy0
         HD2cIBxh1612bDkct8ezWaS4uOEvp5gyEJg0/VrAMCvWIEXEFizE75kOgj3ay+hs
         Dj4C7TRlRvQdRbaNdCb3R27WMK9GPwDJgqjifZQu13pWkkAxQLpZBs+wbnWvh01X
         QWQdf7xr3PQUkm+uVE2fU7j+c1vQs5oJLMdvSuzSN59IvLiEmRFrkQF7yj3WCesh
@@ -149,11 +165,12 @@
         QAHrNeb/kRMcw7l6I3eL94W7ndfCZK7/XhHqYB4m88Jnbaklxih2gjJGWu50eQc7
         EXt0dl6BQeKlMtLWfgtBY4RzEglr1u99DSEqotJTlpSqUQ79rYwzKNvjI1Xc7yJc
         lLNwRJTtoWd8sUc0njlemxtVELNHUj0ahpQgMTqw1WbJu+FJxaTcRdbu6fYwl7hc
         k1Bt6Qyyn4qWD19aV6yClqyhJwZB2uoSKHvBmPIu31nHRYNr9SWD75dht8YODsmF
         QxtFWD7kfutDc40U0GjukbcPsfni1BH9AZZbUsm6YS7JMxoh1Rk=
         =92HM
         -----END PGP SIGNATURE-----
-        """).strip()
+        """
+        ).strip()
         new_message, fingerprint = validate_pgp_signature(message, signature)
         assert new_message is None
         assert fingerprint is None
```

### Comparing `irrd-4.2.8/irrd/utils/tests/test_text.py` & `irrd-4.3.0/irrd/utils/tests/test_text.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,56 +1,66 @@
 from io import StringIO
 
+import pytest
+
 from irrd.conf import PASSWORD_HASH_DUMMY_VALUE
 from irrd.utils.rpsl_samples import SAMPLE_MNTNER
-import pytest
-from ..text import (splitline_unicodesafe, split_paragraphs_rpsl, remove_auth_hashes,
-                    remove_last_modified, snake_to_camel_case)
+
+from ..text import (
+    remove_auth_hashes,
+    remove_last_modified,
+    snake_to_camel_case,
+    split_paragraphs_rpsl,
+    splitline_unicodesafe,
+)
 
 
 def test_remove_auth_hashes():
     with pytest.raises(ValueError):
-        remove_auth_hashes(['a', 'b'])
+        remove_auth_hashes(["a", "b"])
 
     original_text = SAMPLE_MNTNER
-    assert 'CRYPT-Pw LEuuhsBJNFV0Q' in original_text
-    assert 'MD5-pw $1$fgW84Y9r$kKEn9MUq8PChNKpQhO6BM.' in original_text
+    assert "CRYPT-Pw LEuuhsBJNFV0Q" in original_text
+    assert "MD5-pw $1$fgW84Y9r$kKEn9MUq8PChNKpQhO6BM." in original_text
+    assert "bcrypt-pw $2b$12$RMrlONJ0tasnpo.zHDF.yuYm/Gb1ARmIjP097ZoIWBn9YLIM2ao5W" in original_text
 
     result = remove_auth_hashes(original_text)
-    assert 'CRYPT-Pw ' + PASSWORD_HASH_DUMMY_VALUE in result
-    assert 'CRYPT-Pw LEuuhsBJNFV0Q' not in result
-    assert 'MD5-pw ' + PASSWORD_HASH_DUMMY_VALUE in result
-    assert 'MD5-pw $1$fgW84Y9r$kKEn9MUq8PChNKpQhO6BM.' not in result
-    assert 'other_text' == remove_auth_hashes('other_text')
+    assert "CRYPT-Pw " + PASSWORD_HASH_DUMMY_VALUE in result
+    assert "CRYPT-Pw LEuuhsBJNFV0Q" not in result
+    assert "MD5-pw " + PASSWORD_HASH_DUMMY_VALUE in result
+    assert "MD5-pw $1$fgW84Y9r$kKEn9MUq8PChNKpQhO6BM." not in result
+    assert "bcrypt-pw " + PASSWORD_HASH_DUMMY_VALUE in result
+    assert "bcrypt-pw $2b$12$RMrlONJ0tasnpo.zHDF.yuYm/Gb1ARmIjP097ZoIWBn9YLIM2ao5W" not in result
+    assert "other_text" == remove_auth_hashes("other_text")
 
 
 def test_remove_last_modified():
     # This descr line should be kept, only real last-modified attributes should be removed
-    expected_text = SAMPLE_MNTNER + 'descr: last-modified:  2020-01-01T00:00:00Z\n'
-    result = remove_last_modified(expected_text + 'last-modified:  2020-01-01T00:00:00Z\n')
+    expected_text = SAMPLE_MNTNER + "descr: last-modified:  2020-01-01T00:00:00Z\n"
+    result = remove_last_modified(expected_text + "last-modified:  2020-01-01T00:00:00Z\n")
     assert result == expected_text
 
 
 def test_splitline_unicodesafe():
     # U+2028 is the unicode line separator
-    assert list(splitline_unicodesafe('')) == []
-    assert list(splitline_unicodesafe('\nfoo\n\rb\u2028ar\n')) == ['foo', 'b\u2028ar']
+    assert list(splitline_unicodesafe("")) == []
+    assert list(splitline_unicodesafe("\nfoo\n\rb\u2028ar\n")) == ["foo", "b\u2028ar"]
 
 
 def test_split_paragraphs_rpsl():
-    paragraphs = split_paragraphs_rpsl('\n% ignore\npar 1\npar 1\n\npar 2\npar \u20282')
+    paragraphs = split_paragraphs_rpsl("\n% ignore\npar 1\npar 1\n\npar 2\npar \u20282")
     assert list(paragraphs) == [
-        'par 1\npar 1\n',
-        'par 2\npar \u20282\n',
+        "par 1\npar 1\n",
+        "par 2\npar \u20282\n",
     ]
 
-    paragraphs = split_paragraphs_rpsl(StringIO('\n% include\n\npar 1\npar 1\n\npar 2\npar \u20282'), False)
+    paragraphs = split_paragraphs_rpsl(StringIO("\n% include\n\npar 1\npar 1\n\npar 2\npar \u20282"), False)
     assert list(paragraphs) == [
-        '% include\n',
-        'par 1\npar 1\n',
-        'par 2\npar \u20282\n',
+        "% include\n",
+        "par 1\npar 1\n",
+        "par 2\npar \u20282\n",
     ]
 
 
 def test_snake_to_camel_case():
-    assert snake_to_camel_case('foo1_bar') == 'foo1Bar'
-    assert snake_to_camel_case(['foo1_bar', 'second_item']) == ['foo1Bar', 'secondItem']
+    assert snake_to_camel_case("foo1_bar") == "foo1Bar"
+    assert snake_to_camel_case(["foo1_bar", "second_item"]) == ["foo1Bar", "secondItem"]
```

### Comparing `irrd-4.2.8/irrd/utils/tests/test_validators.py` & `irrd-4.3.0/irrd/utils/tests/test_validators.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,59 +1,72 @@
 import pydantic
 import pytest
 from pytest import raises
 
-from ..validators import parse_as_number, ValidationError, RPSLChangeSubmission
+from ..validators import RPSLChangeSubmission, ValidationError, parse_as_number
 
 
 def test_validate_as_number():
-    assert parse_as_number('AS00') == ('AS0', 0)
-    assert parse_as_number('AS012345') == ('AS12345', 12345)
-    assert parse_as_number('as4294967295') == ('AS4294967295', 4294967295)
-    assert parse_as_number('012345', permit_plain=True) == ('AS12345', 12345)
-    assert parse_as_number(12345, permit_plain=True) == ('AS12345', 12345)
+    assert parse_as_number("AS00") == ("AS0", 0)
+    assert parse_as_number("AS012345") == ("AS12345", 12345)
+    assert parse_as_number("as4294967295") == ("AS4294967295", 4294967295)
+    assert parse_as_number("012345", permit_plain=True) == ("AS12345", 12345)
+    assert parse_as_number(12345, permit_plain=True) == ("AS12345", 12345)
 
     with raises(ValidationError) as ve:
-        parse_as_number('12345')
-    assert 'must start with' in str(ve.value)
+        parse_as_number("12345")
+    assert "must start with" in str(ve.value)
 
     with raises(ValidationError) as ve:
-        parse_as_number('ASFOO')
-    assert 'number part is not numeric' in str(ve.value)
+        parse_as_number("ASFOO")
+    assert "number part is not numeric" in str(ve.value)
 
     with raises(ValidationError) as ve:
-        parse_as_number('AS4294967296')
-    assert 'valid range is' in str(ve.value)
+        parse_as_number("AS4294967296")
+    assert "valid range is" in str(ve.value)
 
 
 def test_validate_rpsl_change_submission():
-    result = RPSLChangeSubmission.parse_obj({
-        'objects': [
-            {'object_text': 'text'},
-            {'attributes': [
-                {'name': 'name1', 'value': 'value1'},
-                {'name': 'name1', 'value': ['list1', 'list2']},
-            ]},
-        ],
-        'passwords': ['password'],
-        'override': 'override',
-        'delete_reason': 'delete reason',
-    })
-    assert result.objects[1].attributes[1].value == 'list1, list2'
+    result = RPSLChangeSubmission.parse_obj(
+        {
+            "objects": [
+                {"object_text": "text"},
+                {
+                    "attributes": [
+                        {"name": "name1", "value": "value1"},
+                        {"name": "name1", "value": ["list1", "list2"]},
+                    ]
+                },
+            ],
+            "passwords": ["password"],
+            "override": "override",
+            "delete_reason": "delete reason",
+        }
+    )
+    assert result.objects[1].attributes[1].value == "list1, list2"
 
     with pytest.raises(pydantic.ValidationError):
-        RPSLChangeSubmission.parse_obj({
-            'objects': [
-                {'attributes': [
-                    {'name': 'name1', 'missing-value': 'value1'},
-                ]},
-            ],
-        })
+        RPSLChangeSubmission.parse_obj(
+            {
+                "objects": [
+                    {
+                        "attributes": [
+                            {"name": "name1", "missing-value": "value1"},
+                        ]
+                    },
+                ],
+            }
+        )
 
     with pytest.raises(pydantic.ValidationError):
-        RPSLChangeSubmission.parse_obj({
-            'objects': [
-                {'object_text': 'text', 'attributes': [
-                    {'name': 'name1', 'value': 'value1'},
-                ]},
-            ],
-        })
+        RPSLChangeSubmission.parse_obj(
+            {
+                "objects": [
+                    {
+                        "object_text": "text",
+                        "attributes": [
+                            {"name": "name1", "value": "value1"},
+                        ],
+                    },
+                ],
+            }
+        )
```

### Comparing `irrd-4.2.8/irrd/utils/tests/test_whois_client.py` & `irrd-4.3.0/irrd/utils/tests/test_whois_client.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,289 +1,287 @@
 import socket
 from typing import Optional
 from unittest.mock import Mock
 
 import pytest
 
 from ..test_utils import flatten_mock_calls
-from ..whois_client import whois_query, whois_query_irrd, whois_query_source_status, WhoisQueryError
+from ..whois_client import (
+    WhoisQueryError,
+    whois_query,
+    whois_query_irrd,
+    whois_query_source_status,
+)
 
 
 class TestWhoisQuery:
     recv_calls = 0
 
     def test_query_end_line(self, monkeypatch):
         self.recv_calls = 0
         mock_socket = Mock()
-        monkeypatch.setattr('irrd.utils.whois_client.socket.create_connection', lambda address, timeout: mock_socket)
+        monkeypatch.setattr(
+            "irrd.utils.whois_client.socket.create_connection", lambda address, timeout: mock_socket
+        )
 
         def mock_socket_recv(bytes) -> bytes:
             self.recv_calls += 1
             if self.recv_calls > 2:
-                return b'END'
-            return str(self.recv_calls).encode('utf-8')
+                return b"END"
+            return str(self.recv_calls).encode("utf-8")
 
         mock_socket.recv = mock_socket_recv
-        response = whois_query('192.0.2.1', 43, 'query', ['END'])
-        assert response == '12END'
+        response = whois_query("192.0.2.1", 43, "query", ["END"])
+        assert response == "12END"
 
-        assert flatten_mock_calls(mock_socket) == [
-            ['sendall', (b'query\n',), {}],
-            ['close', (), {}]
-        ]
+        assert flatten_mock_calls(mock_socket) == [["sendall", (b"query\n",), {}], ["close", (), {}]]
         assert self.recv_calls == 3
 
     def test_no_more_data(self, monkeypatch):
         self.recv_calls = 0
         mock_socket = Mock()
-        monkeypatch.setattr('irrd.utils.whois_client.socket.create_connection', lambda address, timeout: mock_socket)
+        monkeypatch.setattr(
+            "irrd.utils.whois_client.socket.create_connection", lambda address, timeout: mock_socket
+        )
 
         def mock_socket_recv(bytes) -> bytes:
             self.recv_calls += 1
             if self.recv_calls > 2:
-                return b''
-            return str(self.recv_calls).encode('utf-8')
+                return b""
+            return str(self.recv_calls).encode("utf-8")
 
         mock_socket.recv = mock_socket_recv
-        response = whois_query('192.0.2.1', 43, 'query', ['END'])
-        assert response == '12'
+        response = whois_query("192.0.2.1", 43, "query", ["END"])
+        assert response == "12"
 
-        assert flatten_mock_calls(mock_socket) == [
-            ['sendall', (b'query\n',), {}],
-            ['close', (), {}]
-        ]
+        assert flatten_mock_calls(mock_socket) == [["sendall", (b"query\n",), {}], ["close", (), {}]]
         assert self.recv_calls == 3
 
     def test_query_timeout(self, monkeypatch):
         self.recv_calls = 0
         mock_socket = Mock()
-        monkeypatch.setattr('irrd.utils.whois_client.socket.create_connection', lambda address, timeout: mock_socket)
+        monkeypatch.setattr(
+            "irrd.utils.whois_client.socket.create_connection", lambda address, timeout: mock_socket
+        )
 
         def mock_socket_recv(bytes) -> bytes:
             self.recv_calls += 1
             if self.recv_calls > 2:
                 raise socket.timeout
-            return str(self.recv_calls).encode('utf-8')
+            return str(self.recv_calls).encode("utf-8")
 
         mock_socket.recv = mock_socket_recv
-        response = whois_query('192.0.2.1', 43, 'query')
-        assert response == '12'
+        response = whois_query("192.0.2.1", 43, "query")
+        assert response == "12"
 
-        assert flatten_mock_calls(mock_socket) == [
-            ['sendall', (b'query\n',), {}],
-            ['close', (), {}]
-        ]
+        assert flatten_mock_calls(mock_socket) == [["sendall", (b"query\n",), {}], ["close", (), {}]]
         assert self.recv_calls == 3
 
 
 class TestWhoisQueryIRRD:
     recv_calls = 0
 
     def test_query_valid(self, monkeypatch):
         self.recv_calls = 0
         mock_socket = Mock()
-        monkeypatch.setattr('irrd.utils.whois_client.socket.create_connection', lambda address, timeout: mock_socket)
+        monkeypatch.setattr(
+            "irrd.utils.whois_client.socket.create_connection", lambda address, timeout: mock_socket
+        )
 
         def mock_socket_recv(bytes) -> bytes:
             self.recv_calls += 1
             if self.recv_calls == 1:
-                return b'A2\n'
+                return b"A2\n"
             if self.recv_calls > 2:
-                return b'C\n'
-            return str(self.recv_calls).encode('utf-8')
+                return b"C\n"
+            return str(self.recv_calls).encode("utf-8")
 
         mock_socket.recv = mock_socket_recv
-        response = whois_query_irrd('192.0.2.1', 43, 'query')
-        assert response == '2'
+        response = whois_query_irrd("192.0.2.1", 43, "query")
+        assert response == "2"
 
-        assert flatten_mock_calls(mock_socket) == [
-            ['sendall', (b'query\n',), {}],
-            ['close', (), {}]
-        ]
+        assert flatten_mock_calls(mock_socket) == [["sendall", (b"query\n",), {}], ["close", (), {}]]
         assert self.recv_calls == 3
 
     def test_query_valid_empty_c_response(self, monkeypatch):
         self.recv_calls = 0
         mock_socket = Mock()
-        monkeypatch.setattr('irrd.utils.whois_client.socket.create_connection', lambda address, timeout: mock_socket)
+        monkeypatch.setattr(
+            "irrd.utils.whois_client.socket.create_connection", lambda address, timeout: mock_socket
+        )
 
         def mock_socket_recv(bytes) -> bytes:
             self.recv_calls += 1
-            return b'C\n'
+            return b"C\n"
 
         mock_socket.recv = mock_socket_recv
-        response = whois_query_irrd('192.0.2.1', 43, 'query')
+        response = whois_query_irrd("192.0.2.1", 43, "query")
         assert response is None
 
-        assert flatten_mock_calls(mock_socket) == [
-            ['sendall', (b'query\n',), {}],
-            ['close', (), {}]
-        ]
+        assert flatten_mock_calls(mock_socket) == [["sendall", (b"query\n",), {}], ["close", (), {}]]
         assert self.recv_calls == 1
 
     def test_query_valid_empty_d_response(self, monkeypatch):
         self.recv_calls = 0
         mock_socket = Mock()
-        monkeypatch.setattr('irrd.utils.whois_client.socket.create_connection', lambda address, timeout: mock_socket)
+        monkeypatch.setattr(
+            "irrd.utils.whois_client.socket.create_connection", lambda address, timeout: mock_socket
+        )
 
         def mock_socket_recv(bytes) -> bytes:
             self.recv_calls += 1
-            return b'D\n'
+            return b"D\n"
 
         mock_socket.recv = mock_socket_recv
-        response = whois_query_irrd('192.0.2.1', 43, 'query')
+        response = whois_query_irrd("192.0.2.1", 43, "query")
         assert response is None
 
-        assert flatten_mock_calls(mock_socket) == [
-            ['sendall', (b'query\n',), {}],
-            ['close', (), {}]
-        ]
+        assert flatten_mock_calls(mock_socket) == [["sendall", (b"query\n",), {}], ["close", (), {}]]
         assert self.recv_calls == 1
 
     def test_query_invalid_f_response(self, monkeypatch):
         self.recv_calls = 0
         mock_socket = Mock()
-        monkeypatch.setattr('irrd.utils.whois_client.socket.create_connection', lambda address, timeout: mock_socket)
+        monkeypatch.setattr(
+            "irrd.utils.whois_client.socket.create_connection", lambda address, timeout: mock_socket
+        )
 
         def mock_socket_recv(bytes) -> bytes:
             self.recv_calls += 1
-            return b'F unrecognized command\n'
+            return b"F unrecognized command\n"
 
         mock_socket.recv = mock_socket_recv
         with pytest.raises(WhoisQueryError) as wqe:
-            whois_query_irrd('192.0.2.1', 43, 'query')
-        assert 'unrecognized command' in str(wqe.value)
+            whois_query_irrd("192.0.2.1", 43, "query")
+        assert "unrecognized command" in str(wqe.value)
 
-        assert flatten_mock_calls(mock_socket) == [
-            ['sendall', (b'query\n',), {}],
-            ['close', (), {}]
-        ]
+        assert flatten_mock_calls(mock_socket) == [["sendall", (b"query\n",), {}], ["close", (), {}]]
         assert self.recv_calls == 1
 
     def test_no_valid_start(self, monkeypatch):
         self.recv_calls = 0
         mock_socket = Mock()
-        monkeypatch.setattr('irrd.utils.whois_client.socket.create_connection', lambda address, timeout: mock_socket)
+        monkeypatch.setattr(
+            "irrd.utils.whois_client.socket.create_connection", lambda address, timeout: mock_socket
+        )
 
         def mock_socket_recv(bytes) -> bytes:
             self.recv_calls += 1
             if self.recv_calls > 2:
-                return b''
-            return str(self.recv_calls).encode('utf-8')
+                return b""
+            return str(self.recv_calls).encode("utf-8")
 
         mock_socket.recv = mock_socket_recv
         with pytest.raises(ValueError) as ve:
-            whois_query_irrd('192.0.2.1', 43, 'query')
-        assert 'without a valid IRRD-format response' in str(ve.value)
+            whois_query_irrd("192.0.2.1", 43, "query")
+        assert "without a valid IRRD-format response" in str(ve.value)
 
-        assert flatten_mock_calls(mock_socket) == [
-            ['sendall', (b'query\n',), {}],
-            ['close', (), {}]
-        ]
+        assert flatten_mock_calls(mock_socket) == [["sendall", (b"query\n",), {}], ["close", (), {}]]
         assert self.recv_calls == 3
 
     def test_no_more_data(self, monkeypatch):
         self.recv_calls = 0
         mock_socket = Mock()
-        monkeypatch.setattr('irrd.utils.whois_client.socket.create_connection', lambda address, timeout: mock_socket)
+        monkeypatch.setattr(
+            "irrd.utils.whois_client.socket.create_connection", lambda address, timeout: mock_socket
+        )
 
         def mock_socket_recv(bytes) -> bytes:
             self.recv_calls += 1
             if self.recv_calls > 2:
-                return b''
+                return b""
             if self.recv_calls == 1:
-                return 'A2\n'.encode('utf-8')
-            return str(self.recv_calls).encode('utf-8')
+                return b"A2\n"
+            return str(self.recv_calls).encode("utf-8")
 
         mock_socket.recv = mock_socket_recv
         with pytest.raises(ValueError) as ve:
-            whois_query_irrd('192.0.2.1', 43, 'query')
-        assert 'Unable to receive ' in str(ve.value)
+            whois_query_irrd("192.0.2.1", 43, "query")
+        assert "Unable to receive " in str(ve.value)
 
-        assert flatten_mock_calls(mock_socket) == [
-            ['sendall', (b'query\n',), {}],
-            ['close', (), {}]
-        ]
+        assert flatten_mock_calls(mock_socket) == [["sendall", (b"query\n",), {}], ["close", (), {}]]
         assert self.recv_calls == 3
 
     def test_query_timeout(self, monkeypatch):
         self.recv_calls = 0
         mock_socket = Mock()
-        monkeypatch.setattr('irrd.utils.whois_client.socket.create_connection', lambda address, timeout: mock_socket)
+        monkeypatch.setattr(
+            "irrd.utils.whois_client.socket.create_connection", lambda address, timeout: mock_socket
+        )
 
         def mock_socket_recv(bytes) -> bytes:
             self.recv_calls += 1
             if self.recv_calls > 2:
                 raise socket.timeout
             if self.recv_calls == 1:
-                return 'A2\n'.encode('utf-8')
-            return str(self.recv_calls).encode('utf-8')
+                return b"A2\n"
+            return str(self.recv_calls).encode("utf-8")
 
         mock_socket.recv = mock_socket_recv
         with pytest.raises(ValueError) as ve:
-            whois_query_irrd('192.0.2.1', 43, 'query')
-        assert 'Unable to receive ' in str(ve.value)
+            whois_query_irrd("192.0.2.1", 43, "query")
+        assert "Unable to receive " in str(ve.value)
 
-        assert flatten_mock_calls(mock_socket) == [
-            ['sendall', (b'query\n',), {}],
-            ['close', (), {}]
-        ]
+        assert flatten_mock_calls(mock_socket) == [["sendall", (b"query\n",), {}], ["close", (), {}]]
         assert self.recv_calls == 3
 
 
 class TestQuerySourceStatus:
-
     def test_query_valid_with_export(self, monkeypatch):
         def mock_whois_query_irrd(host: str, port: int, query: str) -> Optional[str]:
-            assert host == 'host'
+            assert host == "host"
             assert port == 43
-            assert query == '!jTEST'
-            return 'TEST:Y:1-2:1'
+            assert query == "!jTEST"
+            return "TEST:Y:1-2:1"
 
-        monkeypatch.setattr('irrd.utils.whois_client.whois_query_irrd', mock_whois_query_irrd)
+        monkeypatch.setattr("irrd.utils.whois_client.whois_query_irrd", mock_whois_query_irrd)
 
-        mirrorable, serial_oldest, serial_newest, export_serial = whois_query_source_status('host', 43, 'TEST')
+        mirrorable, serial_oldest, serial_newest, export_serial = whois_query_source_status(
+            "host", 43, "TEST"
+        )
         assert mirrorable is True
         assert serial_oldest == 1
         assert serial_newest == 2
         assert export_serial == 1
 
     def test_query_valid_without_export(self, monkeypatch):
         def mock_whois_query_irrd(host: str, port: int, query: str) -> Optional[str]:
-            assert host == 'host'
+            assert host == "host"
             assert port == 43
-            assert query == '!jTEST'
-            return 'TEST:X:1-2'
+            assert query == "!jTEST"
+            return "TEST:X:1-2"
 
-        monkeypatch.setattr('irrd.utils.whois_client.whois_query_irrd', mock_whois_query_irrd)
+        monkeypatch.setattr("irrd.utils.whois_client.whois_query_irrd", mock_whois_query_irrd)
 
-        mirrorable, serial_oldest, serial_newest, export_serial = whois_query_source_status('host', 43, 'TEST')
+        mirrorable, serial_oldest, serial_newest, export_serial = whois_query_source_status(
+            "host", 43, "TEST"
+        )
         assert mirrorable is None
         assert serial_oldest == 1
         assert serial_newest == 2
         assert export_serial is None
 
     def test_query_invalid_source(self, monkeypatch):
         def mock_whois_query_irrd(host: str, port: int, query: str) -> Optional[str]:
-            assert host == 'host'
+            assert host == "host"
             assert port == 43
-            assert query == '!jTEST'
-            return 'NOT-TEST:Y:1-2:1'
+            assert query == "!jTEST"
+            return "NOT-TEST:Y:1-2:1"
 
-        monkeypatch.setattr('irrd.utils.whois_client.whois_query_irrd', mock_whois_query_irrd)
+        monkeypatch.setattr("irrd.utils.whois_client.whois_query_irrd", mock_whois_query_irrd)
 
         with pytest.raises(ValueError) as ve:
-            whois_query_source_status('host', 43, 'TEST')
-        assert 'Received invalid source NOT-TEST' in str(ve.value)
+            whois_query_source_status("host", 43, "TEST")
+        assert "Received invalid source NOT-TEST" in str(ve.value)
 
     def test_query_empty_response(self, monkeypatch):
         def mock_whois_query_irrd(host: str, port: int, query: str) -> Optional[str]:
-            assert host == 'host'
+            assert host == "host"
             assert port == 43
-            assert query == '!jTEST'
+            assert query == "!jTEST"
             return None
 
-        monkeypatch.setattr('irrd.utils.whois_client.whois_query_irrd', mock_whois_query_irrd)
+        monkeypatch.setattr("irrd.utils.whois_client.whois_query_irrd", mock_whois_query_irrd)
 
         with pytest.raises(ValueError) as ve:
-            whois_query_source_status('host', 43, 'TEST')
-        assert 'empty response' in str(ve.value)
+            whois_query_source_status("host", 43, "TEST")
+        assert "empty response" in str(ve.value)
```

### Comparing `irrd-4.2.8/irrd/utils/text.py` & `irrd-4.3.0/irrd/utils/text.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,98 +1,99 @@
 import re
-from typing import Iterator, Union, TextIO, Optional, List, Set
+from typing import Iterator, List, Optional, Set, TextIO, Union
 
 from irrd.conf import PASSWORD_HASH_DUMMY_VALUE
-from irrd.rpsl.config import PASSWORD_HASHERS
+from irrd.rpsl.passwords import PASSWORD_HASHERS_ALL
 
-re_remove_passwords = re.compile(r'(%s)[^\n]+' % '|'.join(PASSWORD_HASHERS.keys()), flags=re.IGNORECASE)
-re_remove_last_modified = re.compile(r'^last-modified: [^\n]+\n', flags=re.MULTILINE)
+re_remove_passwords = re.compile(r"(%s)[^\n]+" % "|".join(PASSWORD_HASHERS_ALL.keys()), flags=re.IGNORECASE)
+re_remove_last_modified = re.compile(r"^last-modified: [^\n]+\n", flags=re.MULTILINE)
 
 
 def remove_auth_hashes(input: Optional[str]):
     if not input:
         return input
     if not isinstance(input, str):
-        raise ValueError('Auth hash removal only supported for strings')
+        raise ValueError("Auth hash removal only supported for strings")
     # If there are no hashes, skip the RE for performance.
     input_lower = input.lower()
-    if not any([pw_hash.lower() in input_lower for pw_hash in PASSWORD_HASHERS.keys()]):
+    if not any([pw_hash.lower() in input_lower for pw_hash in PASSWORD_HASHERS_ALL.keys()]):
         return input
-    return re_remove_passwords.sub(r'\1 %s  # Filtered for security' % PASSWORD_HASH_DUMMY_VALUE, input)
+    return re_remove_passwords.sub(r"\1 %s  # Filtered for security" % PASSWORD_HASH_DUMMY_VALUE, input)
 
 
 def remove_last_modified(rpsl_text: str):
     """
     Remove all last-modified attributes from an RPSL text with less overhead
     than using the full RPSL parser.
     Assumes the last-modified value is single line. This is a safe assumption
     when the input is guaranteed to have been generated by IRRd.
     """
-    return re_remove_last_modified.sub('', rpsl_text)
+    return re_remove_last_modified.sub("", rpsl_text)
 
 
 def splitline_unicodesafe(input: str) -> Iterator[str]:
     """
     Split an input string by newlines, and return an iterator of the lines.
 
     This is a replacement for Python's built-in splitlines, which also splits
     on characters such as unicode line separator (U+2028). In RPSL, that should
     not be considered a line separator.
     """
     if not input:
         return
-    for line in input.strip('\n').split('\n'):
-        yield line.strip('\r')
+    for line in input.strip("\n").split("\n"):
+        yield line.strip("\r")
 
 
 def split_paragraphs_rpsl(input: Union[str, TextIO], strip_comments=True) -> Iterator[str]:
     """
     Split an input into paragraphs, and return an iterator of the paragraphs.
 
     A paragraph is a block of text, separated by at least one empty line.
     Note that a line with other whitespace, e.g. a space, is not considered
     empty.
 
     If strip_comments=True, any line starting with % or # is entirely ignored,
     both within a paragraph and between paragraphs.
     """
-    current_paragraph = ''
+    current_paragraph = ""
     if isinstance(input, str):
         generator = splitline_unicodesafe(input)
     else:
         generator = input
 
     for line in generator:
-        line = line.strip('\r\n')
-        if strip_comments and line.startswith('%') or line.startswith('#'):
+        line = line.strip("\r\n")
+        if strip_comments and line.startswith("%") or line.startswith("#"):
             continue
         if line:
-            current_paragraph += line + '\n'
+            current_paragraph += line + "\n"
         if not line:
             if current_paragraph:
                 yield current_paragraph
-            current_paragraph = ''
+            current_paragraph = ""
 
     if current_paragraph.strip():
         yield current_paragraph
 
 
 def snake_to_camel_case(snake: Union[Set[str], List[str], str]):
     """
     Convert a snake case string to camel case, with lowercase first
     letter. Can also accept a list or set of strings.
     """
+
     def _str_to_camel_case(snake_str: str):
-        components = snake_str.replace('_', '-').split('-')
-        return components[0] + ''.join(x.title() for x in components[1:])
+        components = snake_str.replace("_", "-").split("-")
+        return components[0] + "".join(x.title() for x in components[1:])
 
     if isinstance(snake, (set, list)):
         return [_str_to_camel_case(s) for s in snake]
     return _str_to_camel_case(snake)
 
 
 # Turn "IP('193.0.1.1/21') has invalid prefix length (21)" into "invalid prefix length (21)"
 re_clean_ip_error = re.compile(r"IP\('[A-F0-9:./]+'\) has ", re.IGNORECASE)
 
 
 def clean_ip_value_error(value_error):
-    return re.sub(re_clean_ip_error, '', str(value_error))
+    return re.sub(re_clean_ip_error, "", str(value_error))
```

### Comparing `irrd-4.2.8/irrd/utils/validators.py` & `irrd-4.3.0/irrd/utils/validators.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,66 +1,88 @@
-from typing import Tuple, List, Union, Optional
+from typing import List, Optional, Tuple, Union
 
 import pydantic
 
+from irrd.updates.parser_state import SuspensionRequestType
+
 
 def parse_as_number(value: Union[str, int], permit_plain=False) -> Tuple[str, int]:
     """Validate and clean an AS number. Returns it in ASxxxx and numeric format."""
     if isinstance(value, str):
         value = value.upper()
-        if not permit_plain and not value.startswith('AS'):
+        if not permit_plain and not value.startswith("AS"):
             raise ValidationError(f'Invalid AS number {value}: must start with "AS"')
 
-        start_index = 2 if value.startswith('AS') else 0
+        start_index = 2 if value.startswith("AS") else 0
 
         if not value[start_index:].isnumeric():
-            raise ValidationError(f'Invalid AS number {value}: number part is not numeric')
+            raise ValidationError(f"Invalid AS number {value}: number part is not numeric")
 
         value_int = int(value[start_index:])
     else:
         value_int = value
 
     if not 0 <= value_int <= 4294967295:
-        raise ValidationError(f'Invalid AS number {value}: valid range is 0-4294967295')
+        raise ValidationError(f"Invalid AS number {value}: valid range is 0-4294967295")
 
-    return 'AS' + str(value_int), value_int
+    return "AS" + str(value_int), value_int
 
 
 class ValidationError(ValueError):
     pass
 
 
 class RPSLChangeSubmissionObjectAttribute(pydantic.main.BaseModel):
     """
     Model for a single name/value pair of an RPSL attribute
     in an object in an RPSL change submission
     """
+
     name: str
     value: Union[str, List[str]]
 
-    @pydantic.validator('value')
+    @pydantic.validator("value")
     def translate_list_to_str(cls, value):  # noqa: N805
         """Translate lists to RPSL-compatible strings"""
         if not isinstance(value, str):
-            return ', '.join(value)
+            return ", ".join(value)
         return value
 
 
 class RPSLChangeSubmissionObject(pydantic.main.BaseModel):
     """Model for a single object in an RPSL change submission"""
+
     object_text: Optional[str]
     attributes: Optional[List[RPSLChangeSubmissionObjectAttribute]]
 
     @pydantic.root_validator(pre=True)
     def check_text_xor_attributes_present(cls, values):  # noqa: N805
-        if bool(values.get('object_text')) == bool(values.get('attributes')):
-            raise ValueError('You must describe each object with either '
-                             '"object_text" or "attributes"')
+        if bool(values.get("object_text")) == bool(values.get("attributes")):
+            raise ValueError('You must describe each object with either "object_text" or "attributes"')
         return values
 
 
 class RPSLChangeSubmission(pydantic.main.BaseModel):
     """Model for an RPSL change submission"""
+
     objects: List[RPSLChangeSubmissionObject]
     passwords: List[str] = []
     override: Optional[str]
-    delete_reason: str = '(No reason provided)'
+    delete_reason: str = "(No reason provided)"
+
+
+class RPSLSuspensionSubmissionObject(pydantic.main.BaseModel):
+    """
+    Model for a single key/source pair for a suspension/
+    reactivation request
+    """
+
+    mntner: str
+    source: str
+    request_type: SuspensionRequestType
+
+
+class RPSLSuspensionSubmission(pydantic.main.BaseModel):
+    """Model for an RPSL suspension submission"""
+
+    objects: List[RPSLSuspensionSubmissionObject]
+    override: Optional[str]
```

### Comparing `irrd-4.2.8/irrd/utils/whois_client.py` & `irrd-4.3.0/irrd/utils/whois_client.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,131 +1,139 @@
 import logging
 import socket
-from typing import List, Tuple, Optional
+from typing import List, Optional, Tuple
 
 logger = logging.getLogger(__name__)
 
 
 class WhoisQueryError(ValueError):
     pass
 
 
-def whois_query(host: str, port: int, query: str, end_markings: List[str]=None) -> str:
+def whois_query(host: str, port: int, query: str, end_markings: Optional[List[str]] = None) -> str:
     """
     Perform a query on a whois server, connecting to the specified host and port.
 
     Will continue to read until no more data can be read, until no more data has
     been sent for 5 seconds, or until an optional end_marking is encountered.
     The end marking could be e.g. 'END NTTCOM' in case of an NRTM stream.
     """
-    query = query.strip() + '\n'
-    logger.debug(f'Running whois query {query.strip()} on {host} port {port}')
+    query = query.strip() + "\n"
+    logger.debug(f"Running whois query {query.strip()} on {host} port {port}")
     if end_markings:
-        end_markings_bytes = [mark.encode('utf-8') for mark in end_markings]
+        end_markings_bytes = [mark.encode("utf-8") for mark in end_markings]
     else:
         end_markings_bytes = []
 
     s = socket.create_connection((host, port), timeout=5)
-    s.sendall(query.encode('utf-8'))
+    s.sendall(query.encode("utf-8"))
 
-    buffer = b''
+    buffer = b""
     while not any([end_marking in buffer for end_marking in end_markings_bytes]):
         try:
-            data = s.recv(1024*1024)
+            data = s.recv(1024 * 1024)
         except socket.timeout:
             break
         if not data:
             break
         buffer += data
     s.close()
 
-    return buffer.decode('utf-8', errors='backslashreplace')
+    return buffer.decode("utf-8", errors="backslashreplace")
 
 
 def whois_query_irrd(host: str, port: int, query: str) -> Optional[str]:
     """
     Perform a whois query, expecting an IRRD-style output format.
 
     This is a variant of whois_query(), as it uses the additional metadata
     provided by the IRRD output format to know when the full response has
     been received, and whether the full response has been received.
     """
-    query = query.strip() + '\n'
-    logger.debug(f'Running IRRD whois query {query.strip()} on {host} port {port}')
+    query = query.strip() + "\n"
+    logger.debug(f"Running IRRD whois query {query.strip()} on {host} port {port}")
 
     s = socket.create_connection((host, port), timeout=5)
-    s.sendall(query.encode('utf-8'))
+    s.sendall(query.encode("utf-8"))
 
-    buffer = b''
+    buffer = b""
     expected_data_length = None
     data_offset = None
     error = None
 
     while True:
         try:
             data = s.recv(1024)
         except (socket.timeout, ConnectionError):
             break
         if not data:
             break
         buffer += data
         if not expected_data_length:
-            decoded_buffer = buffer.decode('utf-8', errors='backslashreplace')
-            if '\n' in decoded_buffer:
+            decoded_buffer = buffer.decode("utf-8", errors="backslashreplace")
+            if "\n" in decoded_buffer:
                 length_line = decoded_buffer.splitlines()[0]
-                if length_line.startswith('A'):
+                if length_line.startswith("A"):
                     expected_data_length = int(length_line[1:])
                     data_offset = len(length_line) + 1
-                elif length_line in ['C', 'D']:
+                elif length_line in ["C", "D"]:
                     break
-                elif length_line.startswith('F'):
+                elif length_line.startswith("F"):
                     error = length_line[2:]
                     break
         if expected_data_length and data_offset and len(buffer) > (expected_data_length + data_offset):
             break
     s.close()
 
     if error:
         raise WhoisQueryError(error)
-    if not expected_data_length and buffer in [b'C\n', b'D\n']:
+    if not expected_data_length and buffer in [b"C\n", b"D\n"]:
         return None
     if not expected_data_length or not data_offset:
-        raise ValueError(f'Data receiving ended without a valid IRRD-format response, query {query},'
-                         f'received: {buffer.decode("ascii", "ignore")}')
+        raise ValueError(
+            f"Data receiving ended without a valid IRRD-format response, query {query},"
+            f"received: {buffer.decode('ascii', 'ignore')}"
+        )
     if len(buffer) < (expected_data_length + data_offset):
-        raise ValueError(f'Unable to receive all expected {expected_data_length} bytes')
+        raise ValueError(f"Unable to receive all expected {expected_data_length} bytes")
 
-    return buffer[data_offset:expected_data_length+data_offset-1].decode('utf-8', errors='backslashreplace')
+    return buffer[data_offset : expected_data_length + data_offset - 1].decode(
+        "utf-8", errors="backslashreplace"
+    )
 
 
-def whois_query_source_status(host: str, port: int, source: str) -> Tuple[Optional[bool], int, int, Optional[int]]:
+def whois_query_source_status(
+    host: str, port: int, source: str
+) -> Tuple[Optional[bool], int, int, Optional[int]]:
     """
     Query the status of a particular source against an NRTM server,
     which supports IRRD-style !j queries.
 
     Will return a tuple with:
     - is this server mirrorable
     - the oldest serial available
     - the newest serial available
     - the serial of the latest export
     """
-    remote_status = whois_query_irrd(host, port, f'!j{source}')
+    remote_status = whois_query_irrd(host, port, f"!j{source}")
 
     # Fields are: source, mirrorable, serials in journal, optional last export serial
     if not remote_status:
-        raise ValueError(f'Source status query on {host}:{port} failed: empty response')
-    fields = remote_status.split(':')
+        raise ValueError(f"Source status query on {host}:{port} failed: empty response")
+    fields = remote_status.split(":")
     match_source = fields[0].upper()
     if match_source != source.upper():
-        raise ValueError(f'Received invalid source {match_source}, expecting {source}, in status: {remote_status}')
+        raise ValueError(
+            f"Received invalid source {match_source}, expecting {source}, in status: {remote_status}"
+        )
 
-    mirrorable_choices = {'Y': True, 'N': False}
+    mirrorable_choices = {"Y": True, "N": False}
     mirrorable = mirrorable_choices.get(fields[1].upper())
 
-    serial_oldest, serial_newest = fields[2].split('-')
+    serial_oldest, serial_newest = fields[2].split("-")
     export_serial: Optional[int]
     try:
         export_serial = int(fields[3])
     except IndexError:
         export_serial = None
 
     return mirrorable, int(serial_oldest), int(serial_newest), export_serial
```

### Comparing `irrd-4.2.8/irrd/vendor/dotted/collection.py` & `irrd-4.3.0/irrd/vendor/dotted/collection.py`

 * *Files 5% similar despite different names*

```diff
@@ -23,33 +23,29 @@
 # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 # SOFTWARE.
 
 import collections
 import json
 import re
-
 from abc import ABCMeta, abstractmethod
 
-from six import add_metaclass, string_types as basestring, iteritems
-
-
 SPLIT_REGEX = r"(?<!\\)(\.)"
 
 
 def is_dotted_key(key):
     """Returns True if the key has any not-escaped dot inside"""
     return len(re.findall(SPLIT_REGEX, key)) > 0
 
 
 def split_key(key, max_keys=0):
     """Splits a key but allows dots in the key name if they're scaped properly.
 
     Args:
-        key (basestring): The key to be splitted.
+        key (str): The key to be splitted.
         max_keys (int): The maximum number of keys to be extracted. 0 means no
             limits.
 
     Returns:
         A list of keys
     """
     parts = [x for x in re.split(SPLIT_REGEX, key) if x != "."]
@@ -60,16 +56,15 @@
         result.append(parts.pop(0))
 
     if len(parts) > 0:
         result.append(".".join(parts))
     return result
 
 
-@add_metaclass(ABCMeta)
-class DottedCollection(object):
+class DottedCollection(metaclass=ABCMeta):
     """Abstract Base Class for DottedDict and DottedDict"""
 
     @classmethod
     def factory(cls, initial=None):
         """Returns a DottedDict or a DottedList based on the type of the
         initial value, that must be a dict or a list. In other case the same
         original value will be returned.
@@ -89,55 +84,54 @@
     @classmethod
     def _factory_by_index(cls, dotted_key):
         """Returns the proper DottedCollection that best suits the next key in
         the dotted_key string. First guesses the next key and then analyzes it.
         If the next key is numeric then returns a DottedList. In other case a
         DottedDict is returned.
         """
-        if not isinstance(dotted_key, basestring):
+        if not isinstance(dotted_key, str):
             next_key = str(dotted_key)
         elif not is_dotted_key(dotted_key):
             next_key = dotted_key
         else:
             next_key, tmp = split_key(dotted_key, 1)
 
         return DottedCollection.factory([] if next_key.isdigit() else {})
 
     def __init__(self, initial):
         """Base constructor. If there are nested dicts or lists they are
         transformed into DottedCollection instances.
         """
         if not isinstance(initial, list) and not isinstance(initial, dict):
-            raise ValueError('initial value must be a list or a dict')
+            raise ValueError("initial value must be a list or a dict")
 
         self._validate_initial(initial)
 
         self.store = initial
 
         if isinstance(self.store, list):
             data = enumerate(self.store)
         else:
-            data = iteritems(self.store)
+            data = self.store.items()
 
         for key, value in data:
             try:
                 self.store[key] = DottedCollection.factory(value)
             except ValueError:
                 pass
 
     def _validate_initial(self, initial):
         """Validates data so no unescaped dotted key is present."""
         if isinstance(initial, list):
             for item in initial:
                 self._validate_initial(item)
         elif isinstance(initial, dict):
-            for key, item in iteritems(initial):
+            for key, item in initial.items():
                 if is_dotted_key(key):
-                    raise ValueError("{0} is not a valid key inside a "
-                                     "DottedCollection!".format(key))
+                    raise ValueError("{} is not a valid key inside a DottedCollection!".format(key))
                 self._validate_initial(item)
 
     def __len__(self):
         return len(self.store)
 
     def __iter__(self):
         return iter(self.store)
@@ -165,93 +159,79 @@
         pass
 
 
 class DottedList(DottedCollection, collections.abc.MutableSequence):
     """A list with support for the dotted path syntax"""
 
     def __init__(self, initial=None):
-        DottedCollection.__init__(
-            self,
-            [] if initial is None else list(initial)
-        )
+        DottedCollection.__init__(self, [] if initial is None else list(initial))
 
     def __getitem__(self, index):
         if isinstance(index, slice):
             return self.store[index]
 
-        if isinstance(index, int) \
-                or (isinstance(index, basestring) and index.isdigit()):
+        if isinstance(index, int) or (isinstance(index, str) and index.isdigit()):
             return self.store[int(index)]
 
-        elif isinstance(index, basestring) and is_dotted_key(index):
+        elif isinstance(index, str) and is_dotted_key(index):
             my_index, alt_index = split_key(index, 1)
             target = self.store[int(my_index)]
 
             # required by the dotted path
             if not isinstance(target, DottedCollection):
-                raise IndexError('cannot get "{0}" in "{1}" ({2})'.format(
-                    alt_index,
-                    my_index,
-                    repr(target)
-                ))
+                raise IndexError('cannot get "{}" in "{}" ({})'.format(alt_index, my_index, repr(target)))
 
             return target[alt_index]
 
         else:
-            raise IndexError('cannot get %s in %s' % (index, repr(self.store)))
+            raise IndexError(f"cannot get {index} in {repr(self.store)}")
 
     def __setitem__(self, index, value):
-        if isinstance(index, int) \
-                or (isinstance(index, basestring) and index.isdigit()):
+        if isinstance(index, int) or (isinstance(index, str) and index.isdigit()):
             # If the index does not exist in the list but it's the same index
             # we would obtain by appending the value to the list we actually
             # append the value. (***)
             if int(index) not in self.store and int(index) == len(self.store):
                 self.store.append(DottedCollection.factory(value))
             else:
                 self.store[int(index)] = DottedCollection.factory(value)
 
-        elif isinstance(index, basestring) and is_dotted_key(index):
+        elif isinstance(index, str) and is_dotted_key(index):
             my_index, alt_index = split_key(index, 1)
 
             # (***)
-            if int(my_index) not in self.store \
-                    and int(my_index) == len(self.store):
-                self.store.append(
-                    DottedCollection._factory_by_index(alt_index))
+            if int(my_index) not in self.store and int(my_index) == len(self.store):
+                self.store.append(DottedCollection._factory_by_index(alt_index))
 
             if not isinstance(self[int(my_index)], DottedCollection):
-                raise IndexError('cannot set "%s" in "%s" (%s)' % (
-                    alt_index, my_index, repr(self[int(my_index)])))
+                raise IndexError(
+                    'cannot set "{}" in "{}" ({})'.format(alt_index, my_index, repr(self[int(my_index)]))
+                )
 
             self[int(my_index)][alt_index] = DottedCollection.factory(value)
 
         else:
-            raise IndexError('cannot use %s as index in %s' % (
-                index, repr(self.store)))
+            raise IndexError("cannot use {} as index in {}".format(index, repr(self.store)))
 
     def __delitem__(self, index):
-        if isinstance(index, int) \
-                or (isinstance(index, basestring) and index.isdigit()):
+        if isinstance(index, int) or (isinstance(index, str) and index.isdigit()):
             del self.store[int(index)]
 
-        elif isinstance(index, basestring) and is_dotted_key(index):
+        elif isinstance(index, str) and is_dotted_key(index):
             my_index, alt_index = split_key(index, 1)
             target = self.store[int(my_index)]
 
             # required by the dotted path
             if not isinstance(target, DottedCollection):
-                raise IndexError('cannot delete "%s" in "%s" (%s)' % (
-                    alt_index, my_index, repr(target)))
+                raise IndexError('cannot delete "{}" in "{}" ({})'.format(alt_index, my_index, repr(target)))
 
             del target[alt_index]
 
         else:
-            raise IndexError('cannot delete %s in %s' % (
-                index, repr(self.store)))
+            raise IndexError("cannot delete {} in {}".format(index, repr(self.store)))
 
     def to_python(self):
         """Returns a plain python list and converts to plain python objects all
         this object's descendants.
         """
         result = list(self)
 
@@ -263,105 +243,95 @@
 
     def insert(self, index, value):
         self.store.insert(index, value)
 
 
 class DottedDict(DottedCollection, collections.abc.MutableMapping):
     """A dict with support for the dotted path syntax"""
+
     def __init__(self, initial=None):
-        DottedCollection.__init__(
-            self,
-            {} if initial is None else dict(initial)
-        )
+        DottedCollection.__init__(self, {} if initial is None else dict(initial))
 
     def __getitem__(self, k):
         key = self.__keytransform__(k)
 
-        if not isinstance(k, basestring) or not is_dotted_key(key):
+        if not isinstance(k, str) or not is_dotted_key(key):
             return self.store[key]
 
         my_key, alt_key = split_key(key, 1)
         target = self.store[my_key]
 
         # required by the dotted path
         if not isinstance(target, DottedCollection):
-            raise KeyError('cannot get "{0}" in "{1}" ({2})'.format(
-                alt_key,
-                my_key,
-                repr(target)
-            ))
+            raise KeyError('cannot get "{}" in "{}" ({})'.format(alt_key, my_key, repr(target)))
 
         return target[alt_key]
 
     def __setitem__(self, k, value):
         key = self.__keytransform__(k)
 
-        if not isinstance(k, basestring):
-            raise KeyError('DottedDict keys must be str or unicode')
+        if not isinstance(k, str):
+            raise KeyError("DottedDict keys must be str or unicode")
         elif not is_dotted_key(key):
             self.store[key] = DottedCollection.factory(value)
         else:
             my_key, alt_key = split_key(key, 1)
 
             if my_key not in self.store:
                 self.store[my_key] = DottedCollection._factory_by_index(alt_key)
 
             self.store[my_key][alt_key] = value
 
     def __delitem__(self, k):
         key = self.__keytransform__(k)
 
-        if not isinstance(k, basestring) or not is_dotted_key(key):
+        if not isinstance(k, str) or not is_dotted_key(key):
             del self.store[key]
 
         else:
             my_key, alt_key = split_key(key, 1)
             target = self.store[my_key]
 
             if not isinstance(target, DottedCollection):
-                raise KeyError('cannot delete "{0}" in "{1}" ({2})'.format(
-                    alt_key,
-                    my_key,
-                    repr(target)
-                ))
+                raise KeyError('cannot delete "{}" in "{}" ({})'.format(alt_key, my_key, repr(target)))
 
             del target[alt_key]
 
     def to_python(self):
         """Returns a plain python dict and converts to plain python objects all
         this object's descendants.
         """
         result = dict(self)
 
-        for key, value in iteritems(result):
+        for key, value in result.items():
             if isinstance(value, DottedCollection):
                 result[key] = value.to_python()
 
         return result
 
     __getattr__ = __getitem__
 
     # self.store does not exist before __init__() initializes it
 
     def __setattr__(self, key, value):
-        if key in self.__dict__ or key == 'store':
+        if key in self.__dict__ or key == "store":
             object.__setattr__(self, key, value)
         else:
             self.__setitem__(key, value)
 
     def __delattr__(self, key):
-        if key in self.__dict__ or key == 'store':
+        if key in self.__dict__ or key == "store":
             object.__delattr__(self, key)
         else:
             self.__delitem__(key)
 
     def __contains__(self, k):
         key = self.__keytransform__(k)
 
-        if not isinstance(k, basestring) or not is_dotted_key(key):
+        if not isinstance(k, str) or not is_dotted_key(key):
             return self.store.__contains__(key)
 
         my_key, alt_key = split_key(key, 1)
         target = self.store[my_key]
 
         if not isinstance(target, DottedCollection):
             return False
```

### Comparing `irrd-4.2.8/irrd/vendor/postgres_copy/__init__.py` & `irrd-4.3.0/irrd/vendor/postgres_copy/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -18,20 +18,20 @@
 # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 # THE SOFTWARE.
 
+from sqlalchemy.dialects import postgresql
 from sqlalchemy.exc import SQLAlchemyError
 from sqlalchemy.orm import Mapper, class_mapper
 from sqlalchemy.sql.operators import ColumnOperators
-from sqlalchemy.dialects import postgresql
 
-__version__ = '0.5.0'
+__version__ = "0.5.0"
 
 
 def copy_to(source, dest, engine_or_conn, **flags):
     """Export a query or select to a file. For flags, see the PostgreSQL
     documentation at http://www.postgresql.org/docs/9.5/static/sql-copy.html.
 
     Examples: ::
@@ -48,21 +48,21 @@
     :param engine_or_conn: SQLAlchemy engine, connection, or raw_connection
     :param **flags: Options passed through to COPY
 
     If an existing connection is passed to `engine_or_conn`, it is the caller's
     responsibility to commit and close.
     """
     dialect = postgresql.dialect()
-    statement = getattr(source, 'statement', source)
+    statement = getattr(source, "statement", source)
     compiled = statement.compile(dialect=dialect)
     conn, autoclose = raw_connection_from(engine_or_conn)
     cursor = conn.cursor()
     query = cursor.mogrify(compiled.string, compiled.params).decode()
-    formatted_flags = '({})'.format(format_flags(flags)) if flags else ''
-    copy = 'COPY ({}) TO STDOUT {}'.format(query, formatted_flags)
+    formatted_flags = f"({format_flags(flags)})" if flags else ""
+    copy = f"COPY ({query}) TO STDOUT {formatted_flags}"
     cursor.copy_expert(copy, dest)
     if autoclose:
         conn.close()
 
 
 def copy_from(source, dest, engine_or_conn, columns=(), **flags):
     """Import a table from a file. For flags, see the PostgreSQL documentation
@@ -87,18 +87,18 @@
     The `columns` flag can be set to a tuple of strings to specify the column
     order. Passing `header` alone will not handle out of order columns, it simply tells
     postgres to ignore the first line of `source`.
     """
     tbl = dest.__table__ if is_model(dest) else dest
     conn, autoclose = raw_connection_from(engine_or_conn)
     cursor = conn.cursor()
-    relation = '.'.join('"{}"'.format(part) for part in (tbl.schema, tbl.name) if part)
-    formatted_columns = '({})'.format(','.join(columns)) if columns else ''
-    formatted_flags = '({})'.format(format_flags(flags)) if flags else ''
-    copy = 'COPY {} {} FROM STDIN {}'.format(
+    relation = ".".join(f'"{part}"' for part in (tbl.schema, tbl.name) if part)
+    formatted_columns = "({})".format(",".join(columns)) if columns else ""
+    formatted_flags = f"({format_flags(flags)})" if flags else ""
+    copy = "COPY {} {} FROM STDIN {}".format(
         relation,
         formatted_columns,
         formatted_flags,
     )
     cursor.copy_expert(copy, source)
     if autoclose:
         conn.commit()
@@ -106,72 +106,59 @@
 
 
 def raw_connection_from(engine_or_conn):
     """Extract a raw_connection and determine if it should be automatically closed.
 
     Only connections opened by this package will be closed automatically.
     """
-    if hasattr(engine_or_conn, 'cursor'):
+    if hasattr(engine_or_conn, "cursor"):
         return engine_or_conn, False
-    if hasattr(engine_or_conn, 'connection'):
+    if hasattr(engine_or_conn, "connection"):
         return engine_or_conn.connection, False
     return engine_or_conn.raw_connection(), True
 
 
 def format_flags(flags):
-    return ', '.join(
-        '{} {}'.format(key.upper(), format_flag(value))
-        for key, value in flags.items()
-    )
+    return ", ".join(f"{key.upper()} {format_flag(value)}" for key, value in flags.items())
 
 
 def format_flag(value):
-    return (
-        str(value).upper()
-        if isinstance(value, bool)
-        else repr(value)
-    )
+    return str(value).upper() if isinstance(value, bool) else repr(value)
 
 
 def relabel_query(query):
     """Relabel query entities according to mappings defined in the SQLAlchemy
     ORM. Useful when table column names differ from corresponding attribute
     names. See http://docs.sqlalchemy.org/en/latest/orm/mapping_columns.html
     for details.
 
     :param query: SQLAlchemy query
     """
     return query.with_entities(*query_entities(query))
 
 
 def query_entities(query):
-    return sum(
-        [desc_entities(desc) for desc in query.column_descriptions],
-        []
-    )
+    return sum([desc_entities(desc) for desc in query.column_descriptions], [])
 
 
 def desc_entities(desc):
-    expr, name = desc['expr'], desc['name']
+    expr, name = desc["expr"], desc["name"]
     if isinstance(expr, Mapper):
         return mapper_entities(expr)
     elif is_model(expr):
         return mapper_entities(expr.__mapper__)
     elif isinstance(expr, ColumnOperators):
         return [expr.label(name)]
     else:
-        raise ValueError('Unrecognized query entity {!r}'.format(expr))
+        raise ValueError(f"Unrecognized query entity {expr!r}")
 
 
 def mapper_entities(mapper):
     model = mapper.class_
-    return [
-        getattr(model, prop.key).label(prop.key)
-        for prop in mapper.column_attrs
-    ]
+    return [getattr(model, prop.key).label(prop.key) for prop in mapper.column_attrs]
 
 
 def is_model(class_):
     try:
         class_mapper(class_)
         return True
     except SQLAlchemyError:
```

### Comparing `irrd-4.2.8/irrd.egg-info/PKG-INFO` & `irrd-4.3.0/irrd.egg-info/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,23 +1,23 @@
 Metadata-Version: 2.1
 Name: irrd
-Version: 4.2.8
+Version: 4.3.0
 Summary: Internet Routing Registry daemon (IRRd)
 Home-page: https://github.com/irrdnet/irrd
 Author: Reliably Coded for NTT Ltd. and others
 Author-email: irrd@reliablycoded.nl
 Classifier: License :: OSI Approved :: BSD License
 Classifier: Programming Language :: Python :: 3
-Classifier: Programming Language :: Python :: 3.6
 Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
+Classifier: Programming Language :: Python :: 3.11
 Classifier: Operating System :: OS Independent
-Requires-Python: >=3.6
+Requires-Python: >=3.7
 Description-Content-Type: text/x-rst
 License-File: LICENSE
 
 Internet Routing Registry Daemon (IRRd) Version 4
 =================================================
 
 .. image:: https://circleci.com/gh/irrdnet/irrd.svg?style=svg
```

### Comparing `irrd-4.2.8/irrd.egg-info/SOURCES.txt` & `irrd-4.3.0/irrd.egg-info/SOURCES.txt`

 * *Files 14% similar despite different names*

```diff
@@ -1,9 +1,11 @@
 .coveragerc
+.git-blame-ignore-revs
 .gitignore
+.mergify.yml
 .pyup.yml
 CONTRIBUTING.md
 LICENSE
 README.rst
 SECURITY.rst
 alembic.ini
 conftest.py
@@ -16,24 +18,28 @@
 docs/Makefile
 docs/conf.py
 docs/index.rst
 docs/license.rst
 docs/make.bat
 docs/security.rst
 docs/spelling_wordlist.txt
+docs/_static/irr_rpsl_submit.py
 docs/_static/logo.png
 docs/admins/availability-and-migration.rst
 docs/admins/configuration.rst
 docs/admins/deployment.rst
 docs/admins/faq.rst
 docs/admins/migrating-legacy-irrd.rst
+docs/admins/object-suppression.rst
 docs/admins/object-validation.rst
+docs/admins/route-object-preference.rst
 docs/admins/rpki.rst
 docs/admins/scopefilter.rst
 docs/admins/status_page.rst
+docs/admins/suspension.rst
 docs/development/architecture.rst
 docs/development/development-setup.rst
 docs/development/storage.rst
 docs/releases/4.0.1.rst
 docs/releases/4.0.2.rst
 docs/releases/4.0.3.rst
 docs/releases/4.0.4.rst
@@ -55,30 +61,33 @@
 docs/releases/4.2.2.rst
 docs/releases/4.2.3.rst
 docs/releases/4.2.4.rst
 docs/releases/4.2.5.rst
 docs/releases/4.2.6.rst
 docs/releases/4.2.7.rst
 docs/releases/4.2.8.rst
+docs/releases/4.3.0.rst
 docs/releases/index.rst
 docs/users/database-changes.rst
 docs/users/mirroring.rst
+docs/users/queries/event-stream.rst
 docs/users/queries/graphql.rst
 docs/users/queries/index.rst
 docs/users/queries/whois.rst
 irrd/__init__.py
 irrd.egg-info/PKG-INFO
 irrd.egg-info/SOURCES.txt
 irrd.egg-info/dependency_links.txt
 irrd.egg-info/entry_points.txt
 irrd.egg-info/requires.txt
 irrd.egg-info/top_level.txt
 irrd/conf/__init__.py
 irrd/conf/default_config.yaml
 irrd/conf/defaults.py
+irrd/conf/known_keys.py
 irrd/conf/test_conf.py
 irrd/daemon/__init__.py
 irrd/daemon/main.py
 irrd/integration_tests/__init__.py
 irrd/integration_tests/constants.py
 irrd/integration_tests/mailserver.tac
 irrd/integration_tests/run.py
@@ -93,51 +102,60 @@
 irrd/mirroring/tests/nrtm_samples.py
 irrd/mirroring/tests/test_mirror_runners_export.py
 irrd/mirroring/tests/test_mirror_runners_import.py
 irrd/mirroring/tests/test_nrtm_generator.py
 irrd/mirroring/tests/test_nrtm_operation.py
 irrd/mirroring/tests/test_parsers.py
 irrd/mirroring/tests/test_scheduler.py
+irrd/routepref/__init__.py
+irrd/routepref/routepref.py
+irrd/routepref/status.py
+irrd/routepref/tests/__init__.py
+irrd/routepref/tests/test_routepref.py
 irrd/rpki/__init__.py
 irrd/rpki/importer.py
 irrd/rpki/notifications.py
 irrd/rpki/status.py
 irrd/rpki/validators.py
 irrd/rpki/tests/__init__.py
 irrd/rpki/tests/test_importer.py
 irrd/rpki/tests/test_notifications.py
 irrd/rpki/tests/test_validators.py
 irrd/rpsl/__init__.py
-irrd/rpsl/config.py
 irrd/rpsl/fields.py
 irrd/rpsl/parser.py
 irrd/rpsl/parser_state.py
+irrd/rpsl/passwords.py
 irrd/rpsl/rpsl_objects.py
 irrd/rpsl/tests/__init__.py
 irrd/rpsl/tests/test_fields.py
 irrd/rpsl/tests/test_rpsl_objects.py
 irrd/scopefilter/__init__.py
 irrd/scopefilter/status.py
 irrd/scopefilter/validators.py
 irrd/scopefilter/tests/__init__.py
 irrd/scopefilter/tests/test_scopefilter.py
 irrd/scripts/__init__.py
 irrd/scripts/database_downgrade.py
 irrd/scripts/database_upgrade.py
+irrd/scripts/expire_journal.py
+irrd/scripts/irr_rpsl_submit.py
 irrd/scripts/load_database.py
 irrd/scripts/load_pgp_keys.py
 irrd/scripts/load_test.py
 irrd/scripts/mirror_force_reload.py
 irrd/scripts/query_qa_comparison.py
 irrd/scripts/rpsl_read.py
 irrd/scripts/set_last_modified_auth.py
 irrd/scripts/submit_changes.py
 irrd/scripts/submit_email.py
 irrd/scripts/update_database.py
 irrd/scripts/tests/__init__.py
+irrd/scripts/tests/test_expire_journal.py
+irrd/scripts/tests/test_irr_rpsl_submit.py
 irrd/scripts/tests/test_load_database.py
 irrd/scripts/tests/test_load_pgp_keys.py
 irrd/scripts/tests/test_mirror_force_reload.py
 irrd/scripts/tests/test_rpsl_read.py
 irrd/scripts/tests/test_set_last_modified_auth.py
 irrd/scripts/tests/test_submit_email.py
 irrd/scripts/tests/test_submit_update.py
@@ -154,75 +172,87 @@
 irrd/server/graphql/tests/__init__.py
 irrd/server/graphql/tests/test_extensions.py
 irrd/server/graphql/tests/test_resolvers.py
 irrd/server/graphql/tests/test_schema_generator.py
 irrd/server/http/__init__.py
 irrd/server/http/app.py
 irrd/server/http/endpoints.py
+irrd/server/http/event_stream.py
 irrd/server/http/server.py
 irrd/server/http/status_generator.py
 irrd/server/http/tests/__init__.py
 irrd/server/http/tests/test_endpoints.py
+irrd/server/http/tests/test_event_stream.py
 irrd/server/http/tests/test_status_generator.py
 irrd/server/tests/__init__.py
 irrd/server/tests/test_query_resolver.py
 irrd/server/whois/__init__.py
 irrd/server/whois/query_parser.py
 irrd/server/whois/query_response.py
 irrd/server/whois/server.py
 irrd/server/whois/tests/__init__.py
 irrd/server/whois/tests/test_query_parser.py
 irrd/server/whois/tests/test_query_response.py
 irrd/server/whois/tests/test_server.py
 irrd/storage/__init__.py
 irrd/storage/database_handler.py
+irrd/storage/event_stream.py
 irrd/storage/models.py
 irrd/storage/preload.py
 irrd/storage/queries.py
 irrd/storage/alembic/__init__.py
 irrd/storage/alembic/env.py
 irrd/storage/alembic/script.py.mako
+irrd/storage/alembic/versions/0548f1aa4f10_add_rpsl_objects_suspended.py
 irrd/storage/alembic/versions/1743f98a456d_add_serial_newest_mirror.py
 irrd/storage/alembic/versions/181670a62643_add_journal_entry_origin.py
 irrd/storage/alembic/versions/28dc1cd85bdc_initial_db.py
 irrd/storage/alembic/versions/39e4f15ed80c_add_bogon_status.py
 irrd/storage/alembic/versions/4a514ead8fc2_bogon_to_scope_filter.py
 irrd/storage/alembic/versions/64a3d6faf6d4_add_prefix_length_rpki_status_to_rpsl_objects.py
 irrd/storage/alembic/versions/8744b4b906bb_fix_rpsl_unique_key.py
 irrd/storage/alembic/versions/893d0d5363b3_add_rpsl_prefix_idx.py
+irrd/storage/alembic/versions/8b8357acd333_add_global_serial.py
 irrd/storage/alembic/versions/__init__.py
 irrd/storage/alembic/versions/a7766c144d61_add_synchronised_serial_to_database_.py
 irrd/storage/alembic/versions/a8609af97aa3_set_prefix_length_in_existing_rpsl_.py
 irrd/storage/alembic/versions/b175c262448f_set_rpsl_prefix.py
 irrd/storage/alembic/versions/e07863eac52f_add_roa_object_table.py
 irrd/storage/alembic/versions/f4c837d8258c_add_rpsl_prefix.py
+irrd/storage/alembic/versions/fd4473bc1a10_add_route_preference_status.py
 irrd/storage/tests/__init__.py
 irrd/storage/tests/test_database.py
+irrd/storage/tests/test_event_stream.py
 irrd/storage/tests/test_preload.py
 irrd/updates/__init__.py
 irrd/updates/email.py
 irrd/updates/handler.py
 irrd/updates/parser.py
 irrd/updates/parser_state.py
+irrd/updates/suspension.py
 irrd/updates/validators.py
 irrd/updates/tests/__init__.py
 irrd/updates/tests/test_email.py
 irrd/updates/tests/test_handler.py
 irrd/updates/tests/test_parser.py
+irrd/updates/tests/test_suspension.py
+irrd/updates/tests/test_validators.py
 irrd/utils/__init__.py
 irrd/utils/email.py
+irrd/utils/misc.py
 irrd/utils/pgp.py
 irrd/utils/process_support.py
 irrd/utils/rpsl_samples.py
 irrd/utils/test_utils.py
 irrd/utils/text.py
 irrd/utils/validators.py
 irrd/utils/whois_client.py
 irrd/utils/tests/__init__.py
 irrd/utils/tests/test_email.py
+irrd/utils/tests/test_misc.py
 irrd/utils/tests/test_pgp.py
 irrd/utils/tests/test_text.py
 irrd/utils/tests/test_validators.py
 irrd/utils/tests/test_whois_client.py
 irrd/vendor/__init__.py
 irrd/vendor/dotted/__init__.py
 irrd/vendor/dotted/collection.py
```

### Comparing `irrd-4.2.8/irrd.egg-info/entry_points.txt` & `irrd-4.3.0/irrd.egg-info/entry_points.txt`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,12 @@
 [console_scripts]
+irr_rpsl_submit = irrd.scripts.irr_rpsl_submit:main
 irrd = irrd.daemon.main:main
 irrd_database_downgrade = irrd.scripts.database_downgrade:main
 irrd_database_upgrade = irrd.scripts.database_upgrade:main
+irrd_expire_journal = irrd.scripts.expire_journal:main
 irrd_load_database = irrd.scripts.load_database:main
 irrd_load_pgp_keys = irrd.scripts.load_pgp_keys:main
 irrd_mirror_force_reload = irrd.scripts.mirror_force_reload:main
 irrd_set_last_modified_auth = irrd.scripts.set_last_modified_auth:main
 irrd_submit_email = irrd.scripts.submit_email:main
 irrd_update_database = irrd.scripts.update_database:main
```

### Comparing `irrd-4.2.8/irrd.egg-info/requires.txt` & `irrd-4.3.0/irrd.egg-info/requires.txt`

 * *Files 19% similar despite different names*

```diff
@@ -1,37 +1,40 @@
-python-gnupg==0.4.7
+python-gnupg==0.5.0
 passlib==1.7.4
+bcrypt==4.0.1
 IPy==1.01
-ordered-set==4.0.2
+ordered-set==4.1.0
 beautifultable==0.8.0
-PyYAML==5.4.1
+PyYAML==6.0
 datrie==0.8.2
-setproctitle==1.2.2
-python-daemon==2.3.0
+setproctitle==1.3.2
+python-daemon==2.3.2
 pid==3.0.4
-redis==3.5.3
-hiredis==2.0.0
-requests==2.26.0
-pytz==2021.1
-ariadne==0.13.0
-uvicorn==0.15.0
-starlette==0.14.2
-psutil==5.8.0
-asgiref==3.4.1
-pydantic==1.8.2
-six==1.13.0
+redis==4.5.1
+hiredis==2.2.2
+coredis==4.10.2
+requests==2.28.2
+pytz==2022.7.1
+ariadne==0.17.1
+uvicorn==0.20.0
+starlette==0.20.4
+psutil==5.9.4
+asgiref==3.6.0
+pydantic==1.10.5
+typing-extensions==4.5.0
+py-radix-sr==1.0.0post1
 SQLAlchemy==1.3.24
-alembic==1.7.1
-ujson==4.1.0
-wheel==0.37.0
+alembic==1.9.4
+ujson==5.7.0
+wheel==0.38.4
 
 [:platform_python_implementation == "CPython"]
-psycopg2-binary==2.9.1
+psycopg2-binary==2.9.5
 
 [:platform_python_implementation == "PyPy"]
 psycopg2cffi==2.9.0
 
-[:python_version < "3.7"]
-dataclasses==0.8
+[:python_version < "3.8"]
+websockets==10.4
 
 [:python_version > "3.7"]
-uvicorn[standard]==0.15.0
+uvicorn[standard]==0.20.0
```

### Comparing `irrd-4.2.8/requirements.txt` & `irrd-4.3.0/requirements.txt`

 * *Files 21% similar despite different names*

```diff
@@ -1,56 +1,65 @@
 # Many of these packages are also specified in setup.py, and versions
 # should be kept in sync. The list in setup.py is shorter, as it only
 # includes packages needed for deployment.
 
-python-gnupg==0.4.7    # Integration with gpg for key-cert objects
+python-gnupg==0.5.0    # Integration with gpg for key-cert objects
 passlib==1.7.4         # Password hashing for CRYPT-PW and MD5-PW
+bcrypt==4.0.1          # bcrypt support
 IPy==1.01              # IP address parsing
-dataclasses==0.8; python_version < '3.7'       # PEP 557 dataclasses for python<3.7
-ordered-set==4.0.2     # Ordered sets for simple unique ordered storage
+ordered-set==4.1.0
 beautifultable==0.8.0  # pyup: <1.0.0
-PyYAML==5.4.1          # parsing YAML config files
+PyYAML==6.0          # parsing YAML config files
 datrie==0.8.2           # efficient route searching, used for RPKI
-setproctitle==1.2.2   # updating the process title for useful ps output
-python-daemon==2.3.0   # daemonising the main process
+setproctitle==1.3.2   # updating the process title for useful ps output
+python-daemon==2.3.2   # daemonising the main process
 pid==3.0.4             # managing PID files
-redis==3.5.3           # preloaded data storage & sync through redis
-hiredis==2.0.0         # improved redis response parsing performance
-requests==2.26.0       # HTTP request handling
-pytz==2021.1
-ariadne==0.13.0
-uvicorn==0.15.0        # ASGI server
-uvicorn[standard]==0.15.0; python_version > '3.7'  # ASGI server extras
-starlette==0.14.2 # pyup: <0.15      # ASGI framework - pinned to <0.15 for ariadne
-psutil==5.8.0          # Process management
-asgiref==3.4.1         # ASGI utilities
-pydantic==1.8.2        # Input validation
-six==1.13.0            # Used in vendored dotteddict
+redis==4.5.1           # preloaded data storage & sync through redis
+hiredis==2.2.2         # improved redis response parsing performance
+coredis==4.10.2         # async redis
+requests==2.31.0       # HTTP request handling
+pytz==2022.7.1
+ariadne==0.17.1
+uvicorn==0.20.0        # ASGI server
+uvicorn[standard]==0.20.0; python_version > '3.7'  # ASGI server extras
+websockets==10.4; python_version < '3.8'  # serving websockets
+starlette==0.20.4      # pyup: <0.21  # ariadne conflict
+psutil==5.9.4          # Process management
+asgiref==3.6.0         # ASGI utilities
+pydantic==1.10.5        # Input validation
+typing-extensions==4.5.0
+py-radix-sr==1.0.0post1
+cryptography==39.0.2  # Pinned to allow PyPy3.7 compat
 
 # Database connections and management
-psycopg2-binary==2.9.1; platform_python_implementation == "CPython"
+psycopg2-binary==2.9.5; platform_python_implementation == "CPython"
 psycopg2cffi==2.9.0; platform_python_implementation == "PyPy"
-SQLAlchemy==1.3.24 # pyup: <1.4  # https://github.com/irrdnet/irrd/issues/475
-alembic==1.7.1
-ujson==4.1.0
+SQLAlchemy==1.3.24 # pyup: <1.4  #475
+alembic==1.9.4
+ujson==5.7.0
 
 # Testing and related packages
-pytest==6.2.5
-pytest-cov==2.12.1
-coverage==6.0b1
-twisted==21.7.0    # Used for a mock email server in integration tests
-python-graphql-client==0.4.2
+pytest==7.2.1
+pytest-cov==4.0.0
+pytest-env==0.8.1
+coverage==7.1.0
+twisted==22.10.0    # Used for a mock email server in integration tests
+python-graphql-client==0.4.3
+pytest-asyncio==0.20.3
+freezegun==1.2.2
+pytest-freezegun==0.4.2
+asyncmock==0.4.2; python_version < '3.8'
 
 # Documentation generation
-Sphinx==4.2.0
-sphinxcontrib-spelling==7.2.1
-sphinx-material==0.0.34
+Sphinx==4.3.2  # pyup: <4.4  # importlib-metadata conflict with flake8
+sphinxcontrib-spelling==7.7.0
+sphinx-material==0.0.35
 
 # Code style and type checks
-mypy==0.910; platform_python_implementation == "CPython"
-flake8==3.9.2
-pep8-naming==0.12.1
+mypy==1.0.0; platform_python_implementation == "CPython"
+flake8==6.0.0; python_version >= '3.8'
+pep8-naming==0.13.3
 
 # Creating python packages
-setuptools==58.0.4
-wheel==0.37.0  # also require for installing, #488
-twine==3.4.2
+setuptools==67.3.2
+wheel==0.38.4  # also require for installing, #488
+twine==4.0.2
```

### Comparing `irrd-4.2.8/setup.cfg` & `irrd-4.3.0/setup.cfg`

 * *Files 10% similar despite different names*

```diff
@@ -1,23 +1,24 @@
 [flake8]
-ignore = E501,W503,E226,E252,W504
+ignore = E501,W503,E226,E252,W504,E203,E231
 
 [run]
 omit = setup.py
 
 [report]
 exclude_lines = 
 	pragma: no cover
 	def __repr__
 	if self.debug:
 	if settings.DEBUG
 	raise AssertionError
 	raise NotImplementedError
 	if 0:
 	if __name__ == '__main__':
+	if __name__ == "__main__":
 omit = 
 	irrd/daemon/main.py
 	irrd/server/http/app.py
 	irrd/server/graphql/schema_builder.py
 	irrd/server/http/server.py
 	
 	irrd/storage/alembic/*
@@ -26,14 +27,15 @@
 	irrd/scripts/database_downgrade.py
 	irrd/scripts/load_test.py
 	irrd/integration_tests/*
 	irrd/vendor/*
 
 [tool:pytest]
 log_level = DEBUG
+asyncio_mode = auto
 
 [mypy]
 ignore_missing_imports = True
 install_types = True
 non_interactive = True
 exclude = irrd/storage/alembic/*
```

### Comparing `irrd-4.2.8/setup.py` & `irrd-4.3.0/setup.py`

 * *Files 14% similar despite different names*

```diff
@@ -13,77 +13,82 @@
     description='Internet Routing Registry daemon (IRRd)',
     long_description=long_description,
     long_description_content_type='text/x-rst',
     url='https://github.com/irrdnet/irrd',
     packages=setuptools.find_packages(
         exclude=['*.tests', '*.tests.*', 'tests.*', 'tests', 'irrd.integration_tests']
     ),
-    python_requires='>=3.6',
+    python_requires='>=3.7',
     package_data={'': ['*.txt', '*.yaml', '*.mako']},
     install_requires=[
         # This list must be kept in sync with requirements.txt version-wise,
         # but should not include packages used for testing, generating docs
         # or packages.
-        'python-gnupg==0.4.7',
+        'python-gnupg==0.5.0',
         'passlib==1.7.4',
+        'bcrypt==4.0.1',
         'IPy==1.01',
-        'ordered-set==4.0.2',
+        'ordered-set==4.1.0',
         'beautifultable==0.8.0',
-        'PyYAML==5.4.1',
+        'PyYAML==6.0',
         'datrie==0.8.2',
-        'setproctitle==1.2.2',
-        'python-daemon==2.3.0',
+        'setproctitle==1.3.2',
+        'python-daemon==2.3.2',
         'pid==3.0.4',
-        'redis==3.5.3',
-        'hiredis==2.0.0',
-        'requests==2.26.0',
-        'pytz==2021.1',
-        'ariadne==0.13.0',
-        'uvicorn==0.15.0',
-        'starlette==0.14.2',
-        'psutil==5.8.0',
-        'asgiref==3.4.1',
-        'pydantic==1.8.2',
-        'six==1.13.0',
+        'redis==4.5.1',
+        'hiredis==2.2.2',
+        'coredis==4.10.2',
+        'requests==2.28.2',
+        'pytz==2022.7.1',
+        'ariadne==0.17.1',
+        'uvicorn==0.20.0',
+        'starlette==0.20.4',
+        'psutil==5.9.4',
+        'asgiref==3.6.0',
+        'pydantic==1.10.5',
+        'typing-extensions==4.5.0',
+        'py-radix-sr==1.0.0post1',
         'SQLAlchemy==1.3.24',
-        'alembic==1.7.1',
-        'ujson==4.1.0',
-        'wheel==0.37.0',
+        'alembic==1.9.4',
+        'ujson==5.7.0',
+        'wheel==0.38.4',
     ],
     extras_require={
-        ':python_version < "3.7"': [
-            'dataclasses==0.8',
-        ],
         ':python_version > "3.7"': [
-            'uvicorn[standard]==0.15.0',
+            'uvicorn[standard]==0.20.0',
+        ],
+        ':python_version < "3.8"': [
+            'websockets==10.4',
         ],
         ':platform_python_implementation == "CPython"': [
-            'psycopg2-binary==2.9.1',
+            'psycopg2-binary==2.9.5',
         ],
         ':platform_python_implementation == "PyPy"': [
             'psycopg2cffi==2.9.0',
         ],
     },
     entry_points={
         'console_scripts': [
             'irrd = irrd.daemon.main:main',
             'irrd_submit_email = irrd.scripts.submit_email:main',
             'irrd_database_upgrade = irrd.scripts.database_upgrade:main',
             'irrd_database_downgrade = irrd.scripts.database_downgrade:main',
             'irrd_load_database = irrd.scripts.load_database:main',
             'irrd_update_database = irrd.scripts.update_database:main',
             'irrd_set_last_modified_auth = irrd.scripts.set_last_modified_auth:main',
+            'irrd_expire_journal = irrd.scripts.expire_journal:main',
             'irrd_mirror_force_reload = irrd.scripts.mirror_force_reload:main',
+            'irr_rpsl_submit = irrd.scripts.irr_rpsl_submit:main',
             'irrd_load_pgp_keys = irrd.scripts.load_pgp_keys:main',
         ],
     },
     classifiers=[
         'License :: OSI Approved :: BSD License',
         'Programming Language :: Python :: 3',
-        'Programming Language :: Python :: 3.6',
         'Programming Language :: Python :: 3.7',
         'Programming Language :: Python :: 3.8',
         'Programming Language :: Python :: 3.9',
         'Programming Language :: Python :: 3.10',
+        'Programming Language :: Python :: 3.11',
         'Operating System :: OS Independent',
     ],
 )
```

