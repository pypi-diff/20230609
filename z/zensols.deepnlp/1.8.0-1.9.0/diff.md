# Comparing `tmp/zensols.deepnlp-1.8.0-py3-none-any.whl.zip` & `tmp/zensols.deepnlp-1.9.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,70 +1,75 @@
-Zip file size: 106015 bytes, number of entries: 68
--rw-rw-r--  2.0 unx      503 b- defN 23-Apr-06 01:10 zensols/deepnlp/__init__.py
--rw-rw-r--  2.0 unx       22 b- defN 23-Apr-06 01:10 zensols/deepnlp/batch/__init__.py
--rw-rw-r--  2.0 unx     1899 b- defN 23-Apr-06 01:10 zensols/deepnlp/batch/domain.py
--rw-rw-r--  2.0 unx       85 b- defN 23-Apr-06 01:10 zensols/deepnlp/classify/__init__.py
--rw-rw-r--  2.0 unx     6934 b- defN 23-Apr-06 01:10 zensols/deepnlp/classify/domain.py
--rw-rw-r--  2.0 unx     4057 b- defN 23-Apr-06 01:10 zensols/deepnlp/classify/facade.py
--rw-rw-r--  2.0 unx     3625 b- defN 23-Apr-06 01:10 zensols/deepnlp/classify/model.py
--rw-rw-r--  2.0 unx     4079 b- defN 23-Apr-06 01:10 zensols/deepnlp/classify/pred.py
--rw-rw-r--  2.0 unx       19 b- defN 23-Apr-06 01:10 zensols/deepnlp/cli/__init__.py
--rw-rw-r--  2.0 unx     5076 b- defN 23-Apr-06 01:10 zensols/deepnlp/cli/app.py
--rw-rw-r--  2.0 unx      115 b- defN 23-Apr-06 01:10 zensols/deepnlp/embed/__init__.py
--rw-rw-r--  2.0 unx     7981 b- defN 23-Apr-06 01:10 zensols/deepnlp/embed/domain.py
--rw-rw-r--  2.0 unx     2281 b- defN 23-Apr-06 01:10 zensols/deepnlp/embed/fasttext.py
--rw-rw-r--  2.0 unx     1569 b- defN 23-Apr-06 01:10 zensols/deepnlp/embed/glove.py
--rw-rw-r--  2.0 unx     3703 b- defN 23-Apr-06 01:10 zensols/deepnlp/embed/word2vec.py
--rw-rw-r--  2.0 unx     9385 b- defN 23-Apr-06 01:10 zensols/deepnlp/embed/wordtext.py
--rw-rw-r--  2.0 unx       21 b- defN 23-Apr-06 01:10 zensols/deepnlp/feature/__init__.py
--rw-rw-r--  2.0 unx     2674 b- defN 23-Apr-06 01:10 zensols/deepnlp/feature/stash.py
--rw-rw-r--  2.0 unx      124 b- defN 23-Apr-06 01:10 zensols/deepnlp/index/__init__.py
--rw-rw-r--  2.0 unx     3935 b- defN 23-Apr-06 01:10 zensols/deepnlp/index/domain.py
--rw-rw-r--  2.0 unx     3733 b- defN 23-Apr-06 01:10 zensols/deepnlp/index/lda.py
--rw-rw-r--  2.0 unx     3546 b- defN 23-Apr-06 01:10 zensols/deepnlp/index/lsi.py
--rw-rw-r--  2.0 unx      148 b- defN 23-Apr-06 01:10 zensols/deepnlp/layer/__init__.py
--rw-rw-r--  2.0 unx    10663 b- defN 23-Apr-06 01:10 zensols/deepnlp/layer/conv.py
--rw-rw-r--  2.0 unx    16971 b- defN 23-Apr-06 01:10 zensols/deepnlp/layer/embed.py
--rw-rw-r--  2.0 unx     8717 b- defN 23-Apr-06 01:10 zensols/deepnlp/layer/embrecurcrf.py
--rw-rw-r--  2.0 unx     2449 b- defN 23-Apr-06 01:10 zensols/deepnlp/layer/wordvec.py
--rw-rw-r--  2.0 unx       46 b- defN 23-Apr-06 01:10 zensols/deepnlp/model/__init__.py
--rw-rw-r--  2.0 unx    12956 b- defN 23-Apr-06 01:10 zensols/deepnlp/model/facade.py
--rw-rw-r--  2.0 unx     7916 b- defN 23-Apr-06 01:10 zensols/deepnlp/model/sequence.py
--rw-rw-r--  2.0 unx     1980 b- defN 23-Apr-06 01:10 zensols/deepnlp/resources/bigbird.conf
--rw-rw-r--  2.0 unx      469 b- defN 23-Apr-06 01:10 zensols/deepnlp/resources/classify-batch.yml
--rw-rw-r--  2.0 unx     3468 b- defN 23-Apr-06 01:10 zensols/deepnlp/resources/classify.conf
--rw-rw-r--  2.0 unx      286 b- defN 23-Apr-06 01:10 zensols/deepnlp/resources/cleaner.conf
--rw-rw-r--  2.0 unx      649 b- defN 23-Apr-06 01:10 zensols/deepnlp/resources/cli.conf
--rw-rw-r--  2.0 unx      765 b- defN 23-Apr-06 01:10 zensols/deepnlp/resources/default.conf
--rw-rw-r--  2.0 unx     1560 b- defN 23-Apr-06 01:10 zensols/deepnlp/resources/embed-crf.conf
--rw-rw-r--  2.0 unx     2631 b- defN 23-Apr-06 01:10 zensols/deepnlp/resources/fasttext.conf
--rw-rw-r--  2.0 unx     2505 b- defN 23-Apr-06 01:10 zensols/deepnlp/resources/feature.conf
--rw-rw-r--  2.0 unx     2528 b- defN 23-Apr-06 01:10 zensols/deepnlp/resources/glove.conf
--rw-rw-r--  2.0 unx     1840 b- defN 23-Apr-06 01:10 zensols/deepnlp/resources/lang-batch.yml
--rw-rw-r--  2.0 unx      610 b- defN 23-Apr-06 01:10 zensols/deepnlp/resources/obj.conf
--rw-rw-r--  2.0 unx     2195 b- defN 23-Apr-06 01:10 zensols/deepnlp/resources/token-classify.yml
--rw-rw-r--  2.0 unx     2124 b- defN 23-Apr-06 01:10 zensols/deepnlp/resources/transformer-expander.conf
--rw-rw-r--  2.0 unx      466 b- defN 23-Apr-06 01:10 zensols/deepnlp/resources/transformer-expander.yml
--rw-rw-r--  2.0 unx     4917 b- defN 23-Apr-06 01:10 zensols/deepnlp/resources/transformer-sent.conf
--rw-rw-r--  2.0 unx     4427 b- defN 23-Apr-06 01:10 zensols/deepnlp/resources/transformer.conf
--rw-rw-r--  2.0 unx     2804 b- defN 23-Apr-06 01:10 zensols/deepnlp/resources/vectorizer.conf
--rw-rw-r--  2.0 unx     1488 b- defN 23-Apr-06 01:10 zensols/deepnlp/resources/word2vec.conf
--rw-rw-r--  2.0 unx      482 b- defN 23-Apr-06 01:10 zensols/deepnlp/resources/wordpiece.conf
--rw-rw-r--  2.0 unx     1085 b- defN 23-Apr-06 01:10 zensols/deepnlp/transformer/__init__.py
--rw-rw-r--  2.0 unx    10621 b- defN 23-Apr-06 01:10 zensols/deepnlp/transformer/domain.py
--rw-rw-r--  2.0 unx     8493 b- defN 23-Apr-06 01:10 zensols/deepnlp/transformer/embed.py
--rw-rw-r--  2.0 unx     9254 b- defN 23-Apr-06 01:10 zensols/deepnlp/transformer/layer.py
--rw-rw-r--  2.0 unx     3914 b- defN 23-Apr-06 01:10 zensols/deepnlp/transformer/optimizer.py
--rw-rw-r--  2.0 unx     8297 b- defN 23-Apr-06 01:10 zensols/deepnlp/transformer/resource.py
--rw-rw-r--  2.0 unx     5497 b- defN 23-Apr-06 01:10 zensols/deepnlp/transformer/tokenizer.py
--rw-rw-r--  2.0 unx    20736 b- defN 23-Apr-06 01:10 zensols/deepnlp/transformer/vectorizers.py
--rw-rw-r--  2.0 unx    13722 b- defN 23-Apr-06 01:10 zensols/deepnlp/transformer/wordpiece.py
--rw-rw-r--  2.0 unx      172 b- defN 23-Apr-06 01:10 zensols/deepnlp/vectorize/__init__.py
--rw-rw-r--  2.0 unx     5656 b- defN 23-Apr-06 01:10 zensols/deepnlp/vectorize/embed.py
--rw-rw-r--  2.0 unx    18836 b- defN 23-Apr-06 01:10 zensols/deepnlp/vectorize/manager.py
--rw-rw-r--  2.0 unx     6267 b- defN 23-Apr-06 01:10 zensols/deepnlp/vectorize/spacy.py
--rw-rw-r--  2.0 unx    29843 b- defN 23-Apr-06 01:10 zensols/deepnlp/vectorize/vectorizers.py
--rw-rw-r--  2.0 unx     8071 b- defN 23-Apr-06 01:10 zensols.deepnlp-1.8.0.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-Apr-06 01:10 zensols.deepnlp-1.8.0.dist-info/WHEEL
--rw-rw-r--  2.0 unx       16 b- defN 23-Apr-06 01:10 zensols.deepnlp-1.8.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     6260 b- defN 23-Apr-06 01:10 zensols.deepnlp-1.8.0.dist-info/RECORD
-68 files, 320258 bytes uncompressed, 95913 bytes compressed:  70.1%
+Zip file size: 110359 bytes, number of entries: 73
+-rw-rw-r--  2.0 unx      503 b- defN 23-Jun-09 21:37 zensols/deepnlp/__init__.py
+-rw-rw-r--  2.0 unx       22 b- defN 23-Jun-09 21:37 zensols/deepnlp/batch/__init__.py
+-rw-rw-r--  2.0 unx     1899 b- defN 23-Jun-09 21:37 zensols/deepnlp/batch/domain.py
+-rw-rw-r--  2.0 unx       85 b- defN 23-Jun-09 21:37 zensols/deepnlp/classify/__init__.py
+-rw-rw-r--  2.0 unx     6934 b- defN 23-Jun-09 21:37 zensols/deepnlp/classify/domain.py
+-rw-rw-r--  2.0 unx     4101 b- defN 23-Jun-09 21:37 zensols/deepnlp/classify/facade.py
+-rw-rw-r--  2.0 unx     3625 b- defN 23-Jun-09 21:37 zensols/deepnlp/classify/model.py
+-rw-rw-r--  2.0 unx     4286 b- defN 23-Jun-09 21:37 zensols/deepnlp/classify/pred.py
+-rw-rw-r--  2.0 unx       19 b- defN 23-Jun-09 21:37 zensols/deepnlp/cli/__init__.py
+-rw-rw-r--  2.0 unx     5076 b- defN 23-Jun-09 21:37 zensols/deepnlp/cli/app.py
+-rw-rw-r--  2.0 unx      134 b- defN 23-Jun-09 21:37 zensols/deepnlp/embed/__init__.py
+-rw-rw-r--  2.0 unx     3372 b- defN 23-Jun-09 21:37 zensols/deepnlp/embed/doc.py
+-rw-rw-r--  2.0 unx     8650 b- defN 23-Jun-09 21:37 zensols/deepnlp/embed/domain.py
+-rw-rw-r--  2.0 unx     2281 b- defN 23-Jun-09 21:37 zensols/deepnlp/embed/fasttext.py
+-rw-rw-r--  2.0 unx     1569 b- defN 23-Jun-09 21:37 zensols/deepnlp/embed/glove.py
+-rw-rw-r--  2.0 unx     3703 b- defN 23-Jun-09 21:37 zensols/deepnlp/embed/word2vec.py
+-rw-rw-r--  2.0 unx     9385 b- defN 23-Jun-09 21:37 zensols/deepnlp/embed/wordtext.py
+-rw-rw-r--  2.0 unx       21 b- defN 23-Jun-09 21:37 zensols/deepnlp/feature/__init__.py
+-rw-rw-r--  2.0 unx     2674 b- defN 23-Jun-09 21:37 zensols/deepnlp/feature/stash.py
+-rw-rw-r--  2.0 unx      124 b- defN 23-Jun-09 21:37 zensols/deepnlp/index/__init__.py
+-rw-rw-r--  2.0 unx     4001 b- defN 23-Jun-09 21:37 zensols/deepnlp/index/domain.py
+-rw-rw-r--  2.0 unx     3733 b- defN 23-Jun-09 21:37 zensols/deepnlp/index/lda.py
+-rw-rw-r--  2.0 unx     4663 b- defN 23-Jun-09 21:37 zensols/deepnlp/index/lsi.py
+-rw-rw-r--  2.0 unx      148 b- defN 23-Jun-09 21:37 zensols/deepnlp/layer/__init__.py
+-rw-rw-r--  2.0 unx    10663 b- defN 23-Jun-09 21:37 zensols/deepnlp/layer/conv.py
+-rw-rw-r--  2.0 unx    16971 b- defN 23-Jun-09 21:37 zensols/deepnlp/layer/embed.py
+-rw-rw-r--  2.0 unx     8717 b- defN 23-Jun-09 21:37 zensols/deepnlp/layer/embrecurcrf.py
+-rw-rw-r--  2.0 unx     2449 b- defN 23-Jun-09 21:37 zensols/deepnlp/layer/wordvec.py
+-rw-rw-r--  2.0 unx       46 b- defN 23-Jun-09 21:37 zensols/deepnlp/model/__init__.py
+-rw-rw-r--  2.0 unx    12956 b- defN 23-Jun-09 21:37 zensols/deepnlp/model/facade.py
+-rw-rw-r--  2.0 unx     7916 b- defN 23-Jun-09 21:37 zensols/deepnlp/model/sequence.py
+-rw-rw-r--  2.0 unx     1980 b- defN 23-Jun-09 21:37 zensols/deepnlp/resources/bigbird.conf
+-rw-rw-r--  2.0 unx      469 b- defN 23-Jun-09 21:37 zensols/deepnlp/resources/classify-batch.yml
+-rw-rw-r--  2.0 unx     3468 b- defN 23-Jun-09 21:37 zensols/deepnlp/resources/classify.conf
+-rw-rw-r--  2.0 unx      286 b- defN 23-Jun-09 21:37 zensols/deepnlp/resources/cleaner.conf
+-rw-rw-r--  2.0 unx      649 b- defN 23-Jun-09 21:37 zensols/deepnlp/resources/cli.conf
+-rw-rw-r--  2.0 unx      867 b- defN 23-Jun-09 21:37 zensols/deepnlp/resources/default.conf
+-rw-rw-r--  2.0 unx     1560 b- defN 23-Jun-09 21:37 zensols/deepnlp/resources/embed-crf.conf
+-rw-rw-r--  2.0 unx     2631 b- defN 23-Jun-09 21:37 zensols/deepnlp/resources/fasttext.conf
+-rw-rw-r--  2.0 unx     2505 b- defN 23-Jun-09 21:37 zensols/deepnlp/resources/feature.conf
+-rw-rw-r--  2.0 unx     2528 b- defN 23-Jun-09 21:37 zensols/deepnlp/resources/glove.conf
+-rw-rw-r--  2.0 unx     1840 b- defN 23-Jun-09 21:37 zensols/deepnlp/resources/lang-batch.yml
+-rw-rw-r--  2.0 unx      610 b- defN 23-Jun-09 21:37 zensols/deepnlp/resources/obj.conf
+-rw-rw-r--  2.0 unx      212 b- defN 23-Jun-09 21:37 zensols/deepnlp/resources/score.yml
+-rw-rw-r--  2.0 unx     2195 b- defN 23-Jun-09 21:37 zensols/deepnlp/resources/token-classify.yml
+-rw-rw-r--  2.0 unx     2124 b- defN 23-Jun-09 21:37 zensols/deepnlp/resources/transformer-expander.conf
+-rw-rw-r--  2.0 unx      466 b- defN 23-Jun-09 21:37 zensols/deepnlp/resources/transformer-expander.yml
+-rw-rw-r--  2.0 unx     4999 b- defN 23-Jun-09 21:37 zensols/deepnlp/resources/transformer-sent.conf
+-rw-rw-r--  2.0 unx     4509 b- defN 23-Jun-09 21:37 zensols/deepnlp/resources/transformer.conf
+-rw-rw-r--  2.0 unx     2804 b- defN 23-Jun-09 21:37 zensols/deepnlp/resources/vectorizer.conf
+-rw-rw-r--  2.0 unx     1488 b- defN 23-Jun-09 21:37 zensols/deepnlp/resources/word2vec.conf
+-rw-rw-r--  2.0 unx      215 b- defN 23-Jun-09 21:37 zensols/deepnlp/resources/wordembed.conf
+-rw-rw-r--  2.0 unx     1017 b- defN 23-Jun-09 21:37 zensols/deepnlp/resources/wordpiece.conf
+-rw-rw-r--  2.0 unx       25 b- defN 23-Jun-09 21:37 zensols/deepnlp/score/__init__.py
+-rw-rw-r--  2.0 unx     2259 b- defN 23-Jun-09 21:37 zensols/deepnlp/score/bertscore.py
+-rw-rw-r--  2.0 unx     1084 b- defN 23-Jun-09 21:37 zensols/deepnlp/transformer/__init__.py
+-rw-rw-r--  2.0 unx    10621 b- defN 23-Jun-09 21:37 zensols/deepnlp/transformer/domain.py
+-rw-rw-r--  2.0 unx     8493 b- defN 23-Jun-09 21:37 zensols/deepnlp/transformer/embed.py
+-rw-rw-r--  2.0 unx     9254 b- defN 23-Jun-09 21:37 zensols/deepnlp/transformer/layer.py
+-rw-rw-r--  2.0 unx     3914 b- defN 23-Jun-09 21:37 zensols/deepnlp/transformer/optimizer.py
+-rw-rw-r--  2.0 unx     7945 b- defN 23-Jun-09 21:37 zensols/deepnlp/transformer/resource.py
+-rw-rw-r--  2.0 unx     5351 b- defN 23-Jun-09 21:37 zensols/deepnlp/transformer/tokenizer.py
+-rw-rw-r--  2.0 unx    20736 b- defN 23-Jun-09 21:37 zensols/deepnlp/transformer/vectorizers.py
+-rw-rw-r--  2.0 unx    14458 b- defN 23-Jun-09 21:37 zensols/deepnlp/transformer/wordpiece.py
+-rw-rw-r--  2.0 unx      172 b- defN 23-Jun-09 21:37 zensols/deepnlp/vectorize/__init__.py
+-rw-rw-r--  2.0 unx     5656 b- defN 23-Jun-09 21:37 zensols/deepnlp/vectorize/embed.py
+-rw-rw-r--  2.0 unx    18836 b- defN 23-Jun-09 21:37 zensols/deepnlp/vectorize/manager.py
+-rw-rw-r--  2.0 unx     6267 b- defN 23-Jun-09 21:37 zensols/deepnlp/vectorize/spacy.py
+-rw-rw-r--  2.0 unx    29843 b- defN 23-Jun-09 21:37 zensols/deepnlp/vectorize/vectorizers.py
+-rw-rw-r--  2.0 unx     8071 b- defN 23-Jun-09 21:37 zensols.deepnlp-1.9.0.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Jun-09 21:37 zensols.deepnlp-1.9.0.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       16 b- defN 23-Jun-09 21:37 zensols.deepnlp-1.9.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     6712 b- defN 23-Jun-09 21:37 zensols.deepnlp-1.9.0.dist-info/RECORD
+73 files, 329953 bytes uncompressed, 99537 bytes compressed:  69.8%
```

## zipnote {}

```diff
@@ -27,14 +27,17 @@
 
 Filename: zensols/deepnlp/cli/app.py
 Comment: 
 
 Filename: zensols/deepnlp/embed/__init__.py
 Comment: 
 
+Filename: zensols/deepnlp/embed/doc.py
+Comment: 
+
 Filename: zensols/deepnlp/embed/domain.py
 Comment: 
 
 Filename: zensols/deepnlp/embed/fasttext.py
 Comment: 
 
 Filename: zensols/deepnlp/embed/glove.py
@@ -120,14 +123,17 @@
 
 Filename: zensols/deepnlp/resources/lang-batch.yml
 Comment: 
 
 Filename: zensols/deepnlp/resources/obj.conf
 Comment: 
 
+Filename: zensols/deepnlp/resources/score.yml
+Comment: 
+
 Filename: zensols/deepnlp/resources/token-classify.yml
 Comment: 
 
 Filename: zensols/deepnlp/resources/transformer-expander.conf
 Comment: 
 
 Filename: zensols/deepnlp/resources/transformer-expander.yml
@@ -141,17 +147,26 @@
 
 Filename: zensols/deepnlp/resources/vectorizer.conf
 Comment: 
 
 Filename: zensols/deepnlp/resources/word2vec.conf
 Comment: 
 
+Filename: zensols/deepnlp/resources/wordembed.conf
+Comment: 
+
 Filename: zensols/deepnlp/resources/wordpiece.conf
 Comment: 
 
+Filename: zensols/deepnlp/score/__init__.py
+Comment: 
+
+Filename: zensols/deepnlp/score/bertscore.py
+Comment: 
+
 Filename: zensols/deepnlp/transformer/__init__.py
 Comment: 
 
 Filename: zensols/deepnlp/transformer/domain.py
 Comment: 
 
 Filename: zensols/deepnlp/transformer/embed.py
@@ -186,20 +201,20 @@
 
 Filename: zensols/deepnlp/vectorize/spacy.py
 Comment: 
 
 Filename: zensols/deepnlp/vectorize/vectorizers.py
 Comment: 
 
-Filename: zensols.deepnlp-1.8.0.dist-info/METADATA
+Filename: zensols.deepnlp-1.9.0.dist-info/METADATA
 Comment: 
 
-Filename: zensols.deepnlp-1.8.0.dist-info/WHEEL
+Filename: zensols.deepnlp-1.9.0.dist-info/WHEEL
 Comment: 
 
-Filename: zensols.deepnlp-1.8.0.dist-info/top_level.txt
+Filename: zensols.deepnlp-1.9.0.dist-info/top_level.txt
 Comment: 
 
-Filename: zensols.deepnlp-1.8.0.dist-info/RECORD
+Filename: zensols.deepnlp-1.9.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## zensols/deepnlp/classify/facade.py

```diff
@@ -90,17 +90,17 @@
     """A token level classification model facade.
 
     """
     predictions_datafrmae_factory_class: Type[PredictionsDataFrameFactory] = \
         field(default=SequencePredictionsDataFrameFactory)
 
     def get_predictions(self, *args, **kwargs) -> pd.DataFrame:
-        """Return a Pandas dataframe of the predictions with columns that include the
-        correct label, the prediction, the text and the length of the text of
-        the text.
+        """Return a Pandas dataframe of the predictions with columns that
+        include the correct label, the prediction, the text and the length of
+        the text of the text.  This uses the token norms of the document.
 
         :see: :meth:`get_predictions_factory`
 
         :param args: arguments passed to :meth:`get_predictions_factory`
 
         :param kwargs: arguments passed to :meth:`get_predictions_factory`
```

## zensols/deepnlp/classify/pred.py

```diff
@@ -14,16 +14,16 @@
 from zensols.deeplearn.result import ResultsContainer
 from zensols.deepnlp.vectorize import FeatureDocumentVectorizerManager
 from . import LabeledFeatureDocument
 
 
 @dataclass
 class ClassificationPredictionMapper(PredictionMapper):
-    """A prediction mapper for text classification.  This mapper works at any level
-    (document, sentence, token).
+    """A prediction mapper for text classification.  This mapper works at any
+    level (document, sentence, token).
 
     """
     vec_manager: FeatureDocumentVectorizerManager = field()
     """The vectorizer manager used to parse and get the label vectorizer."""
 
     label_feature_id: str = field()
     """The feature ID for the label vectorizer."""
@@ -88,14 +88,19 @@
             setattr(doc, self.pred_attribute, cl)
             setattr(doc, self.softmax_logit_attribute, sms)
         return tuple(docs)
 
 
 @dataclass
 class SequencePredictionMapper(ClassificationPredictionMapper):
+    """Predicts sequences as a :class:`~zensols.config.serial.Settings` with
+    keys `classes` as the token level predictions and `docs` containing the
+    parsed documents from the sentence text.
+
+    """
     def _create_features(self, sent_text: str) -> Tuple[FeatureSentence]:
         doc: FeatureDocument = self.vec_manager.parse(sent_text)
         self._docs.append(doc)
         return doc.sents
 
     def map_results(self, result: ResultsContainer) -> Settings:
         classes = self._map_classes(result)
```

## zensols/deepnlp/embed/__init__.py

```diff
@@ -1,5 +1,6 @@
 from .domain import *
+from .doc import *
 from .wordtext import *
 from .word2vec import *
 from .glove import *
 from .fasttext import *
```

## zensols/deepnlp/embed/domain.py

```diff
@@ -1,13 +1,13 @@
 """Interface file for word vectors, aka non-contextual word embeddings.
 
 """
 __author__ = 'Paul Landes'
 
-from typing import List, Dict, Tuple, Iterable
+from typing import List, Dict, Tuple, Iterable, ClassVar, Optional
 from dataclasses import dataclass, field
 from abc import ABCMeta, abstractmethod
 import logging
 import numpy as np
 import torch
 from torch import Tensor
 import gensim
@@ -15,32 +15,35 @@
 from zensols.persist import persisted, PersistableContainer, PersistedWork
 from zensols.deeplearn import TorchConfig, DeepLearnError
 
 logger = logging.getLogger(__name__)
 
 
 class WordEmbedError(DeepLearnError):
-    """Raised for any errors pertaining to word vectors."""
+    """Raised for any errors pertaining to word vectors.
+
+    """
+    pass
 
 
 @dataclass
 class WordVectorModel(object):
     """Vector data from the model
 
     """
     vectors: np.ndarray = field()
     """The word vectors."""
 
     word2vec: Dict[str, np.ndarray] = field()
     """The word to word vector mapping."""
 
-    words: Dict[str, int] = field()
-    """The string vocabulary."""
+    words: List[str] = field()
+    """The vocabulary."""
 
-    word2idx: Dict[str, np.ndarray] = field()
+    word2idx: Dict[str, int] = field()
     """The word to word vector index mapping."""
 
     def __post_init__(self):
         self.tensors = {}
 
     def to_matrix(self, torch_config: TorchConfig) -> torch.Tensor:
         dev = torch_config.device
@@ -54,19 +57,19 @@
             vecs = torch_config.from_numpy(self.vectors)
             self.tensors[dev] = vecs
         return vecs
 
 
 @dataclass
 class _WordEmbedVocabAdapter(object):
-    """Adapts a :class:`.WordEmbedModel` to a gensim :class:`.KeyedVectors`, which
-    is used in :meth:`.WordEmbedModel._create_keyed_vectors`.
+    """Adapts a :class:`.WordEmbedModel` to a gensim :class:`.KeyedVectors`,
+    which is used in :meth:`.WordEmbedModel._create_keyed_vectors`.
 
     """
-    model: WordVectorModel
+    model: WordVectorModel = field()
 
     def __post_init__(self):
         self._index = -1
 
     @property
     def index(self):
         return self._index
@@ -85,21 +88,21 @@
 
 @dataclass
 class WordEmbedModel(PersistableContainer, metaclass=ABCMeta):
     """This is an abstract base class that represents a set of word vectors
     (i.e. GloVe).
 
     """
-    UNKNOWN = '<unk>'
+    UNKNOWN: ClassVar[str] = '<unk>'
     """The unknown symbol used for out of vocabulary words."""
 
-    ZERO = UNKNOWN
+    ZERO: ClassVar[str] = UNKNOWN
     """The zero vector symbol used for padding vectors."""
 
-    _CACHE = {}
+    _CACHE: ClassVar[Dict[str, WordVectorModel]] = {}
     """Contains cached embedding model that point to the same source."""
 
     name: str = field()
     """The name of the model given by the configuration and must be unique
     across word vector type and dimension.
 
     """
@@ -161,18 +164,19 @@
 
         """
         return self._get_model_id()
 
     @persisted('_data_inst', transient=True)
     def _data(self) -> WordVectorModel:
         model_id = self.model_id
-        wv_model = self._CACHE.get(model_id)
+        wv_model: WordVectorModel = self._CACHE.get(model_id)
         if wv_model is None:
             wv_model = self._create_data()
-            self._CACHE[model_id] = wv_model
+            if self.cache:
+                self._CACHE[model_id] = wv_model
         return wv_model
 
     @property
     def matrix(self) -> np.ndarray:
         """The word vector matrix."""
         return self._data().vectors
 
@@ -205,47 +209,61 @@
 
     def keys(self) -> Iterable[str]:
         """Return the keys, which are the word2vec words.
 
         """
         return self.vectors.keys()
 
-    def word2idx_or_unk(self, word: str) -> int:
-        """Return the index of ``word`` or :obj:UNKONWN if not indexed.
+    @property
+    @persisted('_unk_idx')
+    def unk_idx(self) -> int:
+        """The ID to the out-of-vocabulary index"""
+        model: WordVectorModel = self._data()
+        word2idx: Dict[str, int] = model.word2idx
+        return word2idx.get(self.UNKNOWN)
+
+    def word2idx(self, word: str, default: int = None) -> Optional[int]:
+        """Return the index of ``word`` or :obj:`UNKONWN` if not indexed.
 
         """
         if self.lowercase:
             word = word.lower()
-        word2idx = self._data().word2idx
-        idx = word2idx.get(word)
+        model: WordVectorModel = self._data()
+        word2idx: Dict[str, int] = model.word2idx
+        idx: int = word2idx.get(word)
         if idx is None:
-            idx = word2idx.get(self.UNKNOWN)
+            idx = default
         return idx
 
+    def word2idx_or_unk(self, word: str) -> int:
+        """Return the index of ``word`` or :obj:`UNKONWN` if not indexed.
+
+        """
+        return self.word2idx(word, self.unk_idx)
+
     def prime(self):
         pass
 
     def get(self, key: str, default: np.ndarray = None) -> np.ndarray:
         """Just like a ``dict.get()``, but but return the vector for a word.
 
         :param key: the word to get the vector
 
         :param default: what to return if ``key`` doesn't exist in the dict
 
         :return: the word vector
         """
         if self.lowercase:
             key = key.lower()
-        if key not in self.vectors:
-            key = self.UNKNOWN
         return self.vectors.get(key, default)
 
     @property
     @persisted('_keyed_vectors', transient=True)
     def keyed_vectors(self) -> KeyedVectors:
+        """Adapt instances of this class to a gensim keyed vector instance."""
         return self._create_keyed_vectors()
 
     def _create_keyed_vectors(self) -> KeyedVectors:
         kv = Word2VecKeyedVectors(vector_size=self.vector_dimension)
         if gensim.__version__[0] >= '4':
             kv.key_to_index = self._data().word2idx
         else:
```

## zensols/deepnlp/index/domain.py

```diff
@@ -12,30 +12,30 @@
 from zensols.util import time
 from zensols.persist import (
     persisted,
     PersistedWork,
     PersistableContainer,
     Primeable
 )
-from zensols.nlp import FeatureDocument
+from zensols.nlp import FeatureToken, FeatureDocument
 from zensols.deepnlp.vectorize import FeatureDocumentVectorizer
 
 logger = logging.getLogger(__name__)
 
 
 @dataclass
 class IndexedDocumentFactory(ABC):
     """Creates training documents used to generate indexed features (i.e. latent
     dirichlet allocation, latent semantic indexing etc).
 
     :see: :class:`.DocumentIndexVectorizer`
 
     """
     @abstractmethod
-    def create_training_docs(self) -> Tuple[FeatureDocument]:
+    def create_training_docs(self) -> Iterable[FeatureDocument]:
         """Create the documents used to index in the model during training.
 
         """
         pass
 
 
 @dataclass
@@ -67,48 +67,50 @@
     index_path: Path = field()
     """The path to the pickeled cache file of the trained model.
 
     """
     def __post_init__(self):
         PersistableContainer.__init__(self)
         self.index_path.parent.mkdir(parents=True, exist_ok=True)
-        self._model_pw = PersistedWork(self.index_path, self)
+        self._model = PersistedWork(self.index_path, self)
 
     @staticmethod
-    def feat_to_tokens(docs: Tuple[FeatureDocument]) -> Tuple[str]:
+    def feat_to_tokens(docs: Tuple[FeatureDocument, ...]) -> Tuple[str, ...]:
         """Create a tuple of string tokens from a set of documents suitable for
         document indexing.  The strings are the lemmas of the tokens.
 
         **Important**: this method must remain static since the LSI instance of
         this class uses it as a factory function in the a vectorizer.
 
         """
-        toks = map(lambda d: d.lemma.lower(),
-                   filter(lambda d: not d.is_stop and not d.is_punctuation,
-                          chain.from_iterable(
-                              map(lambda d: d.tokens, docs))))
+        def filter_tok(t: FeatureToken) -> bool:
+            return not t.is_space and not t.is_stop and not t.is_punctuation
+
+        toks = map(lambda d: d.lemma_.lower(),
+                   filter(filter_tok, chain.from_iterable(
+                       map(lambda d: d.tokens, docs))))
         return tuple(toks)
 
     @abstractmethod
     def _create_model(self, docs: Iterable[FeatureDocument]) -> Any:
         """Create the model for this indexer.  The model is implementation specific.
         The model must be pickelabel and is cached in as :obj:`model`.
 
         """
         pass
 
     @property
-    @persisted('_model_pw')
+    @persisted('_model')
     def model(self):
         """Return the trained model for this vectorizer.  See the class docs on how it
         is cached and cleared.
 
         """
-        with time('created training documents'):
-            docs = self.doc_factory.create_training_docs()
+        docs: Iterable[FeatureDocument] = \
+            self.doc_factory.create_training_docs()
         with time('trained model'):
             if logger.isEnabledFor(logging.INFO):
                 logger.info(f'creating model at {self.index_path}')
             return self._create_model(docs)
 
     def __getstate__(self):
         return self.__dict__
```

## zensols/deepnlp/index/lsi.py

```diff
@@ -1,89 +1,119 @@
-from typing import Tuple, Iterable, Any
+"""A Deerwester latent semantic index vectorizer implementation.
+
+"""
+__author__ = 'Paul Landes'
+
+from typing import Tuple, Iterable, Any, Dict
 from dataclasses import dataclass, field
 import logging
 import numpy as np
 from sklearn.decomposition import TruncatedSVD
 from sklearn.pipeline import make_pipeline, Pipeline
 from sklearn.preprocessing import Normalizer
 from sklearn.feature_extraction.text import TfidfVectorizer
+from scipy.sparse import csr_matrix
 from zensols.util import time
 from zensols.nlp import FeatureDocument, TokenContainer
+from zensols.deeplearn import DeepLearnError
 from zensols.deeplearn.vectorize import FeatureContext, TensorFeatureContext
 from zensols.deepnlp.vectorize import TextFeatureType
 from . import DocumentIndexVectorizer
 
 logger = logging.getLogger(__name__)
 
 
 @dataclass
 class LatentSemanticDocumentIndexerVectorizer(DocumentIndexVectorizer):
-    """Train a latent semantic indexing (LSI, aka LSA) model.
+    """Train a latent semantic indexing (LSI, aka LSA) model from::
 
-    Citation:
+      Deerwester, S., Dumais, S.T., Furnas, G.W., Landauer, T.K., and Harshman,
+      R. 1990.  Indexing by Latent Semantic Analysis. Journal of the American
+      Society for Information Science; New York, N.Y. 41, 6, 391–407.
 
-    Deerwester, S., Dumais, S.T., Furnas, G.W., Landauer, T.K., and Harshman,
-    R. 1990.  Indexing by Latent Semantic Analysis. Journal of the American
-    Society for Information Science; New York, N.Y. 41, 6, 391–407.
+    This class can be used only to index TF/IDF.  To skip the LSI training, set
+    :obj:`iterations` to zero.
 
     :shape: ``(1,)``
 
     :see: :class:`sklearn.decomposition.TruncatedSVD`
 
     """
     DESCRIPTION = 'latent semantic indexing'
     FEATURE_TYPE = TextFeatureType.DOCUMENT
 
     components: int = field(default=100)
     """The number of components for the output."""
 
     iterations: int = field(default=10)
-    """Number of iterations for randomized SVD solver."""
+    """Number of iterations for randomized SVD solver if greater than 0 (see
+    class docs).
 
+    """
+    vectorizer_params: Dict[str, Any] = field(default_factory=dict)
+    """Additional parameters passed to
+    :class:`~sklearn.feature_extraction.text.TfidfVectorizer` when vectorizing
+    TF/IDF features.
+
+    """
     def _get_shape(self) -> Tuple[int, int]:
         return 1,
 
-    def _create_model(self, docs: Iterable[FeatureDocument]) -> Any:
-        """Train using a singular value decomposition, then truncate to get the most
-        salient terms in a document/term matrics.
+    def _create_model(self, docs: Iterable[FeatureDocument]) -> Dict[str, Any]:
+        """Train using a singular value decomposition, then truncate to get the
+        most salient terms in a document/term matrics.
 
         """
         vectorizer = TfidfVectorizer(
             lowercase=False,
-            tokenizer=self.feat_to_tokens
+            tokenizer=self.feat_to_tokens,
+            **self.vectorizer_params,
         )
+        model: Dict[str, Any] = {'vectorizer': vectorizer}
         with time('TF/IDF vectorized {X_train_tfidf.shape[0]} documents'):
             X_train_tfidf = vectorizer.fit_transform(docs)
         if logger.isEnabledFor(logging.DEBUG):
             logger.debug(f'tfidf shape: {X_train_tfidf.shape}')
         svd = TruncatedSVD(self.components, n_iter=self.iterations)
-        lsa: Pipeline = make_pipeline(svd, Normalizer(copy=False))
-        with time('SVD complete'):
-            X_train_lsa = lsa.fit_transform(X_train_tfidf)
-        if logger.isEnabledFor(logging.INFO):
-            logger.info(f'created model with {self.components} components, ' +
-                        f'over {self.iterations} iterations with ' +
-                        f'TF/IDF matrix shape: {X_train_tfidf.shape}, ' +
-                        f'SVD matrix shape: {X_train_lsa.shape}')
-        return {'vectorizer': vectorizer,
-                'lsa': lsa}
+        if self.iterations > 0:
+            lsa: Pipeline = make_pipeline(svd, Normalizer(copy=False))
+            with time('SVD complete'):
+                X_train_lsa = lsa.fit_transform(X_train_tfidf)
+            if logger.isEnabledFor(logging.INFO):
+                logger.info(f'created model w/{self.components} components, ' +
+                            f'over {self.iterations} iterations with ' +
+                            f'TF/IDF matrix shape: {X_train_tfidf.shape}, ' +
+                            f'SVD matrix shape: {X_train_lsa.shape}')
+            model['lsa'] = lsa
+        return model
+
+    @property
+    def vectorizer(self) -> TfidfVectorizer:
+        """The vectorizer trained on the document set."""
+        return self.model['vectorizer']
+
+    @property
+    def lsa(self) -> Pipeline:
+        """The LSA pipeline trained on the document set."""
+        if 'lsa' not in self.model:
+            raise DeepLearnError('SVD model was not trained')
+        return self.model['lsa']
 
     def _transform_doc(self, doc: FeatureDocument, vectorizer: TfidfVectorizer,
                        lsa: Pipeline) -> np.ndarray:
-        X_test_tfidf = vectorizer.transform([doc])
-        X_test_lsa = lsa.transform(X_test_tfidf)
+        X_test_tfidf: csr_matrix = vectorizer.transform([doc])
+        X_test_lsa: csr_matrix = lsa.transform(X_test_tfidf)
         return X_test_lsa
 
     def similarity(self, a: FeatureDocument, b: FeatureDocument) -> float:
         """Return the semantic similarity between two documents.
 
         """
-        model = self.model
-        vectorizer = model['vectorizer']
-        lsa = model['lsa']
+        vectorizer: TfidfVectorizer = self.vectorizer
+        lsa: Pipeline = self.lsa
         emb_a = self._transform_doc(a, vectorizer, lsa)
         emb_b = self._transform_doc(b, vectorizer, lsa)
         return np.dot(emb_a, emb_b.T)[0][0]
 
     def _encode(self, containers: Tuple[TokenContainer]) -> FeatureContext:
         measure = self.similarity(*containers)
         arr = self.torch_config.singleton([measure])
```

## zensols/deepnlp/resources/default.conf

```diff
@@ -16,9 +16,12 @@
 word2vec_encode_transformed = False
 fasttext_encode_transformed = False
 
 # from_pretrain extra arguments; speeds things up, but set to True for the
 # first pretrained download
 transformer_local_files_only = False
 
+# max wordpiece length for transformer tokenization; default to no limit
+word_piece_token_length = 0
+
 # word_piece_document_factory default embedding name
 wordpiece_embedding = transformer_sent_fixed
```

## zensols/deepnlp/resources/transformer-sent.conf

```diff
@@ -30,15 +30,15 @@
 
 [transformer_sent_trainable_tokenizer]
 class_name = zensols.deepnlp.transformer.TransformerDocumentTokenizer
 resource = instance: transformer_sent_trainable_resource
 # the max number of word peice tokens; the word piece length is always the same
 # or greater in count than linguistic tokens because the word piece algorithm
 # tokenizes on characters; set to 0 set length to longest sentence per batch
-word_piece_token_length = 0
+word_piece_token_length = ${deepnlp_default:word_piece_token_length}
 
 [transformer_sent_trainable_embedding]
 class_name = zensols.deepnlp.transformer.TransformerEmbedding
 tokenizer = instance: transformer_sent_trainable_tokenizer
 
 [transformer_sent_trainable_feature_vectorizer]
 class_name = zensols.deepnlp.transformer.TransformerEmbeddingFeatureVectorizer
@@ -79,15 +79,15 @@
 
 [transformer_sent_fixed_tokenizer]
 class_name = zensols.deepnlp.transformer.TransformerDocumentTokenizer
 resource = instance: transformer_sent_fixed_resource
 # the max number of word peice tokens; the word piece length is always the same
 # or greater in count than linguistic tokens because the word piece algorithm
 # tokenizes on characters; set to 0 set length to longest sentence per batch
-word_piece_token_length = 0
+word_piece_token_length = ${deepnlp_default:word_piece_token_length}
 
 [transformer_sent_fixed_embedding]
 class_name = zensols.deepnlp.transformer.TransformerEmbedding
 tokenizer = instance: transformer_sent_fixed_tokenizer
 
 [transformer_sent_fixed_feature_vectorizer]
 class_name = zensols.deepnlp.transformer.TransformerEmbeddingFeatureVectorizer
```

## zensols/deepnlp/resources/transformer.conf

```diff
@@ -19,15 +19,15 @@
 
 [transformer_trainable_tokenizer]
 class_name = zensols.deepnlp.transformer.TransformerDocumentTokenizer
 resource = instance: transformer_trainable_resource
 # the max number of word peice tokens; the word piece length is always the same
 # or greater in count than linguistic tokens because the word piece algorithm
 # tokenizes on characters; set to 0 set length to longest sentence per batch
-word_piece_token_length = 0
+word_piece_token_length = ${deepnlp_default:word_piece_token_length}
 
 [transformer_trainable_embedding]
 class_name = zensols.deepnlp.transformer.TransformerEmbedding
 tokenizer = instance: transformer_trainable_tokenizer
 # uncomment to use the last layer (rather than the CLS token) as output
 #output = last_hidden_state
 
@@ -69,15 +69,15 @@
 
 [transformer_fixed_tokenizer]
 class_name = zensols.deepnlp.transformer.TransformerDocumentTokenizer
 resource = instance: transformer_fixed_resource
 # the max number of word peice tokens; the word piece length is always the same
 # or greater in count than linguistic tokens because the word piece algorithm
 # tokenizes on characters; set to 0 set length to longest sentence per batch
-word_piece_token_length = 0
+word_piece_token_length = ${deepnlp_default:word_piece_token_length}
 
 [transformer_fixed_embedding]
 class_name = zensols.deepnlp.transformer.TransformerEmbedding
 tokenizer = instance: transformer_fixed_tokenizer
 # uncomment to use the last layer (rather than the CLS token) as output
 #output = last_hidden_state
```

## zensols/deepnlp/resources/wordpiece.conf

```diff
@@ -1,11 +1,22 @@
 ## description: sentence BERT transformer
 
 [word_piece_doc_factory]
 class_name = zensols.deepnlp.transformer.WordPieceFeatureDocumentFactory
 tokenizer = instance: ${deepnlp_default:wordpiece_embedding}_tokenizer
 embed_model = instance: ${deepnlp_default:wordpiece_embedding}_embedding
 
-[word_piece_feature_doc_factory]
-class_name = zensols.deepnlp.transformer.WordPieceFeatureDocumentParser
-delegate = instance: doc_factory
+[word_piece_doc_caching_factory_stash]
+class_name = zensols.persist.DirectoryStash
+path = eval({'import': ['zensols.persist as p']}):
+  Path('${default:data_dir}/word-piece-doc',
+       p.FileTextUtil.normalize_text('${transformer_sent_fixed_resource:model_id}'))
+
+[word_piece_doc_caching_factory]
+class_name = zensols.deepnlp.transformer.CachingWordPieceFeatureDocumentFactory
+tokenizer = instance: ${deepnlp_default:wordpiece_embedding}_tokenizer
+embed_model = instance: ${deepnlp_default:wordpiece_embedding}_embedding
+stash = instance: word_piece_doc_caching_factory_stash
+
+[word_piece_doc_decorator]
+class_name = zensols.deepnlp.transformer.WordPieceDocumentDecorator
 word_piece_doc_factory = instance: word_piece_doc_factory
```

## zensols/deepnlp/transformer/__init__.py

```diff
@@ -36,8 +36,7 @@
 from .optimizer import *
 from .resource import *
 from .tokenizer import *
 from .embed import *
 from .vectorizers import *
 from .layer import *
 from .wordpiece import *
-
```

## zensols/deepnlp/transformer/resource.py

```diff
@@ -18,16 +18,16 @@
 from zensols.persist import persisted, PersistedWork, PersistableContainer
 from zensols.deeplearn import TorchConfig, DeepLearnError
 
 logger = logging.getLogger(__name__)
 
 
 class TransformerError(DeepLearnError):
-    """Raised for any transformer specific errors in this and child modules of the
-    parent.
+    """Raised for any transformer specific errors in this and child modules of
+    the parent.
 
     """
     pass
 
 
 @dataclass
 class TransformerResource(PersistableContainer, Dictable):
@@ -114,20 +114,17 @@
             if self.model_id.find('uncased') >= 0:
                 self.cased = False
             else:
                 logger.info("'cased' not given--assuming a cased model")
                 self.cased = True
         self._tokenizer = PersistedWork('_tokenzier', self, cache)
         self._model = PersistedWork('_model', self, cache)
-        if self.cache_dir is not None and not self.cache_dir.exists():
-            if logger.isEnabledFor(logging.DEBUG):
-                logger.info(f'creating cache directory: {self.cache_dir}')
-            self.cache_dir.mkdir(parents=True, exist_ok=True)
         if logger.isEnabledFor(logging.DEBUG):
-            logger.debug(f'id: {self.model_id}, cased: {self.cased}')
+            logger.debug(f'id: {self.model_id}, cased: {self.cased}, ' +
+                         f'cached: {self.cache}')
 
     @property
     def cached(self) -> bool:
         """If the model is cached.
 
         :see: :obj:`cache`
 
@@ -183,16 +180,14 @@
         params.update(self.args)
         params.update(self.model_args)
         if logger.isEnabledFor(logging.DEBUG):
             logger.debug(f'creating model using: {params}')
         with time(f'loaded model from pretrained {self.model_id}'):
             cls = self._create_model_class()
             model = cls.from_pretrained(self.model_id, **params)
-            #print('position types', model.embeddings.position_ids.dtype)
-            #print('token types', model.embedding.token_type_ids.dtype)
         # put the model in `evaluation` mode, meaning feed-forward operation.
         if self.trainable:
             logger.debug('model is trainable')
         else:
             logger.debug('turning off grad for non-trainable transformer')
             model.eval()
             for param in model.base_model.parameters():
```

## zensols/deepnlp/transformer/tokenizer.py

```diff
@@ -16,34 +16,30 @@
 from . import TransformerError, TokenizedFeatureDocument
 
 logger = logging.getLogger(__name__)
 
 
 @dataclass
 class TransformerDocumentTokenizer(PersistableContainer):
-    MAX_TOKEN_LENGTH: ClassVar[int] = 512
-    """The maximum token length to truncate before converting to IDs.  If this
-    isn't done, the following error is raised:
-
-      ``error: CUDA error: device-side assert triggered``
-
-    """
     resource: TransformerResource = field()
     """Contains the model used to create the tokenizer."""
 
-    word_piece_token_length: int = field(default=MAX_TOKEN_LENGTH)
+    word_piece_token_length: int = field(default=None)
     """The max number of word piece tokens.  The word piece length is always the
     same or greater in count than linguistic tokens because the word piece
     algorithm tokenizes on characters.
 
     If this value is less than 0, than do not fix sentence lengths.
 
     """
     def __post_init__(self):
         super().__init__()
+        if self.word_piece_token_length is None:
+            self.word_piece_token_length = \
+                self.resource.tokenizer.model_max_length
 
     @property
     @persisted('_id2tok')
     def id2tok(self) -> Dict[int, str]:
         vocab = self.resource.tokenizer.vocab
         return {vocab[k]: k for k in vocab.keys()}
 
@@ -66,33 +62,33 @@
 
     def _from_tokens(self, sents: List[List[str]], doc: FeatureDocument,
                      tokenizer_kwargs: Dict[str, Any] = None) -> \
             TokenizedFeatureDocument:
         torch_config: TorchConfig = self.resource.torch_config
         tlen: int = self.word_piece_token_length
         tokenizer: PreTrainedTokenizer = self.resource.tokenizer
-        params = {'return_offsets_mapping': True,
-                  'is_split_into_words': True,
-                  'return_special_tokens_mask': True}
+        params: Dict[str, bool] = {
+            'return_offsets_mapping': True,
+            'is_split_into_words': True,
+            'return_special_tokens_mask': True,
+            'padding': 'longest'}
 
         for i, sent in enumerate(sents):
             if len(sent) == 0:
                 raise TransformerError(
                     f'Sentence {i} is empty: can not tokenize')
 
         if logger.isEnabledFor(logging.DEBUG):
             logger.debug(f'parsing {sents} with token length: {tlen}')
 
         if tlen > 0:
-            params.update({'padding': 'max_length',
-                           'truncation': True,
+            params.update({'truncation': True,
                            'max_length': tlen})
         else:
-            params.update({'padding': 'longest',
-                           'truncation': False})
+            params.update({'truncation': False})
 
         if logger.isEnabledFor(logging.DEBUG):
             logger.debug(f'using tokenizer parameters: {params}')
 
         if tokenizer_kwargs is not None:
             params.update(tokenizer_kwargs)
         tok_dat: BatchEncoding = tokenizer(sents, **params)
```

## zensols/deepnlp/transformer/wordpiece.py

```diff
@@ -12,30 +12,34 @@
 
 """
 from __future__ import annotations
 __author__ = 'Paul Landes'
 from typing import Tuple, List, Dict, Any, Union, Iterable, ClassVar
 from dataclasses import dataclass, field
 from abc import ABCMeta
+import logging
 import sys
-from cachetools import LRUCache, cached
 from itertools import chain
 from io import TextIOBase
 import torch
 from torch import Tensor
-from zensols.persist import PersistableContainer
+from zensols.util import Hasher
+from zensols.persist import PersistableContainer, Stash
 from zensols.config import Dictable
 from zensols.nlp import (
     TokenContainer, FeatureToken, FeatureSentence, FeatureDocument,
-    FeatureDocumentParser,
+    FeatureDocumentDecorator,
 )
 from . import (
-    TokenizedFeatureDocument, TransformerDocumentTokenizer, TransformerEmbedding
+    TransformerError, TokenizedFeatureDocument, TransformerDocumentTokenizer,
+    TransformerEmbedding,
 )
 
+logger = logging.getLogger(__name__)
+
 
 @dataclass(repr=False)
 class WordPiece(PersistableContainer, Dictable):
     """The word piece data.
 
     """
     UNKNOWN_TOKEN: ClassVar[str] = '[UNK]'
@@ -253,23 +257,16 @@
     """Whether to add :class:`.WordPieceFeatureToken.embeddings`.
 
     """
     sent_embeddings: bool = field(default=True)
     """Whether to add class:`.WordPieceFeatureSentence.embeddings`.
 
     """
-    cache_size: int = field(default=0)
-    """If higher than zero, the number word piece documents LRU cached."""
-
     def __post_init__(self):
-        if self.cache_size > 0:
-            self.create = cached(
-                LRUCache(maxsize=self.cache_size),
-                key=lambda doc, tdoc: _WordPieceDocKey(doc, tdoc),
-            )(self.create)
+        FeatureToken.SKIP_COMPARE_FEATURE_IDS.add('embedding')
 
     def add_token_embeddings(self, doc: WordPieceFeatureDocument, arr: Tensor):
         """Add token embeddings to the sentences of ``doc``.  This assumes
         tokens are of type :class:`.WordPieceFeatureToken` since the token
         indices are needed.
 
         :param doc: sentences of this doc have ``embeddings`` set to the
@@ -314,14 +311,16 @@
 
         """
         def map_tok(ftok: FeatureToken, wps: Tuple[str, int, int]) -> \
                 WordPieceFeatureToken:
             words = tuple(map(lambda t: WordPiece(*t), wps))
             return ftok.clone(cls=WordPieceFeatureToken, words=words)
 
+        if logger.isEnabledFor(logging.DEBUG):
+            logger.debug(f'creating embeddings for: {fdoc}')
         tdoc = self.tokenizer.tokenize(fdoc) if tdoc is None else tdoc
         sents: List[WordPieceFeatureSentence] = []
         wps: List[Dict[str, Any]] = tdoc.map_to_word_pieces(
             sentences=fdoc,
             map_wp=self.tokenizer.id2tok,
             add_indices=True)
         wp: Dict[str, Any]
@@ -351,30 +350,59 @@
     def __call__(self, fdoc: FeatureDocument,
                  tdoc: TokenizedFeatureDocument = None) -> \
             WordPieceFeatureDocument:
         return self.create(fdoc, tdoc)
 
 
 @dataclass
-class WordPieceFeatureDocumentParser(FeatureDocumentParser):
-    """A document parser that adds word piece embeddings using
-    :class:`.WordPieceFeatureDocumentFactory`.  It does this by first using
-    :obj:`delegate` to parse the document, then uses
-    :obj:`word_piece_document_factory` to create the embeddings.  Finally, the
-    embeddings are copied to the original parsed document.
-
-    This is useful when there is some feature document structure that already
-    inherits from :class:`~zensols.nlp.container.FeatureDocument` and you want
-    to keep it intact.
+class CachingWordPieceFeatureDocumentFactory(WordPieceFeatureDocumentFactory):
+    """Caches the documents and their embeddings in a
+    :class:`~zensols.persist.stash.Stash`.  For those that are cached, the
+    embeddings are copied over to the passed document in :meth:`create`.
+
+    """
+    stash: Stash = field(default=None)
+    """The stash that persists the feature document instances.  If this is not
+    provided, no caching will happen.
 
     """
-    delegate: FeatureDocumentParser = field()
-    """The delegate that parses text in to feature documents."""
+    hasher: Hasher = field(default_factory=Hasher)
+    """Used to hash the natural langauge text in to string keys."""
 
+    def _hash_text(self, text: str) -> str:
+        self.hasher.reset()
+        self.hasher.update(text)
+        return self.hasher()
+
+    def create(self, fdoc: FeatureDocument,
+               tdoc: TokenizedFeatureDocument = None) -> \
+            WordPieceFeatureDocument:
+        key: str = self._hash_text(fdoc.text)
+        wdoc: WordPieceFeatureDocument = self.stash.load(key)
+        if wdoc is None:
+            wdoc = super().create(fdoc, tdoc)
+            if self.stash is not None:
+                self.stash.dump(key, wdoc)
+        else:
+            if wdoc.text != fdoc.text:
+                raise TransformerError('Document text does not match: ' +
+                                       f'<{wdoc.text}> != >{fdoc.text}>')
+        return wdoc
+
+    def clear(self):
+        """Clear the caching stash."""
+        self.stash.clear()
+
+
+@dataclass
+class WordPieceDocumentDecorator(FeatureDocumentDecorator):
+    """Populates sentence and token embeddings in the documents.
+
+    :see: :class:`.WordPieceFeatureDocumentFactory`
+
+    """
     word_piece_doc_factory: WordPieceFeatureDocumentFactory = field()
     """The feature document factory that populates embeddings."""
 
-    def parse(self, text: str, *args, **kwargs) -> FeatureDocument:
-        doc: FeatureDocument = self.delegate.parse(text, *args, **kwargs)
+    def decorate(self, doc: FeatureDocument):
         wpdoc: WordPieceFeatureDocument = self.word_piece_doc_factory(doc)
         wpdoc.copy_embedding(doc)
-        return doc
```

## Comparing `zensols.deepnlp-1.8.0.dist-info/METADATA` & `zensols.deepnlp-1.9.0.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,27 +1,27 @@
 Metadata-Version: 2.1
 Name: zensols.deepnlp
-Version: 1.8.0
+Version: 1.9.0
 Summary: Deep learning utility library for natural language processing that aids in feature engineering and embedding layers.
 Home-page: https://github.com/plandes/deepnlp
-Download-URL: https://github.com/plandes/deepnlp/releases/download/v1.8.0/zensols.deepnlp-1.8.0-py3-none-any.whl
+Download-URL: https://github.com/plandes/deepnlp/releases/download/v1.9.0/zensols.deepnlp-1.9.0-py3-none-any.whl
 Author: Paul Landes
 Author-email: landes@mailc.net
 Keywords: tooling
 Description-Content-Type: text/markdown
 Requires-Dist: cachetools (~=5.2.0)
-Requires-Dist: gensim (~=4.1.2)
+Requires-Dist: gensim (~=4.3.1)
 Requires-Dist: h5py (>=3.3.0)
 Requires-Dist: transformers (~=4.19.4)
 Requires-Dist: huggingface-hub
 Requires-Dist: sentence-transformers (~=2.2.2)
 Requires-Dist: sentencepiece (~=0.1.97)
 Requires-Dist: protobuf (~=3.20.1)
-Requires-Dist: zensols.nlp (~=1.6.0)
-Requires-Dist: zensols.deeplearn (~=1.6.0)
+Requires-Dist: zensols.nlp (~=1.7.0)
+Requires-Dist: zensols.deeplearn (~=1.7.0)
 
 # DeepZensols Natural Language Processing
 
 [![PyPI][pypi-badge]][pypi-link]
 [![Python 3.9][python39-badge]][python39-link]
 [![Python 3.10][python310-badge]][python310-link]
 [![Build Status][build-badge]][build-link]
```

## Comparing `zensols.deepnlp-1.8.0.dist-info/RECORD` & `zensols.deepnlp-1.9.0.dist-info/RECORD`

 * *Files 8% similar despite different names*

```diff
@@ -1,68 +1,73 @@
 zensols/deepnlp/__init__.py,sha256=g_ohneJmr4rr5ycvuMHpVnJXGFrfJus6UvVqZAco3ig,503
 zensols/deepnlp/batch/__init__.py,sha256=TCHeQCNvWQtbMMsh_9nJ_DWubTP5f_UB9L_HSFac7b4,22
 zensols/deepnlp/batch/domain.py,sha256=uYgZuHDY08PCKms8_D3VMxK7l8MPR06-1q-mWxcqFHU,1899
 zensols/deepnlp/classify/__init__.py,sha256=JkQdM4b1QUEQLJtmyyPRvmEY57Lv62yb3WXn16YIDcU,85
 zensols/deepnlp/classify/domain.py,sha256=nty6WHorbPkKh_7-wit0lccnP5Vm32GsQ1WIfj7bwwY,6934
-zensols/deepnlp/classify/facade.py,sha256=hDDwAgtXX0B4yaQq7Dyz4PrWLDdT5ouUt9fS3BLMz4g,4057
+zensols/deepnlp/classify/facade.py,sha256=0017PET1NhfH18pup4RaFDSS53Afa6-9L7H5YrqJEWw,4101
 zensols/deepnlp/classify/model.py,sha256=cQfzXWCE274pdU_dGx42eW2lSybH61z8Bn1J40FSor8,3625
-zensols/deepnlp/classify/pred.py,sha256=KQ7qpgb8gn4t2UGleZyI7fJ88DJleEwxjlpyRlzCYow,4079
+zensols/deepnlp/classify/pred.py,sha256=LdBrVrQPDgDlKBQpeTKqCgCsIarFLc0hbXFZOK6FdCc,4286
 zensols/deepnlp/cli/__init__.py,sha256=K3UdhLC275kO67S3w-Y3kweoAbrk9EegwNaCRpAx7d0,19
 zensols/deepnlp/cli/app.py,sha256=F2m_ky0_Ri03VnhT_x14F2sqU8mrZN_B5Z6_SpsZuGw,5076
-zensols/deepnlp/embed/__init__.py,sha256=TMXICZKDs093ow34i3_0PigeXcyU3yAXixlY3nkOc7U,115
-zensols/deepnlp/embed/domain.py,sha256=7MG430E2jvRYSpY3mcDYbXZtrPEsBlq8Awa-2sNZgKM,7981
+zensols/deepnlp/embed/__init__.py,sha256=FTewSS_d6FiZvdL5X3xhjo20FDtB0HqG_KXCGymWvy4,134
+zensols/deepnlp/embed/doc.py,sha256=7VviKNBhKIvAn_BqUC_SC_Ju62Ciws9_-6i9upRsSoM,3372
+zensols/deepnlp/embed/domain.py,sha256=xOHXvfgrDhTxmlhH9STfiYVds6L7D6bfOOpRlfMrVcA,8650
 zensols/deepnlp/embed/fasttext.py,sha256=c71pe6dQW88XUgjQHLmCgaQYS4dYiLQnpNU4Y0uQjnQ,2281
 zensols/deepnlp/embed/glove.py,sha256=6k0W-8M6t5vb6yPa8qcZN0D981kKEXlSO147saTL0XU,1569
 zensols/deepnlp/embed/word2vec.py,sha256=oEYh_M4Al9HSqtyVAZzXoyso5_q9jfs1Ib5vmsNV4xY,3703
 zensols/deepnlp/embed/wordtext.py,sha256=CDgD8SgxpBCP-WcGenDE8lTQ73xz-TFdLfzwcQLnAC8,9385
 zensols/deepnlp/feature/__init__.py,sha256=KZ05jYtiTuvC-jhKEdssew4mQ_o0By6Unk4AVImS5q0,21
 zensols/deepnlp/feature/stash.py,sha256=uGfG0AGwYNNdovQe_pBimTMNSxOsNrDiyEzgUN9ziPs,2674
 zensols/deepnlp/index/__init__.py,sha256=IAz5v2t81BSVR76TrZ-6qRrnyJUUyJDPlrGyhwTbsa4,124
-zensols/deepnlp/index/domain.py,sha256=_g48xe9HgCUexe0ANXrqJ7GFxeNUE1epX49LZjZTo7o,3935
+zensols/deepnlp/index/domain.py,sha256=1LOHFPlkUabBSC6QuvCM70jigq9h4v5bgm7JE8XMtIY,4001
 zensols/deepnlp/index/lda.py,sha256=IxyNlAUfO08HEg4QTsYnoXnzbGFAF4FYr9tEzB16q_s,3733
-zensols/deepnlp/index/lsi.py,sha256=wQCQqEzebgtUPwO_eNCVj6fhI9ClL_GZmRgUC5Es-cE,3546
+zensols/deepnlp/index/lsi.py,sha256=4gga5L94844ZPCrdjElbpClfnprY_KoxG-WBmKjrvCg,4663
 zensols/deepnlp/layer/__init__.py,sha256=WQmSOOPVfl_EVwplJc2nQyzdifXhUePzyIwz6TfXF-o,148
 zensols/deepnlp/layer/conv.py,sha256=xdl7plOm3tVumWxYxAfgPoAK2RjLoE1lflRJBuzIJjo,10663
 zensols/deepnlp/layer/embed.py,sha256=J8ECiG7GE0Qj-vEnxA0t_6zvMACJo7qco49h9foaFA0,16971
 zensols/deepnlp/layer/embrecurcrf.py,sha256=aqk_LVoLCS8bQQzr7PhePankVa56B8r3mQT0ZAxzPPA,8717
 zensols/deepnlp/layer/wordvec.py,sha256=zbxX6nkBR9ZJU0ovBEHnAX_ojFKaR4AMexazaVZRnvE,2449
 zensols/deepnlp/model/__init__.py,sha256=K3MRUib6cEmtxFU5oOf13binFVw_zCkzKtDqpxgv43I,46
 zensols/deepnlp/model/facade.py,sha256=U9NEYLC4kqwoMngJ5Uo33bZytCrs5aP-IdYAHfM7QKA,12956
 zensols/deepnlp/model/sequence.py,sha256=_47GJTe197FiAEEDYfrSr1CDL5bCfYFLxYAbxTsktz4,7916
 zensols/deepnlp/resources/bigbird.conf,sha256=wIC7klg9mfFZdeJ0wZVua57h9zU8lOzzlCPjJp3hXtg,1980
 zensols/deepnlp/resources/classify-batch.yml,sha256=uUN4aMj01noMhCb6FE1vMrkID9CX-xT9GnxfVAEmP60,469
 zensols/deepnlp/resources/classify.conf,sha256=-p-D1uMv_NPL87bXq550ksgm5X6O1csjDCTbRqLkGTc,3468
 zensols/deepnlp/resources/cleaner.conf,sha256=OlWdbpV6v_7kPEnpW6CCeQwm_FZHYIWkYlCDK34wYHA,286
 zensols/deepnlp/resources/cli.conf,sha256=7nQcC8_sYMI8fFrhqZHT3n3Zy51ngjZMkW9Jm_Uoe30,649
-zensols/deepnlp/resources/default.conf,sha256=lPRAaKdwV6PqMqmiGumpJQIjfa7ZBXEaFOuAiyyWX3E,765
+zensols/deepnlp/resources/default.conf,sha256=EKtvAA0KcFLgm-txvh47UzhdDDAP007zXkiIDhvlFkM,867
 zensols/deepnlp/resources/embed-crf.conf,sha256=5p4ZJNPL1hyoF1vYYB1LniBC-86DuCH2wCuz3sMbM9E,1560
 zensols/deepnlp/resources/fasttext.conf,sha256=2VAVDIRpbSCQHcaFqXa6Z68mxVlRNoZDAdZsSE6Iskc,2631
 zensols/deepnlp/resources/feature.conf,sha256=SgrRL0RpvnvuoRusmTFyxNW_fmyQNPR8grfJMW9AhZA,2505
 zensols/deepnlp/resources/glove.conf,sha256=iRmM4qBWaSPPzoqwYvme-zk72xgDS1NP1eqAsop8N-8,2528
 zensols/deepnlp/resources/lang-batch.yml,sha256=v5xUozQ7W3wifB77Co9_gOj-YFLlhh7Bt0lUdGW41vo,1840
 zensols/deepnlp/resources/obj.conf,sha256=sMeIszQU9ghki_7F0Bxov5fgnmAVLX_I-dJcM6RMEvQ,610
+zensols/deepnlp/resources/score.yml,sha256=Iuebx5ofjsEWq67_08dq3zfrAKgNAK9VgG2si4W-L8M,212
 zensols/deepnlp/resources/token-classify.yml,sha256=KoqC0U1Q0QaqllQ5F0mzFqkDN-qEN-MUYokdy5sZ7zw,2195
 zensols/deepnlp/resources/transformer-expander.conf,sha256=RGvqGyQhqD0rlmtVKYYkk--P5mjjkRgblj4xp-r8wtg,2124
 zensols/deepnlp/resources/transformer-expander.yml,sha256=akI6DeF4WcmSsJmOCTGT-CWnxjRvhwrNIjoBaPD1EiU,466
-zensols/deepnlp/resources/transformer-sent.conf,sha256=5ehhOqgs2kEfReGhv_bi8TkvsWGbQAyhvyatL0l2yvI,4917
-zensols/deepnlp/resources/transformer.conf,sha256=kHbwtAH0fMLn_ezOkJWtfzei87ANg-p9Y-VGWkl6Gy0,4427
+zensols/deepnlp/resources/transformer-sent.conf,sha256=qZS0JCR6omUfHMBJXyQ4hUo2pic7QVIiJPwtyHTsDLM,4999
+zensols/deepnlp/resources/transformer.conf,sha256=la8N6q4FNBGtZPReD-imP28C-WLEfLwEIBI8D3-uuYM,4509
 zensols/deepnlp/resources/vectorizer.conf,sha256=jdUBjf1rObFQQiK8obzO__wkOfggTQprHXJsfdv4n8M,2804
 zensols/deepnlp/resources/word2vec.conf,sha256=SOGdc-vGRl7CDEDrx_tdxDpFm_OtNZlnnYQ1QQIq0Mw,1488
-zensols/deepnlp/resources/wordpiece.conf,sha256=PGHfZH5iO487_GQX0JvIXetQ0Uypo5fRtgR3jf18N0o,482
-zensols/deepnlp/transformer/__init__.py,sha256=D73FN5CKhq75h6dldLHV5Eh1iTajq5osAKAXAtxip8E,1085
+zensols/deepnlp/resources/wordembed.conf,sha256=tLUa8CaRdwSuvRzgh3O4y31ly-G53lmit1eU40gd-fc,215
+zensols/deepnlp/resources/wordpiece.conf,sha256=mv0FYf3RFtqRDrie6KncNHi1BXWI9uwJqs8YAfd9fvI,1017
+zensols/deepnlp/score/__init__.py,sha256=uxuQoXk0PzOKrKcEFctUeB8c0S74zgkQA6PTq1puK6w,25
+zensols/deepnlp/score/bertscore.py,sha256=IaTHMn6O7ITlkGI3I7Gm_lFCEZyl13fzsMEKWKJ5ocA,2259
+zensols/deepnlp/transformer/__init__.py,sha256=swF-bshyUJWwZeG3o7wfJh_3EKa0JSjPib_nYkhJx3s,1084
 zensols/deepnlp/transformer/domain.py,sha256=F6nhpqpztM35i8u8XFp24PSb7QLK-pwgzuQwj9T98NI,10621
 zensols/deepnlp/transformer/embed.py,sha256=8GwWA-f7stYEXgbfNsOVHF2r8QRCUww0Ax0l01iCZ3I,8493
 zensols/deepnlp/transformer/layer.py,sha256=ff9n2xsuhPSrXcm3XKvBuB_w7yxrflUBMtEweFKgWSM,9254
 zensols/deepnlp/transformer/optimizer.py,sha256=UWHNQ80OX5oKEnqvGQtwjdYSpe_KFiqtfQxDHAQYIBM,3914
-zensols/deepnlp/transformer/resource.py,sha256=82B3Lm1ig_YhfZIR8IYdjEzXT5GibnMnU-Gf7cRcD7g,8297
-zensols/deepnlp/transformer/tokenizer.py,sha256=O4gq1v2svpUr4WV_Q8l4BhFYWjraIKeeKWIG7i9MAxw,5497
+zensols/deepnlp/transformer/resource.py,sha256=Dmgv8a18EwphpMNDDy0fIkrQ3mf4kkco9-HEXTQDoms,7945
+zensols/deepnlp/transformer/tokenizer.py,sha256=Pq2vHyufY4SIgagYw0kgSz7I515KeggiOi9hrYv4YGE,5351
 zensols/deepnlp/transformer/vectorizers.py,sha256=nSK14l-HojHA65hb9hwbEfZm4ETxTS5qQ9pNGHkiIqo,20736
-zensols/deepnlp/transformer/wordpiece.py,sha256=aLL1BBI4btSnKsnKwKzxs5eYs5kyYQDpYGCy2Jrv52k,13722
+zensols/deepnlp/transformer/wordpiece.py,sha256=NGooNFhUBspK6ddSTxNM3Zme8mR2K7901eQ05l2OAm8,14458
 zensols/deepnlp/vectorize/__init__.py,sha256=HseSSWRP9TKh2wjoUqVmrd-LDzUAJzQv2jqe4BL197c,172
 zensols/deepnlp/vectorize/embed.py,sha256=yPTROX-n5GOj5vsGcVxe1qvDAnWdAFLjgSJGjG9f90s,5656
 zensols/deepnlp/vectorize/manager.py,sha256=ZxYYXFG3u8LyqmXqdlf36OUP34kTjfdLI5G8CJjs5Ek,18836
 zensols/deepnlp/vectorize/spacy.py,sha256=gSpxk5jaZK7cGZOnDqX_PbIUXTRXicD70UoxlOoEypQ,6267
 zensols/deepnlp/vectorize/vectorizers.py,sha256=Jevgao8P0gtaFviBguPQiRCF8--t2q-Bzm2GjVI0-g4,29843
-zensols.deepnlp-1.8.0.dist-info/METADATA,sha256=L3eC6kW2Wvb2-3uq-_s37PpdnoAjWPWtWSBkC-_nD2A,8071
-zensols.deepnlp-1.8.0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-zensols.deepnlp-1.8.0.dist-info/top_level.txt,sha256=dz9R2vn_cJoPt2tH08mu2QAd4pyWPkFTl66lxe2Q4TI,16
-zensols.deepnlp-1.8.0.dist-info/RECORD,,
+zensols.deepnlp-1.9.0.dist-info/METADATA,sha256=DpPf_riF4hnpoEo3IGlBWlITJ3MnQVIwv533rD3L3Ns,8071
+zensols.deepnlp-1.9.0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+zensols.deepnlp-1.9.0.dist-info/top_level.txt,sha256=dz9R2vn_cJoPt2tH08mu2QAd4pyWPkFTl66lxe2Q4TI,16
+zensols.deepnlp-1.9.0.dist-info/RECORD,,
```

