# Comparing `tmp/habu_databricks_cli-1.1.0-py3-none-any.whl.zip` & `tmp/habu_databricks_cli-1.1.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,21 +1,21 @@
-Zip file size: 15249 bytes, number of entries: 19
--rw-r--r--  2.0 unx     3764 b- defN 23-Jun-01 19:24 README.md
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-01 19:24 hdb/__init__.py
--rw-r--r--  2.0 unx     5989 b- defN 23-Jun-01 19:24 hdb/cli.py
--rw-r--r--  2.0 unx     5623 b- defN 23-Jun-01 19:24 hdb/cluster.py
--rw-r--r--  2.0 unx     3478 b- defN 23-Jun-01 19:24 hdb/config.py
--rw-r--r--  2.0 unx     3638 b- defN 23-Jun-01 19:24 hdb/init.py
--rw-r--r--  2.0 unx     5348 b- defN 23-Jun-01 19:24 hdb/job.py
--rw-r--r--  2.0 unx      854 b- defN 23-Jun-01 19:24 hdb/util.py
--rw-r--r--  2.0 unx     3246 b- defN 23-Jun-01 19:24 hdb/cli_installer/cli-installer.sql
--rw-r--r--  2.0 unx     3721 b- defN 23-Jun-01 19:24 hdb/commands/cleanroom-request.sql
--rw-r--r--  2.0 unx      277 b- defN 23-Jun-01 19:24 hdb/commands/create-cleanroom.sql
--rw-r--r--  2.0 unx      912 b- defN 23-Jun-01 19:24 hdb/commands/create-dataset.sql
--rw-r--r--  2.0 unx     1883 b- defN 23-Jun-01 19:24 hdb/commands/new-data-connection.sql
--rw-r--r--  2.0 unx      594 b- defN 23-Jun-01 19:24 hdb/resource/config.yaml
--rw-r--r--  2.0 unx      388 b- defN 23-Jun-01 19:25 habu_databricks_cli-1.1.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jun-01 19:25 habu_databricks_cli-1.1.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       40 b- defN 23-Jun-01 19:25 habu_databricks_cli-1.1.0.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        4 b- defN 23-Jun-01 19:25 habu_databricks_cli-1.1.0.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     1528 b- defN 23-Jun-01 19:25 habu_databricks_cli-1.1.0.dist-info/RECORD
-19 files, 41379 bytes uncompressed, 12755 bytes compressed:  69.2%
+Zip file size: 15324 bytes, number of entries: 19
+-rw-r--r--  2.0 unx     3764 b- defN 23-Jun-09 18:57 README.md
+-rw-r--r--  2.0 unx        0 b- defN 23-Jun-09 18:57 hdb/__init__.py
+-rw-r--r--  2.0 unx     6112 b- defN 23-Jun-09 18:57 hdb/cli.py
+-rw-r--r--  2.0 unx     5623 b- defN 23-Jun-09 18:57 hdb/cluster.py
+-rw-r--r--  2.0 unx     3478 b- defN 23-Jun-09 18:57 hdb/config.py
+-rw-r--r--  2.0 unx     3638 b- defN 23-Jun-09 18:57 hdb/init.py
+-rw-r--r--  2.0 unx     5348 b- defN 23-Jun-09 18:57 hdb/job.py
+-rw-r--r--  2.0 unx      854 b- defN 23-Jun-09 18:57 hdb/util.py
+-rw-r--r--  2.0 unx     3246 b- defN 23-Jun-09 18:57 hdb/cli_installer/cli-installer.sql
+-rw-r--r--  2.0 unx     3721 b- defN 23-Jun-09 18:57 hdb/commands/cleanroom-request.sql
+-rw-r--r--  2.0 unx      277 b- defN 23-Jun-09 18:57 hdb/commands/create-cleanroom.sql
+-rw-r--r--  2.0 unx      912 b- defN 23-Jun-09 18:57 hdb/commands/create-dataset.sql
+-rw-r--r--  2.0 unx     1883 b- defN 23-Jun-09 18:57 hdb/commands/new-data-connection.sql
+-rw-r--r--  2.0 unx      594 b- defN 23-Jun-09 18:57 hdb/resource/config.yaml
+-rw-r--r--  2.0 unx      388 b- defN 23-Jun-09 18:57 habu_databricks_cli-1.1.1.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jun-09 18:57 habu_databricks_cli-1.1.1.dist-info/WHEEL
+-rw-r--r--  2.0 unx       40 b- defN 23-Jun-09 18:57 habu_databricks_cli-1.1.1.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx        4 b- defN 23-Jun-09 18:57 habu_databricks_cli-1.1.1.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     1528 b- defN 23-Jun-09 18:57 habu_databricks_cli-1.1.1.dist-info/RECORD
+19 files, 41502 bytes uncompressed, 12830 bytes compressed:  69.1%
```

## zipnote {}

```diff
@@ -36,23 +36,23 @@
 
 Filename: hdb/commands/new-data-connection.sql
 Comment: 
 
 Filename: hdb/resource/config.yaml
 Comment: 
 
-Filename: habu_databricks_cli-1.1.0.dist-info/METADATA
+Filename: habu_databricks_cli-1.1.1.dist-info/METADATA
 Comment: 
 
-Filename: habu_databricks_cli-1.1.0.dist-info/WHEEL
+Filename: habu_databricks_cli-1.1.1.dist-info/WHEEL
 Comment: 
 
-Filename: habu_databricks_cli-1.1.0.dist-info/entry_points.txt
+Filename: habu_databricks_cli-1.1.1.dist-info/entry_points.txt
 Comment: 
 
-Filename: habu_databricks_cli-1.1.0.dist-info/top_level.txt
+Filename: habu_databricks_cli-1.1.1.dist-info/top_level.txt
 Comment: 
 
-Filename: habu_databricks_cli-1.1.0.dist-info/RECORD
+Filename: habu_databricks_cli-1.1.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## hdb/cli.py

```diff
@@ -38,25 +38,26 @@
 @cli.command()
 @click.option('--config-file', '-c', default='habu_databricks_config.yaml',
               help='File name where config will get saved (default is habu_databricks_config.yaml in current directory)')
 @click.option('--databricks-instance', '-i', prompt='Databricks instance id',
               help=' Databricks instance id (<instance-id>.cloud.databricks.com)')
 @click.option('--login', '-u', prompt="Databricks user name",
               help='Databricks user name (me@example.com)')
-@click.password_option('--token', '-t', prompt='Token',
+@click.password_option('--token', '-t', prompt='Token', confirmation_prompt=False,
                        help='Enter the token generated from Databricks user settings')
 @click.option('--log-level', '-l', type=click.Choice(['CRITICAL', 'ERROR', 'WARNING', 'INFO', 'DEBUG'],
                                                      case_sensitive=False), default='INFO',
               help='Select log level while running the commands, default level is set to INFO')
 @click.option('--auto-termination-mins', '-m', default=10, type=click.UNPROCESSED, callback=validate_mins,
               help='Automatically terminate the cluster after it is inactive for this time in minutes.'
                    ' If not set, the cluster will not be automatically terminated.'
                    'The threshold must be between 10 and 10000 minutes. You can also set this value to 0 '
                    'to explicitly disable automatic termination.')
-def config(config_file: str, databricks_instance: str, login: str, token: str, log_level: str, auto_termination_mins: int):
+def config(config_file: str, databricks_instance: str, login: str, token: str, log_level: str,
+           auto_termination_mins: int):
     """
     Command to generate the config for the habu databricks agent.
     The config is required to set up the habu databricks framework
     """
     setup_log(getattr(logging, log_level.upper()))
     config_params = hdb_config.create_config(config_file, databricks_instance, login, token, auto_termination_mins)
     if config_params is not None:
@@ -83,17 +84,18 @@
             print(node)
     else:
         logging.error(f'Failed to read config from {config_file}, use config cmd to generate config')
 
 
 @cli.command()
 @click.option('--org-id', '-o', prompt="Organization id", help='Habu Org Id')
-@click.option('--habu-sharing-id', '-s', prompt="Habu Sharing Id", help='Habu Sharing Id (aws:<region>:<id>)')
+@click.option('--habu-sharing-id', '-s', prompt="Habu Sharing Id", help='Habu Sharing Id (aws:<region>:<id>)',
+              default='aws:us-west-2:23eb138f-f773-4922-9805-53c286bbd24d')
 @click.option('--orchestrator', '-oc', prompt="Orchestrator Name", help='Orchestrator name',
-              default='habu-stage-orchestrator')
+              default='habudbprodorchestrator_metastore')
 @click.option('--config-file', '-c', default="./habu_databricks_config.yaml",
               help='Databricks Configuration file')
 @click.option('--node-type', '-n', prompt='cluster node type',
               help='Choose cluster node type (case sensitive), '
                    'use list-nodes command to find acceptable node-type values')
 @click.option('--log-level', '-l', type=click.Choice(['CRITICAL', 'ERROR', 'WARNING', 'INFO', 'DEBUG'],
                                                      case_sensitive=False), default='INFO',
```

## Comparing `habu_databricks_cli-1.1.0.dist-info/RECORD` & `habu_databricks_cli-1.1.1.dist-info/RECORD`

 * *Files 20% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 README.md,sha256=fqeBg96AMadrv9ff8Z8Z1W3ea3K-b3A6gWbRVDPMbbI,3764
 hdb/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-hdb/cli.py,sha256=hWIvHAmNCN19_051NPf-xTlpd3VHn3Nvkb6b8y1qdqk,5989
+hdb/cli.py,sha256=P0Nb5u5gVoTZPz49Impnq4809zJ6vxrXwY7c77ioUuQ,6112
 hdb/cluster.py,sha256=XvXK-gzvvw0QyNull4-4ZyS0iIFS-kGFvaFjnZARZi4,5623
 hdb/config.py,sha256=nAlDhoqb5mftGeBYB28ujeRMtHKd3dC4TkvMcz3QYTI,3478
 hdb/init.py,sha256=0IW_u9BvG0UMeS7UG4ph6jIS25BD2f4cZpIMUY-sc4k,3638
 hdb/job.py,sha256=FJvs3PCQ_wM5yjzYmBoy9HBhiuCcZnUsaYgitUcNNkk,5348
 hdb/util.py,sha256=tYfGJp_NA8kBfO0LqoA11JObYOPzsuxrKCjqpDFvFEU,854
 hdb/cli_installer/cli-installer.sql,sha256=qSuja0hew3YoyS8grN9eZKFtEtSvKqgjB7qxbozAuHo,3246
 hdb/commands/cleanroom-request.sql,sha256=iyUZ5OW2VZYfbU60dszHTsdme5HycZZoT7gMHd41Zio,3721
 hdb/commands/create-cleanroom.sql,sha256=NcadqD1wEGejr8G2Wqd7ajukzVbdZn588JA8JbU2Lps,277
 hdb/commands/create-dataset.sql,sha256=DgtagXn7eLCQkJxgGdUKo36tvTHVJi_6HZRQ-EHwzZM,912
 hdb/commands/new-data-connection.sql,sha256=SlV8qwK7cIfMBE07bOw-q-3AhJxlIV9676zRYDpY0As,1883
 hdb/resource/config.yaml,sha256=8CMoKN7sqljcw0oWI15U6yb2hCn8u1ItftVgdk9rBPY,594
-habu_databricks_cli-1.1.0.dist-info/METADATA,sha256=Pu3mci7kuTRG2qagZK73yeaqO9KzkiQgEDVt5oaRiEg,388
-habu_databricks_cli-1.1.0.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-habu_databricks_cli-1.1.0.dist-info/entry_points.txt,sha256=ncoJOHelyx6usEFcFZ-lmQC5N0TCmPdFQ0jodLc2Z3E,40
-habu_databricks_cli-1.1.0.dist-info/top_level.txt,sha256=G1s1EaggMBt7hVcx2WCCSPJFZ4_mpLsMEENCmnDbE34,4
-habu_databricks_cli-1.1.0.dist-info/RECORD,,
+habu_databricks_cli-1.1.1.dist-info/METADATA,sha256=TeMC3QPKlSJcVzvq1w3XSE0D5ApX1JwNSayXtlm2bNY,388
+habu_databricks_cli-1.1.1.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+habu_databricks_cli-1.1.1.dist-info/entry_points.txt,sha256=ncoJOHelyx6usEFcFZ-lmQC5N0TCmPdFQ0jodLc2Z3E,40
+habu_databricks_cli-1.1.1.dist-info/top_level.txt,sha256=G1s1EaggMBt7hVcx2WCCSPJFZ4_mpLsMEENCmnDbE34,4
+habu_databricks_cli-1.1.1.dist-info/RECORD,,
```

