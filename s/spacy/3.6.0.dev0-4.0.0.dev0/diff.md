# Comparing `tmp/spacy-3.6.0.dev0.tar.gz` & `tmp/spacy-4.0.0.dev0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "spacy-3.6.0.dev0.tar", last modified: Mon May  8 15:54:19 2023, max compression
+gzip compressed data, was "spacy-4.0.0.dev0.tar", last modified: Thu Jan 19 08:47:20 2023, max compression
```

## Comparing `spacy-3.6.0.dev0.tar` & `spacy-4.0.0.dev0.tar`

### file list

```diff
@@ -1,1105 +1,1102 @@
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.343003 spacy-3.6.0.dev0/
--rw-r--r--   0 vsts      (1001) docker     (122)     1128 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/LICENSE
--rw-r--r--   0 vsts      (1001) docker     (122)      247 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/MANIFEST.in
--rw-r--r--   0 vsts      (1001) docker     (122)    22567 2023-05-08 15:54:19.343003 spacy-3.6.0.dev0/PKG-INFO
--rw-r--r--   0 vsts      (1001) docker     (122)    20725 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/README.md
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.199005 spacy-3.6.0.dev0/licenses/
--rw-r--r--   0 vsts      (1001) docker     (122)     6541 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/licenses/3rd_party_licenses.txt
--rw-r--r--   0 vsts      (1001) docker     (122)      250 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/pyproject.toml
--rw-r--r--   0 vsts      (1001) docker     (122)     3383 2023-05-08 15:54:19.343003 spacy-3.6.0.dev0/setup.cfg
--rwxr-xr-x   0 vsts      (1001) docker     (122)     7664 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/setup.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.203004 spacy-3.6.0.dev0/spacy/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/__init__.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)     2954 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)       80 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/__main__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      326 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/about.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1155 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/attrs.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)     5748 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/attrs.pyx
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.207004 spacy-3.6.0.dev0/spacy/cli/
--rw-r--r--   0 vsts      (1001) docker     (122)     1988 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/cli/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)    24341 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/cli/_util.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4845 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/cli/apply.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2574 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/cli/assemble.py
--rw-r--r--   0 vsts      (1001) docker     (122)     5217 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/cli/benchmark_speed.py
--rw-r--r--   0 vsts      (1001) docker     (122)     9401 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/cli/convert.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4546 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/cli/debug_config.py
--rw-r--r--   0 vsts      (1001) docker     (122)    50116 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/cli/debug_data.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3580 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/cli/debug_diff.py
--rw-r--r--   0 vsts      (1001) docker     (122)     8909 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/cli/debug_model.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4665 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/cli/download.py
--rw-r--r--   0 vsts      (1001) docker     (122)     8559 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/cli/evaluate.py
--rw-r--r--   0 vsts      (1001) docker     (122)     9380 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/cli/find_threshold.py
--rw-r--r--   0 vsts      (1001) docker     (122)     6343 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/cli/info.py
--rw-r--r--   0 vsts      (1001) docker     (122)    10376 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/cli/init_config.py
--rw-r--r--   0 vsts      (1001) docker     (122)     5644 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/cli/init_pipeline.py
--rw-r--r--   0 vsts      (1001) docker     (122)    21385 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/cli/package.py
--rw-r--r--   0 vsts      (1001) docker     (122)     5260 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/cli/pretrain.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3452 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/cli/profile.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.207004 spacy-3.6.0.dev0/spacy/cli/project/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/cli/project/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     8208 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/cli/project/assets.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4754 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/cli/project/clone.py
--rw-r--r--   0 vsts      (1001) docker     (122)     5138 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/cli/project/document.py
--rw-r--r--   0 vsts      (1001) docker     (122)     8543 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/cli/project/dvc.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2942 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/cli/project/pull.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2774 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/cli/project/push.py
--rw-r--r--   0 vsts      (1001) docker     (122)     8236 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/cli/project/remote_storage.py
--rw-r--r--   0 vsts      (1001) docker     (122)    15838 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/cli/project/run.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.207004 spacy-3.6.0.dev0/spacy/cli/templates/
--rw-r--r--   0 vsts      (1001) docker     (122)    15155 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/cli/templates/quickstart_training.jinja
--rw-r--r--   0 vsts      (1001) docker     (122)     6294 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/cli/templates/quickstart_training_recommendations.yml
--rw-r--r--   0 vsts      (1001) docker     (122)     3399 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/cli/train.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4575 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/cli/validate.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1346 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/compat.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4350 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/default_config.cfg
--rw-r--r--   0 vsts      (1001) docker     (122)      738 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/default_config_pretraining.cfg
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.207004 spacy-3.6.0.dev0/spacy/displacy/
--rw-r--r--   0 vsts      (1001) docker     (122)    10356 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/displacy/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)    23810 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/displacy/render.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4656 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/displacy/templates.py
--rw-r--r--   0 vsts      (1001) docker     (122)    66899 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/errors.py
--rw-r--r--   0 vsts      (1001) docker     (122)       73 2023-05-08 15:54:18.000000 spacy-3.6.0.dev0/spacy/git_info.py
--rw-r--r--   0 vsts      (1001) docker     (122)    13185 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/glossary.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.207004 spacy-3.6.0.dev0/spacy/kb/
--rw-r--r--   0 vsts      (1001) docker     (122)      144 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/kb/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      390 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/kb/candidate.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)     2606 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/kb/candidate.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)      265 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/kb/kb.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)     4460 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/kb/kb.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)     6797 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/kb/kb_in_memory.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)    26490 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/kb/kb_in_memory.pyx
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.211004 spacy-3.6.0.dev0/spacy/lang/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/__init__.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.211004 spacy-3.6.0.dev0/spacy/lang/af/
--rw-r--r--   0 vsts      (1001) docker     (122)      255 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/af/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      291 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/af/stop_words.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.211004 spacy-3.6.0.dev0/spacy/lang/am/
--rw-r--r--   0 vsts      (1001) docker     (122)      831 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/am/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      792 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/am/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2192 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/am/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)      544 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/am/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3202 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/am/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)      291 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/am/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.211004 spacy-3.6.0.dev0/spacy/lang/ar/
--rw-r--r--   0 vsts      (1001) docker     (122)      572 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ar/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      890 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ar/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1309 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ar/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)      460 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ar/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3180 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ar/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1502 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ar/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.211004 spacy-3.6.0.dev0/spacy/lang/az/
--rw-r--r--   0 vsts      (1001) docker     (122)      329 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/az/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      747 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/az/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1697 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/az/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)      966 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/az/stop_words.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.211004 spacy-3.6.0.dev0/spacy/lang/bg/
--rw-r--r--   0 vsts      (1001) docker     (122)      919 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/bg/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      636 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/bg/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2048 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/bg/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4748 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/bg/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)     9110 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/bg/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.211004 spacy-3.6.0.dev0/spacy/lang/bn/
--rw-r--r--   0 vsts      (1001) docker     (122)     1175 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/bn/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      305 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/bn/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1270 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/bn/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)     6156 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/bn/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)      971 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/bn/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.215004 spacy-3.6.0.dev0/spacy/lang/ca/
--rwxr-xr-x   0 vsts      (1001) docker     (122)     1344 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ca/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      595 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ca/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2843 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ca/lemmatizer.py
--rw-r--r--   0 vsts      (1001) docker     (122)      956 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ca/lex_attrs.py
--rwxr-xr-x   0 vsts      (1001) docker     (122)     1650 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ca/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1619 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ca/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1994 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ca/syntax_iterators.py
--rwxr-xr-x   0 vsts      (1001) docker     (122)     1962 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ca/tokenizer_exceptions.py
--rw-r--r--   0 vsts      (1001) docker     (122)    14678 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/char_classes.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.215004 spacy-3.6.0.dev0/spacy/lang/cs/
--rw-r--r--   0 vsts      (1001) docker     (122)      305 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/cs/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1514 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/cs/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1074 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/cs/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2263 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/cs/stop_words.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.215004 spacy-3.6.0.dev0/spacy/lang/da/
--rw-r--r--   0 vsts      (1001) docker     (122)      628 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/da/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      568 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/da/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3575 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/da/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)      912 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/da/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1345 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/da/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2188 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/da/syntax_iterators.py
--rw-r--r--   0 vsts      (1001) docker     (122)     8956 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/da/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.215004 spacy-3.6.0.dev0/spacy/lang/de/
--rw-r--r--   0 vsts      (1001) docker     (122)      616 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/de/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      679 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/de/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1413 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/de/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3661 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/de/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1850 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/de/syntax_iterators.py
--rw-r--r--   0 vsts      (1001) docker     (122)     5889 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/de/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.215004 spacy-3.6.0.dev0/spacy/lang/dsb/
--rw-r--r--   0 vsts      (1001) docker     (122)      334 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/dsb/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      553 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/dsb/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1906 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/dsb/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)      118 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/dsb/stop_words.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.215004 spacy-3.6.0.dev0/spacy/lang/el/
--rw-r--r--   0 vsts      (1001) docker     (122)     1329 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/el/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1972 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/el/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2086 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/el/get_pos_from_wiktionary.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2195 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/el/lemmatizer.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2320 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/el/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3411 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/el/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)     8100 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/el/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2290 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/el/syntax_iterators.py
--rw-r--r--   0 vsts      (1001) docker     (122)    10077 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/el/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.219004 spacy-3.6.0.dev0/spacy/lang/en/
--rw-r--r--   0 vsts      (1001) docker     (122)     1235 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/en/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      556 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/en/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1480 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/en/lemmatizer.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1586 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/en/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)      569 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/en/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2148 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/en/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1570 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/en/syntax_iterators.py
--rw-r--r--   0 vsts      (1001) docker     (122)    14252 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/en/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.219004 spacy-3.6.0.dev0/spacy/lang/es/
--rw-r--r--   0 vsts      (1001) docker     (122)     1288 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/es/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      778 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/es/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)    16020 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/es/lemmatizer.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1796 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/es/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1234 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/es/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3388 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/es/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2714 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/es/syntax_iterators.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1458 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/es/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.219004 spacy-3.6.0.dev0/spacy/lang/et/
--rw-r--r--   0 vsts      (1001) docker     (122)      251 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/et/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      246 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/et/stop_words.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.219004 spacy-3.6.0.dev0/spacy/lang/eu/
--rw-r--r--   0 vsts      (1001) docker     (122)      387 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/eu/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      419 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/eu/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1093 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/eu/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)       78 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/eu/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)      760 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/eu/stop_words.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.219004 spacy-3.6.0.dev0/spacy/lang/fa/
--rw-r--r--   0 vsts      (1001) docker     (122)     1305 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/fa/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      515 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/fa/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)    14856 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/fa/generate_verbs_exc.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1386 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/fa/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)      505 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/fa/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3768 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/fa/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1555 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/fa/syntax_iterators.py
--rw-r--r--   0 vsts      (1001) docker     (122)    64921 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/fa/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.223004 spacy-3.6.0.dev0/spacy/lang/fi/
--rw-r--r--   0 vsts      (1001) docker     (122)      632 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/fi/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      545 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/fi/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1077 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/fi/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)      857 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/fi/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)     6436 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/fi/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2379 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/fi/syntax_iterators.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2468 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/fi/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.223004 spacy-3.6.0.dev0/spacy/lang/fr/
--rw-r--r--   0 vsts      (1001) docker     (122)     1404 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/fr/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)   360608 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/fr/_tokenizer_exceptions_list.py
--rw-r--r--   0 vsts      (1001) docker     (122)      938 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/fr/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3016 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/fr/lemmatizer.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1627 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/fr/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1476 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/fr/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3504 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/fr/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3124 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/fr/syntax_iterators.py
--rw-r--r--   0 vsts      (1001) docker     (122)    11232 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/fr/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.223004 spacy-3.6.0.dev0/spacy/lang/ga/
--rw-r--r--   0 vsts      (1001) docker     (122)      819 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ga/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4936 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ga/lemmatizer.py
--rw-r--r--   0 vsts      (1001) docker     (122)      608 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ga/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1883 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ga/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.223004 spacy-3.6.0.dev0/spacy/lang/grc/
--rw-r--r--   0 vsts      (1001) docker     (122)      620 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/grc/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1066 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/grc/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     6641 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/grc/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1082 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/grc/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)     9364 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/grc/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)     6768 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/grc/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.223004 spacy-3.6.0.dev0/spacy/lang/gu/
--rw-r--r--   0 vsts      (1001) docker     (122)      251 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/gu/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1215 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/gu/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1004 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/gu/stop_words.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.223004 spacy-3.6.0.dev0/spacy/lang/he/
--rw-r--r--   0 vsts      (1001) docker     (122)      391 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/he/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      994 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/he/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1759 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/he/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1856 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/he/stop_words.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.227004 spacy-3.6.0.dev0/spacy/lang/hi/
--rw-r--r--   0 vsts      (1001) docker     (122)      305 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/hi/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1518 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/hi/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     5889 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/hi/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2975 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/hi/stop_words.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.227004 spacy-3.6.0.dev0/spacy/lang/hr/
--rw-r--r--   0 vsts      (1001) docker     (122)      251 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/hr/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      489 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/hr/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)      970 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/hr/lemma_lookup_license.txt
--rw-r--r--   0 vsts      (1001) docker     (122)     1999 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/hr/stop_words.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.227004 spacy-3.6.0.dev0/spacy/lang/hsb/
--rw-r--r--   0 vsts      (1001) docker     (122)      437 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/hsb/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      640 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/hsb/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1791 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/hsb/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)      123 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/hsb/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)      386 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/hsb/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.227004 spacy-3.6.0.dev0/spacy/lang/hu/
--rw-r--r--   0 vsts      (1001) docker     (122)      584 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/hu/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      407 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/hu/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1491 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/hu/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1384 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/hu/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)     8401 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/hu/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.227004 spacy-3.6.0.dev0/spacy/lang/hy/
--rw-r--r--   0 vsts      (1001) docker     (122)      317 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/hy/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      441 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/hy/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1160 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/hy/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1067 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/hy/stop_words.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.227004 spacy-3.6.0.dev0/spacy/lang/id/
--rw-r--r--   0 vsts      (1001) docker     (122)      698 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/id/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)    53599 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/id/_tokenizer_exceptions_list.py
--rw-r--r--   0 vsts      (1001) docker     (122)      726 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/id/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1277 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/id/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2139 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/id/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)     6507 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/id/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1538 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/id/syntax_iterators.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4205 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/id/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.227004 spacy-3.6.0.dev0/spacy/lang/is/
--rw-r--r--   0 vsts      (1001) docker     (122)      255 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/is/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1019 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/is/stop_words.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.231004 spacy-3.6.0.dev0/spacy/lang/it/
--rw-r--r--   0 vsts      (1001) docker     (122)     1229 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/it/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      471 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/it/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4615 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/it/lemmatizer.py
--rw-r--r--   0 vsts      (1001) docker     (122)      873 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/it/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4114 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/it/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3137 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/it/syntax_iterators.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1161 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/it/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.231004 spacy-3.6.0.dev0/spacy/lang/ja/
--rw-r--r--   0 vsts      (1001) docker     (122)    12623 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ja/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      499 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ja/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1328 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ja/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1639 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ja/syntax_iterators.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1647 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ja/tag_bigram_map.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3741 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ja/tag_map.py
--rw-r--r--   0 vsts      (1001) docker     (122)      546 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ja/tag_orth_map.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.231004 spacy-3.6.0.dev0/spacy/lang/kn/
--rw-r--r--   0 vsts      (1001) docker     (122)      247 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/kn/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1211 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/kn/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1253 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/kn/stop_words.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.231004 spacy-3.6.0.dev0/spacy/lang/ko/
--rw-r--r--   0 vsts      (1001) docker     (122)     4301 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ko/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      531 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ko/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1052 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ko/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)      268 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ko/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)      349 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ko/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1928 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ko/tag_map.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.231004 spacy-3.6.0.dev0/spacy/lang/ky/
--rw-r--r--   0 vsts      (1001) docker     (122)      487 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ky/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      917 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ky/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)      925 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ky/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)      848 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ky/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1064 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ky/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2049 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ky/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.235004 spacy-3.6.0.dev0/spacy/lang/la/
--rw-r--r--   0 vsts      (1001) docker     (122)      495 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/la/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1146 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/la/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2371 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/la/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)      619 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/la/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2391 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/la/syntax_iterators.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1236 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/la/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.235004 spacy-3.6.0.dev0/spacy/lang/lb/
--rw-r--r--   0 vsts      (1001) docker     (122)      515 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/lb/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      906 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/lb/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1343 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/lb/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)      641 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/lb/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1127 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/lb/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1168 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/lb/tokenizer_exceptions.py
--rw-r--r--   0 vsts      (1001) docker     (122)     5944 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/lex_attrs.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.235004 spacy-3.6.0.dev0/spacy/lang/lg/
--rw-r--r--   0 vsts      (1001) docker     (122)      388 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/lg/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      522 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/lg/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2680 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/lg/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)      569 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/lg/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1361 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/lg/stop_words.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.235004 spacy-3.6.0.dev0/spacy/lang/lij/
--rw-r--r--   0 vsts      (1001) docker     (122)      430 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/lij/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      397 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/lij/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)      270 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/lij/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)      853 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/lij/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)      871 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/lij/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.235004 spacy-3.6.0.dev0/spacy/lang/lt/
--rw-r--r--   0 vsts      (1001) docker     (122)      557 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/lt/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      602 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/lt/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)    22464 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/lt/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)      716 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/lt/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)    19708 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/lt/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)      383 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/lt/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.235004 spacy-3.6.0.dev0/spacy/lang/lv/
--rw-r--r--   0 vsts      (1001) docker     (122)      247 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/lv/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1085 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/lv/stop_words.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.235004 spacy-3.6.0.dev0/spacy/lang/mk/
--rw-r--r--   0 vsts      (1001) docker     (122)     1689 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/mk/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1718 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/mk/lemmatizer.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3468 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/mk/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)     9106 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/mk/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3577 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/mk/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.239004 spacy-3.6.0.dev0/spacy/lang/ml/
--rw-r--r--   0 vsts      (1001) docker     (122)      321 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ml/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1403 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ml/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2345 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ml/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)      184 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ml/stop_words.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.239004 spacy-3.6.0.dev0/spacy/lang/mr/
--rw-r--r--   0 vsts      (1001) docker     (122)      247 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/mr/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2456 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/mr/stop_words.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.239004 spacy-3.6.0.dev0/spacy/lang/nb/
--rw-r--r--   0 vsts      (1001) docker     (122)     1296 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/nb/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      428 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/nb/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1663 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/nb/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1160 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/nb/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1523 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/nb/syntax_iterators.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3069 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/nb/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.239004 spacy-3.6.0.dev0/spacy/lang/ne/
--rw-r--r--   0 vsts      (1001) docker     (122)      309 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ne/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1104 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ne/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3276 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ne/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)     7135 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ne/stop_words.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.239004 spacy-3.6.0.dev0/spacy/lang/nl/
--rw-r--r--   0 vsts      (1001) docker     (122)     1354 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/nl/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      441 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/nl/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4618 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/nl/lemmatizer.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1303 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/nl/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1539 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/nl/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3090 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/nl/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2868 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/nl/syntax_iterators.py
--rw-r--r--   0 vsts      (1001) docker     (122)    24299 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/nl/tokenizer_exceptions.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1425 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/norm_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.239004 spacy-3.6.0.dev0/spacy/lang/pl/
--rw-r--r--   0 vsts      (1001) docker     (122)     1383 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/pl/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      685 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/pl/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3566 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/pl/lemmatizer.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1193 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/pl/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1362 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/pl/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2360 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/pl/stop_words.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.239004 spacy-3.6.0.dev0/spacy/lang/pt/
--rw-r--r--   0 vsts      (1001) docker     (122)      644 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/pt/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      472 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/pt/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2038 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/pt/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)      456 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/pt/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2566 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/pt/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3088 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/pt/syntax_iterators.py
--rw-r--r--   0 vsts      (1001) docker     (122)      716 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/pt/tokenizer_exceptions.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2192 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/punctuation.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.243004 spacy-3.6.0.dev0/spacy/lang/ro/
--rw-r--r--   0 vsts      (1001) docker     (122)      802 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ro/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      601 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ro/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1680 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ro/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3158 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ro/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2945 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ro/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1461 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ro/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.243004 spacy-3.6.0.dev0/spacy/lang/ru/
--rw-r--r--   0 vsts      (1001) docker     (122)     1317 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ru/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4581 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ru/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     7976 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ru/lemmatizer.py
--rw-r--r--   0 vsts      (1001) docker     (122)    18734 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ru/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)     8356 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ru/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)    25394 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ru/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.243004 spacy-3.6.0.dev0/spacy/lang/sa/
--rw-r--r--   0 vsts      (1001) docker     (122)      317 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/sa/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      781 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/sa/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4211 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/sa/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)     9364 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/sa/stop_words.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.243004 spacy-3.6.0.dev0/spacy/lang/si/
--rw-r--r--   0 vsts      (1001) docker     (122)      313 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/si/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      944 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/si/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1253 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/si/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2407 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/si/stop_words.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.243004 spacy-3.6.0.dev0/spacy/lang/sk/
--rw-r--r--   0 vsts      (1001) docker     (122)      309 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/sk/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      729 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/sk/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1070 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/sk/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2639 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/sk/stop_words.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.243004 spacy-3.6.0.dev0/spacy/lang/sl/
--rw-r--r--   0 vsts      (1001) docker     (122)      607 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/sl/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      575 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/sl/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     6603 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/sl/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3274 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/sl/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2478 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/sl/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)    13428 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/sl/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.247004 spacy-3.6.0.dev0/spacy/lang/sq/
--rw-r--r--   0 vsts      (1001) docker     (122)      251 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/sq/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      467 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/sq/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1210 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/sq/stop_words.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.247004 spacy-3.6.0.dev0/spacy/lang/sr/
--rw-r--r--   0 vsts      (1001) docker     (122)      545 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/sr/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      910 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/sr/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1549 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/sr/lemma_lookup_licence.txt
--rw-r--r--   0 vsts      (1001) docker     (122)     2977 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/sr/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)      964 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/sr/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3895 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/sr/stop_words.py
--rwxr-xr-x   0 vsts      (1001) docker     (122)     3662 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/sr/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.247004 spacy-3.6.0.dev0/spacy/lang/sv/
--rw-r--r--   0 vsts      (1001) docker     (122)     1274 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/sv/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      441 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/sv/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)      954 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/sv/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1023 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/sv/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2545 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/sv/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1531 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/sv/syntax_iterators.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3655 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/sv/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.247004 spacy-3.6.0.dev0/spacy/lang/ta/
--rw-r--r--   0 vsts      (1001) docker     (122)      305 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ta/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2703 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ta/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2288 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ta/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2018 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ta/stop_words.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.247004 spacy-3.6.0.dev0/spacy/lang/te/
--rw-r--r--   0 vsts      (1001) docker     (122)      309 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/te/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1243 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/te/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1263 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/te/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1107 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/te/stop_words.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.247004 spacy-3.6.0.dev0/spacy/lang/th/
--rw-r--r--   0 vsts      (1001) docker     (122)     1372 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/th/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1478 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/th/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)    19463 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/th/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)    18334 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/th/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.247004 spacy-3.6.0.dev0/spacy/lang/ti/
--rw-r--r--   0 vsts      (1001) docker     (122)      835 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ti/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      821 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ti/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1508 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ti/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)      545 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ti/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1977 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ti/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)      339 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ti/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.251004 spacy-3.6.0.dev0/spacy/lang/tl/
--rw-r--r--   0 vsts      (1001) docker     (122)      416 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/tl/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      943 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/tl/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)      965 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/tl/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)      688 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/tl/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.251004 spacy-3.6.0.dev0/spacy/lang/tn/
--rw-r--r--   0 vsts      (1001) docker     (122)      392 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/tn/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      436 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/tn/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2050 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/tn/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)      588 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/tn/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)      816 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/tn/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3476 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.251004 spacy-3.6.0.dev0/spacy/lang/tr/
--rw-r--r--   0 vsts      (1001) docker     (122)      546 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/tr/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      609 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/tr/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1674 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/tr/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4506 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/tr/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1853 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/tr/syntax_iterators.py
--rw-r--r--   0 vsts      (1001) docker     (122)     6092 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/tr/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.251004 spacy-3.6.0.dev0/spacy/lang/tt/
--rw-r--r--   0 vsts      (1001) docker     (122)      483 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/tt/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      897 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/tt/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1117 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/tt/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)      799 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/tt/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)    18567 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/tt/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1761 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/tt/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.251004 spacy-3.6.0.dev0/spacy/lang/uk/
--rw-r--r--   0 vsts      (1001) docker     (122)     1332 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/uk/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1798 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/uk/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1716 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/uk/lemmatizer.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1575 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/uk/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4887 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/uk/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1389 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/uk/tokenizer_exceptions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.251004 spacy-3.6.0.dev0/spacy/lang/ur/
--rw-r--r--   0 vsts      (1001) docker     (122)      461 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ur/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      303 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ur/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2237 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ur/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)       78 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ur/punctuation.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4722 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/ur/stop_words.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.251004 spacy-3.6.0.dev0/spacy/lang/vi/
--rw-r--r--   0 vsts      (1001) docker     (122)     5575 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/vi/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      769 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/vi/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1397 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/vi/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)    20595 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/vi/stop_words.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.251004 spacy-3.6.0.dev0/spacy/lang/xx/
--rw-r--r--   0 vsts      (1001) docker     (122)      266 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/xx/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     8161 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/xx/examples.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.255004 spacy-3.6.0.dev0/spacy/lang/yo/
--rw-r--r--   0 vsts      (1001) docker     (122)      309 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/yo/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1269 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/yo/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2523 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/yo/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)      608 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/yo/stop_words.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.255004 spacy-3.6.0.dev0/spacy/lang/zh/
--rw-r--r--   0 vsts      (1001) docker     (122)    12738 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/zh/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      792 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/zh/examples.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1613 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/zh/lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)    13409 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lang/zh/stop_words.py
--rw-r--r--   0 vsts      (1001) docker     (122)   101537 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/language.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2621 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lexeme.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)     1446 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lexeme.pyi
--rw-r--r--   0 vsts      (1001) docker     (122)    16850 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lexeme.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)    10851 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/lookups.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.255004 spacy-3.6.0.dev0/spacy/matcher/
--rw-r--r--   0 vsts      (1001) docker     (122)      232 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/matcher/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2124 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/matcher/dependencymatcher.pyi
--rw-r--r--   0 vsts      (1001) docker     (122)    17792 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/matcher/dependencymatcher.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)   227182 2023-05-08 15:53:32.000000 spacy-3.6.0.dev0/spacy/matcher/levenshtein.c
--rw-r--r--   0 vsts      (1001) docker     (122)      945 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/matcher/levenshtein.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)     1530 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/matcher/matcher.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)     1846 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/matcher/matcher.pyi
--rw-r--r--   0 vsts      (1001) docker     (122)    49548 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/matcher/matcher.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)      555 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/matcher/phrasematcher.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)     1038 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/matcher/phrasematcher.pyi
--rw-r--r--   0 vsts      (1001) docker     (122)    14362 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/matcher/phrasematcher.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)     9571 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/matcher/polyleven.c
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.255004 spacy-3.6.0.dev0/spacy/ml/
--rw-r--r--   0 vsts      (1001) docker     (122)      109 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/ml/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1993 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/ml/_character_embed.py
--rw-r--r--   0 vsts      (1001) docker     (122)     5834 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/ml/_precomputable_affine.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3787 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/ml/callbacks.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1191 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/ml/extract_ngrams.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2303 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/ml/extract_spans.py
--rw-r--r--   0 vsts      (1001) docker     (122)      969 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/ml/featureextractor.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.259004 spacy-3.6.0.dev0/spacy/ml/models/
--rw-r--r--   0 vsts      (1001) docker     (122)      224 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/ml/models/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4262 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/ml/models/entity_linker.py
--rw-r--r--   0 vsts      (1001) docker     (122)     9260 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/ml/models/multi_task.py
--rw-r--r--   0 vsts      (1001) docker     (122)     6896 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/ml/models/parser.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2370 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/ml/models/spancat.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1252 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/ml/models/tagger.py
--rw-r--r--   0 vsts      (1001) docker     (122)     6748 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/ml/models/textcat.py
--rw-r--r--   0 vsts      (1001) docker     (122)    14022 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/ml/models/tok2vec.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1166 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/ml/parser_model.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)    18181 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/ml/parser_model.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)     3851 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/ml/staticvectors.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1464 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/ml/tb_framework.py
--rw-r--r--   0 vsts      (1001) docker     (122)      808 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/morphology.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)     7983 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/morphology.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)      256 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/parts_of_speech.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)      434 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/parts_of_speech.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)     6241 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipe_analysis.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.259004 spacy-3.6.0.dev0/spacy/pipeline/
--rw-r--r--   0 vsts      (1001) docker     (122)     1199 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/__init__.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.263004 spacy-3.6.0.dev0/spacy/pipeline/_edit_tree_internals/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/_edit_tree_internals/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3449 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/_edit_tree_internals/edit_trees.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)    10747 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/_edit_tree_internals/edit_trees.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)     1474 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/_edit_tree_internals/schemas.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.263004 spacy-3.6.0.dev0/spacy/pipeline/_parser_internals/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/_parser_internals/__init__.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/_parser_internals/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      254 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/_parser_internals/_beam_utils.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)    11651 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/_parser_internals/_beam_utils.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)    12328 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/_parser_internals/_state.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/_parser_internals/_state.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)      211 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/_parser_internals/arc_eager.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)    30412 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/_parser_internals/arc_eager.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)      104 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/_parser_internals/ner.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)    23344 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/_parser_internals/ner.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)      187 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/_parser_internals/nonproj.hh
--rw-r--r--   0 vsts      (1001) docker     (122)      133 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/_parser_internals/nonproj.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)     8603 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/_parser_internals/nonproj.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)      744 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/_parser_internals/stateclass.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)     4622 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/_parser_internals/stateclass.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)     1724 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/_parser_internals/transition_system.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)     9235 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/_parser_internals/transition_system.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)    13693 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/attributeruler.py
--rw-r--r--   0 vsts      (1001) docker     (122)    13096 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/dep_parser.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)    15348 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/edit_tree_lemmatizer.py
--rw-r--r--   0 vsts      (1001) docker     (122)    27432 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/entity_linker.py
--rw-r--r--   0 vsts      (1001) docker     (122)    21071 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/entityruler.py
--rw-r--r--   0 vsts      (1001) docker     (122)     6703 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/functions.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.263004 spacy-3.6.0.dev0/spacy/pipeline/legacy/
--rw-r--r--   0 vsts      (1001) docker     (122)       74 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/legacy/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)    18808 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/legacy/entity_linker.py
--rw-r--r--   0 vsts      (1001) docker     (122)    12178 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/lemmatizer.py
--rw-r--r--   0 vsts      (1001) docker     (122)    12733 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/morphologizer.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)     7579 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/multitask.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)    10064 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/ner.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)       42 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/pipe.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)     1224 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/pipe.pyi
--rw-r--r--   0 vsts      (1001) docker     (122)     5627 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/pipe.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)     6836 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/sentencizer.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)     6525 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/senter.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)    21522 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/span_ruler.py
--rw-r--r--   0 vsts      (1001) docker     (122)    28930 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/spancat.py
--rw-r--r--   0 vsts      (1001) docker     (122)    12404 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/tagger.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)    14943 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/textcat.py
--rw-r--r--   0 vsts      (1001) docker     (122)     6606 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/textcat_multilabel.py
--rw-r--r--   0 vsts      (1001) docker     (122)    13389 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/tok2vec.py
--rw-r--r--   0 vsts      (1001) docker     (122)      198 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/trainable_pipe.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)    14041 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/trainable_pipe.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)      715 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/transition_parser.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)    27552 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/pipeline/transition_parser.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/py.typed
--rw-r--r--   0 vsts      (1001) docker     (122)    23889 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/schemas.py
--rw-r--r--   0 vsts      (1001) docker     (122)    41002 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/scorer.py
--rw-r--r--   0 vsts      (1001) docker     (122)      727 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/strings.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)     1098 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/strings.pyi
--rw-r--r--   0 vsts      (1001) docker     (122)    10621 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/strings.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)     2534 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/structs.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)     6939 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/symbols.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)    14176 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/symbols.pyx
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.267004 spacy-3.6.0.dev0/spacy/tests/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)    11509 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/conftest.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.267004 spacy-3.6.0.dev0/spacy/tests/doc/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/doc/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1803 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/doc/test_add_entities.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4988 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/doc/test_array.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2650 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/doc/test_creation.py
--rw-r--r--   0 vsts      (1001) docker     (122)    35903 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/doc/test_doc_api.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1819 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/doc/test_graph.py
--rw-r--r--   0 vsts      (1001) docker     (122)    13658 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/doc/test_json_doc_conversion.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3145 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/doc/test_morphanalysis.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1470 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/doc/test_pickle_doc.py
--rw-r--r--   0 vsts      (1001) docker     (122)    18955 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/doc/test_retokenize_merge.py
--rw-r--r--   0 vsts      (1001) docker     (122)    10937 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/doc/test_retokenize_split.py
--rw-r--r--   0 vsts      (1001) docker     (122)    25422 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/doc/test_span.py
--rw-r--r--   0 vsts      (1001) docker     (122)     7685 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/doc/test_span_group.py
--rw-r--r--   0 vsts      (1001) docker     (122)    11164 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/doc/test_token_api.py
--rw-r--r--   0 vsts      (1001) docker     (122)     5564 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/doc/test_underscore.py
--rw-r--r--   0 vsts      (1001) docker     (122)       45 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/enable_gpu.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.267004 spacy-3.6.0.dev0/spacy/tests/lang/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/__init__.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.267004 spacy-3.6.0.dev0/spacy/tests/lang/af/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/af/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      931 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/af/test_text.py
--rw-r--r--   0 vsts      (1001) docker     (122)      710 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/af/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.267004 spacy-3.6.0.dev0/spacy/tests/lang/am/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/am/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/am/test_exception.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1837 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/am/test_text.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.267004 spacy-3.6.0.dev0/spacy/tests/lang/ar/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ar/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      621 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ar/test_exceptions.py
--rw-r--r--   0 vsts      (1001) docker     (122)      821 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ar/test_text.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.267004 spacy-3.6.0.dev0/spacy/tests/lang/bn/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/bn/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3623 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/bn/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.275004 spacy-3.6.0.dev0/spacy/tests/lang/ca/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ca/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      621 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ca/test_exception.py
--rw-r--r--   0 vsts      (1001) docker     (122)      493 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ca/test_prefix_suffix_infix.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1900 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ca/test_text.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.275004 spacy-3.6.0.dev0/spacy/tests/lang/cs/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/cs/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      510 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/cs/test_text.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.275004 spacy-3.6.0.dev0/spacy/tests/lang/da/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/da/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1824 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/da/test_exceptions.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2063 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/da/test_noun_chunks.py
--rw-r--r--   0 vsts      (1001) docker     (122)     5423 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/da/test_prefix_suffix_infix.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1219 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/da/test_text.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.275004 spacy-3.6.0.dev0/spacy/tests/lang/de/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/de/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      598 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/de/test_exceptions.py
--rw-r--r--   0 vsts      (1001) docker     (122)      266 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/de/test_noun_chunks.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1188 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/de/test_parser.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3395 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/de/test_prefix_suffix_infix.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1505 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/de/test_text.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.275004 spacy-3.6.0.dev0/spacy/tests/lang/dsb/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/dsb/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      560 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/dsb/test_text.py
--rw-r--r--   0 vsts      (1001) docker     (122)      749 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/dsb/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.275004 spacy-3.6.0.dev0/spacy/tests/lang/el/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/el/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      514 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/el/test_exception.py
--rw-r--r--   0 vsts      (1001) docker     (122)      306 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/el/test_noun_chunks.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1768 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/el/test_text.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.275004 spacy-3.6.0.dev0/spacy/tests/lang/en/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/en/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3798 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/en/test_customized_tokenizer.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4158 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/en/test_exceptions.py
--rw-r--r--   0 vsts      (1001) docker     (122)      723 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/en/test_indices.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1548 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/en/test_noun_chunks.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2954 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/en/test_parser.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4254 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/en/test_prefix_suffix_infix.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4422 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/en/test_punct.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1707 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/en/test_sbd.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1962 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/en/test_text.py
--rw-r--r--   0 vsts      (1001) docker     (122)     5938 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/en/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.275004 spacy-3.6.0.dev0/spacy/tests/lang/es/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/es/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      549 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/es/test_exception.py
--rw-r--r--   0 vsts      (1001) docker     (122)     9966 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/es/test_noun_chunks.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2025 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/es/test_text.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.279004 spacy-3.6.0.dev0/spacy/tests/lang/et/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/et/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      933 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/et/test_text.py
--rw-r--r--   0 vsts      (1001) docker     (122)      737 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/et/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.279004 spacy-3.6.0.dev0/spacy/tests/lang/eu/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/eu/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      488 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/eu/test_text.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.279004 spacy-3.6.0.dev0/spacy/tests/lang/fa/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/fa/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      296 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/fa/test_noun_chunks.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.279004 spacy-3.6.0.dev0/spacy/tests/lang/fi/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/fi/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     7258 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/fi/test_noun_chunks.py
--rw-r--r--   0 vsts      (1001) docker     (122)      542 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/fi/test_text.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2900 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/fi/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.279004 spacy-3.6.0.dev0/spacy/tests/lang/fr/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/fr/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2219 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/fr/test_exceptions.py
--rw-r--r--   0 vsts      (1001) docker     (122)     8353 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/fr/test_noun_chunks.py
--rw-r--r--   0 vsts      (1001) docker     (122)      772 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/fr/test_prefix_suffix_infix.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1011 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/fr/test_text.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.279004 spacy-3.6.0.dev0/spacy/tests/lang/ga/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ga/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      685 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ga/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.279004 spacy-3.6.0.dev0/spacy/tests/lang/grc/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/grc/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      543 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/grc/test_text.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1327 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/grc/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.279004 spacy-3.6.0.dev0/spacy/tests/lang/gu/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/gu/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      685 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/gu/test_text.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.279004 spacy-3.6.0.dev0/spacy/tests/lang/he/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/he/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2255 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/he/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.279004 spacy-3.6.0.dev0/spacy/tests/lang/hi/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/hi/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1841 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/hi/test_lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)      400 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/hi/test_text.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.279004 spacy-3.6.0.dev0/spacy/tests/lang/hr/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/hr/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      954 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/hr/test_text.py
--rw-r--r--   0 vsts      (1001) docker     (122)      801 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/hr/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.279004 spacy-3.6.0.dev0/spacy/tests/lang/hsb/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/hsb/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      566 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/hsb/test_text.py
--rw-r--r--   0 vsts      (1001) docker     (122)      866 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/hsb/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.279004 spacy-3.6.0.dev0/spacy/tests/lang/hu/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/hu/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)    14492 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/hu/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.283004 spacy-3.6.0.dev0/spacy/tests/lang/hy/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/hy/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      210 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/hy/test_text.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1351 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/hy/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.283004 spacy-3.6.0.dev0/spacy/tests/lang/id/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/id/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      256 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/id/test_noun_chunks.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3492 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/id/test_prefix_suffix_infix.py
--rw-r--r--   0 vsts      (1001) docker     (122)      205 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/id/test_text.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.287004 spacy-3.6.0.dev0/spacy/tests/lang/is/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/is/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      980 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/is/test_text.py
--rw-r--r--   0 vsts      (1001) docker     (122)      783 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/is/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.287004 spacy-3.6.0.dev0/spacy/tests/lang/it/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/it/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     8639 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/it/test_noun_chunks.py
--rw-r--r--   0 vsts      (1001) docker     (122)      359 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/it/test_prefix_suffix_infix.py
--rw-r--r--   0 vsts      (1001) docker     (122)      449 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/it/test_stopwords.py
--rw-r--r--   0 vsts      (1001) docker     (122)      411 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/it/test_text.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.291004 spacy-3.6.0.dev0/spacy/tests/lang/ja/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ja/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      694 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ja/test_lemmatization.py
--rw-r--r--   0 vsts      (1001) docker     (122)      243 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ja/test_morphologizer_factory.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1306 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ja/test_serialize.py
--rw-r--r--   0 vsts      (1001) docker     (122)     7900 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ja/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.291004 spacy-3.6.0.dev0/spacy/tests/lang/ko/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ko/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      309 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ko/test_lemmatization.py
--rw-r--r--   0 vsts      (1001) docker     (122)      712 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ko/test_serialize.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2893 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ko/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.291004 spacy-3.6.0.dev0/spacy/tests/lang/ky/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ky/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4014 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ky/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.291004 spacy-3.6.0.dev0/spacy/tests/lang/la/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/la/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      236 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/la/test_exception.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1627 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/la/test_noun_chunks.py
--rw-r--r--   0 vsts      (1001) docker     (122)      803 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/la/test_text.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.291004 spacy-3.6.0.dev0/spacy/tests/lang/lb/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/lb/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      590 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/lb/test_exceptions.py
--rw-r--r--   0 vsts      (1001) docker     (122)      584 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/lb/test_prefix_suffix_infix.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1302 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/lb/test_text.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.291004 spacy-3.6.0.dev0/spacy/tests/lang/lg/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/lg/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      449 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/lg/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.295004 spacy-3.6.0.dev0/spacy/tests/lang/lt/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/lt/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1644 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/lt/test_text.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.295004 spacy-3.6.0.dev0/spacy/tests/lang/lv/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/lv/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1018 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/lv/test_text.py
--rw-r--r--   0 vsts      (1001) docker     (122)      778 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/lv/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.295004 spacy-3.6.0.dev0/spacy/tests/lang/mk/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/mk/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4352 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/mk/test_text.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.295004 spacy-3.6.0.dev0/spacy/tests/lang/ml/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ml/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1075 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ml/test_text.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.295004 spacy-3.6.0.dev0/spacy/tests/lang/nb/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/nb/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      277 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/nb/test_noun_chunks.py
--rw-r--r--   0 vsts      (1001) docker     (122)      630 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/nb/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.295004 spacy-3.6.0.dev0/spacy/tests/lang/ne/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ne/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      783 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ne/test_text.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.295004 spacy-3.6.0.dev0/spacy/tests/lang/nl/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/nl/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4303 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/nl/test_noun_chunks.py
--rw-r--r--   0 vsts      (1001) docker     (122)      693 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/nl/test_text.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.295004 spacy-3.6.0.dev0/spacy/tests/lang/pl/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/pl/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      522 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/pl/test_text.py
--rw-r--r--   0 vsts      (1001) docker     (122)      555 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/pl/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.299004 spacy-3.6.0.dev0/spacy/tests/lang/pt/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/pt/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     7748 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/pt/test_noun_chunks.py
--rw-r--r--   0 vsts      (1001) docker     (122)      219 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/pt/test_text.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.299004 spacy-3.6.0.dev0/spacy/tests/lang/ro/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ro/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      734 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ro/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.299004 spacy-3.6.0.dev0/spacy/tests/lang/ru/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ru/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      353 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ru/test_exceptions.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3958 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ru/test_lemmatizer.py
--rw-r--r--   0 vsts      (1001) docker     (122)      220 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ru/test_text.py
--rw-r--r--   0 vsts      (1001) docker     (122)     5983 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ru/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.299004 spacy-3.6.0.dev0/spacy/tests/lang/sa/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/sa/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1297 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/sa/test_text.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.299004 spacy-3.6.0.dev0/spacy/tests/lang/sk/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/sk/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1485 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/sk/test_text.py
--rw-r--r--   0 vsts      (1001) docker     (122)      453 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/sk/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.299004 spacy-3.6.0.dev0/spacy/tests/lang/sl/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/sl/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      993 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/sl/test_text.py
--rw-r--r--   0 vsts      (1001) docker     (122)      831 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/sl/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.303004 spacy-3.6.0.dev0/spacy/tests/lang/sq/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/sq/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1178 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/sq/test_text.py
--rw-r--r--   0 vsts      (1001) docker     (122)      817 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/sq/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.303004 spacy-3.6.0.dev0/spacy/tests/lang/sr/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/sr/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      391 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/sr/test_exceptions.py
--rw-r--r--   0 vsts      (1001) docker     (122)      484 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/sr/test_lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4302 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/sr/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.303004 spacy-3.6.0.dev0/spacy/tests/lang/sv/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/sv/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2453 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/sv/test_exceptions.py
--rw-r--r--   0 vsts      (1001) docker     (122)      683 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/sv/test_lex_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1857 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/sv/test_noun_chunks.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1265 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/sv/test_prefix_suffix_infix.py
--rw-r--r--   0 vsts      (1001) docker     (122)      723 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/sv/test_text.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1005 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/sv/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.303004 spacy-3.6.0.dev0/spacy/tests/lang/ta/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ta/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2737 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ta/test_text.py
--rw-r--r--   0 vsts      (1001) docker     (122)     7829 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ta/test_tokenizer.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3572 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/test_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)      922 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/test_initialize.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1910 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/test_lemmatizers.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.307004 spacy-3.6.0.dev0/spacy/tests/lang/th/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/th/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      706 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/th/test_serialize.py
--rw-r--r--   0 vsts      (1001) docker     (122)      318 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/th/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.307004 spacy-3.6.0.dev0/spacy/tests/lang/ti/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ti/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ti/test_exception.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2120 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ti/test_text.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.307004 spacy-3.6.0.dev0/spacy/tests/lang/tl/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/tl/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      257 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/tl/test_indices.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4420 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/tl/test_punct.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2479 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/tl/test_text.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.307004 spacy-3.6.0.dev0/spacy/tests/lang/tr/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/tr/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      422 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/tr/test_noun_chunks.py
--rw-r--r--   0 vsts      (1001) docker     (122)    19784 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/tr/test_parser.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1796 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/tr/test_text.py
--rw-r--r--   0 vsts      (1001) docker     (122)    19589 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/tr/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.307004 spacy-3.6.0.dev0/spacy/tests/lang/tt/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/tt/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3721 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/tt/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.311004 spacy-3.6.0.dev0/spacy/tests/lang/uk/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/uk/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      837 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/uk/test_lemmatizer.py
--rw-r--r--   0 vsts      (1001) docker     (122)     5416 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/uk/test_tokenizer.py
--rw-r--r--   0 vsts      (1001) docker     (122)      364 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/uk/test_tokenizer_exc.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.311004 spacy-3.6.0.dev0/spacy/tests/lang/ur/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ur/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      229 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ur/test_prefix_suffix_infix.py
--rw-r--r--   0 vsts      (1001) docker     (122)      479 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/ur/test_text.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.311004 spacy-3.6.0.dev0/spacy/tests/lang/vi/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/vi/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1308 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/vi/test_serialize.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1677 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/vi/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.311004 spacy-3.6.0.dev0/spacy/tests/lang/xx/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/xx/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1720 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/xx/test_text.py
--rw-r--r--   0 vsts      (1001) docker     (122)      669 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/xx/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.311004 spacy-3.6.0.dev0/spacy/tests/lang/yo/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/yo/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1490 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/yo/test_text.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.311004 spacy-3.6.0.dev0/spacy/tests/lang/zh/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/zh/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1245 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/zh/test_serialize.py
--rw-r--r--   0 vsts      (1001) docker     (122)      454 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/zh/test_text.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2819 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/lang/zh/test_tokenizer.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.315003 spacy-3.6.0.dev0/spacy/tests/matcher/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/matcher/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)    14870 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/matcher/test_dependency_matcher.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2797 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/matcher/test_levenshtein.py
--rw-r--r--   0 vsts      (1001) docker     (122)    29916 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/matcher/test_matcher_api.py
--rw-r--r--   0 vsts      (1001) docker     (122)    27183 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/matcher/test_matcher_logic.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3334 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/matcher/test_pattern_validation.py
--rw-r--r--   0 vsts      (1001) docker     (122)    17967 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/matcher/test_phrase_matcher.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.315003 spacy-3.6.0.dev0/spacy/tests/morphology/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/morphology/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      856 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/morphology/test_morph_converters.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1348 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/morphology/test_morph_features.py
--rw-r--r--   0 vsts      (1001) docker     (122)      668 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/morphology/test_morph_pickle.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.315003 spacy-3.6.0.dev0/spacy/tests/package/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/package/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)      250 2023-05-08 15:54:18.000000 spacy-3.6.0.dev0/spacy/tests/package/pyproject.toml
--rw-r--r--   0 vsts      (1001) docker     (122)      982 2023-05-08 15:54:18.000000 spacy-3.6.0.dev0/spacy/tests/package/requirements.txt
--rw-r--r--   0 vsts      (1001) docker     (122)     3744 2023-05-08 15:54:18.000000 spacy-3.6.0.dev0/spacy/tests/package/setup.cfg
--rw-r--r--   0 vsts      (1001) docker     (122)     3191 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/package/test_requirements.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.319003 spacy-3.6.0.dev0/spacy/tests/parser/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/parser/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4919 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/parser/test_add_label.py
--rw-r--r--   0 vsts      (1001) docker     (122)    10085 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/parser/test_arc_eager_oracle.py
--rw-r--r--   0 vsts      (1001) docker     (122)    29122 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/parser/test_ner.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2859 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/parser/test_neural_parser.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3600 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/parser/test_nn_beam.py
--rw-r--r--   0 vsts      (1001) docker     (122)     6289 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/parser/test_nonproj.py
--rw-r--r--   0 vsts      (1001) docker     (122)    20158 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/parser/test_parse.py
--rw-r--r--   0 vsts      (1001) docker     (122)     6215 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/parser/test_parse_navigate.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2560 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/parser/test_preset_sbd.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2927 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/parser/test_space_attachment.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1894 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/parser/test_state.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.323004 spacy-3.6.0.dev0/spacy/tests/pipeline/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/pipeline/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3456 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/pipeline/test_analysis.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3192 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/pipeline/test_annotates_on_update.py
--rw-r--r--   0 vsts      (1001) docker     (122)     9535 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/pipeline/test_attributeruler.py
--rw-r--r--   0 vsts      (1001) docker     (122)    10448 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/pipeline/test_edit_tree_lemmatizer.py
--rw-r--r--   0 vsts      (1001) docker     (122)    45209 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/pipeline/test_entity_linker.py
--rw-r--r--   0 vsts      (1001) docker     (122)    25923 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/pipeline/test_entity_ruler.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3188 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/pipeline/test_functions.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2350 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/pipeline/test_initialize.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3725 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/pipeline/test_lemmatizer.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3843 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/pipeline/test_models.py
--rw-r--r--   0 vsts      (1001) docker     (122)     8095 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/pipeline/test_morphologizer.py
--rw-r--r--   0 vsts      (1001) docker     (122)    19895 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/pipeline/test_pipe_factories.py
--rw-r--r--   0 vsts      (1001) docker     (122)    23309 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/pipeline/test_pipe_methods.py
--rw-r--r--   0 vsts      (1001) docker     (122)    18273 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/pipeline/test_sentencizer.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3313 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/pipeline/test_senter.py
--rw-r--r--   0 vsts      (1001) docker     (122)    16221 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/pipeline/test_span_ruler.py
--rw-r--r--   0 vsts      (1001) docker     (122)    21356 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/pipeline/test_spancat.py
--rw-r--r--   0 vsts      (1001) docker     (122)     7653 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/pipeline/test_tagger.py
--rw-r--r--   0 vsts      (1001) docker     (122)    35737 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/pipeline/test_textcat.py
--rw-r--r--   0 vsts      (1001) docker     (122)    18811 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/pipeline/test_tok2vec.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.327004 spacy-3.6.0.dev0/spacy/tests/serialize/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/serialize/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4185 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/serialize/test_resource_warning.py
--rw-r--r--   0 vsts      (1001) docker     (122)    16435 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/serialize/test_serialize_config.py
--rw-r--r--   0 vsts      (1001) docker     (122)     7370 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/serialize/test_serialize_doc.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4028 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/serialize/test_serialize_docbin.py
--rw-r--r--   0 vsts      (1001) docker     (122)      945 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/serialize/test_serialize_extension_attrs.py
--rw-r--r--   0 vsts      (1001) docker     (122)     7066 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/serialize/test_serialize_kb.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3594 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/serialize/test_serialize_language.py
--rw-r--r--   0 vsts      (1001) docker     (122)    16859 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/serialize/test_serialize_pipeline.py
--rw-r--r--   0 vsts      (1001) docker     (122)     8768 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/serialize/test_serialize_span_groups.py
--rw-r--r--   0 vsts      (1001) docker     (122)     5442 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/serialize/test_serialize_tokenizer.py
--rw-r--r--   0 vsts      (1001) docker     (122)     7067 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/serialize/test_serialize_vocab_strings.py
--rw-r--r--   0 vsts      (1001) docker     (122)      447 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/test_architectures.py
--rw-r--r--   0 vsts      (1001) docker     (122)    50002 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/test_cli.py
--rw-r--r--   0 vsts      (1001) docker     (122)     7936 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/test_cli_app.py
--rw-r--r--   0 vsts      (1001) docker     (122)    13600 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/test_displacy.py
--rw-r--r--   0 vsts      (1001) docker     (122)      333 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/test_errors.py
--rw-r--r--   0 vsts      (1001) docker     (122)    26992 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/test_language.py
--rw-r--r--   0 vsts      (1001) docker     (122)    14454 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/test_misc.py
--rw-r--r--   0 vsts      (1001) docker     (122)     9089 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/test_models.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2022 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/test_pickles.py
--rw-r--r--   0 vsts      (1001) docker     (122)    16857 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/test_scorer.py
--rw-r--r--   0 vsts      (1001) docker     (122)      748 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/test_ty.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.327004 spacy-3.6.0.dev0/spacy/tests/tokenizer/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/tokenizer/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2591 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/tokenizer/sun.txt
--rw-r--r--   0 vsts      (1001) docker     (122)     1756 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/tokenizer/test_exceptions.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4850 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/tokenizer/test_explain.py
--rw-r--r--   0 vsts      (1001) docker     (122)     7466 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/tokenizer/test_naughty_strings.py
--rw-r--r--   0 vsts      (1001) docker     (122)    18589 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/tokenizer/test_tokenizer.py
--rw-r--r--   0 vsts      (1001) docker     (122)     6463 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/tokenizer/test_urls.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1295 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/tokenizer/test_whitespace.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.331004 spacy-3.6.0.dev0/spacy/tests/training/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/training/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)    10607 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/training/test_augmenters.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1941 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/training/test_corpus.py
--rw-r--r--   0 vsts      (1001) docker     (122)      600 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/training/test_logger.py
--rw-r--r--   0 vsts      (1001) docker     (122)    16145 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/training/test_new_example.py
--rw-r--r--   0 vsts      (1001) docker     (122)    12027 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/training/test_pretraining.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3947 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/training/test_readers.py
--rw-r--r--   0 vsts      (1001) docker     (122)     6405 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/training/test_rehearse.py
--rw-r--r--   0 vsts      (1001) docker     (122)    45564 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/training/test_training.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3279 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/util.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.331004 spacy-3.6.0.dev0/spacy/tests/vocab_vectors/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/vocab_vectors/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2852 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/vocab_vectors/test_lexeme.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4651 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/vocab_vectors/test_lookups.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3834 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/vocab_vectors/test_similarity.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3338 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/vocab_vectors/test_stringstore.py
--rw-r--r--   0 vsts      (1001) docker     (122)    22581 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/vocab_vectors/test_vectors.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2147 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tests/vocab_vectors/test_vocab_api.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2359 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tokenizer.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)    36550 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tokenizer.pyx
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.339003 spacy-3.6.0.dev0/spacy/tokens/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tokens/__init__.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)      251 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tokens/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     4515 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tokens/_dict_proxies.py
--rw-r--r--   0 vsts      (1001) docker     (122)      702 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tokens/_retokenize.pyi
--rw-r--r--   0 vsts      (1001) docker     (122)    20451 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tokens/_retokenize.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)    12101 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tokens/_serialize.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1666 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tokens/doc.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)     5931 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tokens/doc.pyi
--rw-r--r--   0 vsts      (1001) docker     (122)    83023 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tokens/doc.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)      295 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tokens/graph.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)    23113 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tokens/graph.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)      209 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tokens/morphanalysis.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)      860 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tokens/morphanalysis.pyi
--rw-r--r--   0 vsts      (1001) docker     (122)     2992 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tokens/morphanalysis.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)      521 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tokens/span.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)     3735 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tokens/span.pyi
--rw-r--r--   0 vsts      (1001) docker     (122)    32945 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tokens/span.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)      244 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tokens/span_group.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)      835 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tokens/span_group.pyi
--rw-r--r--   0 vsts      (1001) docker     (122)    10992 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tokens/span_group.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)     3419 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tokens/token.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)     5398 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tokens/token.pyi
--rw-r--r--   0 vsts      (1001) docker     (122)    33280 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tokens/token.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)     5589 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/tokens/underscore.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.343003 spacy-3.6.0.dev0/spacy/training/
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/training/__init__.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)      831 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/training/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     3194 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/training/align.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)      614 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/training/alignment.py
--rw-r--r--   0 vsts      (1001) docker     (122)      170 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/training/alignment_array.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)     2128 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/training/alignment_array.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)    13279 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/training/augment.py
--rw-r--r--   0 vsts      (1001) docker     (122)     9110 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/training/batchers.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1269 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/training/callbacks.py
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.343003 spacy-3.6.0.dev0/spacy/training/converters/
--rw-r--r--   0 vsts      (1001) docker     (122)      224 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/training/converters/__init__.py
--rw-r--r--   0 vsts      (1001) docker     (122)     6177 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/training/converters/conll_ner_to_docs.py
--rw-r--r--   0 vsts      (1001) docker     (122)    10275 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/training/converters/conllu_to_docs.py
--rw-r--r--   0 vsts      (1001) docker     (122)     2356 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/training/converters/iob_to_docs.py
--rw-r--r--   0 vsts      (1001) docker     (122)      883 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/training/converters/json_to_docs.py
--rw-r--r--   0 vsts      (1001) docker     (122)    11991 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/training/corpus.py
--rw-r--r--   0 vsts      (1001) docker     (122)      327 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/training/example.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)    24796 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/training/example.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)     8027 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/training/gold_io.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)    13741 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/training/initialize.py
--rw-r--r--   0 vsts      (1001) docker     (122)     9075 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/training/iob_utils.py
--rw-r--r--   0 vsts      (1001) docker     (122)     7819 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/training/loggers.py
--rw-r--r--   0 vsts      (1001) docker     (122)    15001 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/training/loop.py
--rw-r--r--   0 vsts      (1001) docker     (122)     9702 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/training/pretrain.py
--rw-r--r--   0 vsts      (1001) docker     (122)     1299 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/ty.py
--rw-r--r--   0 vsts      (1001) docker     (122)      283 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/typedefs.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/typedefs.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)    66761 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/util.py
--rw-r--r--   0 vsts      (1001) docker     (122)    25940 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/vectors.pyx
--rw-r--r--   0 vsts      (1001) docker     (122)     1374 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/vocab.pxd
--rw-r--r--   0 vsts      (1001) docker     (122)     2752 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/vocab.pyi
--rw-r--r--   0 vsts      (1001) docker     (122)    23369 2023-05-08 15:53:11.000000 spacy-3.6.0.dev0/spacy/vocab.pyx
-drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-05-08 15:54:19.203004 spacy-3.6.0.dev0/spacy.egg-info/
--rw-r--r--   0 vsts      (1001) docker     (122)    22567 2023-05-08 15:54:18.000000 spacy-3.6.0.dev0/spacy.egg-info/PKG-INFO
--rw-r--r--   0 vsts      (1001) docker     (122)    29226 2023-05-08 15:54:19.000000 spacy-3.6.0.dev0/spacy.egg-info/SOURCES.txt
--rw-r--r--   0 vsts      (1001) docker     (122)        1 2023-05-08 15:54:18.000000 spacy-3.6.0.dev0/spacy.egg-info/dependency_links.txt
--rw-r--r--   0 vsts      (1001) docker     (122)       46 2023-05-08 15:54:18.000000 spacy-3.6.0.dev0/spacy.egg-info/entry_points.txt
--rw-r--r--   0 vsts      (1001) docker     (122)        1 2023-05-08 15:54:18.000000 spacy-3.6.0.dev0/spacy.egg-info/not-zip-safe
--rw-r--r--   0 vsts      (1001) docker     (122)     1475 2023-05-08 15:54:18.000000 spacy-3.6.0.dev0/spacy.egg-info/requires.txt
--rw-r--r--   0 vsts      (1001) docker     (122)        6 2023-05-08 15:54:18.000000 spacy-3.6.0.dev0/spacy.egg-info/top_level.txt
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:20.001715 spacy-4.0.0.dev0/
+-rw-r--r--   0 vsts      (1001) docker     (122)     1128 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/LICENSE
+-rw-r--r--   0 vsts      (1001) docker     (122)      247 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/MANIFEST.in
+-rw-r--r--   0 vsts      (1001) docker     (122)    22433 2023-01-19 08:47:20.001715 spacy-4.0.0.dev0/PKG-INFO
+-rw-r--r--   0 vsts      (1001) docker     (122)    20591 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/README.md
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.833714 spacy-4.0.0.dev0/licenses/
+-rw-r--r--   0 vsts      (1001) docker     (122)     6541 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/licenses/3rd_party_licenses.txt
+-rw-r--r--   0 vsts      (1001) docker     (122)      255 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/pyproject.toml
+-rw-r--r--   0 vsts      (1001) docker     (122)     3242 2023-01-19 08:47:20.001715 spacy-4.0.0.dev0/setup.cfg
+-rwxr-xr-x   0 vsts      (1001) docker     (122)     7754 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/setup.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.841714 spacy-4.0.0.dev0/spacy/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/__init__.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)     2954 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)       80 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/__main__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      326 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/about.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1277 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/attrs.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)     2664 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/attrs.pyx
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.845714 spacy-4.0.0.dev0/spacy/cli/
+-rw-r--r--   0 vsts      (1001) docker     (122)     1925 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/cli/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    24161 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/cli/_util.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4845 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/cli/apply.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2574 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/cli/assemble.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     9401 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/cli/convert.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4546 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/cli/debug_config.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    46061 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/cli/debug_data.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3580 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/cli/debug_diff.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     8909 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/cli/debug_model.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4419 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/cli/download.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     8191 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/cli/evaluate.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     9379 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/cli/find_threshold.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     6403 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/cli/info.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    10376 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/cli/init_config.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     5644 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/cli/init_pipeline.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    21380 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/cli/package.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     5141 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/cli/pretrain.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3452 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/cli/profile.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.845714 spacy-4.0.0.dev0/spacy/cli/project/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/cli/project/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     8208 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/cli/project/assets.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4754 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/cli/project/clone.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     5138 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/cli/project/document.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     8543 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/cli/project/dvc.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2862 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/cli/project/pull.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2764 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/cli/project/push.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     8236 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/cli/project/remote_storage.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    15834 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/cli/project/run.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.845714 spacy-4.0.0.dev0/spacy/cli/templates/
+-rw-r--r--   0 vsts      (1001) docker     (122)    12928 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/cli/templates/quickstart_training.jinja
+-rw-r--r--   0 vsts      (1001) docker     (122)     6294 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/cli/templates/quickstart_training_recommendations.yml
+-rw-r--r--   0 vsts      (1001) docker     (122)     3399 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/cli/train.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4575 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/cli/validate.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1346 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/compat.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4350 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/default_config.cfg
+-rw-r--r--   0 vsts      (1001) docker     (122)      738 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/default_config_pretraining.cfg
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.845714 spacy-4.0.0.dev0/spacy/displacy/
+-rw-r--r--   0 vsts      (1001) docker     (122)    10266 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/displacy/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    23810 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/displacy/render.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4656 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/displacy/templates.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    65718 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/errors.py
+-rw-r--r--   0 vsts      (1001) docker     (122)       73 2023-01-19 08:46:30.000000 spacy-4.0.0.dev0/spacy/git_info.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    13185 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/glossary.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.845714 spacy-4.0.0.dev0/spacy/kb/
+-rw-r--r--   0 vsts      (1001) docker     (122)      144 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/kb/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      390 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/kb/candidate.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)     2606 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/kb/candidate.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)      265 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/kb/kb.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)     4460 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/kb/kb.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)     6797 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/kb/kb_in_memory.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)    26431 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/kb/kb_in_memory.pyx
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.849714 spacy-4.0.0.dev0/spacy/lang/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/__init__.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.849714 spacy-4.0.0.dev0/spacy/lang/af/
+-rw-r--r--   0 vsts      (1001) docker     (122)      255 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/af/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      291 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/af/stop_words.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.849714 spacy-4.0.0.dev0/spacy/lang/am/
+-rw-r--r--   0 vsts      (1001) docker     (122)      831 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/am/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      792 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/am/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2192 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/am/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      544 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/am/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3202 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/am/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      291 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/am/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.849714 spacy-4.0.0.dev0/spacy/lang/ar/
+-rw-r--r--   0 vsts      (1001) docker     (122)      572 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ar/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      890 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ar/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1309 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ar/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      460 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ar/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3180 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ar/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1502 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ar/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.849714 spacy-4.0.0.dev0/spacy/lang/az/
+-rw-r--r--   0 vsts      (1001) docker     (122)      329 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/az/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      747 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/az/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1697 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/az/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      966 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/az/stop_words.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.849714 spacy-4.0.0.dev0/spacy/lang/bg/
+-rw-r--r--   0 vsts      (1001) docker     (122)      919 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/bg/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      636 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/bg/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2048 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/bg/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4748 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/bg/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     9110 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/bg/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.853714 spacy-4.0.0.dev0/spacy/lang/bn/
+-rw-r--r--   0 vsts      (1001) docker     (122)     1175 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/bn/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      305 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/bn/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1270 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/bn/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     6156 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/bn/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      971 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/bn/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.853714 spacy-4.0.0.dev0/spacy/lang/ca/
+-rwxr-xr-x   0 vsts      (1001) docker     (122)     1344 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ca/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      595 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ca/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2843 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ca/lemmatizer.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      956 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ca/lex_attrs.py
+-rwxr-xr-x   0 vsts      (1001) docker     (122)     1650 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ca/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1619 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ca/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1994 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ca/syntax_iterators.py
+-rwxr-xr-x   0 vsts      (1001) docker     (122)     1962 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ca/tokenizer_exceptions.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    14678 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/char_classes.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.853714 spacy-4.0.0.dev0/spacy/lang/cs/
+-rw-r--r--   0 vsts      (1001) docker     (122)      305 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/cs/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1514 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/cs/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1074 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/cs/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2263 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/cs/stop_words.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.853714 spacy-4.0.0.dev0/spacy/lang/da/
+-rw-r--r--   0 vsts      (1001) docker     (122)      628 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/da/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      568 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/da/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3575 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/da/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      912 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/da/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1345 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/da/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2188 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/da/syntax_iterators.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     8956 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/da/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.857714 spacy-4.0.0.dev0/spacy/lang/de/
+-rw-r--r--   0 vsts      (1001) docker     (122)      616 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/de/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      679 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/de/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1413 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/de/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3661 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/de/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1850 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/de/syntax_iterators.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     5889 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/de/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.857714 spacy-4.0.0.dev0/spacy/lang/dsb/
+-rw-r--r--   0 vsts      (1001) docker     (122)      334 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/dsb/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      553 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/dsb/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1906 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/dsb/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      118 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/dsb/stop_words.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.857714 spacy-4.0.0.dev0/spacy/lang/el/
+-rw-r--r--   0 vsts      (1001) docker     (122)     1329 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/el/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1972 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/el/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2086 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/el/get_pos_from_wiktionary.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2195 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/el/lemmatizer.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2320 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/el/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3411 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/el/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     8100 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/el/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2290 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/el/syntax_iterators.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    10077 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/el/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.857714 spacy-4.0.0.dev0/spacy/lang/en/
+-rw-r--r--   0 vsts      (1001) docker     (122)     1235 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/en/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      556 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/en/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1480 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/en/lemmatizer.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1586 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/en/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      569 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/en/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2148 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/en/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1570 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/en/syntax_iterators.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    14252 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/en/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.861714 spacy-4.0.0.dev0/spacy/lang/es/
+-rw-r--r--   0 vsts      (1001) docker     (122)     1288 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/es/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      778 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/es/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    16020 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/es/lemmatizer.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1796 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/es/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1234 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/es/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3388 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/es/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2714 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/es/syntax_iterators.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1458 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/es/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.861714 spacy-4.0.0.dev0/spacy/lang/et/
+-rw-r--r--   0 vsts      (1001) docker     (122)      251 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/et/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      246 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/et/stop_words.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.861714 spacy-4.0.0.dev0/spacy/lang/eu/
+-rw-r--r--   0 vsts      (1001) docker     (122)      387 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/eu/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      419 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/eu/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1093 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/eu/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)       78 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/eu/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      760 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/eu/stop_words.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.861714 spacy-4.0.0.dev0/spacy/lang/fa/
+-rw-r--r--   0 vsts      (1001) docker     (122)     1305 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/fa/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      515 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/fa/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    14856 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/fa/generate_verbs_exc.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1386 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/fa/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      505 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/fa/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3768 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/fa/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1555 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/fa/syntax_iterators.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    64921 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/fa/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.861714 spacy-4.0.0.dev0/spacy/lang/fi/
+-rw-r--r--   0 vsts      (1001) docker     (122)      632 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/fi/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      545 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/fi/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1077 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/fi/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      857 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/fi/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     6436 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/fi/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2379 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/fi/syntax_iterators.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2468 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/fi/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.865714 spacy-4.0.0.dev0/spacy/lang/fr/
+-rw-r--r--   0 vsts      (1001) docker     (122)     1404 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/fr/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)   360608 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/fr/_tokenizer_exceptions_list.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      938 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/fr/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3016 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/fr/lemmatizer.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1627 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/fr/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1476 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/fr/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3504 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/fr/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3124 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/fr/syntax_iterators.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    11232 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/fr/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.865714 spacy-4.0.0.dev0/spacy/lang/ga/
+-rw-r--r--   0 vsts      (1001) docker     (122)      819 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ga/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4936 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ga/lemmatizer.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      608 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ga/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1883 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ga/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.865714 spacy-4.0.0.dev0/spacy/lang/grc/
+-rw-r--r--   0 vsts      (1001) docker     (122)      620 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/grc/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1066 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/grc/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     6641 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/grc/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1082 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/grc/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     9364 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/grc/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     6768 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/grc/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.865714 spacy-4.0.0.dev0/spacy/lang/gu/
+-rw-r--r--   0 vsts      (1001) docker     (122)      251 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/gu/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1215 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/gu/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1004 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/gu/stop_words.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.865714 spacy-4.0.0.dev0/spacy/lang/he/
+-rw-r--r--   0 vsts      (1001) docker     (122)      391 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/he/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      994 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/he/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1759 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/he/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1856 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/he/stop_words.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.869714 spacy-4.0.0.dev0/spacy/lang/hi/
+-rw-r--r--   0 vsts      (1001) docker     (122)      305 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/hi/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1518 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/hi/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     5889 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/hi/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2975 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/hi/stop_words.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.869714 spacy-4.0.0.dev0/spacy/lang/hr/
+-rw-r--r--   0 vsts      (1001) docker     (122)      251 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/hr/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      489 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/hr/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      970 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/hr/lemma_lookup_license.txt
+-rw-r--r--   0 vsts      (1001) docker     (122)     1999 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/hr/stop_words.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.869714 spacy-4.0.0.dev0/spacy/lang/hsb/
+-rw-r--r--   0 vsts      (1001) docker     (122)      437 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/hsb/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      640 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/hsb/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1791 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/hsb/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      123 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/hsb/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      386 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/hsb/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.869714 spacy-4.0.0.dev0/spacy/lang/hu/
+-rw-r--r--   0 vsts      (1001) docker     (122)      584 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/hu/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      407 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/hu/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1491 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/hu/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1384 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/hu/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     8401 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/hu/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.869714 spacy-4.0.0.dev0/spacy/lang/hy/
+-rw-r--r--   0 vsts      (1001) docker     (122)      317 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/hy/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      441 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/hy/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1160 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/hy/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1067 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/hy/stop_words.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.869714 spacy-4.0.0.dev0/spacy/lang/id/
+-rw-r--r--   0 vsts      (1001) docker     (122)      698 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/id/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    53599 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/id/_tokenizer_exceptions_list.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      726 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/id/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1277 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/id/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2139 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/id/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     6507 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/id/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1538 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/id/syntax_iterators.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4205 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/id/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.873714 spacy-4.0.0.dev0/spacy/lang/is/
+-rw-r--r--   0 vsts      (1001) docker     (122)      255 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/is/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1019 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/is/stop_words.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.873714 spacy-4.0.0.dev0/spacy/lang/it/
+-rw-r--r--   0 vsts      (1001) docker     (122)     1229 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/it/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      471 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/it/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4615 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/it/lemmatizer.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      873 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/it/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4114 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/it/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3137 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/it/syntax_iterators.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1161 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/it/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.873714 spacy-4.0.0.dev0/spacy/lang/ja/
+-rw-r--r--   0 vsts      (1001) docker     (122)    12623 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ja/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      499 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ja/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1328 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ja/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1639 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ja/syntax_iterators.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1647 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ja/tag_bigram_map.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3741 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ja/tag_map.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      546 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ja/tag_orth_map.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.873714 spacy-4.0.0.dev0/spacy/lang/kn/
+-rw-r--r--   0 vsts      (1001) docker     (122)      247 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/kn/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1211 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/kn/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1253 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/kn/stop_words.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.873714 spacy-4.0.0.dev0/spacy/lang/ko/
+-rw-r--r--   0 vsts      (1001) docker     (122)     6887 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ko/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      531 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ko/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1052 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ko/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      268 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ko/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      349 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ko/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1928 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ko/tag_map.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.877714 spacy-4.0.0.dev0/spacy/lang/ky/
+-rw-r--r--   0 vsts      (1001) docker     (122)      487 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ky/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      917 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ky/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      925 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ky/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      848 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ky/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1064 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ky/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2049 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ky/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.877714 spacy-4.0.0.dev0/spacy/lang/la/
+-rw-r--r--   0 vsts      (1001) docker     (122)      408 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/la/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      741 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/la/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      619 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/la/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1175 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/la/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.877714 spacy-4.0.0.dev0/spacy/lang/lb/
+-rw-r--r--   0 vsts      (1001) docker     (122)      515 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/lb/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      906 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/lb/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1343 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/lb/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      641 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/lb/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1127 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/lb/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1168 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/lb/tokenizer_exceptions.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     5944 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/lex_attrs.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.877714 spacy-4.0.0.dev0/spacy/lang/lg/
+-rw-r--r--   0 vsts      (1001) docker     (122)      388 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/lg/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      522 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/lg/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2680 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/lg/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      569 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/lg/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1361 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/lg/stop_words.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.877714 spacy-4.0.0.dev0/spacy/lang/lij/
+-rw-r--r--   0 vsts      (1001) docker     (122)      430 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/lij/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      397 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/lij/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      270 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/lij/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      853 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/lij/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      871 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/lij/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.881714 spacy-4.0.0.dev0/spacy/lang/lt/
+-rw-r--r--   0 vsts      (1001) docker     (122)      557 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/lt/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      602 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/lt/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    22464 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/lt/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      716 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/lt/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    19708 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/lt/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      383 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/lt/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.881714 spacy-4.0.0.dev0/spacy/lang/lv/
+-rw-r--r--   0 vsts      (1001) docker     (122)      247 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/lv/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1085 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/lv/stop_words.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.881714 spacy-4.0.0.dev0/spacy/lang/mk/
+-rw-r--r--   0 vsts      (1001) docker     (122)     1689 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/mk/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1718 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/mk/lemmatizer.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3468 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/mk/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     9106 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/mk/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3577 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/mk/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.881714 spacy-4.0.0.dev0/spacy/lang/ml/
+-rw-r--r--   0 vsts      (1001) docker     (122)      321 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ml/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1403 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ml/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2345 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ml/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      184 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ml/stop_words.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.881714 spacy-4.0.0.dev0/spacy/lang/mr/
+-rw-r--r--   0 vsts      (1001) docker     (122)      247 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/mr/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2456 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/mr/stop_words.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.881714 spacy-4.0.0.dev0/spacy/lang/nb/
+-rw-r--r--   0 vsts      (1001) docker     (122)     1296 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/nb/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      428 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/nb/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1663 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/nb/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1160 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/nb/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1523 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/nb/syntax_iterators.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3069 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/nb/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.881714 spacy-4.0.0.dev0/spacy/lang/ne/
+-rw-r--r--   0 vsts      (1001) docker     (122)      309 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ne/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1104 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ne/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3276 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ne/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     7135 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ne/stop_words.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.885714 spacy-4.0.0.dev0/spacy/lang/nl/
+-rw-r--r--   0 vsts      (1001) docker     (122)     1354 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/nl/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      441 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/nl/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4618 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/nl/lemmatizer.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1303 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/nl/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1539 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/nl/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3090 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/nl/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2868 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/nl/syntax_iterators.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    24299 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/nl/tokenizer_exceptions.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1425 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/norm_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.885714 spacy-4.0.0.dev0/spacy/lang/pl/
+-rw-r--r--   0 vsts      (1001) docker     (122)     1383 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/pl/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      685 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/pl/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3566 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/pl/lemmatizer.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1193 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/pl/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1362 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/pl/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2360 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/pl/stop_words.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.885714 spacy-4.0.0.dev0/spacy/lang/pt/
+-rw-r--r--   0 vsts      (1001) docker     (122)      644 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/pt/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      472 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/pt/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2038 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/pt/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      456 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/pt/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2566 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/pt/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3088 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/pt/syntax_iterators.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      716 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/pt/tokenizer_exceptions.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2192 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/punctuation.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.885714 spacy-4.0.0.dev0/spacy/lang/ro/
+-rw-r--r--   0 vsts      (1001) docker     (122)      802 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ro/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      601 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ro/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1680 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ro/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3158 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ro/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2945 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ro/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1461 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ro/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.889715 spacy-4.0.0.dev0/spacy/lang/ru/
+-rw-r--r--   0 vsts      (1001) docker     (122)     1317 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ru/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4581 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ru/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     7976 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ru/lemmatizer.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    18734 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ru/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     8356 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ru/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    25394 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ru/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.889715 spacy-4.0.0.dev0/spacy/lang/sa/
+-rw-r--r--   0 vsts      (1001) docker     (122)      317 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/sa/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      781 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/sa/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4211 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/sa/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     9364 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/sa/stop_words.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.889715 spacy-4.0.0.dev0/spacy/lang/si/
+-rw-r--r--   0 vsts      (1001) docker     (122)      313 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/si/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      944 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/si/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1253 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/si/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2407 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/si/stop_words.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.889715 spacy-4.0.0.dev0/spacy/lang/sk/
+-rw-r--r--   0 vsts      (1001) docker     (122)      309 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/sk/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      729 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/sk/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1070 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/sk/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2639 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/sk/stop_words.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.889715 spacy-4.0.0.dev0/spacy/lang/sl/
+-rw-r--r--   0 vsts      (1001) docker     (122)      607 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/sl/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      575 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/sl/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     6603 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/sl/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3274 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/sl/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2478 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/sl/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    13428 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/sl/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.889715 spacy-4.0.0.dev0/spacy/lang/sq/
+-rw-r--r--   0 vsts      (1001) docker     (122)      251 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/sq/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      467 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/sq/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1210 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/sq/stop_words.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.893715 spacy-4.0.0.dev0/spacy/lang/sr/
+-rw-r--r--   0 vsts      (1001) docker     (122)      416 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/sr/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      910 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/sr/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1549 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/sr/lemma_lookup_licence.txt
+-rw-r--r--   0 vsts      (1001) docker     (122)     1426 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/sr/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3895 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/sr/stop_words.py
+-rwxr-xr-x   0 vsts      (1001) docker     (122)     3524 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/sr/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.893715 spacy-4.0.0.dev0/spacy/lang/sv/
+-rw-r--r--   0 vsts      (1001) docker     (122)     1313 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/sv/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      441 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/sv/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      954 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/sv/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2545 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/sv/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1531 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/sv/syntax_iterators.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3655 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/sv/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.893715 spacy-4.0.0.dev0/spacy/lang/ta/
+-rw-r--r--   0 vsts      (1001) docker     (122)      305 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ta/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2703 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ta/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2288 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ta/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2018 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ta/stop_words.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.893715 spacy-4.0.0.dev0/spacy/lang/te/
+-rw-r--r--   0 vsts      (1001) docker     (122)      309 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/te/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1243 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/te/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1263 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/te/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1107 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/te/stop_words.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.897714 spacy-4.0.0.dev0/spacy/lang/th/
+-rw-r--r--   0 vsts      (1001) docker     (122)     1372 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/th/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1478 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/th/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    19463 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/th/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    18334 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/th/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.897714 spacy-4.0.0.dev0/spacy/lang/ti/
+-rw-r--r--   0 vsts      (1001) docker     (122)      835 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ti/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      821 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ti/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1508 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ti/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      545 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ti/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1977 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ti/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      339 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ti/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.897714 spacy-4.0.0.dev0/spacy/lang/tl/
+-rw-r--r--   0 vsts      (1001) docker     (122)      416 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/tl/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      943 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/tl/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      965 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/tl/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      688 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/tl/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.897714 spacy-4.0.0.dev0/spacy/lang/tn/
+-rw-r--r--   0 vsts      (1001) docker     (122)      392 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/tn/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      436 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/tn/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2050 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/tn/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      588 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/tn/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      816 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/tn/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3304 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.901715 spacy-4.0.0.dev0/spacy/lang/tr/
+-rw-r--r--   0 vsts      (1001) docker     (122)      546 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/tr/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      609 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/tr/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1674 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/tr/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4506 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/tr/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1853 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/tr/syntax_iterators.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     6092 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/tr/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.901715 spacy-4.0.0.dev0/spacy/lang/tt/
+-rw-r--r--   0 vsts      (1001) docker     (122)      483 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/tt/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      897 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/tt/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1117 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/tt/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      799 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/tt/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    18567 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/tt/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1761 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/tt/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.901715 spacy-4.0.0.dev0/spacy/lang/uk/
+-rw-r--r--   0 vsts      (1001) docker     (122)     1332 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/uk/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1798 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/uk/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1716 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/uk/lemmatizer.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1575 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/uk/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4887 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/uk/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1389 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/uk/tokenizer_exceptions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.905715 spacy-4.0.0.dev0/spacy/lang/ur/
+-rw-r--r--   0 vsts      (1001) docker     (122)      461 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ur/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      303 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ur/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2237 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ur/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)       78 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ur/punctuation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4722 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/ur/stop_words.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.905715 spacy-4.0.0.dev0/spacy/lang/vi/
+-rw-r--r--   0 vsts      (1001) docker     (122)     5575 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/vi/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      769 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/vi/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1397 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/vi/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    20595 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/vi/stop_words.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.905715 spacy-4.0.0.dev0/spacy/lang/xx/
+-rw-r--r--   0 vsts      (1001) docker     (122)      266 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/xx/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     8161 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/xx/examples.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.905715 spacy-4.0.0.dev0/spacy/lang/yo/
+-rw-r--r--   0 vsts      (1001) docker     (122)      309 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/yo/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1269 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/yo/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2523 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/yo/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      608 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/yo/stop_words.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.909715 spacy-4.0.0.dev0/spacy/lang/zh/
+-rw-r--r--   0 vsts      (1001) docker     (122)    12738 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/zh/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      792 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/zh/examples.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1613 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/zh/lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    13409 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lang/zh/stop_words.py
+-rw-r--r--   0 vsts      (1001) docker     (122)   101241 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/language.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2587 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lexeme.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)     1411 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lexeme.pyi
+-rw-r--r--   0 vsts      (1001) docker     (122)    16255 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lexeme.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)    10851 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/lookups.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.909715 spacy-4.0.0.dev0/spacy/matcher/
+-rw-r--r--   0 vsts      (1001) docker     (122)      232 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/matcher/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2124 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/matcher/dependencymatcher.pyi
+-rw-r--r--   0 vsts      (1001) docker     (122)    17097 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/matcher/dependencymatcher.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)   226181 2023-01-19 08:46:35.000000 spacy-4.0.0.dev0/spacy/matcher/levenshtein.c
+-rw-r--r--   0 vsts      (1001) docker     (122)      945 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/matcher/levenshtein.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)     1530 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/matcher/matcher.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)     1824 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/matcher/matcher.pyi
+-rw-r--r--   0 vsts      (1001) docker     (122)    49465 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/matcher/matcher.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)      555 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/matcher/phrasematcher.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)     1273 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/matcher/phrasematcher.pyi
+-rw-r--r--   0 vsts      (1001) docker     (122)    14701 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/matcher/phrasematcher.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)     9571 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/matcher/polyleven.c
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.913715 spacy-4.0.0.dev0/spacy/ml/
+-rw-r--r--   0 vsts      (1001) docker     (122)      109 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/ml/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3819 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/ml/callbacks.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1993 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/ml/character_embed.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1191 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/ml/extract_ngrams.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2317 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/ml/extract_spans.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      969 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/ml/featureextractor.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.913715 spacy-4.0.0.dev0/spacy/ml/models/
+-rw-r--r--   0 vsts      (1001) docker     (122)      224 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/ml/models/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3973 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/ml/models/entity_linker.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     8745 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/ml/models/multi_task.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3984 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/ml/models/parser.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2370 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/ml/models/spancat.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1252 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/ml/models/tagger.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     6748 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/ml/models/textcat.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    14020 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/ml/models/tok2vec.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3851 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/ml/staticvectors.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      475 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/ml/tb_framework.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)    23395 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/ml/tb_framework.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)     1297 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/morphology.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)     9560 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/morphology.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)      538 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/parts_of_speech.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)      434 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/parts_of_speech.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)     6241 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipe_analysis.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.921715 spacy-4.0.0.dev0/spacy/pipeline/
+-rw-r--r--   0 vsts      (1001) docker     (122)     1144 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/__init__.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.921715 spacy-4.0.0.dev0/spacy/pipeline/_edit_tree_internals/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/_edit_tree_internals/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3449 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/_edit_tree_internals/edit_trees.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)    10747 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/_edit_tree_internals/edit_trees.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)     1474 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/_edit_tree_internals/schemas.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.925715 spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/__init__.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      249 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/_beam_utils.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)    11618 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/_beam_utils.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)      146 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/_parser_utils.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)      633 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/_parser_utils.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)    12621 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/_state.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/_state.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)      211 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/arc_eager.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)    30513 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/arc_eager.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)       27 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/batch.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)     1299 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/batch.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)      104 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/ner.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)    23467 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/ner.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)      187 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/nonproj.hh
+-rw-r--r--   0 vsts      (1001) docker     (122)      133 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/nonproj.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)     8603 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/nonproj.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)     2652 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/search.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)    12326 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/search.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)      744 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/stateclass.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)     4831 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/stateclass.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)     1975 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/transition_system.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)    11911 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/transition_system.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)    13693 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/attribute_ruler.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    13081 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/dep_parser.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    14817 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/edit_tree_lemmatizer.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    30367 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/entity_linker.py
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/entityruler.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     6703 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/functions.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.925715 spacy-4.0.0.dev0/spacy/pipeline/legacy/
+-rw-r--r--   0 vsts      (1001) docker     (122)       74 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/legacy/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    18808 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/legacy/entity_linker.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    12178 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/lemmatizer.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    13516 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/morphologizer.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)    10150 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/ner.py
+-rw-r--r--   0 vsts      (1001) docker     (122)       42 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/pipe.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)     1224 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/pipe.pyi
+-rw-r--r--   0 vsts      (1001) docker     (122)     5382 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/pipe.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)     6836 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/sentencizer.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)     7351 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/senter.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)    21727 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/span_ruler.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    19114 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/spancat.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    13982 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/tagger.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)    15579 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/textcat.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     6910 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/textcat_multilabel.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    13389 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/tok2vec.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      230 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/trainable_pipe.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)    17601 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/trainable_pipe.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)    31472 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/pipeline/transition_parser.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/py.typed
+-rw-r--r--   0 vsts      (1001) docker     (122)    23759 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/schemas.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    41002 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/scorer.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      726 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/strings.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)     1226 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/strings.pyi
+-rw-r--r--   0 vsts      (1001) docker     (122)    11406 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/strings.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)     2433 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/structs.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)     7063 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/symbols.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)    14118 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/symbols.pyx
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.929715 spacy-4.0.0.dev0/spacy/tests/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    12745 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/conftest.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.933715 spacy-4.0.0.dev0/spacy/tests/doc/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/doc/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2675 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/doc/test_add_entities.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4988 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/doc/test_array.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2650 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/doc/test_creation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    37138 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/doc/test_doc_api.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1819 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/doc/test_graph.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    13658 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/doc/test_json_doc_conversion.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2993 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/doc/test_morphanalysis.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1470 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/doc/test_pickle_doc.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    18955 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/doc/test_retokenize_merge.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    10937 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/doc/test_retokenize_split.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    23446 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/doc/test_span.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     7761 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/doc/test_span_group.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    11164 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/doc/test_token_api.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    10014 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/doc/test_underscore.py
+-rw-r--r--   0 vsts      (1001) docker     (122)       45 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/enable_gpu.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.933715 spacy-4.0.0.dev0/spacy/tests/lang/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/__init__.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.933715 spacy-4.0.0.dev0/spacy/tests/lang/af/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/af/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      931 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/af/test_text.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      710 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/af/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.933715 spacy-4.0.0.dev0/spacy/tests/lang/am/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/am/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/am/test_exception.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1837 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/am/test_text.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.937715 spacy-4.0.0.dev0/spacy/tests/lang/ar/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ar/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      621 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ar/test_exceptions.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      821 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ar/test_text.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.937715 spacy-4.0.0.dev0/spacy/tests/lang/bn/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/bn/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3623 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/bn/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.937715 spacy-4.0.0.dev0/spacy/tests/lang/ca/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ca/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      621 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ca/test_exception.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      493 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ca/test_prefix_suffix_infix.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1900 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ca/test_text.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.937715 spacy-4.0.0.dev0/spacy/tests/lang/cs/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/cs/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      510 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/cs/test_text.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.937715 spacy-4.0.0.dev0/spacy/tests/lang/da/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/da/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1824 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/da/test_exceptions.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2063 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/da/test_noun_chunks.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     5423 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/da/test_prefix_suffix_infix.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1219 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/da/test_text.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.937715 spacy-4.0.0.dev0/spacy/tests/lang/de/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/de/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      598 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/de/test_exceptions.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      266 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/de/test_noun_chunks.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1188 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/de/test_parser.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3395 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/de/test_prefix_suffix_infix.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1505 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/de/test_text.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.941715 spacy-4.0.0.dev0/spacy/tests/lang/dsb/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/dsb/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      560 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/dsb/test_text.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      749 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/dsb/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.941715 spacy-4.0.0.dev0/spacy/tests/lang/el/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/el/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      514 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/el/test_exception.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      306 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/el/test_noun_chunks.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1768 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/el/test_text.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.941715 spacy-4.0.0.dev0/spacy/tests/lang/en/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/en/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3798 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/en/test_customized_tokenizer.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4158 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/en/test_exceptions.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      723 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/en/test_indices.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1548 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/en/test_noun_chunks.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2954 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/en/test_parser.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4254 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/en/test_prefix_suffix_infix.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4422 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/en/test_punct.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1707 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/en/test_sbd.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1962 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/en/test_text.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     5938 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/en/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.945715 spacy-4.0.0.dev0/spacy/tests/lang/es/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/es/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      549 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/es/test_exception.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     9966 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/es/test_noun_chunks.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2025 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/es/test_text.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.945715 spacy-4.0.0.dev0/spacy/tests/lang/et/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/et/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      933 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/et/test_text.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      737 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/et/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.945715 spacy-4.0.0.dev0/spacy/tests/lang/eu/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/eu/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      488 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/eu/test_text.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.945715 spacy-4.0.0.dev0/spacy/tests/lang/fa/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/fa/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      296 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/fa/test_noun_chunks.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.945715 spacy-4.0.0.dev0/spacy/tests/lang/fi/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/fi/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     7258 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/fi/test_noun_chunks.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      542 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/fi/test_text.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2900 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/fi/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.949715 spacy-4.0.0.dev0/spacy/tests/lang/fr/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/fr/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2219 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/fr/test_exceptions.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     8353 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/fr/test_noun_chunks.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      772 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/fr/test_prefix_suffix_infix.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1011 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/fr/test_text.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.949715 spacy-4.0.0.dev0/spacy/tests/lang/ga/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ga/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      685 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ga/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.949715 spacy-4.0.0.dev0/spacy/tests/lang/grc/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/grc/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      543 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/grc/test_text.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1327 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/grc/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.949715 spacy-4.0.0.dev0/spacy/tests/lang/gu/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/gu/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      685 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/gu/test_text.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.949715 spacy-4.0.0.dev0/spacy/tests/lang/he/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/he/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2255 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/he/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.949715 spacy-4.0.0.dev0/spacy/tests/lang/hi/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/hi/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1841 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/hi/test_lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      400 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/hi/test_text.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.949715 spacy-4.0.0.dev0/spacy/tests/lang/hr/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/hr/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      954 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/hr/test_text.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      801 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/hr/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.949715 spacy-4.0.0.dev0/spacy/tests/lang/hsb/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/hsb/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      566 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/hsb/test_text.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      866 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/hsb/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.953715 spacy-4.0.0.dev0/spacy/tests/lang/hu/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/hu/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    14492 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/hu/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.953715 spacy-4.0.0.dev0/spacy/tests/lang/hy/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/hy/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      210 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/hy/test_text.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1351 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/hy/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.953715 spacy-4.0.0.dev0/spacy/tests/lang/id/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/id/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      256 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/id/test_noun_chunks.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3492 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/id/test_prefix_suffix_infix.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      205 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/id/test_text.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.953715 spacy-4.0.0.dev0/spacy/tests/lang/is/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/is/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      980 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/is/test_text.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      783 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/is/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.953715 spacy-4.0.0.dev0/spacy/tests/lang/it/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/it/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     8639 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/it/test_noun_chunks.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      359 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/it/test_prefix_suffix_infix.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      449 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/it/test_stopwords.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      411 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/it/test_text.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.957715 spacy-4.0.0.dev0/spacy/tests/lang/ja/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ja/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      694 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ja/test_lemmatization.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      243 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ja/test_morphologizer_factory.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1306 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ja/test_serialize.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     7900 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ja/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.957715 spacy-4.0.0.dev0/spacy/tests/lang/ko/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ko/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      622 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ko/test_lemmatization.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1403 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ko/test_serialize.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4279 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ko/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.957715 spacy-4.0.0.dev0/spacy/tests/lang/ky/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ky/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4014 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ky/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.957715 spacy-4.0.0.dev0/spacy/tests/lang/la/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/la/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      236 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/la/test_exception.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      803 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/la/test_text.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.957715 spacy-4.0.0.dev0/spacy/tests/lang/lb/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/lb/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      590 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/lb/test_exceptions.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      584 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/lb/test_prefix_suffix_infix.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1302 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/lb/test_text.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.957715 spacy-4.0.0.dev0/spacy/tests/lang/lg/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/lg/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      449 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/lg/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.961715 spacy-4.0.0.dev0/spacy/tests/lang/lt/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/lt/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1644 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/lt/test_text.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.961715 spacy-4.0.0.dev0/spacy/tests/lang/lv/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/lv/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1018 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/lv/test_text.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      778 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/lv/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.961715 spacy-4.0.0.dev0/spacy/tests/lang/mk/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/mk/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4352 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/mk/test_text.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.961715 spacy-4.0.0.dev0/spacy/tests/lang/ml/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ml/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1075 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ml/test_text.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.961715 spacy-4.0.0.dev0/spacy/tests/lang/nb/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/nb/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      277 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/nb/test_noun_chunks.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      630 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/nb/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.961715 spacy-4.0.0.dev0/spacy/tests/lang/ne/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ne/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      783 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ne/test_text.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.961715 spacy-4.0.0.dev0/spacy/tests/lang/nl/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/nl/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4303 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/nl/test_noun_chunks.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      693 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/nl/test_text.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.965715 spacy-4.0.0.dev0/spacy/tests/lang/pl/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/pl/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      522 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/pl/test_text.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      555 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/pl/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.965715 spacy-4.0.0.dev0/spacy/tests/lang/pt/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/pt/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     7748 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/pt/test_noun_chunks.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      219 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/pt/test_text.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.965715 spacy-4.0.0.dev0/spacy/tests/lang/ro/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ro/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      734 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ro/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.965715 spacy-4.0.0.dev0/spacy/tests/lang/ru/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ru/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      353 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ru/test_exceptions.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3958 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ru/test_lemmatizer.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      220 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ru/test_text.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     5983 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ru/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.965715 spacy-4.0.0.dev0/spacy/tests/lang/sa/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/sa/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1297 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/sa/test_text.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.965715 spacy-4.0.0.dev0/spacy/tests/lang/sk/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/sk/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1485 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/sk/test_text.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      453 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/sk/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.965715 spacy-4.0.0.dev0/spacy/tests/lang/sl/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/sl/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      993 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/sl/test_text.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      831 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/sl/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.969715 spacy-4.0.0.dev0/spacy/tests/lang/sq/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/sq/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1178 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/sq/test_text.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      817 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/sq/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.969715 spacy-4.0.0.dev0/spacy/tests/lang/sr/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/sr/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      510 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/sr/test_exceptions.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4302 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/sr/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.969715 spacy-4.0.0.dev0/spacy/tests/lang/sv/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/sv/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2453 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/sv/test_exceptions.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      683 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/sv/test_lex_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1857 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/sv/test_noun_chunks.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1053 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/sv/test_prefix_suffix_infix.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      723 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/sv/test_text.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1005 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/sv/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.969715 spacy-4.0.0.dev0/spacy/tests/lang/ta/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ta/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2737 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ta/test_text.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     7829 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ta/test_tokenizer.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3322 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/test_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      922 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/test_initialize.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1910 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/test_lemmatizers.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.973715 spacy-4.0.0.dev0/spacy/tests/lang/th/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/th/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      706 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/th/test_serialize.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      318 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/th/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.973715 spacy-4.0.0.dev0/spacy/tests/lang/ti/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ti/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ti/test_exception.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2120 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ti/test_text.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.973715 spacy-4.0.0.dev0/spacy/tests/lang/tl/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/tl/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      257 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/tl/test_indices.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4420 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/tl/test_punct.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2479 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/tl/test_text.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.973715 spacy-4.0.0.dev0/spacy/tests/lang/tr/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/tr/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      422 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/tr/test_noun_chunks.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    19784 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/tr/test_parser.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1796 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/tr/test_text.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    19589 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/tr/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.973715 spacy-4.0.0.dev0/spacy/tests/lang/tt/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/tt/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3721 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/tt/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.977715 spacy-4.0.0.dev0/spacy/tests/lang/uk/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/uk/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      837 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/uk/test_lemmatizer.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     5416 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/uk/test_tokenizer.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      364 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/uk/test_tokenizer_exc.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.977715 spacy-4.0.0.dev0/spacy/tests/lang/ur/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ur/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      229 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ur/test_prefix_suffix_infix.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      479 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/ur/test_text.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.977715 spacy-4.0.0.dev0/spacy/tests/lang/vi/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/vi/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1308 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/vi/test_serialize.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1677 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/vi/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.977715 spacy-4.0.0.dev0/spacy/tests/lang/xx/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/xx/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1720 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/xx/test_text.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      669 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/xx/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.977715 spacy-4.0.0.dev0/spacy/tests/lang/yo/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/yo/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1490 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/yo/test_text.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.981715 spacy-4.0.0.dev0/spacy/tests/lang/zh/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/zh/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1245 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/zh/test_serialize.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      454 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/zh/test_text.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2819 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/lang/zh/test_tokenizer.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.981715 spacy-4.0.0.dev0/spacy/tests/matcher/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/matcher/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    14304 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/matcher/test_dependency_matcher.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2797 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/matcher/test_levenshtein.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    30055 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/matcher/test_matcher_api.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    27183 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/matcher/test_matcher_logic.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3334 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/matcher/test_pattern_validation.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    17364 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/matcher/test_phrase_matcher.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.985715 spacy-4.0.0.dev0/spacy/tests/morphology/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/morphology/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      856 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/morphology/test_morph_converters.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1348 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/morphology/test_morph_features.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      668 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/morphology/test_morph_pickle.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.985715 spacy-4.0.0.dev0/spacy/tests/package/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/package/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      255 2023-01-19 08:46:30.000000 spacy-4.0.0.dev0/spacy/tests/package/pyproject.toml
+-rw-r--r--   0 vsts      (1001) docker     (122)      992 2023-01-19 08:46:30.000000 spacy-4.0.0.dev0/spacy/tests/package/requirements.txt
+-rw-r--r--   0 vsts      (1001) docker     (122)     3523 2023-01-19 08:46:30.000000 spacy-4.0.0.dev0/spacy/tests/package/setup.cfg
+-rw-r--r--   0 vsts      (1001) docker     (122)     3151 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/package/test_requirements.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.989715 spacy-4.0.0.dev0/spacy/tests/parser/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/parser/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3148 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/parser/_search.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)     4919 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/parser/test_add_label.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    10085 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/parser/test_arc_eager_oracle.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    30390 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/parser/test_ner.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2859 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/parser/test_neural_parser.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3600 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/parser/test_nn_beam.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     6289 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/parser/test_nonproj.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    23784 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/parser/test_parse.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     6215 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/parser/test_parse_navigate.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2560 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/parser/test_preset_sbd.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      108 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/parser/test_search.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2927 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/parser/test_space_attachment.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1894 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/parser/test_state.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.989715 spacy-4.0.0.dev0/spacy/tests/pipeline/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/pipeline/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3456 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/pipeline/test_analysis.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3192 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/pipeline/test_annotates_on_update.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     9535 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/pipeline/test_attributeruler.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    12043 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/pipeline/test_edit_tree_lemmatizer.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    47651 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/pipeline/test_entity_linker.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    20789 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/pipeline/test_entity_ruler.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3188 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/pipeline/test_functions.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2350 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/pipeline/test_initialize.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3725 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/pipeline/test_lemmatizer.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3842 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/pipeline/test_models.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     8088 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/pipeline/test_morphologizer.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    19895 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/pipeline/test_pipe_factories.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    23041 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/pipeline/test_pipe_methods.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    18273 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/pipeline/test_sentencizer.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4294 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/pipeline/test_senter.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    16221 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/pipeline/test_span_ruler.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    15962 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/pipeline/test_spancat.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     8887 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/pipeline/test_tagger.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    37365 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/pipeline/test_textcat.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    18811 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/pipeline/test_tok2vec.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.993715 spacy-4.0.0.dev0/spacy/tests/serialize/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/serialize/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4185 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/serialize/test_resource_warning.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    16025 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/serialize/test_serialize_config.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     6931 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/serialize/test_serialize_doc.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3687 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/serialize/test_serialize_docbin.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      945 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/serialize/test_serialize_extension_attrs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4726 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/serialize/test_serialize_kb.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3594 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/serialize/test_serialize_language.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    15013 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/serialize/test_serialize_pipeline.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     8766 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/serialize/test_serialize_span_groups.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     5442 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/serialize/test_serialize_tokenizer.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     7067 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/serialize/test_serialize_vocab_strings.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      447 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/test_architectures.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    43308 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/test_cli.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1203 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/test_cli_app.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    12974 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/test_displacy.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      333 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/test_errors.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    26991 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/test_language.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    13415 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/test_misc.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     9089 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/test_models.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2022 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/test_pickles.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    16857 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/test_scorer.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    10600 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/test_symbols.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      748 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/test_ty.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.993715 spacy-4.0.0.dev0/spacy/tests/tokenizer/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/tokenizer/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2591 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/tokenizer/sun.txt
+-rw-r--r--   0 vsts      (1001) docker     (122)     1756 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/tokenizer/test_exceptions.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4850 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/tokenizer/test_explain.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     7466 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/tokenizer/test_naughty_strings.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    18589 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/tokenizer/test_tokenizer.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     6605 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/tokenizer/test_urls.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1295 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/tokenizer/test_whitespace.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.993715 spacy-4.0.0.dev0/spacy/tests/training/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/training/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    10607 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/training/test_augmenters.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      600 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/training/test_logger.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    16145 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/training/test_new_example.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    10842 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/training/test_pretraining.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3947 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/training/test_readers.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     6405 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/training/test_rehearse.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    46167 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/training/test_training.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3204 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/util.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.993715 spacy-4.0.0.dev0/spacy/tests/vocab_vectors/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/vocab_vectors/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2852 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/vocab_vectors/test_lexeme.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     4651 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/vocab_vectors/test_lookups.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3834 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/vocab_vectors/test_similarity.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3939 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/vocab_vectors/test_stringstore.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    22581 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/vocab_vectors/test_vectors.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2147 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tests/vocab_vectors/test_vocab_api.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2158 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tokenizer.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)    36322 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tokenizer.pyx
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.997715 spacy-4.0.0.dev0/spacy/tokens/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tokens/__init__.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)      248 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tokens/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1667 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tokens/doc.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)     5947 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tokens/doc.pyi
+-rw-r--r--   0 vsts      (1001) docker     (122)    83730 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tokens/doc.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)    11891 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tokens/doc_bin.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      295 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tokens/graph.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)    23114 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tokens/graph.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)      303 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tokens/morphanalysis.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)      820 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tokens/morphanalysis.pyi
+-rw-r--r--   0 vsts      (1001) docker     (122)     2941 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tokens/morphanalysis.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)      702 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tokens/retokenizer.pyi
+-rw-r--r--   0 vsts      (1001) docker     (122)    20452 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tokens/retokenizer.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)      632 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tokens/span.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)     3821 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tokens/span.pyi
+-rw-r--r--   0 vsts      (1001) docker     (122)    34475 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tokens/span.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)      307 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tokens/span_group.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)      835 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tokens/span_group.pyi
+-rw-r--r--   0 vsts      (1001) docker     (122)    11137 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tokens/span_group.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)     4515 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tokens/span_groups.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3419 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tokens/token.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)     5346 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tokens/token.pyi
+-rw-r--r--   0 vsts      (1001) docker     (122)    33015 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tokens/token.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)     6926 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/tokens/underscore.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:20.001715 spacy-4.0.0.dev0/spacy/training/
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/training/__init__.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)      880 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/training/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     3194 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/training/align.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)      614 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/training/alignment.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      170 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/training/alignment_array.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)     2128 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/training/alignment_array.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)    13279 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/training/augment.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     9446 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/training/batchers.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1267 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/training/callbacks.py
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:20.001715 spacy-4.0.0.dev0/spacy/training/converters/
+-rw-r--r--   0 vsts      (1001) docker     (122)      224 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/training/converters/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     6177 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/training/converters/conll_ner_to_docs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    10275 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/training/converters/conllu_to_docs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     2356 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/training/converters/iob_to_docs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      883 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/training/converters/json_to_docs.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     9323 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/training/corpus.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      327 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/training/example.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)    25036 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/training/example.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)     8027 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/training/gold_io.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)    13716 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/training/initialize.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     9075 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/training/iob_utils.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     7819 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/training/loggers.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    15059 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/training/loop.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     9356 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/training/pretrain.py
+-rw-r--r--   0 vsts      (1001) docker     (122)     1299 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/ty.py
+-rw-r--r--   0 vsts      (1001) docker     (122)      283 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/typedefs.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)        0 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/typedefs.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)    65665 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/util.py
+-rw-r--r--   0 vsts      (1001) docker     (122)    25940 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/vectors.pyx
+-rw-r--r--   0 vsts      (1001) docker     (122)     1305 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/vocab.pxd
+-rw-r--r--   0 vsts      (1001) docker     (122)     2727 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/vocab.pyi
+-rw-r--r--   0 vsts      (1001) docker     (122)    23180 2023-01-19 08:46:07.000000 spacy-4.0.0.dev0/spacy/vocab.pyx
+drwxr-xr-x   0 vsts      (1001) docker     (122)        0 2023-01-19 08:47:19.841714 spacy-4.0.0.dev0/spacy.egg-info/
+-rw-r--r--   0 vsts      (1001) docker     (122)    22433 2023-01-19 08:47:19.000000 spacy-4.0.0.dev0/spacy.egg-info/PKG-INFO
+-rw-r--r--   0 vsts      (1001) docker     (122)    29202 2023-01-19 08:47:19.000000 spacy-4.0.0.dev0/spacy.egg-info/SOURCES.txt
+-rw-r--r--   0 vsts      (1001) docker     (122)        1 2023-01-19 08:47:19.000000 spacy-4.0.0.dev0/spacy.egg-info/dependency_links.txt
+-rw-r--r--   0 vsts      (1001) docker     (122)       46 2023-01-19 08:47:19.000000 spacy-4.0.0.dev0/spacy.egg-info/entry_points.txt
+-rw-r--r--   0 vsts      (1001) docker     (122)        1 2023-01-19 08:47:19.000000 spacy-4.0.0.dev0/spacy.egg-info/not-zip-safe
+-rw-r--r--   0 vsts      (1001) docker     (122)     1478 2023-01-19 08:47:19.000000 spacy-4.0.0.dev0/spacy.egg-info/requires.txt
+-rw-r--r--   0 vsts      (1001) docker     (122)        6 2023-01-19 08:47:19.000000 spacy-4.0.0.dev0/spacy.egg-info/top_level.txt
```

### Comparing `spacy-3.6.0.dev0/LICENSE` & `spacy-4.0.0.dev0/LICENSE`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/PKG-INFO` & `spacy-4.0.0.dev0/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: spacy
-Version: 3.6.0.dev0
+Version: 4.0.0.dev0
 Summary: Industrial-strength Natural Language Processing (NLP) in Python
 Home-page: https://spacy.io
 Author: Explosion
 Author-email: contact@explosion.ai
 License: MIT
 Project-URL: Release notes, https://github.com/explosion/spaCy/releases
 Project-URL: Source, https://github.com/explosion/spaCy
@@ -68,18 +68,15 @@
 state-of-the-art speed and **neural network models** for tagging,
 parsing, **named entity recognition**, **text classification** and more,
 multi-task learning with pretrained **transformers** like BERT, as well as a
 production-ready [**training system**](https://spacy.io/usage/training) and easy
 model packaging, deployment and workflow management. spaCy is commercial
 open-source software, released under the [MIT license](https://github.com/explosion/spaCy/blob/master/LICENSE).
 
- **We'd love to hear more about your experience with spaCy!**
-[Fill out our survey here.](https://form.typeform.com/to/aMel9q9f)
-
- **Version 3.5 out now!**
+ **Version 3.4 out now!**
 [Check out the release notes here.](https://github.com/explosion/spaCy/releases)
 
 [![Azure Pipelines](https://img.shields.io/azure-devops/build/explosion-ai/public/8/master.svg?logo=azure-pipelines&style=flat-square&label=build)](https://dev.azure.com/explosion-ai/public/_build?definitionId=8)
 [![Current Release Version](https://img.shields.io/github/release/explosion/spacy.svg?style=flat-square&logo=github)](https://github.com/explosion/spaCy/releases)
 [![pypi Version](https://img.shields.io/pypi/v/spacy.svg?style=flat-square&logo=pypi&logoColor=white)](https://pypi.org/project/spacy/)
 [![conda Version](https://img.shields.io/conda/vn/conda-forge/spacy.svg?style=flat-square&logo=conda-forge&logoColor=white)](https://anaconda.org/conda-forge/spacy)
 [![Python wheels](https://img.shields.io/badge/wheels-%E2%9C%93-4c1.svg?longCache=true&style=flat-square&logo=python&logoColor=white)](https://github.com/explosion/wheelwright/releases)
```

#### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: spacy Version: 3.6.0.dev0 Summary: Industrial-
+Metadata-Version: 2.1 Name: spacy Version: 4.0.0.dev0 Summary: Industrial-
 strength Natural Language Processing (NLP) in Python Home-page: https://
 spacy.io Author: Explosion Author-email: contact@explosion.ai License: MIT
 Project-URL: Release notes, https://github.com/explosion/spaCy/releases
 Project-URL: Source, https://github.com/explosion/spaCy Classifier: Development
 Status :: 5 - Production/Stable Classifier: Environment :: Console Classifier:
 Intended Audience :: Developers Classifier: Intended Audience :: Science/
 Research Classifier: License :: OSI Approved :: MIT License Classifier:
@@ -30,22 +30,20 @@
 tokenization and training for **70+ languages**. It features state-of-the-art
 speed and **neural network models** for tagging, parsing, **named entity
 recognition**, **text classification** and more, multi-task learning with
 pretrained **transformers** like BERT, as well as a production-ready
 [**training system**](https://spacy.io/usage/training) and easy model
 packaging, deployment and workflow management. spaCy is commercial open-source
 software, released under the [MIT license](https://github.com/explosion/spaCy/
-blob/master/LICENSE).  **We'd love to hear more about your experience with
-spaCy!** [Fill out our survey here.](https://form.typeform.com/to/aMel9q9f)
- **Version 3.5 out now!** [Check out the release notes here.](https://
-github.com/explosion/spaCy/releases) [![Azure Pipelines](https://
-img.shields.io/azure-devops/build/explosion-ai/public/8/master.svg?logo=azure-
-pipelines&style=flat-square&label=build)](https://dev.azure.com/explosion-ai/
-public/_build?definitionId=8) [![Current Release Version](https://
-img.shields.io/github/release/explosion/spacy.svg?style=flat-
+blob/master/LICENSE).  **Version 3.4 out now!** [Check out the release
+notes here.](https://github.com/explosion/spaCy/releases) [![Azure Pipelines]
+(https://img.shields.io/azure-devops/build/explosion-ai/public/8/
+master.svg?logo=azure-pipelines&style=flat-square&label=build)](https://
+dev.azure.com/explosion-ai/public/_build?definitionId=8) [![Current Release
+Version](https://img.shields.io/github/release/explosion/spacy.svg?style=flat-
 square&logo=github)](https://github.com/explosion/spaCy/releases) [![pypi
 Version](https://img.shields.io/pypi/v/spacy.svg?style=flat-
 square&logo=pypi&logoColor=white)](https://pypi.org/project/spacy/) [![conda
 Version](https://img.shields.io/conda/vn/conda-forge/spacy.svg?style=flat-
 square&logo=conda-forge&logoColor=white)](https://anaconda.org/conda-forge/
 spacy) [![Python wheels](https://img.shields.io/badge/wheels-%E2%9C%93-
 4c1.svg?longCache=true&style=flat-square&logo=python&logoColor=white)](https://
```

### Comparing `spacy-3.6.0.dev0/README.md` & `spacy-4.0.0.dev0/README.md`

 * *Files 1% similar despite different names*

```diff
@@ -12,18 +12,15 @@
 state-of-the-art speed and **neural network models** for tagging,
 parsing, **named entity recognition**, **text classification** and more,
 multi-task learning with pretrained **transformers** like BERT, as well as a
 production-ready [**training system**](https://spacy.io/usage/training) and easy
 model packaging, deployment and workflow management. spaCy is commercial
 open-source software, released under the [MIT license](https://github.com/explosion/spaCy/blob/master/LICENSE).
 
- **We'd love to hear more about your experience with spaCy!**
-[Fill out our survey here.](https://form.typeform.com/to/aMel9q9f)
-
- **Version 3.5 out now!**
+ **Version 3.4 out now!**
 [Check out the release notes here.](https://github.com/explosion/spaCy/releases)
 
 [![Azure Pipelines](https://img.shields.io/azure-devops/build/explosion-ai/public/8/master.svg?logo=azure-pipelines&style=flat-square&label=build)](https://dev.azure.com/explosion-ai/public/_build?definitionId=8)
 [![Current Release Version](https://img.shields.io/github/release/explosion/spacy.svg?style=flat-square&logo=github)](https://github.com/explosion/spaCy/releases)
 [![pypi Version](https://img.shields.io/pypi/v/spacy.svg?style=flat-square&logo=pypi&logoColor=white)](https://pypi.org/project/spacy/)
 [![conda Version](https://img.shields.io/conda/vn/conda-forge/spacy.svg?style=flat-square&logo=conda-forge&logoColor=white)](https://anaconda.org/conda-forge/spacy)
 [![Python wheels](https://img.shields.io/badge/wheels-%E2%9C%93-4c1.svg?longCache=true&style=flat-square&logo=python&logoColor=white)](https://github.com/explosion/wheelwright/releases)
```

#### html2text {}

```diff
@@ -5,32 +5,30 @@
 spacy.io/models) and currently supports tokenization and training for **70+
 languages**. It features state-of-the-art speed and **neural network models**
 for tagging, parsing, **named entity recognition**, **text classification** and
 more, multi-task learning with pretrained **transformers** like BERT, as well
 as a production-ready [**training system**](https://spacy.io/usage/training)
 and easy model packaging, deployment and workflow management. spaCy is
 commercial open-source software, released under the [MIT license](https://
-github.com/explosion/spaCy/blob/master/LICENSE).  **We'd love to hear more
-about your experience with spaCy!** [Fill out our survey here.](https://
-form.typeform.com/to/aMel9q9f)  **Version 3.5 out now!** [Check out the
-release notes here.](https://github.com/explosion/spaCy/releases) [![Azure
-Pipelines](https://img.shields.io/azure-devops/build/explosion-ai/public/8/
-master.svg?logo=azure-pipelines&style=flat-square&label=build)](https://
-dev.azure.com/explosion-ai/public/_build?definitionId=8) [![Current Release
-Version](https://img.shields.io/github/release/explosion/spacy.svg?style=flat-
-square&logo=github)](https://github.com/explosion/spaCy/releases) [![pypi
-Version](https://img.shields.io/pypi/v/spacy.svg?style=flat-
-square&logo=pypi&logoColor=white)](https://pypi.org/project/spacy/) [![conda
-Version](https://img.shields.io/conda/vn/conda-forge/spacy.svg?style=flat-
-square&logo=conda-forge&logoColor=white)](https://anaconda.org/conda-forge/
-spacy) [![Python wheels](https://img.shields.io/badge/wheels-%E2%9C%93-
-4c1.svg?longCache=true&style=flat-square&logo=python&logoColor=white)](https://
-github.com/explosion/wheelwright/releases) [![Code style: black](https://
-img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://
-github.com/ambv/black)
+github.com/explosion/spaCy/blob/master/LICENSE).  **Version 3.4 out now!**
+[Check out the release notes here.](https://github.com/explosion/spaCy/
+releases) [![Azure Pipelines](https://img.shields.io/azure-devops/build/
+explosion-ai/public/8/master.svg?logo=azure-pipelines&style=flat-
+square&label=build)](https://dev.azure.com/explosion-ai/public/
+_build?definitionId=8) [![Current Release Version](https://img.shields.io/
+github/release/explosion/spacy.svg?style=flat-square&logo=github)](https://
+github.com/explosion/spaCy/releases) [![pypi Version](https://img.shields.io/
+pypi/v/spacy.svg?style=flat-square&logo=pypi&logoColor=white)](https://
+pypi.org/project/spacy/) [![conda Version](https://img.shields.io/conda/vn/
+conda-forge/spacy.svg?style=flat-square&logo=conda-forge&logoColor=white)]
+(https://anaconda.org/conda-forge/spacy) [![Python wheels](https://
+img.shields.io/badge/wheels-%E2%9C%93-4c1.svg?longCache=true&style=flat-
+square&logo=python&logoColor=white)](https://github.com/explosion/wheelwright/
+releases) [![Code style: black](https://img.shields.io/badge/code%20style-
+black-000000.svg?style=flat-square)](https://github.com/ambv/black)
 [![PyPi downloads](https://static.pepy.tech/personalized-badge/
 spacy?period=total&units=international_system&left_color=grey&right_color=orange&left_text=pip%20downloads)]
 (https://pypi.org/project/spacy/) [![Conda downloads](https://img.shields.io/
 conda/dn/conda-forge/spacy?label=conda%20downloads)](https://anaconda.org/
 conda-forge/spacy) [![spaCy on Twitter](https://img.shields.io/twitter/follow/
 spacy_io.svg?style=social&label=Follow)](https://twitter.com/spacy_io) ## 
 Documentation | Documentation | | | -------------------------------------------
```

### Comparing `spacy-3.6.0.dev0/licenses/3rd_party_licenses.txt` & `spacy-4.0.0.dev0/licenses/3rd_party_licenses.txt`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/setup.cfg` & `spacy-4.0.0.dev0/setup.cfg`

 * *Files 17% similar despite different names*

```diff
@@ -28,98 +28,91 @@
 	Release notes = https://github.com/explosion/spaCy/releases
 	Source = https://github.com/explosion/spaCy
 
 [options]
 zip_safe = false
 include_package_data = true
 python_requires = >=3.6
-setup_requires = 
-	cython>=0.25,<3.0
-	numpy>=1.15.0
-	cymem>=2.0.2,<2.1.0
-	preshed>=3.0.2,<3.1.0
-	murmurhash>=0.28.0,<1.1.0
-	thinc>=8.1.8,<8.2.0
 install_requires = 
 	spacy-legacy>=3.0.11,<3.1.0
 	spacy-loggers>=1.0.0,<2.0.0
 	murmurhash>=0.28.0,<1.1.0
 	cymem>=2.0.2,<2.1.0
 	preshed>=3.0.2,<3.1.0
-	thinc>=8.1.8,<8.2.0
+	thinc>=9.0.0.dev2,<9.1.0
 	wasabi>=0.9.1,<1.2.0
 	srsly>=2.4.3,<3.0.0
 	catalogue>=2.0.6,<2.1.0
 	typer>=0.3.0,<0.8.0
 	pathy>=0.10.0
 	smart-open>=5.2.1,<7.0.0
 	tqdm>=4.38.0,<5.0.0
 	numpy>=1.15.0
 	requests>=2.13.0,<3.0.0
 	pydantic>=1.7.4,!=1.8,!=1.8.1,<1.11.0
 	jinja2
 	setuptools
 	packaging>=20.0
-	typing_extensions>=3.7.4.1,<4.5.0; python_version < "3.8"
+	typing_extensions>=3.7.4,<4.2.0; python_version < "3.8"
 	langcodes>=3.2.0,<4.0.0
 
 [options.entry_points]
 console_scripts = 
 	spacy = spacy.cli:setup_cli
 
 [options.extras_require]
 lookups = 
 	spacy_lookups_data>=1.0.3,<1.1.0
 transformers = 
 	spacy_transformers>=1.1.2,<1.3.0
 ray = 
 	spacy_ray>=0.1.0,<1.0.0
 cuda = 
-	cupy>=5.0.0b4,<13.0.0
+	cupy>=5.0.0b4,<12.0.0
 cuda80 = 
-	cupy-cuda80>=5.0.0b4,<13.0.0
+	cupy-cuda80>=5.0.0b4,<12.0.0
 cuda90 = 
-	cupy-cuda90>=5.0.0b4,<13.0.0
+	cupy-cuda90>=5.0.0b4,<12.0.0
 cuda91 = 
-	cupy-cuda91>=5.0.0b4,<13.0.0
+	cupy-cuda91>=5.0.0b4,<12.0.0
 cuda92 = 
-	cupy-cuda92>=5.0.0b4,<13.0.0
+	cupy-cuda92>=5.0.0b4,<12.0.0
 cuda100 = 
-	cupy-cuda100>=5.0.0b4,<13.0.0
+	cupy-cuda100>=5.0.0b4,<12.0.0
 cuda101 = 
-	cupy-cuda101>=5.0.0b4,<13.0.0
+	cupy-cuda101>=5.0.0b4,<12.0.0
 cuda102 = 
-	cupy-cuda102>=5.0.0b4,<13.0.0
+	cupy-cuda102>=5.0.0b4,<12.0.0
 cuda110 = 
-	cupy-cuda110>=5.0.0b4,<13.0.0
+	cupy-cuda110>=5.0.0b4,<12.0.0
 cuda111 = 
-	cupy-cuda111>=5.0.0b4,<13.0.0
+	cupy-cuda111>=5.0.0b4,<12.0.0
 cuda112 = 
-	cupy-cuda112>=5.0.0b4,<13.0.0
+	cupy-cuda112>=5.0.0b4,<12.0.0
 cuda113 = 
-	cupy-cuda113>=5.0.0b4,<13.0.0
+	cupy-cuda113>=5.0.0b4,<12.0.0
 cuda114 = 
-	cupy-cuda114>=5.0.0b4,<13.0.0
+	cupy-cuda114>=5.0.0b4,<12.0.0
 cuda115 = 
-	cupy-cuda115>=5.0.0b4,<13.0.0
+	cupy-cuda115>=5.0.0b4,<12.0.0
 cuda116 = 
-	cupy-cuda116>=5.0.0b4,<13.0.0
+	cupy-cuda116>=5.0.0b4,<12.0.0
 cuda117 = 
-	cupy-cuda117>=5.0.0b4,<13.0.0
+	cupy-cuda117>=5.0.0b4,<12.0.0
 cuda11x = 
-	cupy-cuda11x>=11.0.0,<13.0.0
+	cupy-cuda11x>=11.0.0,<12.0.0
 cuda-autodetect = 
-	cupy-wheel>=11.0.0,<13.0.0
+	cupy-wheel>=11.0.0,<12.0.0
 apple = 
 	thinc-apple-ops>=0.1.0.dev0,<1.0.0
 ja = 
 	sudachipy>=0.5.2,!=0.6.1
 	sudachidict_core>=20211220
 ko = 
-	natto-py>=0.9.0
+	mecab-ko>=1.0.0
 th = 
 	pythainlp>=2.0
 
 [bdist_wheel]
 universal = false
 
 [sdist]
```

### Comparing `spacy-3.6.0.dev0/setup.py` & `spacy-4.0.0.dev0/setup.py`

 * *Files 3% similar despite different names*

```diff
@@ -29,49 +29,50 @@
     "spacy.strings",
     "spacy.lexeme",
     "spacy.vocab",
     "spacy.attrs",
     "spacy.kb.candidate",
     "spacy.kb.kb",
     "spacy.kb.kb_in_memory",
-    "spacy.ml.parser_model",
+    "spacy.ml.tb_framework",
     "spacy.morphology",
-    "spacy.pipeline.dep_parser",
     "spacy.pipeline._edit_tree_internals.edit_trees",
     "spacy.pipeline.morphologizer",
-    "spacy.pipeline.multitask",
-    "spacy.pipeline.ner",
     "spacy.pipeline.pipe",
     "spacy.pipeline.trainable_pipe",
     "spacy.pipeline.sentencizer",
     "spacy.pipeline.senter",
     "spacy.pipeline.tagger",
     "spacy.pipeline.transition_parser",
     "spacy.pipeline._parser_internals.arc_eager",
+    "spacy.pipeline._parser_internals.batch",
     "spacy.pipeline._parser_internals.ner",
     "spacy.pipeline._parser_internals.nonproj",
+    "spacy.pipeline._parser_internals.search",
     "spacy.pipeline._parser_internals._state",
     "spacy.pipeline._parser_internals.stateclass",
     "spacy.pipeline._parser_internals.transition_system",
     "spacy.pipeline._parser_internals._beam_utils",
+    "spacy.pipeline._parser_internals._parser_utils",
     "spacy.tokenizer",
     "spacy.training.align",
     "spacy.training.gold_io",
     "spacy.tokens.doc",
     "spacy.tokens.span",
     "spacy.tokens.token",
     "spacy.tokens.span_group",
     "spacy.tokens.graph",
     "spacy.tokens.morphanalysis",
-    "spacy.tokens._retokenize",
+    "spacy.tokens.retokenizer",
     "spacy.matcher.matcher",
     "spacy.matcher.phrasematcher",
     "spacy.matcher.dependencymatcher",
     "spacy.symbols",
     "spacy.vectors",
+    "spacy.tests.parser._search",
 ]
 COMPILE_OPTIONS = {
     "msvc": ["/Ox", "/EHsc"],
     "mingw32": ["-O2", "-Wno-strict-prototypes", "-Wno-unused-function"],
     "other": ["-O2", "-Wno-strict-prototypes", "-Wno-unused-function"],
 }
 LINK_OPTIONS = {"msvc": ["-std=c++11"], "mingw32": ["-std=c++11"], "other": []}
```

### Comparing `spacy-3.6.0.dev0/spacy/__init__.py` & `spacy-4.0.0.dev0/spacy/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/cli/__init__.py` & `spacy-4.0.0.dev0/spacy/cli/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 from wasabi import msg
 
 from ._util import app, setup_cli  # noqa: F401
 
 # These are the actual functions, NOT the wrapped CLI commands. The CLI commands
 # are registered automatically and won't have to be imported here.
-from .benchmark_speed import benchmark_speed_cli  # noqa: F401
 from .download import download  # noqa: F401
 from .info import info  # noqa: F401
 from .package import package  # noqa: F401
 from .profile import profile  # noqa: F401
 from .train import train_cli  # noqa: F401
 from .assemble import assemble_cli  # noqa: F401
 from .pretrain import pretrain  # noqa: F401
```

### Comparing `spacy-3.6.0.dev0/spacy/cli/_util.py` & `spacy-4.0.0.dev0/spacy/cli/_util.py`

 * *Files 2% similar despite different names*

```diff
@@ -42,31 +42,28 @@
 fetching its assets like datasets etc. See the project's {PROJECT_FILE} for the
 available commands.
 """
 DEBUG_HELP = """Suite of helpful commands for debugging and profiling. Includes
 commands to check and validate your config files, training and evaluation data,
 and custom model implementations.
 """
-BENCHMARK_HELP = """Commands for benchmarking pipelines."""
 INIT_HELP = """Commands for initializing configs and pipeline packages."""
 
 # Wrappers for Typer's annotations. Initially created to set defaults and to
 # keep the names short, but not needed at the moment.
 Arg = typer.Argument
 Opt = typer.Option
 
 app = typer.Typer(name=NAME, help=HELP)
-benchmark_cli = typer.Typer(name="benchmark", help=BENCHMARK_HELP, no_args_is_help=True)
 project_cli = typer.Typer(name="project", help=PROJECT_HELP, no_args_is_help=True)
 debug_cli = typer.Typer(name="debug", help=DEBUG_HELP, no_args_is_help=True)
 init_cli = typer.Typer(name="init", help=INIT_HELP, no_args_is_help=True)
 
 app.add_typer(project_cli)
 app.add_typer(debug_cli)
-app.add_typer(benchmark_cli)
 app.add_typer(init_cli)
 
 
 def setup_cli() -> None:
     # Make sure the entry-point for CLI runs, so that they get imported.
     registry.cli.get_all()
     # Ensure that the help messages always display the correct prompt
@@ -86,17 +83,17 @@
     RETURNS (Dict[str, Any]): The parsed dict, keyed by nested config setting.
     """
     env_string = os.environ.get(env_var, "") if env_var else ""
     env_overrides = _parse_overrides(split_arg_string(env_string))
     cli_overrides = _parse_overrides(args, is_cli=True)
     if cli_overrides:
         keys = [k for k in cli_overrides if k not in env_overrides]
-        logger.debug("Config overrides from CLI: %s", keys)
+        logger.debug(f"Config overrides from CLI: {keys}")
     if env_overrides:
-        logger.debug("Config overrides from env variables: %s", list(env_overrides))
+        logger.debug(f"Config overrides from env variables: {list(env_overrides)}")
     return {**cli_overrides, **env_overrides}
 
 
 def _parse_overrides(args: List[str], is_cli: bool = False) -> Dict[str, Any]:
     result = {}
     while args:
         opt = args.pop(0)
```

### Comparing `spacy-3.6.0.dev0/spacy/cli/apply.py` & `spacy-4.0.0.dev0/spacy/cli/apply.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/cli/assemble.py` & `spacy-4.0.0.dev0/spacy/cli/assemble.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/cli/convert.py` & `spacy-4.0.0.dev0/spacy/cli/convert.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/cli/debug_config.py` & `spacy-4.0.0.dev0/spacy/cli/debug_config.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/cli/debug_data.py` & `spacy-4.0.0.dev0/spacy/cli/debug_data.py`

 * *Files 3% similar despite different names*

```diff
@@ -3,26 +3,24 @@
 from pathlib import Path
 from collections import Counter
 import sys
 import srsly
 from wasabi import Printer, MESSAGES, msg
 import typer
 import math
-import numpy
 
 from ._util import app, Arg, Opt, show_validation_error, parse_config_overrides
 from ._util import import_code, debug_cli, _format_number
 from ..training import Example, remove_bilu_prefix
 from ..training.initialize import get_sourced_components
 from ..schemas import ConfigSchemaTraining
 from ..pipeline import TrainablePipe
 from ..pipeline._parser_internals import nonproj
 from ..pipeline._parser_internals.nonproj import DELIMITER
 from ..pipeline import Morphologizer, SpanCategorizer
-from ..pipeline._edit_tree_internals.edit_trees import EditTrees
 from ..morphology import Morphology
 from ..language import Language
 from ..util import registry, resolve_dot_names
 from ..compat import Literal
 from ..vectors import Mode as VectorsMode
 from .. import util
 
@@ -333,15 +331,15 @@
             msg.text(
                 "Training data should always include examples of spans "
                 "in context, as well as examples without a given span "
                 "type.",
                 show=verbose,
             )
         else:
-            msg.good("Examples without occurrences available for all labels")
+            msg.good("Examples without ocurrences available for all labels")
 
     if "ner" in factory_names:
         # Get all unique NER labels present in the data
         labels = set(
             label for label in gold_train_data["ner"] if label not in ("O", "-", None)
         )
         label_counts = gold_train_data["ner"]
@@ -518,21 +516,17 @@
                     "Train/dev mismatch: the dev data contains instances "
                     "without mutually-exclusive classes while the train data "
                     "contains only instances with mutually-exclusive classes."
                 )
 
     if "tagger" in factory_names:
         msg.divider("Part-of-speech Tagging")
-        label_list, counts = zip(*gold_train_data["tags"].items())
-        msg.info(f"{len(label_list)} label(s) in train data")
-        p = numpy.array(counts)
-        p = p / p.sum()
-        norm_entropy = (-p * numpy.log2(p)).sum() / numpy.log2(len(label_list))
-        msg.info(f"{norm_entropy} is the normalised label entropy")
+        label_list = [label for label in gold_train_data["tags"]]
         model_labels = _get_labels_from_model(nlp, "tagger")
+        msg.info(f"{len(label_list)} label(s) in train data")
         labels = set(label_list)
         missing_labels = model_labels - labels
         if missing_labels:
             msg.warn(
                 "Some model labels are not present in the train data. The "
                 "model performance may be degraded for these labels after "
                 f"training: {_format_labels(missing_labels)}."
@@ -673,67 +667,14 @@
                 f"projectivized train sentence(s)"
             )
         if gold_train_data["n_cycles"] > 0:
             msg.fail(
                 f"Found {gold_train_data['n_cycles']} projectivized train sentence(s) with cycles"
             )
 
-    if "trainable_lemmatizer" in factory_names:
-        msg.divider("Trainable Lemmatizer")
-        trees_train: Set[str] = gold_train_data["lemmatizer_trees"]
-        trees_dev: Set[str] = gold_dev_data["lemmatizer_trees"]
-        # This is necessary context when someone is attempting to interpret whether the
-        # number of trees exclusively in the dev set is meaningful.
-        msg.info(f"{len(trees_train)} lemmatizer trees generated from training data")
-        msg.info(f"{len(trees_dev)} lemmatizer trees generated from dev data")
-        dev_not_train = trees_dev - trees_train
-
-        if len(dev_not_train) != 0:
-            pct = len(dev_not_train) / len(trees_dev)
-            msg.info(
-                f"{len(dev_not_train)} lemmatizer trees ({pct*100:.1f}% of dev trees)"
-                " were found exclusively in the dev data."
-            )
-        else:
-            # Would we ever expect this case? It seems like it would be pretty rare,
-            # and we might actually want a warning?
-            msg.info("All trees in dev data present in training data.")
-
-        if gold_train_data["n_low_cardinality_lemmas"] > 0:
-            n = gold_train_data["n_low_cardinality_lemmas"]
-            msg.warn(f"{n} training docs with 0 or 1 unique lemmas.")
-
-        if gold_dev_data["n_low_cardinality_lemmas"] > 0:
-            n = gold_dev_data["n_low_cardinality_lemmas"]
-            msg.warn(f"{n} dev docs with 0 or 1 unique lemmas.")
-
-        if gold_train_data["no_lemma_annotations"] > 0:
-            n = gold_train_data["no_lemma_annotations"]
-            msg.warn(f"{n} training docs with no lemma annotations.")
-        else:
-            msg.good("All training docs have lemma annotations.")
-
-        if gold_dev_data["no_lemma_annotations"] > 0:
-            n = gold_dev_data["no_lemma_annotations"]
-            msg.warn(f"{n} dev docs with no lemma annotations.")
-        else:
-            msg.good("All dev docs have lemma annotations.")
-
-        if gold_train_data["partial_lemma_annotations"] > 0:
-            n = gold_train_data["partial_lemma_annotations"]
-            msg.info(f"{n} training docs with partial lemma annotations.")
-        else:
-            msg.good("All training docs have complete lemma annotations.")
-
-        if gold_dev_data["partial_lemma_annotations"] > 0:
-            n = gold_dev_data["partial_lemma_annotations"]
-            msg.info(f"{n} dev docs with partial lemma annotations.")
-        else:
-            msg.good("All dev docs have complete lemma annotations.")
-
     msg.divider("Summary")
     good_counts = msg.counts[MESSAGES.GOOD]
     warn_counts = msg.counts[MESSAGES.WARN]
     fail_counts = msg.counts[MESSAGES.FAIL]
     if good_counts:
         msg.good(f"{good_counts} {'check' if good_counts == 1 else 'checks'} passed")
     if warn_counts:
@@ -787,21 +728,15 @@
         "words_missing_vectors": Counter(),
         "n_sents": 0,
         "n_nonproj": 0,
         "n_cycles": 0,
         "n_cats_multilabel": 0,
         "n_cats_bad_values": 0,
         "texts": set(),
-        "lemmatizer_trees": set(),
-        "no_lemma_annotations": 0,
-        "partial_lemma_annotations": 0,
-        "n_low_cardinality_lemmas": 0,
     }
-    if "trainable_lemmatizer" in factory_names:
-        trees = EditTrees(nlp.vocab.strings)
     for eg in examples:
         gold = eg.reference
         doc = eg.predicted
         valid_words = [x.text for x in gold]
         data["words"].update(valid_words)
         data["n_words"] += len(valid_words)
         align = eg.alignment
@@ -923,33 +858,14 @@
                 if head == i:
                     data["roots"].update([dep])
                     data["n_sents"] += 1
             if nonproj.is_nonproj_tree(aligned_heads):
                 data["n_nonproj"] += 1
             if nonproj.contains_cycle(aligned_heads):
                 data["n_cycles"] += 1
-        if "trainable_lemmatizer" in factory_names:
-            # from EditTreeLemmatizer._labels_from_data
-            if all(token.lemma == 0 for token in gold):
-                data["no_lemma_annotations"] += 1
-                continue
-            if any(token.lemma == 0 for token in gold):
-                data["partial_lemma_annotations"] += 1
-            lemma_set = set()
-            for token in gold:
-                if token.lemma != 0:
-                    lemma_set.add(token.lemma)
-                    tree_id = trees.add(token.text, token.lemma_)
-                    tree_str = trees.tree_to_str(tree_id)
-                    data["lemmatizer_trees"].add(tree_str)
-            # We want to identify cases where lemmas aren't assigned
-            # or are all assigned the same value, as this would indicate
-            # an issue since we're expecting a large set of lemmas
-            if len(lemma_set) < 2 and len(gold) > 1:
-                data["n_low_cardinality_lemmas"] += 1
     return data
 
 
 @overload
 def _format_labels(labels: Iterable[str], counts: Literal[False] = False) -> str:
     ...
```

### Comparing `spacy-3.6.0.dev0/spacy/cli/debug_diff.py` & `spacy-4.0.0.dev0/spacy/cli/debug_diff.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/cli/debug_model.py` & `spacy-4.0.0.dev0/spacy/cli/debug_model.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/cli/download.py` & `spacy-4.0.0.dev0/spacy/cli/download.py`

 * *Files 4% similar despite different names*

```diff
@@ -4,15 +4,14 @@
 from wasabi import msg
 import typer
 
 from ._util import app, Arg, Opt, WHEEL_SUFFIX, SDIST_SUFFIX
 from .. import about
 from ..util import is_package, get_minor_version, run_command
 from ..util import is_prerelease_version
-from ..errors import OLD_MODEL_SHORTCUTS
 
 
 @app.command(
     "download",
     context_settings={"allow_extra_args": True, "ignore_unknown_options": True},
 )
 def download_cli(
@@ -57,36 +56,33 @@
         pip_args = pip_args + ("--no-deps",)
     if direct:
         components = model.split("-")
         model_name = "".join(components[:-1])
         version = components[-1]
     else:
         model_name = model
-        if model in OLD_MODEL_SHORTCUTS:
-            msg.warn(
-                f"As of spaCy v3.0, shortcuts like '{model}' are deprecated. Please "
-                f"use the full pipeline package name '{OLD_MODEL_SHORTCUTS[model]}' instead."
-            )
-            model_name = OLD_MODEL_SHORTCUTS[model]
         compatibility = get_compatibility()
         version = get_version(model_name, compatibility)
 
     filename = get_model_filename(model_name, version, sdist)
 
     download_model(filename, pip_args)
     msg.good(
         "Download and installation successful",
         f"You can now load the package via spacy.load('{model_name}')",
     )
 
 
 def get_model_filename(model_name: str, version: str, sdist: bool = False) -> str:
     dl_tpl = "{m}-{v}/{m}-{v}{s}"
+    egg_tpl = "#egg={m}=={v}"
     suffix = SDIST_SUFFIX if sdist else WHEEL_SUFFIX
     filename = dl_tpl.format(m=model_name, v=version, s=suffix)
+    if sdist:
+        filename += egg_tpl.format(m=model_name, v=version)
     return filename
 
 
 def get_compatibility() -> dict:
     if is_prerelease_version(about.__version__):
         version: Optional[str] = about.__version__
     else:
```

### Comparing `spacy-3.6.0.dev0/spacy/cli/evaluate.py` & `spacy-4.0.0.dev0/spacy/cli/evaluate.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,23 +3,20 @@
 from pathlib import Path
 import re
 import srsly
 from thinc.api import fix_random_seed
 
 from ..training import Corpus
 from ..tokens import Doc
-from ._util import app, Arg, Opt, setup_gpu, import_code, benchmark_cli
+from ._util import app, Arg, Opt, setup_gpu, import_code
 from ..scorer import Scorer
 from .. import util
 from .. import displacy
 
 
-@benchmark_cli.command(
-    "accuracy",
-)
 @app.command("evaluate")
 def evaluate_cli(
     # fmt: off
     model: str = Arg(..., help="Model name or path"),
     data_path: Path = Arg(..., help="Location of binary evaluation data in .spacy format", exists=True),
     output: Optional[Path] = Opt(None, "--output", "-o", help="Output JSON file for metrics", dir_okay=False),
     code_path: Optional[Path] = Opt(None, "--code", "-c", help="Path to Python file with additional code (registered functions) to be imported"),
@@ -35,15 +32,15 @@
     evaluation examples with gold-standard sentences and tokens for the
     predictions. Gold preprocessing helps the annotations align to the
     tokenization, and may result in sequences of more consistent length. However,
     it may reduce runtime accuracy due to train/test skew. To render a sample of
     dependency parses in a HTML file, set as output directory as the
     displacy_path argument.
 
-    DOCS: https://spacy.io/api/cli#benchmark-accuracy
+    DOCS: https://spacy.io/api/cli#evaluate
     """
     import_code(code_path)
     evaluate(
         model,
         data_path,
         output=output,
         use_gpu=use_gpu,
@@ -118,24 +115,21 @@
     data = handle_scores_per_type(scores, data, spans_key=spans_key, silent=silent)
 
     if displacy_path:
         factory_names = [nlp.get_pipe_meta(pipe).factory for pipe in nlp.pipe_names]
         docs = list(nlp.pipe(ex.reference.text for ex in dev_dataset[:displacy_limit]))
         render_deps = "parser" in factory_names
         render_ents = "ner" in factory_names
-        render_spans = "spancat" in factory_names
-
         render_parses(
             docs,
             displacy_path,
             model_name=model,
             limit=displacy_limit,
             deps=render_deps,
             ents=render_ents,
-            spans=render_spans,
         )
         msg.good(f"Generated {displacy_limit} parses as HTML", displacy_path)
 
     if output_path is not None:
         srsly.write_json(output_path, data)
         msg.good(f"Saved results to {output_path}")
     return data
@@ -181,33 +175,27 @@
 def render_parses(
     docs: List[Doc],
     output_path: Path,
     model_name: str = "",
     limit: int = 250,
     deps: bool = True,
     ents: bool = True,
-    spans: bool = True,
 ):
     docs[0].user_data["title"] = model_name
     if ents:
         html = displacy.render(docs[:limit], style="ent", page=True)
         with (output_path / "entities.html").open("w", encoding="utf8") as file_:
             file_.write(html)
     if deps:
         html = displacy.render(
             docs[:limit], style="dep", page=True, options={"compact": True}
         )
         with (output_path / "parses.html").open("w", encoding="utf8") as file_:
             file_.write(html)
 
-    if spans:
-        html = displacy.render(docs[:limit], style="span", page=True)
-        with (output_path / "spans.html").open("w", encoding="utf8") as file_:
-            file_.write(html)
-
 
 def print_prf_per_type(
     msg: Printer, scores: Dict[str, Dict[str, float]], name: str, type: str
 ) -> None:
     data = []
     for key, value in scores.items():
         row = [key]
```

### Comparing `spacy-3.6.0.dev0/spacy/cli/find_threshold.py` & `spacy-4.0.0.dev0/spacy/cli/find_threshold.py`

 * *Files 0% similar despite different names*

```diff
@@ -31,15 +31,15 @@
     pipe_name: str = Arg(..., help="Name of pipe to examine thresholds for"),
     threshold_key: str = Arg(..., help="Key of threshold attribute in component's configuration"),
     scores_key: str = Arg(..., help="Metric to optimize"),
     n_trials: int = Opt(_DEFAULTS["n_trials"], "--n_trials", "-n", help="Number of trials to determine optimal thresholds"),
     code_path: Optional[Path] = Opt(None, "--code", "-c", help="Path to Python file with additional code (registered functions) to be imported"),
     use_gpu: int = Opt(_DEFAULTS["use_gpu"], "--gpu-id", "-g", help="GPU ID or -1 for CPU"),
     gold_preproc: bool = Opt(_DEFAULTS["gold_preproc"], "--gold-preproc", "-G", help="Use gold preprocessing"),
-    verbose: bool = Opt(False, "--verbose", "-V", "-VV", help="Display more information for debugging purposes"),
+    verbose: bool = Opt(False, "--silent", "-V", "-VV", help="Display more information for debugging purposes"),
     # fmt: on
 ):
     """
     Runs prediction trials for a trained model with varying tresholds to maximize
     the specified metric. The search space for the threshold is traversed linearly
     from 0 to 1 in `n_trials` steps. Results are displayed in a table on `stdout`
     (the corresponding API call to `spacy.cli.find_threshold.find_threshold()`
```

### Comparing `spacy-3.6.0.dev0/spacy/cli/info.py` & `spacy-4.0.0.dev0/spacy/cli/info.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 from typing import Optional, Dict, Any, Union, List
 import platform
+import pkg_resources
 import json
 from pathlib import Path
 from wasabi import Printer, MarkdownRenderer
 import srsly
 
 from ._util import app, Arg, Opt, string_to_list
 from .download import get_model_filename, get_latest_version
 from .. import util
 from .. import about
-from ..compat import importlib_metadata
 
 
 @app.command("info")
 def info_cli(
     # fmt: off
     model: Optional[str] = Arg(None, help="Optional loadable spaCy pipeline"),
     markdown: bool = Opt(False, "--markdown", "-md", help="Generate Markdown for GitHub issues"),
@@ -133,22 +133,23 @@
     """Given a pipeline name, get the download URL if available, otherwise
     return None.
 
     This is only available for pipelines installed as modules that have
     dist-info available.
     """
     try:
-        dist = importlib_metadata.distribution(model)
-        text = dist.read_text("direct_url.json")
-        if isinstance(text, str):
-            data = json.loads(text)
-            return data["url"]
+        dist = pkg_resources.get_distribution(model)
+        data = json.loads(dist.get_metadata("direct_url.json"))
+        return data["url"]
+    except pkg_resources.DistributionNotFound:
+        # no such package
+        return None
     except Exception:
-        pass
-    return None
+        # something else, like no file or invalid JSON
+        return None
 
 
 def info_model_url(model: str) -> Dict[str, Any]:
     """Return the download URL for the latest version of a pipeline."""
     version = get_latest_version(model)
 
     filename = get_model_filename(model, version)
```

### Comparing `spacy-3.6.0.dev0/spacy/cli/init_config.py` & `spacy-4.0.0.dev0/spacy/cli/init_config.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/cli/init_pipeline.py` & `spacy-4.0.0.dev0/spacy/cli/init_pipeline.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/cli/package.py` & `spacy-4.0.0.dev0/spacy/cli/package.py`

 * *Files 1% similar despite different names*

```diff
@@ -248,15 +248,15 @@
                 try:
                     func_info = util.registry.find(reg_name, func_name)
                 except RegistryError as regerr:
                     # lang-specific version being absent is not actually an issue
                     raise regerr from None
             module_name = func_info.get("module")  # type: ignore[attr-defined]
             if module_name:  # the code is part of a module, not a --code file
-                modules.add(func_info["module"].split(".")[0])  # type: ignore[union-attr]
+                modules.add(func_info["module"].split(".")[0])  # type: ignore[index]
     dependencies = []
     for module_name in modules:
         if module_name in distributions:
             dist = distributions.get(module_name)
             if dist:
                 pkg = dist[0]
                 if pkg in own_packages or pkg in exclude:
```

### Comparing `spacy-3.6.0.dev0/spacy/cli/pretrain.py` & `spacy-4.0.0.dev0/spacy/cli/pretrain.py`

 * *Files 3% similar despite different names*

```diff
@@ -19,15 +19,14 @@
     ctx: typer.Context,  # This is only used to read additional arguments
     config_path: Path = Arg(..., help="Path to config file", exists=True, dir_okay=False, allow_dash=True),
     output_dir: Path = Arg(..., help="Directory to write weights to on each epoch"),
     code_path: Optional[Path] = Opt(None, "--code", "-c", help="Path to Python file with additional code (registered functions) to be imported"),
     resume_path: Optional[Path] = Opt(None, "--resume-path", "-r", help="Path to pretrained weights from which to resume pretraining"),
     epoch_resume: Optional[int] = Opt(None, "--epoch-resume", "-er", help="The epoch to resume counting from when using --resume-path. Prevents unintended overwriting of existing weight files."),
     use_gpu: int = Opt(-1, "--gpu-id", "-g", help="GPU ID or -1 for CPU"),
-    skip_last: bool = Opt(False, "--skip-last", "-L", help="Skip saving model-last.bin"),
     # fmt: on
 ):
     """
     Pre-train the 'token-to-vector' (tok2vec) layer of pipeline components,
     using an approximate language-modelling objective. Two objective types
     are available, vector-based and character-based.
 
@@ -71,15 +70,14 @@
     pretrain(
         config,
         output_dir,
         resume_path=resume_path,
         epoch_resume=epoch_resume,
         use_gpu=use_gpu,
         silent=False,
-        skip_last=skip_last,
     )
     msg.good("Successfully finished pretrain")
 
 
 def verify_cli_args(config_path, output_dir, resume_path, epoch_resume):
     if not config_path or (str(config_path) != "-" and not config_path.exists()):
         msg.fail("Config file not found", config_path, exits=1)
```

### Comparing `spacy-3.6.0.dev0/spacy/cli/profile.py` & `spacy-4.0.0.dev0/spacy/cli/profile.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/cli/project/assets.py` & `spacy-4.0.0.dev0/spacy/cli/project/assets.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/cli/project/clone.py` & `spacy-4.0.0.dev0/spacy/cli/project/clone.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/cli/project/document.py` & `spacy-4.0.0.dev0/spacy/cli/project/document.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/cli/project/dvc.py` & `spacy-4.0.0.dev0/spacy/cli/project/dvc.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/cli/project/pull.py` & `spacy-4.0.0.dev0/spacy/cli/project/pull.py`

 * *Files 7% similar despite different names*

```diff
@@ -35,33 +35,30 @@
     storage = RemoteStorage(project_dir, remote)
     commands = list(config.get("commands", []))
     # We use a while loop here because we don't know how the commands
     # will be ordered. A command might need dependencies from one that's later
     # in the list.
     while commands:
         for i, cmd in enumerate(list(commands)):
-            logger.debug("CMD: %s.", cmd["name"])
+            logger.debug(f"CMD: {cmd['name']}.")
             deps = [project_dir / dep for dep in cmd.get("deps", [])]
             if all(dep.exists() for dep in deps):
                 cmd_hash = get_command_hash("", "", deps, cmd["script"])
                 for output_path in cmd.get("outputs", []):
                     url = storage.pull(output_path, command_hash=cmd_hash)
                     logger.debug(
-                        "URL: %s for %s with command hash %s",
-                        url,
-                        output_path,
-                        cmd_hash,
+                        f"URL: {url} for {output_path} with command hash {cmd_hash}"
                     )
                     yield url, output_path
 
                 out_locs = [project_dir / out for out in cmd.get("outputs", [])]
                 if all(loc.exists() for loc in out_locs):
                     update_lockfile(project_dir, cmd)
                 # We remove the command from the list here, and break, so that
                 # we iterate over the loop again.
                 commands.pop(i)
                 break
             else:
-                logger.debug("Dependency missing. Skipping %s outputs.", cmd["name"])
+                logger.debug(f"Dependency missing. Skipping {cmd['name']} outputs.")
         else:
             # If we didn't break the for loop, break the while loop.
             break
```

### Comparing `spacy-3.6.0.dev0/spacy/cli/project/push.py` & `spacy-4.0.0.dev0/spacy/cli/project/push.py`

 * *Files 4% similar despite different names*

```diff
@@ -33,33 +33,33 @@
     library can upload to, e.g. gcs, aws, ssh, local directories etc
     """
     config = load_project_config(project_dir)
     if remote in config.get("remotes", {}):
         remote = config["remotes"][remote]
     storage = RemoteStorage(project_dir, remote)
     for cmd in config.get("commands", []):
-        logger.debug("CMD: %s", cmd["name"])
+        logger.debug(f"CMD: cmd['name']")
         deps = [project_dir / dep for dep in cmd.get("deps", [])]
         if any(not dep.exists() for dep in deps):
-            logger.debug("Dependency missing. Skipping %s outputs", cmd["name"])
+            logger.debug(f"Dependency missing. Skipping {cmd['name']} outputs")
             continue
         cmd_hash = get_command_hash(
             "", "", [project_dir / dep for dep in cmd.get("deps", [])], cmd["script"]
         )
-        logger.debug("CMD_HASH: %s", cmd_hash)
+        logger.debug(f"CMD_HASH: {cmd_hash}")
         for output_path in cmd.get("outputs", []):
             output_loc = project_dir / output_path
             if output_loc.exists() and _is_not_empty_dir(output_loc):
                 url = storage.push(
                     output_path,
                     command_hash=cmd_hash,
                     content_hash=get_content_hash(output_loc),
                 )
                 logger.debug(
-                    "URL: %s for output %s with cmd_hash %s", url, output_path, cmd_hash
+                    f"URL: {url} for output {output_path} with cmd_hash {cmd_hash}"
                 )
                 yield output_path, url
 
 
 def _is_not_empty_dir(loc: Path):
     if not loc.is_dir():
         return True
```

### Comparing `spacy-3.6.0.dev0/spacy/cli/project/remote_storage.py` & `spacy-4.0.0.dev0/spacy/cli/project/remote_storage.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/cli/project/run.py` & `spacy-4.0.0.dev0/spacy/cli/project/run.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 from typing import Optional, List, Dict, Sequence, Any, Iterable, Tuple
 import os.path
 from pathlib import Path
 
+import pkg_resources
 from wasabi import msg
 from wasabi.util import locale_escape
 import sys
 import srsly
 import typer
 
 from ... import about
@@ -326,15 +327,14 @@
 
 def _check_requirements(requirements: List[str]) -> Tuple[bool, bool]:
     """Checks whether requirements are installed and free of version conflicts.
     requirements (List[str]): List of requirements.
     RETURNS (Tuple[bool, bool]): Whether (1) any packages couldn't be imported, (2) any packages with version conflicts
         exist.
     """
-    import pkg_resources
 
     failed_pkgs_msgs: List[str] = []
     conflicting_pkgs_msgs: List[str] = []
 
     for req in requirements:
         try:
             pkg_resources.require(req)
```

### Comparing `spacy-3.6.0.dev0/spacy/cli/templates/quickstart_training.jinja` & `spacy-4.0.0.dev0/spacy/cli/templates/quickstart_training.jinja`

 * *Files 14% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 {# This is a template for training configs used for the quickstart widget in
 the docs and the init config command. It encodes various best practices and
 can help generate the best possible configuration, given a user's requirements. #}
 {%- set use_transformer = hardware != "cpu" and transformer_data -%}
 {%- set transformer = transformer_data[optimize] if use_transformer else {} -%}
-{%- set listener_components = ["tagger", "morphologizer", "parser", "ner", "textcat", "textcat_multilabel", "entity_linker", "spancat", "spancat_singlelabel", "trainable_lemmatizer"] -%}
+{%- set listener_components = ["tagger", "morphologizer", "parser", "ner", "textcat", "textcat_multilabel", "entity_linker", "spancat", "trainable_lemmatizer"] -%}
 [paths]
 train = null
 dev = null
 {% if use_transformer or optimize == "efficiency" or not word_vectors -%}
 vectors = null
 {% else -%}
 vectors = "{{ word_vectors }}"
@@ -20,19 +20,16 @@
 gpu_allocator = null
 {% endif %}
 
 [nlp]
 lang = "{{ lang }}"
 {%- set has_textcat = ("textcat" in components or "textcat_multilabel" in components) -%}
 {%- set with_accuracy = optimize == "accuracy" -%}
-{# The BOW textcat doesn't need a source of features, so it can omit the
-tok2vec/transformer. #}
-{%- set with_accuracy_or_transformer = (use_transformer or with_accuracy) -%}
-{%- set textcat_needs_features = has_textcat and with_accuracy_or_transformer -%}
-{%- if ("tagger" in components or "morphologizer" in components or "parser" in components or "ner" in components or "spancat" in components or "spancat_singlelabel" in components or "trainable_lemmatizer" in components or "entity_linker" in components or textcat_needs_features) -%}
+{%- set has_accurate_textcat = has_textcat and with_accuracy -%}
+{%- if ("tagger" in components or "morphologizer" in components or "parser" in components or "ner" in components or "spancat" in components or "trainable_lemmatizer" in components or "entity_linker" in components or has_accurate_textcat) -%}
 {%- set full_pipeline = ["transformer" if use_transformer else "tok2vec"] + components -%}
 {%- else -%}
 {%- set full_pipeline = components -%}
 {%- endif %}
 pipeline = {{ full_pipeline|pprint()|replace("'", '"')|safe }}
 batch_size = {{ 128 if hardware == "gpu" else 1000 }}
 
@@ -86,20 +83,19 @@
 {%- endif %}
 
 {% if "parser" in components -%}
 [components.parser]
 factory = "parser"
 
 [components.parser.model]
-@architectures = "spacy.TransitionBasedParser.v2"
+@architectures = "spacy.TransitionBasedParser.v3"
 state_type = "parser"
 extra_state_tokens = false
 hidden_width = 128
 maxout_pieces = 3
-use_upper = false
 nO = null
 
 [components.parser.model.tok2vec]
 @architectures = "spacy-transformers.TransformerListener.v1"
 grad_factor = 1.0
 
 [components.parser.model.tok2vec.pooling]
@@ -107,20 +103,19 @@
 {%- endif %}
 
 {% if "ner" in components -%}
 [components.ner]
 factory = "ner"
 
 [components.ner.model]
-@architectures = "spacy.TransitionBasedParser.v2"
+@architectures = "spacy.TransitionBasedParser.v3"
 state_type = "ner"
 extra_state_tokens = false
 hidden_width = 64
 maxout_pieces = 2
-use_upper = false
 nO = null
 
 [components.ner.model.tok2vec]
 @architectures = "spacy-transformers.TransformerListener.v1"
 grad_factor = 1.0
 
 [components.ner.model.tok2vec.pooling]
@@ -155,44 +150,14 @@
 @layers = "reduce_mean.v1"
 
 [components.spancat.suggester]
 @misc = "spacy.ngram_suggester.v1"
 sizes = [1,2,3]
 {% endif -%}
 
-{% if "spancat_singlelabel" in components %}
-[components.spancat_singlelabel]
-factory = "spancat_singlelabel"
-negative_weight = 1.0
-allow_overlap = true
-scorer = {"@scorers":"spacy.spancat_scorer.v1"}
-spans_key = "sc"
-
-[components.spancat_singlelabel.model]
-@architectures = "spacy.SpanCategorizer.v1"
-
-[components.spancat_singlelabel.model.reducer]
-@layers = "spacy.mean_max_reducer.v1"
-hidden_size = 128
-
-[components.spancat_singlelabel.model.scorer]
-@layers = "Softmax.v2"
-
-[components.spancat_singlelabel.model.tok2vec]
-@architectures = "spacy-transformers.TransformerListener.v1"
-grad_factor = 1.0
-
-[components.spancat_singlelabel.model.tok2vec.pooling]
-@layers = "reduce_mean.v1"
-
-[components.spancat_singlelabel.suggester]
-@misc = "spacy.ngram_suggester.v1"
-sizes = [1,2,3]
-{% endif %}
-
 {% if "trainable_lemmatizer" in components -%}
 [components.trainable_lemmatizer]
 factory = "trainable_lemmatizer"
 backoff = "orth"
 min_tree_freq = 3
 overwrite = false
 scorer = {"@scorers":"spacy.lemmatizer_scorer.v1"}
@@ -250,24 +215,18 @@
 @architectures = "spacy.TextCatBOW.v2"
 exclusive_classes = true
 ngram_size = 1
 no_output_layer = false
 
 {% else -%}
 [components.textcat.model]
-@architectures = "spacy.TextCatCNN.v2"
+@architectures = "spacy.TextCatBOW.v2"
 exclusive_classes = true
-nO = null
-
-[components.textcat.model.tok2vec]
-@architectures = "spacy-transformers.TransformerListener.v1"
-grad_factor = 1.0
-
-[components.textcat.model.tok2vec.pooling]
-@layers = "reduce_mean.v1"
+ngram_size = 1
+no_output_layer = false
 {%- endif %}
 {%- endif %}
 
 {% if "textcat_multilabel" in components %}
 [components.textcat_multilabel]
 factory = "textcat_multilabel"
 
@@ -287,24 +246,18 @@
 @architectures = "spacy.TextCatBOW.v2"
 exclusive_classes = false
 ngram_size = 1
 no_output_layer = false
 
 {% else -%}
 [components.textcat_multilabel.model]
-@architectures = "spacy.TextCatCNN.v2"
+@architectures = "spacy.TextCatBOW.v2"
 exclusive_classes = false
-nO = null
-
-[components.textcat_multilabel.model.tok2vec]
-@architectures = "spacy-transformers.TransformerListener.v1"
-grad_factor = 1.0
-
-[components.textcat_multilabel.model.tok2vec.pooling]
-@layers = "reduce_mean.v1"
+ngram_size = 1
+no_output_layer = false
 {%- endif %}
 {%- endif %}
 
 {# NON-TRANSFORMER PIPELINE #}
 {% else -%}
 {% if "tok2vec" in full_pipeline -%}
 [components.tok2vec]
@@ -327,29 +280,27 @@
 window_size = 1
 maxout_pieces = 3
 {% endif -%}
 
 {% if "morphologizer" in components %}
 [components.morphologizer]
 factory = "morphologizer"
-label_smoothing = 0.05
 
 [components.morphologizer.model]
 @architectures = "spacy.Tagger.v2"
 nO = null
 
 [components.morphologizer.model.tok2vec]
 @architectures = "spacy.Tok2VecListener.v1"
 width = ${components.tok2vec.model.encode.width}
 {%- endif %}
 
 {% if "tagger" in components %}
 [components.tagger]
 factory = "tagger"
-label_smoothing = 0.05
 
 [components.tagger.model]
 @architectures = "spacy.Tagger.v2"
 nO = null
 
 [components.tagger.model.tok2vec]
 @architectures = "spacy.Tok2VecListener.v1"
@@ -357,38 +308,36 @@
 {%- endif %}
 
 {% if "parser" in components -%}
 [components.parser]
 factory = "parser"
 
 [components.parser.model]
-@architectures = "spacy.TransitionBasedParser.v2"
+@architectures = "spacy.TransitionBasedParser.v3"
 state_type = "parser"
 extra_state_tokens = false
 hidden_width = 128
 maxout_pieces = 3
-use_upper = true
 nO = null
 
 [components.parser.model.tok2vec]
 @architectures = "spacy.Tok2VecListener.v1"
 width = ${components.tok2vec.model.encode.width}
 {%- endif %}
 
 {% if "ner" in components %}
 [components.ner]
 factory = "ner"
 
 [components.ner.model]
-@architectures = "spacy.TransitionBasedParser.v2"
+@architectures = "spacy.TransitionBasedParser.v3"
 state_type = "ner"
 extra_state_tokens = false
 hidden_width = 64
 maxout_pieces = 2
-use_upper = true
 nO = null
 
 [components.ner.model.tok2vec]
 @architectures = "spacy.Tok2VecListener.v1"
 width = ${components.tok2vec.model.encode.width}
 {% endif %}
 
@@ -417,41 +366,14 @@
 width = ${components.tok2vec.model.encode.width}
 
 [components.spancat.suggester]
 @misc = "spacy.ngram_suggester.v1"
 sizes = [1,2,3]
 {% endif %}
 
-{% if "spancat_singlelabel" in components %}
-[components.spancat_singlelabel]
-factory = "spancat_singlelabel"
-negative_weight = 1.0
-allow_overlap = true
-scorer = {"@scorers":"spacy.spancat_scorer.v1"}
-spans_key = "sc"
-
-[components.spancat_singlelabel.model]
-@architectures = "spacy.SpanCategorizer.v1"
-
-[components.spancat_singlelabel.model.reducer]
-@layers = "spacy.mean_max_reducer.v1"
-hidden_size = 128
-
-[components.spancat_singlelabel.model.scorer]
-@layers = "Softmax.v2"
-
-[components.spancat_singlelabel.model.tok2vec]
-@architectures = "spacy.Tok2VecListener.v1"
-width = ${components.tok2vec.model.encode.width}
-
-[components.spancat_singlelabel.suggester]
-@misc = "spacy.ngram_suggester.v1"
-sizes = [1,2,3]
-{% endif %}
-
 {% if "trainable_lemmatizer" in components -%}
 [components.trainable_lemmatizer]
 factory = "trainable_lemmatizer"
 backoff = "orth"
 min_tree_freq = 3
 overwrite = false
 scorer = {"@scorers":"spacy.lemmatizer_scorer.v1"}
```

### Comparing `spacy-3.6.0.dev0/spacy/cli/templates/quickstart_training_recommendations.yml` & `spacy-4.0.0.dev0/spacy/cli/templates/quickstart_training_recommendations.yml`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/cli/train.py` & `spacy-4.0.0.dev0/spacy/cli/train.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/cli/validate.py` & `spacy-4.0.0.dev0/spacy/cli/validate.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/compat.py` & `spacy-4.0.0.dev0/spacy/compat.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/default_config.cfg` & `spacy-4.0.0.dev0/spacy/default_config.cfg`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/default_config_pretraining.cfg` & `spacy-4.0.0.dev0/spacy/default_config_pretraining.cfg`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/displacy/__init__.py` & `spacy-4.0.0.dev0/spacy/displacy/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -102,15 +102,17 @@
     """
     from wsgiref import simple_server
 
     port = find_available_port(port, host, auto_select_port)
 
     if is_in_jupyter():
         warnings.warn(Warnings.W011)
-    render(docs, style=style, page=page, minify=minify, options=options, manual=manual)
+    render(
+        docs, style=style, page=page, minify=minify, options=options, manual=manual
+    )
     httpd = simple_server.make_server(host, port, app)
     print(f"\nUsing the '{style}' visualizer")
     print(f"Serving on http://{host}:{port} ...\n")
     try:
         httpd.serve_forever()
     except KeyboardInterrupt:
         print(f"Shutting down server on port {port}.")
@@ -121,25 +123,21 @@
 def app(environ, start_response):
     headers = [("Content-type", "text/html; charset=utf-8")]
     start_response("200 OK", headers)
     res = _html["parsed"].encode(encoding="utf-8")
     return [res]
 
 
-def parse_deps(
-    orig_doc: Union[Doc, Span], options: Dict[str, Any] = {}
-) -> Dict[str, Any]:
+def parse_deps(orig_doc: Doc, options: Dict[str, Any] = {}) -> Dict[str, Any]:
     """Generate dependency parse in {'words': [], 'arcs': []} format.
 
-    orig_doc (Union[Doc, Span]): Document to parse.
+    orig_doc (Doc): Document to parse.
     options (Dict[str, Any]): Dependency parse specific visualisation options.
     RETURNS (dict): Generated dependency parse keyed by words and arcs.
     """
-    if isinstance(orig_doc, Span):
-        orig_doc = orig_doc.as_doc()
     doc = Doc(orig_doc.vocab).from_bytes(
         orig_doc.to_bytes(exclude=["user_data", "user_hooks"])
     )
     if not doc.has_annotation("DEP"):
         warnings.warn(Warnings.W005)
     if options.get("collapse_phrases", False):
         with doc.retokenize() as retokenizer:
```

### Comparing `spacy-3.6.0.dev0/spacy/displacy/render.py` & `spacy-4.0.0.dev0/spacy/displacy/render.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/displacy/templates.py` & `spacy-4.0.0.dev0/spacy/displacy/templates.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/errors.py` & `spacy-4.0.0.dev0/spacy/errors.py`

 * *Files 2% similar despite different names*

```diff
@@ -127,21 +127,14 @@
             "performance of '{listener}' will be degraded. You can either freeze "
             "both, or neither of the two. If you're sourcing the component from "
             "an existing pipeline, you can use the `replace_listeners` setting in "
             "the config block to replace its token-to-vector listener with a copy "
             "and make it independent. For example, `replace_listeners = "
             "[\"model.tok2vec\"]` See the documentation for details: "
             "https://spacy.io/usage/training#config-components-listeners")
-    W088 = ("The pipeline component {name} implements a `begin_training` "
-            "method, which won't be called by spaCy. As of v3.0, `begin_training` "
-            "has been renamed to `initialize`, so you likely want to rename the "
-            "component method. See the documentation for details: "
-            "https://spacy.io/api/language#initialize")
-    W089 = ("As of spaCy v3.0, the `nlp.begin_training` method has been renamed "
-            "to `nlp.initialize`.")
     W090 = ("Could not locate any {format} files in path '{path}'.")
     W091 = ("Could not clean/remove the temp directory at {dir}: {msg}.")
     W092 = ("Ignoring annotations for sentence starts, as dependency heads are set.")
     W093 = ("Could not find any data to train the {name} on. Is your "
             "input data correctly formatted?")
     W094 = ("Model '{model}' ({model_version}) specifies an under-constrained "
             "spaCy version requirement: {version}. This can lead to compatibility "
@@ -212,14 +205,16 @@
     W121 = ("Attempting to trace non-existent method '{method}' in pipe '{pipe}'")
     W122 = ("Couldn't trace method '{method}' in pipe '{pipe}'. This can happen if the pipe class "
             "is a Cython extension type.")
     W123 = ("Argument `enable` with value {enable} does not contain all values specified in the config option "
             "`enabled` ({enabled}). Be aware that this might affect other components in your pipeline.")
     W124 = ("{host}:{port} is already in use, using the nearest available port {serve_port} as an alternative.")
 
+    W400 = ("`use_upper=False` is ignored, the upper layer is always enabled")
+
 
 class Errors(metaclass=ErrorsWithCodes):
     E001 = ("No component '{name}' found in pipeline. Available names: {opts}")
     E002 = ("Can't find factory for '{name}' for language {lang} ({lang_code}). "
             "This usually happens when spaCy calls `nlp.{method}` with a custom "
             "component name that's not registered on the current language class. "
             "If you're using a Transformer, make sure to install 'spacy-transformers'. "
@@ -247,17 +242,15 @@
             "`nlp.select_pipes` or after restoring the disabled components.")
     E010 = ("Word vectors set to length 0. This may be because you don't have "
             "a model installed or loaded, or because your model doesn't "
             "include word vectors. For more info, see the docs:\n"
             "https://spacy.io/usage/models")
     E011 = ("Unknown operator: '{op}'. Options: {opts}")
     E012 = ("Cannot add pattern for zero tokens to matcher.\nKey: {key}")
-    E016 = ("MultitaskObjective target should be function or one of: dep, "
-            "tag, ent, dep_tag_offset, ent_tag.")
-    E017 = ("Can only add unicode or bytes. Got type: {value_type}")
+    E017 = ("Can only add 'str' inputs to StringStore. Got type: {value_type}")
     E018 = ("Can't retrieve string for hash '{hash_value}'. This usually "
             "refers to an issue with the `Vocab` or `StringStore`.")
     E019 = ("Can't create transition with unknown action ID: {action}. Action "
             "IDs are enumerated in spacy/syntax/{src}.pyx.")
     E022 = ("Could not find a transition with the name '{name}' in the NER "
             "model.")
     E024 = ("Could not find an optimal move to supervise the parser. Usually, "
@@ -440,15 +433,16 @@
             "`--enable-unicode=ucs4 flag`.")
     E132 = ("The vectors for entities and probabilities for alias '{alias}' "
             "should have equal length, but found {entities_length} and "
             "{probabilities_length} respectively.")
     E133 = ("The sum of prior probabilities for alias '{alias}' should not "
             "exceed 1, but found {sum}.")
     E134 = ("Entity '{entity}' is not defined in the Knowledge Base.")
-    E139 = ("Knowledge base for component '{name}' is empty.")
+    E139 = ("Knowledge base for component '{name}' is empty. Use the methods "
+            "`kb.add_entity` and `kb.add_alias` to add entries.")
     E140 = ("The list of entities, prior probabilities and entity vectors "
             "should be of equal length.")
     E141 = ("Entity vectors should be of length {required} instead of the "
             "provided {found}.")
     E143 = ("Labels for component '{name}' not initialized. This can be fixed "
             "by calling add_label, or by providing a representative batch of "
             "examples to the component's `initialize` method.")
@@ -461,21 +455,21 @@
             "each entity in `doc.ents` is assigned to a KB identifier.")
     E149 = ("Error deserializing model. Check that the config used to create "
             "the component matches the model being loaded.")
     E150 = ("The language of the `nlp` object and the `vocab` should be the "
             "same, but found '{nlp}' and '{vocab}' respectively.")
     E152 = ("The attribute {attr} is not supported for token patterns. "
             "Please use the option `validate=True` with the Matcher, PhraseMatcher, "
-            "EntityRuler or AttributeRuler for more details.")
+            "SpanRuler or AttributeRuler for more details.")
     E153 = ("The value type {vtype} is not supported for token patterns. "
             "Please use the option validate=True with Matcher, PhraseMatcher, "
-            "EntityRuler or AttributeRuler for more details.")
+            "SpanRuler or AttributeRuler for more details.")
     E154 = ("One of the attributes or values is not supported for token "
             "patterns. Please use the option `validate=True` with the Matcher, "
-            "PhraseMatcher, or EntityRuler for more details.")
+            "PhraseMatcher, or SpanRuler for more details.")
     E155 = ("The pipeline needs to include a {pipe} in order to use "
             "Matcher or PhraseMatcher with the attribute {attr}. "
             "Try using `nlp()` instead of `nlp.make_doc()` or `list(nlp.pipe())` "
             "instead of `list(nlp.tokenizer.pipe())`.")
     E157 = ("Can't render negative values for dependency arc start or end. "
             "Make sure that you're passing in absolute token indices, not "
             "relative token offsets.\nstart: {start}, end: {end}, label: "
@@ -491,15 +485,15 @@
     E164 = ("x is neither increasing nor decreasing: {x}.")
     E165 = ("Only one class present in the gold labels: {label}. "
             "ROC AUC score is not defined in that case.")
     E166 = ("Can only merge DocBins with the same value for '{param}'.\n"
             "Current DocBin: {current}\nOther DocBin: {other}")
     E169 = ("Can't find module: {module}")
     E170 = ("Cannot apply transition {name}: invalid for the current state.")
-    E171 = ("Matcher.add received invalid 'on_match' callback argument: expected "
+    E171 = ("{name}.add received invalid 'on_match' callback argument: expected "
             "callable or None, but got: {arg_type}")
     E175 = ("Can't remove rule for unknown match pattern ID: {key}")
     E176 = ("Alias '{alias}' is not defined in the Knowledge Base.")
     E177 = ("Ill-formed IOB input detected: {tag}")
     E178 = ("Each pattern should be a list of dicts, but got: {pat}. Maybe you "
             "accidentally passed a single pattern to Matcher.add instead of a "
             "list of patterns? If you only want to add one pattern, make sure "
@@ -545,16 +539,14 @@
     E199 = ("Unable to merge 0-length span at `doc[{start}:{end}]`.")
     E200 = ("Can't set {attr} from Span.")
     E202 = ("Unsupported {name} mode '{mode}'. Supported modes: {modes}.")
     E203 = ("If the {name} embedding layer is not updated "
             "during training, make sure to include it in 'annotating components'")
 
     # New errors added in v3.x
-    E850 = ("The PretrainVectors objective currently only supports default or "
-            "floret vectors, not {mode} vectors.")
     E851 = ("The 'textcat' component labels should only have values of 0 or 1, "
             "but found value of '{val}'.")
     E852 = ("The tar file pulled from the remote attempted an unsafe path "
             "traversal.")
     E853 = ("Unsupported component factory name '{name}'. The character '.' is "
             "not permitted in factory names.")
     E854 = ("Unable to set doc.ents. Check that the 'ents_filter' does not "
@@ -730,35 +722,28 @@
     E930 = ("Received invalid get_examples callback in `{method}`. "
             "Expected function that returns an iterable of Example objects but "
             "got: {obj}")
     E931 = ("Encountered {parent} subclass without `{parent}.{method}` "
             "method in component '{name}'. If you want to use this "
             "method, make sure it's overwritten on the subclass.")
     E940 = ("Found NaN values in scores.")
-    E941 = ("Can't find model '{name}'. It looks like you're trying to load a "
-            "model from a shortcut, which is obsolete as of spaCy v3.0. To "
-            "load the model, use its full name instead:\n\n"
-            "nlp = spacy.load(\"{full}\")\n\nFor more details on the available "
-            "models, see the models directory: https://spacy.io/models. If you "
-            "want to create a blank model, use spacy.blank: "
-            "nlp = spacy.blank(\"{name}\")")
     E942 = ("Executing `after_{name}` callback failed. Expected the function to "
             "return an initialized nlp object but got: {value}. Maybe "
             "you forgot to return the modified object in your function?")
     E943 = ("Executing `before_creation` callback failed. Expected the function to "
             "return an uninitialized Language subclass but got: {value}. Maybe "
             "you forgot to return the modified object in your function or "
             "returned the initialized nlp object instead?")
     E944 = ("Can't copy pipeline component '{name}' from source '{model}': "
             "not found in pipeline. Available components: {opts}")
     E945 = ("Can't copy pipeline component '{name}' from source. Expected "
             "loaded nlp object, but got: {source}")
     E947 = ("`Matcher.add` received invalid `greedy` argument: expected "
             "a string value from {expected} but got: '{arg}'")
-    E948 = ("`Matcher.add` received invalid 'patterns' argument: expected "
+    E948 = ("`{name}.add` received invalid 'patterns' argument: expected "
             "a list, but got: {arg_type}")
     E949 = ("Unable to align tokens for the predicted and reference docs. It "
             "is only possible to align the docs when both texts are the same "
             "except for whitespace and capitalization. The predicted tokens "
             "start with: {x}. The reference tokens start with: {y}.")
     E952 = ("The section '{name}' is not a valid section in the provided config.")
     E953 = ("Mismatched IDs received by the Tok2Vec listener: {id1} vs. {id2}")
@@ -924,16 +909,14 @@
              "statistical model to be installed and loaded. For more info, see "
              "the documentation:\nhttps://spacy.io/usage/models")
     E1020 = ("No `epoch_resume` value specified and could not infer one from "
              "filename. Specify an epoch to resume from.")
     E1021 = ("`pos` value \"{pp}\" is not a valid Universal Dependencies tag. "
              "Non-UD tags should use the `tag` property.")
     E1022 = ("Words must be of type str or int, but input is of type '{wtype}'")
-    E1023 = ("Couldn't read EntityRuler from the {path}. This file doesn't "
-             "exist.")
     E1024 = ("A pattern with {attr_type} '{label}' is not present in "
              "'{component}' patterns.")
     E1025 = ("Cannot intify the value '{value}' as an IOB string. The only "
              "supported values are: 'I', 'O', 'B' and ''")
     E1026 = ("Edit tree has an invalid format:\n{errors}")
     E1027 = ("AlignmentArray only supports slicing with a step of 1.")
     E1028 = ("AlignmentArray only supports indexing using an int or a slice.")
@@ -962,27 +945,26 @@
              "method in '{name}'. If you want to use this method, make "
              "sure it's overwritten on the subclass.")
     E1046 = ("{cls_name} is an abstract class and cannot be instantiated. If you are looking for spaCy's default "
              "knowledge base, use `InMemoryLookupKB`.")
     E1047 = ("`find_threshold()` only supports components with a `scorer` attribute.")
     E1048 = ("Got '{unexpected}' as console progress bar type, but expected one of the following: {expected}")
     E1049 = ("No available port found for displaCy on host {host}. Please specify an available port "
-             "with `displacy.serve(doc, port=port)`")
-    E1050 = ("Port {port} is already in use. Please specify an available port with `displacy.serve(doc, port=port)` "
-             "or use `auto_select_port=True` to pick an available port automatically.")
-    E1051 = ("'allow_overlap' can only be False when max_positive is 1, but found 'max_positive': {max_positive}.")
-
+             "with `displacy.serve(doc, port)`")
+    E1050 = ("Port {port} is already in use. Please specify an available port with `displacy.serve(doc, port)` "
+             "or use `auto_switch_port=True` to pick an available port automatically.")
 
-# Deprecated model shortcuts, only used in errors and warnings
-OLD_MODEL_SHORTCUTS = {
-    "en": "en_core_web_sm", "de": "de_core_news_sm", "es": "es_core_news_sm",
-    "pt": "pt_core_news_sm", "fr": "fr_core_news_sm", "it": "it_core_news_sm",
-    "nl": "nl_core_news_sm", "el": "el_core_news_sm", "nb": "nb_core_news_sm",
-    "lt": "lt_core_news_sm", "xx": "xx_ent_wiki_sm"
-}
+    # v4 error strings
+    E4000 = ("Expected a Doc as input, but got: '{type}'")
+    E4001 = ("Expected input to be one of the following types: ({expected_types}), "
+             "but got '{received_type}'")
+    E4002 = ("Pipe '{name}' requires a teacher pipe for distillation.")
+    E4003 = ("Training examples for distillation must have the exact same tokens in the "
+             "reference and predicted docs.")
+    E4004 = ("Backprop is not supported when is_train is not set.")
 
 
 # fmt: on
 
 
 class MatchPatternError(ValueError):
     def __init__(self, key, errors):
```

### Comparing `spacy-3.6.0.dev0/spacy/glossary.py` & `spacy-4.0.0.dev0/spacy/glossary.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/kb/candidate.pyx` & `spacy-4.0.0.dev0/spacy/kb/candidate.pyx`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/kb/kb.pyx` & `spacy-4.0.0.dev0/spacy/kb/kb.pyx`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/kb/kb_in_memory.pxd` & `spacy-4.0.0.dev0/spacy/kb/kb_in_memory.pxd`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/kb/kb_in_memory.pyx` & `spacy-4.0.0.dev0/spacy/kb/kb_in_memory.pyx`

 * *Files 1% similar despite different names*

```diff
@@ -21,15 +21,15 @@
 from .candidate import Candidate as Candidate
 
 
 cdef class InMemoryLookupKB(KnowledgeBase):
     """An `InMemoryLookupKB` instance stores unique identifiers for entities and their textual aliases,
     to support entity linking of named entities to real-world concepts.
 
-    DOCS: https://spacy.io/api/inmemorylookupkb
+    DOCS: https://spacy.io/api/kb_in_memory
     """
 
     def __init__(self, Vocab vocab, entity_vector_length):
         """Create an InMemoryLookupKB."""
         super().__init__(vocab, entity_vector_length)
         self._entry_index = PreshMap()
         self._alias_index = PreshMap()
@@ -42,17 +42,14 @@
     def _initialize_vectors(self, int64_t nr_entities):
         self._vectors_table = float_matrix(nr_entities + 1)
 
     def _initialize_aliases(self, int64_t nr_aliases):
         self._alias_index = PreshMap(nr_aliases + 1)
         self._aliases_table = alias_vec(nr_aliases + 1)
 
-    def is_empty(self):
-        return len(self) == 0
-
     def __len__(self):
         return self.get_size_entities()
 
     def get_size_entities(self):
         return len(self._entry_index)
 
     def get_entity_strings(self):
```

### Comparing `spacy-3.6.0.dev0/spacy/lang/am/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/am/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/am/examples.py` & `spacy-4.0.0.dev0/spacy/lang/am/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/am/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/am/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/am/punctuation.py` & `spacy-4.0.0.dev0/spacy/lang/am/punctuation.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/am/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/am/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ar/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/ar/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ar/examples.py` & `spacy-4.0.0.dev0/spacy/lang/ar/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ar/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/ar/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ar/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/ar/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ar/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/ar/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/az/examples.py` & `spacy-4.0.0.dev0/spacy/lang/az/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/az/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/az/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/az/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/az/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/bg/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/bg/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/bg/examples.py` & `spacy-4.0.0.dev0/spacy/lang/bg/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/bg/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/bg/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/bg/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/bg/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/bg/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/bg/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/bn/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/bn/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/bn/punctuation.py` & `spacy-4.0.0.dev0/spacy/lang/bn/punctuation.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/bn/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/bn/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/bn/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/bn/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ca/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/ca/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ca/examples.py` & `spacy-4.0.0.dev0/spacy/lang/ca/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ca/lemmatizer.py` & `spacy-4.0.0.dev0/spacy/lang/ca/lemmatizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ca/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/ca/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ca/punctuation.py` & `spacy-4.0.0.dev0/spacy/lang/ca/punctuation.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ca/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/ca/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ca/syntax_iterators.py` & `spacy-4.0.0.dev0/spacy/lang/ca/syntax_iterators.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ca/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/ca/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/char_classes.py` & `spacy-4.0.0.dev0/spacy/lang/char_classes.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/cs/examples.py` & `spacy-4.0.0.dev0/spacy/lang/cs/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/cs/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/cs/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/cs/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/cs/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/da/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/da/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/da/examples.py` & `spacy-4.0.0.dev0/spacy/lang/da/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/da/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/da/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/da/punctuation.py` & `spacy-4.0.0.dev0/spacy/lang/da/punctuation.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/da/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/da/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/da/syntax_iterators.py` & `spacy-4.0.0.dev0/spacy/lang/da/syntax_iterators.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/da/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/da/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/de/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/de/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/de/examples.py` & `spacy-4.0.0.dev0/spacy/lang/de/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/de/punctuation.py` & `spacy-4.0.0.dev0/spacy/lang/de/punctuation.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/de/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/de/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/de/syntax_iterators.py` & `spacy-4.0.0.dev0/spacy/lang/de/syntax_iterators.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/de/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/de/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/dsb/examples.py` & `spacy-4.0.0.dev0/spacy/lang/dsb/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/dsb/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/dsb/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/el/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/el/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/el/examples.py` & `spacy-4.0.0.dev0/spacy/lang/el/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/el/get_pos_from_wiktionary.py` & `spacy-4.0.0.dev0/spacy/lang/el/get_pos_from_wiktionary.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/el/lemmatizer.py` & `spacy-4.0.0.dev0/spacy/lang/el/lemmatizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/el/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/el/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/el/punctuation.py` & `spacy-4.0.0.dev0/spacy/lang/el/punctuation.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/el/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/el/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/el/syntax_iterators.py` & `spacy-4.0.0.dev0/spacy/lang/el/syntax_iterators.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/el/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/el/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/en/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/en/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/en/examples.py` & `spacy-4.0.0.dev0/spacy/lang/en/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/en/lemmatizer.py` & `spacy-4.0.0.dev0/spacy/lang/en/lemmatizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/en/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/en/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/en/punctuation.py` & `spacy-4.0.0.dev0/spacy/lang/en/punctuation.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/en/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/en/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/en/syntax_iterators.py` & `spacy-4.0.0.dev0/spacy/lang/en/syntax_iterators.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/en/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/en/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/es/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/es/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/es/examples.py` & `spacy-4.0.0.dev0/spacy/lang/es/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/es/lemmatizer.py` & `spacy-4.0.0.dev0/spacy/lang/es/lemmatizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/es/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/es/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/es/punctuation.py` & `spacy-4.0.0.dev0/spacy/lang/es/punctuation.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/es/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/es/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/es/syntax_iterators.py` & `spacy-4.0.0.dev0/spacy/lang/es/syntax_iterators.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/es/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/es/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/eu/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/eu/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/eu/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/eu/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/fa/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/fa/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/fa/examples.py` & `spacy-4.0.0.dev0/spacy/lang/fa/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/fa/generate_verbs_exc.py` & `spacy-4.0.0.dev0/spacy/lang/fa/generate_verbs_exc.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/fa/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/fa/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/fa/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/fa/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/fa/syntax_iterators.py` & `spacy-4.0.0.dev0/spacy/lang/fa/syntax_iterators.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/fa/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/fa/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/fi/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/fi/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/fi/examples.py` & `spacy-4.0.0.dev0/spacy/lang/fi/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/fi/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/fi/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/fi/punctuation.py` & `spacy-4.0.0.dev0/spacy/lang/fi/punctuation.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/fi/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/fi/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/fi/syntax_iterators.py` & `spacy-4.0.0.dev0/spacy/lang/fi/syntax_iterators.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/fi/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/fi/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/fr/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/fr/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/fr/_tokenizer_exceptions_list.py` & `spacy-4.0.0.dev0/spacy/lang/fr/_tokenizer_exceptions_list.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/fr/examples.py` & `spacy-4.0.0.dev0/spacy/lang/fr/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/fr/lemmatizer.py` & `spacy-4.0.0.dev0/spacy/lang/fr/lemmatizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/fr/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/fr/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/fr/punctuation.py` & `spacy-4.0.0.dev0/spacy/lang/fr/punctuation.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/fr/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/fr/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/fr/syntax_iterators.py` & `spacy-4.0.0.dev0/spacy/lang/fr/syntax_iterators.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/fr/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/fr/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ga/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/ga/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ga/lemmatizer.py` & `spacy-4.0.0.dev0/spacy/lang/ga/lemmatizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ga/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/ga/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ga/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/ga/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/grc/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/grc/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/grc/examples.py` & `spacy-4.0.0.dev0/spacy/lang/grc/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/grc/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/grc/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/grc/punctuation.py` & `spacy-4.0.0.dev0/spacy/lang/grc/punctuation.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/grc/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/grc/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/grc/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/grc/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/gu/examples.py` & `spacy-4.0.0.dev0/spacy/lang/gu/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/gu/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/gu/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/he/examples.py` & `spacy-4.0.0.dev0/spacy/lang/he/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/he/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/he/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/he/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/he/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/hi/examples.py` & `spacy-4.0.0.dev0/spacy/lang/hi/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/hi/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/hi/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/hi/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/hi/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/hr/lemma_lookup_license.txt` & `spacy-4.0.0.dev0/spacy/lang/hr/lemma_lookup_license.txt`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/hr/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/hr/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/hsb/examples.py` & `spacy-4.0.0.dev0/spacy/lang/hsb/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/hsb/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/hsb/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/hu/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/hu/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/hu/punctuation.py` & `spacy-4.0.0.dev0/spacy/lang/hu/punctuation.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/hu/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/hu/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/hu/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/hu/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/hy/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/hy/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/hy/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/hy/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/id/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/id/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/id/_tokenizer_exceptions_list.py` & `spacy-4.0.0.dev0/spacy/lang/id/_tokenizer_exceptions_list.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/id/examples.py` & `spacy-4.0.0.dev0/spacy/lang/id/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/id/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/id/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/id/punctuation.py` & `spacy-4.0.0.dev0/spacy/lang/id/punctuation.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/id/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/id/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/id/syntax_iterators.py` & `spacy-4.0.0.dev0/spacy/lang/id/syntax_iterators.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/id/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/id/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/is/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/is/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/it/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/it/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/it/lemmatizer.py` & `spacy-4.0.0.dev0/spacy/lang/it/lemmatizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/it/punctuation.py` & `spacy-4.0.0.dev0/spacy/lang/it/punctuation.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/it/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/it/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/it/syntax_iterators.py` & `spacy-4.0.0.dev0/spacy/lang/it/syntax_iterators.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/it/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/it/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ja/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/ja/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ja/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/ja/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ja/syntax_iterators.py` & `spacy-4.0.0.dev0/spacy/lang/ja/syntax_iterators.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ja/tag_bigram_map.py` & `spacy-4.0.0.dev0/spacy/lang/ja/tag_bigram_map.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ja/tag_map.py` & `spacy-4.0.0.dev0/spacy/lang/ja/tag_map.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ja/tag_orth_map.py` & `spacy-4.0.0.dev0/spacy/lang/ja/tag_orth_map.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/kn/examples.py` & `spacy-4.0.0.dev0/spacy/lang/kn/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/kn/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/kn/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ko/examples.py` & `spacy-4.0.0.dev0/spacy/lang/ko/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ko/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/ko/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ko/tag_map.py` & `spacy-4.0.0.dev0/spacy/lang/ko/tag_map.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ky/examples.py` & `spacy-4.0.0.dev0/spacy/lang/ky/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ky/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/ky/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ky/punctuation.py` & `spacy-4.0.0.dev0/spacy/lang/ky/punctuation.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ky/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/ky/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ky/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/ky/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/la/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/la/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/la/syntax_iterators.py` & `spacy-4.0.0.dev0/spacy/lang/pt/syntax_iterators.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,85 +1,85 @@
 from typing import Union, Iterator, Tuple
-from ...tokens import Doc, Span
-from ...symbols import NOUN, PROPN, PRON, VERB, AUX
-from ...errors import Errors
 
-# NB: Modified from da on suggestion from https://github.com/explosion/spaCy/issues/7457#issuecomment-800349751 [PJB]
+from ...symbols import NOUN, PROPN, PRON
+from ...errors import Errors
+from ...tokens import Doc, Span
 
 
 def noun_chunks(doclike: Union[Doc, Span]) -> Iterator[Tuple[int, int, int]]:
-    def is_verb_token(tok):
-        return tok.pos in [VERB, AUX]
-
-    def get_left_bound(root):
-        left_bound = root
-        for tok in reversed(list(root.lefts)):
-            if tok.dep in np_left_deps:
-                left_bound = tok
-        return left_bound
-
-    def get_right_bound(doc, root):
-        right_bound = root
-        for tok in root.rights:
-            if tok.dep in np_right_deps:
-                right = get_right_bound(doc, tok)
-                if list(
-                    filter(
-                        lambda t: is_verb_token(t) or t.dep in stop_deps,
-                        doc[root.i : right.i],
-                    )
-                ):
-                    break
-                else:
-                    right_bound = right
-        return right_bound
-
-    def get_bounds(doc, root):
-        return get_left_bound(root), get_right_bound(doc, root)
-
+    """
+    Detect base noun phrases from a dependency parse. Works on both Doc and Span.
+    """
+    labels = [
+        "nsubj",
+        "nsubj:pass",
+        "obj",
+        "obl",
+        "obl:agent",
+        "nmod",
+        "pcomp",
+        "appos",
+        "ROOT",
+    ]
+    post_modifiers = ["flat", "flat:name", "fixed", "compound"]
     doc = doclike.doc  # Ensure works on both Doc and Span.
-
     if not doc.has_annotation("DEP"):
         raise ValueError(Errors.E029)
-
-    if not len(doc):
-        return
-
-    left_labels = [
-        "det",
-        "fixed",
-        "nmod:poss",
-        "amod",
-        "flat",
-        "goeswith",
-        "nummod",
-        "appos",
-    ]
-    right_labels = [
-        "fixed",
-        "nmod:poss",
-        "amod",
-        "flat",
-        "goeswith",
-        "nummod",
-        "appos",
-        "nmod",
-        "det",
-    ]
-    stop_labels = ["punct"]
-
+    np_deps = {doc.vocab.strings.add(label) for label in labels}
+    np_modifs = {doc.vocab.strings.add(modifier) for modifier in post_modifiers}
     np_label = doc.vocab.strings.add("NP")
-    np_left_deps = [doc.vocab.strings.add(label) for label in left_labels]
-    np_right_deps = [doc.vocab.strings.add(label) for label in right_labels]
-    stop_deps = [doc.vocab.strings.add(label) for label in stop_labels]
-
-    prev_right = -1
-    for token in doclike:
-        if token.pos in [PROPN, NOUN, PRON]:
-            left, right = get_bounds(doc, token)
-            if left.i <= prev_right:
-                continue
-            yield left.i, right.i + 1, np_label
-            prev_right = right.i
+    adj_label = doc.vocab.strings.add("amod")
+    det_label = doc.vocab.strings.add("det")
+    det_pos = doc.vocab.strings.add("DET")
+    adp_label = doc.vocab.strings.add("ADP")
+    conj = doc.vocab.strings.add("conj")
+    conj_pos = doc.vocab.strings.add("CCONJ")
+    prev_end = -1
+    for i, word in enumerate(doclike):
+        if word.pos not in (NOUN, PROPN, PRON):
+            continue
+        # Prevent nested chunks from being produced
+        if word.left_edge.i <= prev_end:
+            continue
+        if word.dep in np_deps:
+            right_childs = list(word.rights)
+            right_child = right_childs[0] if right_childs else None
+
+            if right_child:
+                if (
+                    right_child.dep == adj_label
+                ):  # allow chain of adjectives by expanding to right
+                    right_end = right_child.right_edge
+                elif (
+                    right_child.dep == det_label and right_child.pos == det_pos
+                ):  # cut relative pronouns here
+                    right_end = right_child
+                elif right_child.dep in np_modifs:  # Check if we can expand to right
+                    right_end = word.right_edge
+                else:
+                    right_end = word
+            else:
+                right_end = word
+            prev_end = right_end.i
+
+            left_index = word.left_edge.i
+            left_index = (
+                left_index + 1 if word.left_edge.pos == adp_label else left_index
+            )
+
+            yield left_index, right_end.i + 1, np_label
+        elif word.dep == conj:
+            head = word.head
+            while head.dep == conj and head.head.i < head.i:
+                head = head.head
+            # If the head is an NP, and we're coordinated to it, we're an NP
+            if head.dep in np_deps:
+                prev_end = word.i
+
+                left_index = word.left_edge.i  # eliminate left attached conjunction
+                left_index = (
+                    left_index + 1 if word.left_edge.pos == conj_pos else left_index
+                )
+                yield left_index, word.i + 1, np_label
 
 
 SYNTAX_ITERATORS = {"noun_chunks": noun_chunks}
```

### Comparing `spacy-3.6.0.dev0/spacy/lang/lb/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/lb/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/lb/examples.py` & `spacy-4.0.0.dev0/spacy/lang/lb/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/lb/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/lb/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/lb/punctuation.py` & `spacy-4.0.0.dev0/spacy/lang/lb/punctuation.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/lb/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/lb/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/lb/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/lb/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/lg/examples.py` & `spacy-4.0.0.dev0/spacy/lang/lg/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/lg/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/lg/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/lg/punctuation.py` & `spacy-4.0.0.dev0/spacy/lang/lg/punctuation.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/lg/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/lg/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/lij/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/lij/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/lij/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/lij/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/lt/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/lt/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/lt/examples.py` & `spacy-4.0.0.dev0/spacy/lang/lt/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/lt/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/lt/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/lt/punctuation.py` & `spacy-4.0.0.dev0/spacy/lang/lt/punctuation.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/lt/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/lt/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/lv/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/lv/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/mk/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/mk/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/mk/lemmatizer.py` & `spacy-4.0.0.dev0/spacy/lang/mk/lemmatizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/mk/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/mk/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/mk/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/mk/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/mk/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/mk/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ml/examples.py` & `spacy-4.0.0.dev0/spacy/lang/ml/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ml/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/ml/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/mr/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/mr/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/nb/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/nb/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/nb/punctuation.py` & `spacy-4.0.0.dev0/spacy/lang/nb/punctuation.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/nb/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/nb/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/nb/syntax_iterators.py` & `spacy-4.0.0.dev0/spacy/lang/nb/syntax_iterators.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/nb/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/nb/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ne/examples.py` & `spacy-4.0.0.dev0/spacy/lang/ne/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ne/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/ne/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ne/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/ne/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/nl/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/nl/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/nl/lemmatizer.py` & `spacy-4.0.0.dev0/spacy/lang/nl/lemmatizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/nl/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/nl/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/nl/punctuation.py` & `spacy-4.0.0.dev0/spacy/lang/nl/punctuation.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/nl/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/nl/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/nl/syntax_iterators.py` & `spacy-4.0.0.dev0/spacy/lang/nl/syntax_iterators.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/nl/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/nl/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/norm_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/norm_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/pl/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/pl/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/pl/examples.py` & `spacy-4.0.0.dev0/spacy/lang/pl/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/pl/lemmatizer.py` & `spacy-4.0.0.dev0/spacy/lang/pl/lemmatizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/pl/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/pl/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/pl/punctuation.py` & `spacy-4.0.0.dev0/spacy/lang/pl/punctuation.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/pl/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/pl/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/pt/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/pt/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/pt/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/pt/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/pt/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/pt/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/pt/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/pt/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/punctuation.py` & `spacy-4.0.0.dev0/spacy/lang/punctuation.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ro/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/ro/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ro/examples.py` & `spacy-4.0.0.dev0/spacy/lang/ro/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ro/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/ro/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ro/punctuation.py` & `spacy-4.0.0.dev0/spacy/lang/ro/punctuation.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ro/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/ro/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ro/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/ro/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ru/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/ru/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ru/examples.py` & `spacy-4.0.0.dev0/spacy/lang/ru/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ru/lemmatizer.py` & `spacy-4.0.0.dev0/spacy/lang/ru/lemmatizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ru/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/ru/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ru/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/ru/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ru/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/ru/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/sa/examples.py` & `spacy-4.0.0.dev0/spacy/lang/sa/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/sa/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/sa/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/sa/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/sa/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/si/examples.py` & `spacy-4.0.0.dev0/spacy/lang/si/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/si/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/si/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/si/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/si/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/sk/examples.py` & `spacy-4.0.0.dev0/spacy/lang/sk/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/sk/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/sk/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/sk/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/sk/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/sl/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/sl/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/sl/examples.py` & `spacy-4.0.0.dev0/spacy/lang/sl/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/sl/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/sl/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/sl/punctuation.py` & `spacy-4.0.0.dev0/spacy/lang/sl/punctuation.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/sl/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/sl/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/sl/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/sl/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/sq/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/sq/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/sr/examples.py` & `spacy-4.0.0.dev0/spacy/lang/sr/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/sr/lemma_lookup_licence.txt` & `spacy-4.0.0.dev0/spacy/lang/sr/lemma_lookup_licence.txt`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/sr/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/sr/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/sr/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/sr/tokenizer_exceptions.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,8 +1,7 @@
-from .lex_attrs import _cyr_to_latin_norm
 from ..tokenizer_exceptions import BASE_EXCEPTIONS
 from ...symbols import ORTH, NORM
 from ...util import update_exc
 
 
 _exc = {}
 
@@ -86,11 +85,9 @@
     {ORTH: " '", NORM: " "},
     {ORTH: "'", NORM: ""},
 ]
 
 for slang_desc in _slang_exc:
     _exc[slang_desc[ORTH]] = [slang_desc]
 
-for _exc_key in _exc:
-    _exc[_exc_key][0][NORM] = _cyr_to_latin_norm(_exc[_exc_key][0][NORM])
 
 TOKENIZER_EXCEPTIONS = update_exc(BASE_EXCEPTIONS, _exc)
```

### Comparing `spacy-3.6.0.dev0/spacy/lang/sv/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/sv/__init__.py`

 * *Files 6% similar despite different names*

```diff
@@ -2,15 +2,18 @@
 from thinc.api import Model
 from .tokenizer_exceptions import TOKENIZER_EXCEPTIONS
 from .stop_words import STOP_WORDS
 from .lex_attrs import LEX_ATTRS
 from .syntax_iterators import SYNTAX_ITERATORS
 from ...language import Language, BaseDefaults
 from ...pipeline import Lemmatizer
-from .punctuation import TOKENIZER_INFIXES, TOKENIZER_SUFFIXES
+
+
+# Punctuation stolen from Danish
+from ..da.punctuation import TOKENIZER_INFIXES, TOKENIZER_SUFFIXES
 
 
 class SwedishDefaults(BaseDefaults):
     tokenizer_exceptions = TOKENIZER_EXCEPTIONS
     infixes = TOKENIZER_INFIXES
     suffixes = TOKENIZER_SUFFIXES
     lex_attr_getters = LEX_ATTRS
```

### Comparing `spacy-3.6.0.dev0/spacy/lang/sv/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/sv/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/sv/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/sv/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/sv/syntax_iterators.py` & `spacy-4.0.0.dev0/spacy/lang/sv/syntax_iterators.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/sv/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/sv/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ta/examples.py` & `spacy-4.0.0.dev0/spacy/lang/ta/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ta/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/ta/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ta/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/ta/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/te/examples.py` & `spacy-4.0.0.dev0/spacy/lang/te/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/te/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/te/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/te/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/te/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/th/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/th/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/th/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/th/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/th/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/th/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/th/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/th/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ti/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/ti/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ti/examples.py` & `spacy-4.0.0.dev0/spacy/lang/ti/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ti/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/ti/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ti/punctuation.py` & `spacy-4.0.0.dev0/spacy/lang/ti/punctuation.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ti/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/ti/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/tl/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/tl/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/tl/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/tl/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/tl/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/tl/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/tn/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/tn/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/tn/punctuation.py` & `spacy-4.0.0.dev0/spacy/lang/tn/punctuation.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/tn/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/tn/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/tokenizer_exceptions.py`

 * *Files 4% similar despite different names*

```diff
@@ -13,18 +13,14 @@
     # protocol identifier (mods: make optional and expand schemes)
     # (see: https://www.iana.org/assignments/uri-schemes/uri-schemes.xhtml)
     r"(?:(?:[\w\+\-\.]{2,})://)?"
     # mailto:user or user:pass authentication
     r"(?:\S+(?::\S*)?@)?"
     r"(?:"
     # IP address exclusion
-    # private & local networks
-    r"(?!(?:10|127)(?:\.\d{1,3}){3})"
-    r"(?!(?:169\.254|192\.168)(?:\.\d{1,3}){2})"
-    r"(?!172\.(?:1[6-9]|2\d|3[0-1])(?:\.\d{1,3}){2})"
     # IP address dotted notation octets
     # excludes loopback network 0.0.0.0
     # excludes reserved space >= 224.0.0.0
     # excludes network & broadcast addresses
     # (first & last IP address of each class)
     # MH: Do we really need this? Seems excessive, and seems to have caused
     # Issue #957
```

### Comparing `spacy-3.6.0.dev0/spacy/lang/tr/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/tr/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/tr/examples.py` & `spacy-4.0.0.dev0/spacy/lang/tr/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/tr/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/tr/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/tr/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/tr/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/tr/syntax_iterators.py` & `spacy-4.0.0.dev0/spacy/lang/tr/syntax_iterators.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/tr/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/tr/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/tt/examples.py` & `spacy-4.0.0.dev0/spacy/lang/tt/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/tt/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/tt/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/tt/punctuation.py` & `spacy-4.0.0.dev0/spacy/lang/tt/punctuation.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/tt/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/tt/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/tt/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/tt/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/uk/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/uk/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/uk/examples.py` & `spacy-4.0.0.dev0/spacy/lang/uk/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/uk/lemmatizer.py` & `spacy-4.0.0.dev0/spacy/lang/uk/lemmatizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/uk/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/uk/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/uk/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/uk/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/uk/tokenizer_exceptions.py` & `spacy-4.0.0.dev0/spacy/lang/uk/tokenizer_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ur/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/ur/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/ur/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/ur/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/vi/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/vi/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/vi/examples.py` & `spacy-4.0.0.dev0/spacy/lang/vi/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/vi/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/vi/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/vi/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/vi/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/xx/examples.py` & `spacy-4.0.0.dev0/spacy/lang/xx/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/yo/examples.py` & `spacy-4.0.0.dev0/spacy/lang/yo/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/yo/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/yo/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/yo/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/yo/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/zh/__init__.py` & `spacy-4.0.0.dev0/spacy/lang/zh/__init__.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/zh/examples.py` & `spacy-4.0.0.dev0/spacy/lang/zh/examples.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/zh/lex_attrs.py` & `spacy-4.0.0.dev0/spacy/lang/zh/lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/lang/zh/stop_words.py` & `spacy-4.0.0.dev0/spacy/lang/zh/stop_words.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/language.py` & `spacy-4.0.0.dev0/spacy/language.py`

 * *Files 1% similar despite different names*

```diff
@@ -100,15 +100,15 @@
         )
 
     return tokenizer_factory
 
 
 @registry.misc("spacy.LookupsDataLoader.v1")
 def load_lookups_data(lang, tables):
-    util.logger.debug("Loading lookups from spacy-lookups-data: %s", tables)
+    util.logger.debug(f"Loading lookups from spacy-lookups-data: {tables}")
     lookups = load_lookups(lang=lang, tables=tables)
     return lookups
 
 
 class Language:
     """A text-processing pipeline. Usually you'll load this once per process,
     and pass the instance around your application.
@@ -1235,23 +1235,14 @@
             proc.rehearse(  # type: ignore[attr-defined]
                 examples, sgd=get_grads, losses=losses, **component_cfg.get(name, {})
             )
         for key, (W, dW) in grads.items():
             sgd(key, W, dW)  # type: ignore[call-arg, misc]
         return losses
 
-    def begin_training(
-        self,
-        get_examples: Optional[Callable[[], Iterable[Example]]] = None,
-        *,
-        sgd: Optional[Optimizer] = None,
-    ) -> Optimizer:
-        warnings.warn(Warnings.W089, DeprecationWarning)
-        return self.initialize(get_examples, sgd=sgd)
-
     def initialize(
         self,
         get_examples: Optional[Callable[[], Iterable[Example]]] = None,
         *,
         sgd: Optional[Optimizer] = None,
     ) -> Optimizer:
         """Initialize the pipe for training, using data examples if available.
@@ -1965,15 +1956,15 @@
         if not isinstance(tok2vec, ty.ListenedToComponent):
             raise ValueError(Errors.E888.format(name=tok2vec_name, pipe=type(tok2vec)))
         tok2vec_model = tok2vec.model
         pipe_listeners = tok2vec.listener_map.get(pipe_name, [])
         pipe = self.get_pipe(pipe_name)
         pipe_cfg = self._pipe_configs[pipe_name]
         if listeners:
-            util.logger.debug("Replacing listeners of component '%s'", pipe_name)
+            util.logger.debug(f"Replacing listeners of component '{pipe_name}'")
             if len(list(listeners)) != len(pipe_listeners):
                 # The number of listeners defined in the component model doesn't
                 # match the listeners to replace, so we won't be able to update
                 # the nodes and generate a matching config
                 err = Errors.E887.format(
                     name=pipe_name,
                     tok2vec=tok2vec_name,
```

### Comparing `spacy-3.6.0.dev0/spacy/lexeme.pxd` & `spacy-4.0.0.dev0/spacy/lexeme.pxd`

 * *Files 4% similar despite different names*

```diff
@@ -1,15 +1,14 @@
 from numpy cimport ndarray
 
 from .typedefs cimport attr_t, hash_t, flags_t, len_t, tag_t
 from .attrs cimport attr_id_t
 from .attrs cimport ID, ORTH, LOWER, NORM, SHAPE, PREFIX, SUFFIX, LENGTH, LANG
 
 from .structs cimport LexemeC
-from .strings cimport StringStore
 from .vocab cimport Vocab
 
 
 cdef LexemeC EMPTY_LEXEME
 cdef attr_t OOV_RANK
 
 cdef class Lexeme:
```

### Comparing `spacy-3.6.0.dev0/spacy/lexeme.pyi` & `spacy-4.0.0.dev0/spacy/lexeme.pyi`

 * *Files 6% similar despite different names*

```diff
@@ -16,21 +16,19 @@
     def similarity(self, other: Union[Doc, Span, Token, Lexeme]) -> float: ...
     @property
     def has_vector(self) -> bool: ...
     @property
     def vector_norm(self) -> float: ...
     vector: Floats1d
     rank: int
-    sentiment: float
     @property
     def orth_(self) -> str: ...
     @property
     def text(self) -> str: ...
-    orth: int
-    lower: int
+    lower: str
     norm: int
     shape: int
     prefix: int
     suffix: int
     cluster: int
     lang: int
     prob: float
```

### Comparing `spacy-3.6.0.dev0/spacy/lexeme.pyx` & `spacy-4.0.0.dev0/spacy/lexeme.pyx`

 * *Files 2% similar despite different names*

```diff
@@ -169,41 +169,28 @@
             to index into tables, e.g. for word vectors."""
         def __get__(self):
             return self.c.id
 
         def __set__(self, value):
             self.c.id = value
 
-    property sentiment:
-        """RETURNS (float): A scalar value indicating the positivity or
-            negativity of the lexeme."""
-        def __get__(self):
-            sentiment_table = self.vocab.lookups.get_table("lexeme_sentiment", {})
-            return sentiment_table.get(self.c.orth, 0.0)
-
-        def __set__(self, float x):
-            if "lexeme_sentiment" not in self.vocab.lookups:
-                self.vocab.lookups.add_table("lexeme_sentiment")
-            sentiment_table = self.vocab.lookups.get_table("lexeme_sentiment")
-            sentiment_table[self.c.orth] = x
-
     @property
     def orth_(self):
         """RETURNS (str): The original verbatim text of the lexeme
             (identical to `Lexeme.text`). Exists mostly for consistency with
             the other attributes."""
         return self.vocab.strings[self.c.orth]
 
     @property
     def text(self):
         """RETURNS (str): The original verbatim text of the lexeme."""
         return self.orth_
 
     property lower:
-        """RETURNS (uint64): Lowercase form of the lexeme."""
+        """RETURNS (str): Lowercase form of the lexeme."""
         def __get__(self):
             return self.c.lower
 
         def __set__(self, attr_t x):
             self.c.lower = x
 
     property norm:
```

### Comparing `spacy-3.6.0.dev0/spacy/lookups.py` & `spacy-4.0.0.dev0/spacy/lookups.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/matcher/dependencymatcher.pyi` & `spacy-4.0.0.dev0/spacy/matcher/dependencymatcher.pyi`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/matcher/dependencymatcher.pyx` & `spacy-4.0.0.dev0/spacy/matcher/dependencymatcher.pyx`

 * *Files 4% similar despite different names*

```diff
@@ -78,20 +78,16 @@
             ".*": self._precede,
             ";": self._imm_follow,
             ";*": self._follow,
             "$+": self._imm_right_sib,
             "$-": self._imm_left_sib,
             "$++": self._right_sib,
             "$--": self._left_sib,
-            ">+": self._imm_right_child,
-            ">-": self._imm_left_child,
             ">++": self._right_child,
             ">--": self._left_child,
-            "<+": self._imm_right_parent,
-            "<-": self._imm_left_parent,
             "<++": self._right_parent,
             "<--": self._left_parent,
         }
 
     def __reduce__(self):
         data = (self.vocab, self._raw_patterns, self._callbacks)
         return (unpickle_matcher, data, None, None)
@@ -165,17 +161,17 @@
         """Add a new matcher rule to the matcher.
 
         key (str): The match ID.
         patterns (list): The patterns to add for the given key.
         on_match (callable): Optional callback executed on match.
         """
         if on_match is not None and not hasattr(on_match, "__call__"):
-            raise ValueError(Errors.E171.format(arg_type=type(on_match)))
-        if patterns is None or not isinstance(patterns, List):  # old API
-            raise ValueError(Errors.E948.format(arg_type=type(patterns)))
+            raise ValueError(Errors.E171.format(name="DependencyMatcher", arg_type=type(on_match)))
+        if patterns is None or not isinstance(patterns, List):
+            raise ValueError(Errors.E948.format(name="DependencyMatcher", arg_type=type(patterns)))
         for pattern in patterns:
             if len(pattern) == 0:
                 raise ValueError(Errors.E012.format(key=key))
             self._validate_input(pattern, key)
 
         key = self._normalize_key(key)
 
@@ -427,41 +423,19 @@
 
     def _right_sib(self, doc, node):
         return [doc[child.i] for child in doc[node].head.children if child.i > node]
 
     def _left_sib(self, doc, node):
         return [doc[child.i] for child in doc[node].head.children if child.i < node]
 
-    def _imm_right_child(self, doc, node):
-        for child in doc[node].rights:
-            if child.i == node + 1:
-                return [doc[child.i]]
-        return []
-
-    def _imm_left_child(self, doc, node):
-        for child in doc[node].lefts:
-            if child.i == node - 1:
-                return [doc[child.i]]
-        return []
-
     def _right_child(self, doc, node):
-        return [child for child in doc[node].rights]
+        return [doc[child.i] for child in doc[node].children if child.i > node]
     
     def _left_child(self, doc, node):
-        return [child for child in doc[node].lefts]
-
-    def _imm_right_parent(self, doc, node):
-        if doc[node].head.i == node + 1:
-            return [doc[node].head]
-        return []
-
-    def _imm_left_parent(self, doc, node):
-        if doc[node].head.i == node - 1:
-            return [doc[node].head]
-        return []
+        return [doc[child.i] for child in doc[node].children if child.i < node]
 
     def _right_parent(self, doc, node):
         if doc[node].head.i > node:
             return [doc[node].head]
         return []
     
     def _left_parent(self, doc, node):
```

### Comparing `spacy-3.6.0.dev0/spacy/matcher/levenshtein.c` & `spacy-4.0.0.dev0/spacy/matcher/levenshtein.c`

 * *Files 0% similar despite different names*

```diff
@@ -1,20 +1,20 @@
-/* Generated by Cython 0.29.34 */
+/* Generated by Cython 0.29.33 */
 
 /* BEGIN: Cython Metadata
 {
     "distutils": {
         "depends": [
-            "/opt/hostedtoolcache/Python/3.8.16/x64/include/python3.8/Python.h",
+            "/opt/hostedtoolcache/Python/3.8.15/x64/include/python3.8/Python.h",
             "spacy/matcher/polyleven.c"
         ],
         "include_dirs": [
             "spacy/matcher",
-            "/tmp/build-env-_zzhfsgw/lib/python3.8/site-packages/numpy/core/include",
-            "/opt/hostedtoolcache/Python/3.8.16/x64/include/python3.8"
+            "/opt/hostedtoolcache/Python/3.8.15/x64/lib/python3.8/site-packages/numpy/core/include",
+            "/opt/hostedtoolcache/Python/3.8.15/x64/include/python3.8"
         ],
         "language": "c",
         "name": "spacy.matcher.levenshtein",
         "sources": [
             "spacy/matcher/levenshtein.pyx",
             "spacy/matcher/polyleven.c"
         ]
@@ -28,16 +28,16 @@
 #endif /* PY_SSIZE_T_CLEAN */
 #include "Python.h"
 #ifndef Py_PYTHON_H
     #error Python headers needed to compile C extensions, please install development version of Python.
 #elif PY_VERSION_HEX < 0x02060000 || (0x03000000 <= PY_VERSION_HEX && PY_VERSION_HEX < 0x03030000)
     #error Cython requires Python 2.6+ or Python 3.3+.
 #else
-#define CYTHON_ABI "0_29_34"
-#define CYTHON_HEX_VERSION 0x001D22F0
+#define CYTHON_ABI "0_29_33"
+#define CYTHON_HEX_VERSION 0x001D21F0
 #define CYTHON_FUTURE_DIVISION 0
 #include <stddef.h>
 #ifndef offsetof
   #define offsetof(type, member) ( (size_t) & ((type*)0) -> member )
 #endif
 #if !defined(WIN32) && !defined(MS_WINDOWS)
   #ifndef __stdcall
@@ -222,15 +222,15 @@
   #elif !defined(CYTHON_USE_ASYNC_SLOTS)
     #define CYTHON_USE_ASYNC_SLOTS 1
   #endif
   #if PY_VERSION_HEX < 0x02070000
     #undef CYTHON_USE_PYLONG_INTERNALS
     #define CYTHON_USE_PYLONG_INTERNALS 0
   #elif !defined(CYTHON_USE_PYLONG_INTERNALS)
-    #define CYTHON_USE_PYLONG_INTERNALS (PY_VERSION_HEX < 0x030C00A5)
+    #define CYTHON_USE_PYLONG_INTERNALS 1
   #endif
   #ifndef CYTHON_USE_PYLIST_INTERNALS
     #define CYTHON_USE_PYLIST_INTERNALS 1
   #endif
   #ifndef CYTHON_USE_UNICODE_INTERNALS
     #define CYTHON_USE_UNICODE_INTERNALS 1
   #endif
@@ -261,15 +261,15 @@
   #ifndef CYTHON_PEP489_MULTI_PHASE_INIT
     #define CYTHON_PEP489_MULTI_PHASE_INIT (PY_VERSION_HEX >= 0x03050000)
   #endif
   #ifndef CYTHON_USE_TP_FINALIZE
     #define CYTHON_USE_TP_FINALIZE (PY_VERSION_HEX >= 0x030400a1)
   #endif
   #ifndef CYTHON_USE_DICT_VERSIONS
-    #define CYTHON_USE_DICT_VERSIONS ((PY_VERSION_HEX >= 0x030600B1) && (PY_VERSION_HEX < 0x030C00A5))
+    #define CYTHON_USE_DICT_VERSIONS (PY_VERSION_HEX >= 0x030600B1)
   #endif
   #if PY_VERSION_HEX >= 0x030B00A4
     #undef CYTHON_USE_EXC_INFO_STACK
     #define CYTHON_USE_EXC_INFO_STACK 0
   #elif !defined(CYTHON_USE_EXC_INFO_STACK)
     #define CYTHON_USE_EXC_INFO_STACK (PY_VERSION_HEX >= 0x030700A3)
   #endif
@@ -1467,28 +1467,20 @@
 #define __Pyx_GetModuleGlobalNameUncached(var, name)  (var) = __Pyx__GetModuleGlobalName(name)
 static CYTHON_INLINE PyObject *__Pyx__GetModuleGlobalName(PyObject *name);
 #endif
 
 /* TypeImport.proto */
 #ifndef __PYX_HAVE_RT_ImportType_proto
 #define __PYX_HAVE_RT_ImportType_proto
-#if __STDC_VERSION__ >= 201112L
-#include <stdalign.h>
-#endif
-#if __STDC_VERSION__ >= 201112L || __cplusplus >= 201103L
-#define __PYX_GET_STRUCT_ALIGNMENT(s) alignof(s)
-#else
-#define __PYX_GET_STRUCT_ALIGNMENT(s) sizeof(void*)
-#endif
 enum __Pyx_ImportType_CheckSize {
    __Pyx_ImportType_CheckSize_Error = 0,
    __Pyx_ImportType_CheckSize_Warn = 1,
    __Pyx_ImportType_CheckSize_Ignore = 2
 };
-static PyTypeObject *__Pyx_ImportType(PyObject* module, const char *module_name, const char *class_name, size_t size, size_t alignment, enum __Pyx_ImportType_CheckSize check_size);
+static PyTypeObject *__Pyx_ImportType(PyObject* module, const char *module_name, const char *class_name, size_t size, enum __Pyx_ImportType_CheckSize check_size);
 #endif
 
 /* Import.proto */
 static PyObject *__Pyx_Import(PyObject *name, PyObject *from_list, int level);
 
 /* ImportFrom.proto */
 static PyObject* __Pyx_ImportFrom(PyObject* module, PyObject* name);
@@ -2008,15 +2000,15 @@
  *     else:
  */
     goto __pyx_L3;
   }
 
   /* "spacy/matcher/levenshtein.pyx":26
  *         # allow at least two edits (to allow at least one transposition) and up
- *         # to 30% of the pattern string length
+ *         # to 20% of the pattern string length
  *         max_edits = max(2, round(0.3 * len(pattern_text)))             # <<<<<<<<<<<<<<
  *     return levenshtein(input_text, pattern_text, max_edits) <= max_edits
  * 
  */
   /*else*/ {
     __pyx_t_3 = PyObject_Length(__pyx_v_pattern_text); if (unlikely(__pyx_t_3 == ((Py_ssize_t)-1))) __PYX_ERR(0, 26, __pyx_L1_error)
     __pyx_t_1 = PyFloat_FromDouble((0.3 * __pyx_t_3)); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 26, __pyx_L1_error)
@@ -2046,15 +2038,15 @@
     __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;
     __pyx_v_max_edits = __pyx_t_4;
     __pyx_t_4 = 0;
   }
   __pyx_L3:;
 
   /* "spacy/matcher/levenshtein.pyx":27
- *         # to 30% of the pattern string length
+ *         # to 20% of the pattern string length
  *         max_edits = max(2, round(0.3 * len(pattern_text)))
  *     return levenshtein(input_text, pattern_text, max_edits) <= max_edits             # <<<<<<<<<<<<<<
  * 
  * 
  */
   __pyx_t_9.__pyx_n = 1;
   __pyx_t_9.k = __pyx_v_max_edits;
@@ -2461,17 +2453,17 @@
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("__Pyx_modinit_type_import_code", 0);
   /*--- Type import code ---*/
   __pyx_t_1 = PyImport_ImportModule(__Pyx_BUILTIN_MODULE_NAME); if (unlikely(!__pyx_t_1)) __PYX_ERR(1, 9, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_1);
   __pyx_ptype_7cpython_4type_type = __Pyx_ImportType(__pyx_t_1, __Pyx_BUILTIN_MODULE_NAME, "type", 
   #if defined(PYPY_VERSION_NUM) && PYPY_VERSION_NUM < 0x050B0000
-  sizeof(PyTypeObject), __PYX_GET_STRUCT_ALIGNMENT(PyTypeObject),
+  sizeof(PyTypeObject),
   #else
-  sizeof(PyHeapTypeObject), __PYX_GET_STRUCT_ALIGNMENT(PyHeapTypeObject),
+  sizeof(PyHeapTypeObject),
   #endif
   __Pyx_ImportType_CheckSize_Warn);
    if (!__pyx_ptype_7cpython_4type_type) __PYX_ERR(1, 9, __pyx_L1_error)
   __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;
   __Pyx_RefNannyFinishContext();
   return 0;
   __pyx_L1_error:;
@@ -3487,62 +3479,44 @@
     return __Pyx_GetBuiltinName(name);
 }
 
 /* TypeImport */
 #ifndef __PYX_HAVE_RT_ImportType
 #define __PYX_HAVE_RT_ImportType
 static PyTypeObject *__Pyx_ImportType(PyObject *module, const char *module_name, const char *class_name,
-    size_t size, size_t alignment, enum __Pyx_ImportType_CheckSize check_size)
+    size_t size, enum __Pyx_ImportType_CheckSize check_size)
 {
     PyObject *result = 0;
     char warning[200];
     Py_ssize_t basicsize;
-    Py_ssize_t itemsize;
 #ifdef Py_LIMITED_API
     PyObject *py_basicsize;
-    PyObject *py_itemsize;
 #endif
     result = PyObject_GetAttrString(module, class_name);
     if (!result)
         goto bad;
     if (!PyType_Check(result)) {
         PyErr_Format(PyExc_TypeError,
             "%.200s.%.200s is not a type object",
             module_name, class_name);
         goto bad;
     }
 #ifndef Py_LIMITED_API
     basicsize = ((PyTypeObject *)result)->tp_basicsize;
-    itemsize = ((PyTypeObject *)result)->tp_itemsize;
 #else
     py_basicsize = PyObject_GetAttrString(result, "__basicsize__");
     if (!py_basicsize)
         goto bad;
     basicsize = PyLong_AsSsize_t(py_basicsize);
     Py_DECREF(py_basicsize);
     py_basicsize = 0;
     if (basicsize == (Py_ssize_t)-1 && PyErr_Occurred())
         goto bad;
-    py_itemsize = PyObject_GetAttrString(result, "__itemsize__");
-    if (!py_itemsize)
-        goto bad;
-    itemsize = PyLong_AsSsize_t(py_itemsize);
-    Py_DECREF(py_itemsize);
-    py_itemsize = 0;
-    if (itemsize == (Py_ssize_t)-1 && PyErr_Occurred())
-        goto bad;
 #endif
-    if (itemsize) {
-        if (size % alignment) {
-            alignment = size % alignment;
-        }
-        if (itemsize < (Py_ssize_t)alignment)
-            itemsize = (Py_ssize_t)alignment;
-    }
-    if ((size_t)(basicsize + itemsize) < size) {
+    if ((size_t)basicsize < size) {
         PyErr_Format(PyExc_ValueError,
             "%.200s.%.200s size changed, may indicate binary incompatibility. "
             "Expected %zd from C header, got %zd from PyObject",
             module_name, class_name, size, basicsize);
         goto bad;
     }
     if (check_size == __Pyx_ImportType_CheckSize_Error && (size_t)basicsize != size) {
```

### Comparing `spacy-3.6.0.dev0/spacy/matcher/levenshtein.pyx` & `spacy-4.0.0.dev0/spacy/matcher/levenshtein.pyx`

 * *Files 1% similar despite different names*

```diff
@@ -18,15 +18,15 @@
 
 
 cpdef bint levenshtein_compare(input_text: str, pattern_text: str, fuzzy: int = -1):
     if fuzzy >= 0:
         max_edits = fuzzy
     else:
         # allow at least two edits (to allow at least one transposition) and up
-        # to 30% of the pattern string length
+        # to 20% of the pattern string length
         max_edits = max(2, round(0.3 * len(pattern_text)))
     return levenshtein(input_text, pattern_text, max_edits) <= max_edits
 
 
 @registry.misc("spacy.levenshtein_compare.v1")
 def make_levenshtein_compare():
     return levenshtein_compare
```

### Comparing `spacy-3.6.0.dev0/spacy/matcher/matcher.pxd` & `spacy-4.0.0.dev0/spacy/matcher/matcher.pxd`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/matcher/matcher.pyi` & `spacy-4.0.0.dev0/spacy/matcher/matcher.pyi`

 * *Files 6% similar despite different names*

```diff
@@ -1,20 +1,16 @@
 from typing import Any, List, Dict, Tuple, Optional, Callable, Union
 from typing import Iterator, Iterable, overload
 from ..compat import Literal
 from ..vocab import Vocab
 from ..tokens import Doc, Span
 
 class Matcher:
-    def __init__(
-        self,
-        vocab: Vocab,
-        validate: bool = ...,
-        fuzzy_compare: Callable[[str, str, int], bool] = ...,
-    ) -> None: ...
+    def __init__(self, vocab: Vocab, validate: bool = ...,
+                 fuzzy_compare: Callable[[str, str, int], bool] = ...) -> None: ...
     def __reduce__(self) -> Any: ...
     def __len__(self) -> int: ...
     def __contains__(self, key: str) -> bool: ...
     def add(
         self,
         key: Union[str, int],
         patterns: List[List[Dict[str, Any]]],
```

### Comparing `spacy-3.6.0.dev0/spacy/matcher/matcher.pyx` & `spacy-4.0.0.dev0/spacy/matcher/matcher.pyx`

 * *Files 1% similar despite different names*

```diff
@@ -19,15 +19,15 @@
 from ..tokens.token cimport Token
 from ..tokens.morphanalysis cimport MorphAnalysis
 from ..attrs cimport ID, attr_id_t, NULL_ATTR, ORTH, POS, TAG, DEP, LEMMA, MORPH, ENT_IOB
 
 from .levenshtein import levenshtein_compare
 from ..schemas import validate_token_pattern
 from ..errors import Errors, MatchPatternError, Warnings
-from ..strings import get_string_id
+from ..strings cimport get_string_id
 from ..attrs import IDS
 from ..util import registry
 
 
 DEF PADDING = 5
 
 
@@ -111,17 +111,17 @@
         key (Union[str, int]): The match ID.
         patterns (list): The patterns to add for the given key.
         on_match (callable): Optional callback executed on match.
         greedy (str): Optional filter: "FIRST" or "LONGEST".
         """
         errors = {}
         if on_match is not None and not hasattr(on_match, "__call__"):
-            raise ValueError(Errors.E171.format(arg_type=type(on_match)))
-        if patterns is None or not isinstance(patterns, List):  # old API
-            raise ValueError(Errors.E948.format(arg_type=type(patterns)))
+            raise ValueError(Errors.E171.format(name="Matcher", arg_type=type(on_match)))
+        if patterns is None or not isinstance(patterns, List):
+            raise ValueError(Errors.E948.format(name="Matcher", arg_type=type(patterns)))
         if greedy is not None and greedy not in ["FIRST", "LONGEST"]:
             raise ValueError(Errors.E947.format(expected=["FIRST", "LONGEST"], arg=greedy))
         for i, pattern in enumerate(patterns):
             if len(pattern) == 0:
                 raise ValueError(Errors.E012.format(key=key))
             if not isinstance(pattern, list):
                 raise ValueError(Errors.E178.format(pat=pattern, key=key))
@@ -261,14 +261,18 @@
                                     extensions=self._extensions, predicates=self._extra_predicates, with_alignments=with_alignments)
         final_matches = []
         pairs_by_id = {}
         # For each key, either add all matches, or only the filtered,
         # non-overlapping ones this `match` can be either (start, end) or
         # (start, end, alignments) depending on `with_alignments=` option.
         for key, *match in matches:
+            # Adjust span matches to doc offsets
+            if isinstance(doclike, Span):
+                match[0] += doclike.start
+                match[1] += doclike.start
             span_filter = self._filter.get(key)
             if span_filter is not None:
                 pairs = pairs_by_id.get(key, [])
                 pairs.append(match)
                 pairs_by_id[key] = pairs
             else:
                 final_matches.append((key, *match))
@@ -291,17 +295,14 @@
                 if memcmp(&matched[start], &empty[start], span_len * sizeof(matched[0])) == 0:
                     final_matches.append((key, *match))
                     # Mark tokens that have matched
                     memset(&matched[start], 1, span_len * sizeof(matched[0]))
         if as_spans:
             final_results = []
             for key, start, end, *_ in final_matches:
-                if isinstance(doclike, Span):
-                    start += doclike.start
-                    end += doclike.start
                 final_results.append(Span(doc, start, end, label=key))
         elif with_alignments:
             # convert alignments List[Dict[str, int]] --> List[int]
             # when multiple alignment (belongs to the same length) is found,
             # keeps the alignment that has largest token_idx
             final_results = []
             for key, start, end, alignments in final_matches:
@@ -824,19 +825,14 @@
             attr_values.append((attr, value))
         else:
             # should be caught in validation
             raise ValueError(Errors.E152.format(attr=input_attr))
     return attr_values
 
 
-def _predicate_cache_key(attr, predicate, value, *, regex=False, fuzzy=None):
-    # tuple order affects performance
-    return (attr, regex, fuzzy, predicate, srsly.json_dumps(value, sort_keys=True))
-
-
 # These predicate helper classes are used to match the REGEX, IN, >= etc
 # extensions to the matcher introduced in #3173.
 
 class _FuzzyPredicate:
     operators = ("FUZZY", "FUZZY1", "FUZZY2", "FUZZY3", "FUZZY4", "FUZZY5",
                  "FUZZY6", "FUZZY7", "FUZZY8", "FUZZY9")
 
@@ -848,15 +844,15 @@
         self.predicate = predicate
         self.is_extension = is_extension
         if self.predicate not in self.operators:
             raise ValueError(Errors.E126.format(good=self.operators, bad=self.predicate))
         fuzz = self.predicate[len("FUZZY"):] # number after prefix
         self.fuzzy = int(fuzz) if fuzz else -1
         self.fuzzy_compare = fuzzy_compare
-        self.key = _predicate_cache_key(self.attr, self.predicate, value, fuzzy=self.fuzzy)
+        self.key = (self.attr, self.fuzzy, self.predicate, srsly.json_dumps(value, sort_keys=True))
 
     def __call__(self, Token token):
         if self.is_extension:
             value = token._.get(self.attr)
         else:
             value = token.vocab.strings[get_token_attr_for_matcher(token.c, self.attr)]
         if self.value == value:
@@ -870,15 +866,15 @@
     def __init__(self, i, attr, value, predicate, is_extension=False, vocab=None,
                  regex=False, fuzzy=None, fuzzy_compare=None):
         self.i = i
         self.attr = attr
         self.value = re.compile(value)
         self.predicate = predicate
         self.is_extension = is_extension
-        self.key = _predicate_cache_key(self.attr, self.predicate, value)
+        self.key = (self.attr, self.predicate, srsly.json_dumps(value, sort_keys=True))
         if self.predicate not in self.operators:
             raise ValueError(Errors.E126.format(good=self.operators, bad=self.predicate))
 
     def __call__(self, Token token):
         if self.is_extension:
             value = token._.get(self.attr)
         else:
@@ -906,15 +902,15 @@
             elif self.fuzzy is not None:
                 # add to string store
                 self.value = set(self.vocab.strings.add(v) for v in value)
             else:
                 self.value = set(get_string_id(v) for v in value)
         self.predicate = predicate
         self.is_extension = is_extension
-        self.key = _predicate_cache_key(self.attr, self.predicate, value, regex=self.regex, fuzzy=self.fuzzy)
+        self.key = (self.attr, self.regex, self.fuzzy, self.predicate, srsly.json_dumps(value, sort_keys=True))
         if self.predicate not in self.operators:
             raise ValueError(Errors.E126.format(good=self.operators, bad=self.predicate))
 
     def __call__(self, Token token):
         if self.is_extension:
             value = token._.get(self.attr)
         else:
@@ -978,15 +974,15 @@
     def __init__(self, i, attr, value, predicate, is_extension=False, vocab=None,
                  regex=False, fuzzy=None, fuzzy_compare=None):
         self.i = i
         self.attr = attr
         self.value = value
         self.predicate = predicate
         self.is_extension = is_extension
-        self.key = _predicate_cache_key(self.attr, self.predicate, value)
+        self.key = (self.attr, self.predicate, srsly.json_dumps(value, sort_keys=True))
         if self.predicate not in self.operators:
             raise ValueError(Errors.E126.format(good=self.operators, bad=self.predicate))
 
     def __call__(self, Token token):
         if self.is_extension:
             value = token._.get(self.attr)
         else:
@@ -1093,15 +1089,15 @@
 def _get_extension_extra_predicates(spec, extra_predicates, predicate_types,
         seen_predicates):
     output = []
     for attr, value in spec.items():
         if isinstance(value, dict):
             for type_, cls in predicate_types.items():
                 if type_ in value:
-                    key = _predicate_cache_key(attr, type_, value[type_])
+                    key = (attr, type_, srsly.json_dumps(value[type_], sort_keys=True))
                     if key in seen_predicates:
                         output.append(seen_predicates[key])
                     else:
                         predicate = cls(len(extra_predicates), attr, value[type_], type_,
                                         is_extension=True)
                         extra_predicates.append(predicate)
                         output.append(predicate.i)
```

### Comparing `spacy-3.6.0.dev0/spacy/matcher/phrasematcher.pxd` & `spacy-4.0.0.dev0/spacy/matcher/phrasematcher.pxd`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/matcher/phrasematcher.pyi` & `spacy-4.0.0.dev0/spacy/matcher/phrasematcher.pyi`

 * *Files 5% similar despite different names*

```diff
@@ -16,14 +16,23 @@
         key: str,
         docs: List[Doc],
         *,
         on_match: Optional[
             Callable[[Matcher, Doc, int, List[Tuple[Any, ...]]], Any]
         ] = ...,
     ) -> None: ...
+    def _add_from_arrays(
+        self,
+        key: str,
+        specs: List[List[int]],
+        *,
+        on_match: Optional[
+            Callable[[Matcher, Doc, int, List[Tuple[Any, ...]]], Any]
+        ] = ...,
+    ) -> None: ...
     def remove(self, key: str) -> None: ...
     @overload
     def __call__(
         self,
         doclike: Union[Doc, Span],
         *,
         as_spans: Literal[False] = ...,
```

### Comparing `spacy-3.6.0.dev0/spacy/matcher/phrasematcher.pyx` & `spacy-4.0.0.dev0/spacy/matcher/phrasematcher.pyx`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,10 @@
 # cython: infer_types=True, profile=True
+from typing import List
+from collections import defaultdict
 from libc.stdint cimport uintptr_t
 from preshed.maps cimport map_init, map_set, map_get, map_clear, map_iter
 
 import warnings
 
 from ..attrs cimport ORTH, POS, TAG, DEP, LEMMA, MORPH
 from ..attrs import IDS
@@ -35,15 +37,15 @@
         attr (int / str): Token attribute to match on.
         validate (bool): Perform additional validation when patterns are added.
 
         DOCS: https://spacy.io/api/phrasematcher#init
         """
         self.vocab = vocab
         self._callbacks = {}
-        self._docs = {}
+        self._docs = defaultdict(set)
         self._validate = validate
 
         self.mem = Pool()
         self.c_map = <MapStruct*>self.mem.alloc(1, sizeof(MapStruct))
         self._terminal_hash = 826361138722620965
         map_init(self.mem, self.c_map, 8)
 
@@ -151,74 +153,32 @@
                     result = map_get(current_node, self._terminal_hash)
                     if result:
                         map_clear(<MapStruct*>result, self.vocab.strings[key])
 
         del self._callbacks[key]
         del self._docs[key]
 
-    def add(self, key, docs, *_docs, on_match=None):
-        """Add a match-rule to the phrase-matcher. A match-rule consists of: an ID
-        key, an on_match callback, and one or more patterns.
 
-        Since spaCy v2.2.2, PhraseMatcher.add takes a list of patterns as the
-        second argument, with the on_match callback as an optional keyword
-        argument.
+    def _add_from_arrays(self, key, specs, *, on_match=None):
+        """Add a preprocessed list of specs, with an optional callback.
 
         key (str): The match ID.
-        docs (list): List of `Doc` objects representing match patterns.
+        specs (List[List[int]]): A list of lists of hashes to match.
         on_match (callable): Callback executed on match.
-        *_docs (Doc): For backwards compatibility: list of patterns to add
-            as variable arguments. Will be ignored if a list of patterns is
-            provided as the second argument.
-
-        DOCS: https://spacy.io/api/phrasematcher#add
         """
-        if docs is None or hasattr(docs, "__call__"):  # old API
-            on_match = docs
-            docs = _docs
-
-        _ = self.vocab[key]
-        self._callbacks[key] = on_match
-        self._docs.setdefault(key, set())
-
         cdef MapStruct* current_node
         cdef MapStruct* internal_node
         cdef void* result
 
-        if isinstance(docs, Doc):
-            raise ValueError(Errors.E179.format(key=key))
-        for doc in docs:
-            if len(doc) == 0:
-                continue
-            if isinstance(doc, Doc):
-                attrs = (TAG, POS, MORPH, LEMMA, DEP)
-                has_annotation = {attr: doc.has_annotation(attr) for attr in attrs}
-                for attr in attrs:
-                    if self.attr == attr and not has_annotation[attr]:
-                        if attr == TAG:
-                            pipe = "tagger"
-                        elif attr in (POS, MORPH):
-                            pipe = "morphologizer or tagger+attribute_ruler"
-                        elif attr == LEMMA:
-                            pipe = "lemmatizer"
-                        elif attr == DEP:
-                            pipe = "parser"
-                        error_msg = Errors.E155.format(pipe=pipe, attr=self.vocab.strings.as_string(attr))
-                        raise ValueError(error_msg)
-                if self._validate and any(has_annotation.values()) \
-                        and self.attr not in attrs:
-                    string_attr = self.vocab.strings[self.attr]
-                    warnings.warn(Warnings.W012.format(key=key, attr=string_attr))
-                keyword = self._convert_to_array(doc)
-            else:
-                keyword = doc
-            self._docs[key].add(tuple(keyword))
+        self._callbacks[key] = on_match
+        for spec in specs:
+            self._docs[key].add(tuple(spec))
 
             current_node = self.c_map
-            for token in keyword:
+            for token in spec:
                 if token == self._terminal_hash:
                     warnings.warn(Warnings.W021)
                     break
                 result = <MapStruct*>map_get(current_node, token)
                 if not result:
                     internal_node = <MapStruct*>self.mem.alloc(1, sizeof(MapStruct))
                     map_init(self.mem, internal_node, 8)
@@ -229,14 +189,65 @@
             if not result:
                 internal_node = <MapStruct*>self.mem.alloc(1, sizeof(MapStruct))
                 map_init(self.mem, internal_node, 8)
                 map_set(self.mem, current_node, self._terminal_hash, internal_node)
                 result = internal_node
             map_set(self.mem, <MapStruct*>result, self.vocab.strings[key], NULL)
 
+
+    def add(self, key, docs, *, on_match=None):
+        """Add a match-rule to the phrase-matcher. A match-rule consists of: an ID
+        key, a list of one or more patterns, and (optionally) an on_match callback.
+
+        key (str): The match ID.
+        docs (list): List of `Doc` objects representing match patterns.
+        on_match (callable): Callback executed on match.
+
+        If any of the input Docs are invalid, no internal state will be updated.
+
+        DOCS: https://spacy.io/api/phrasematcher#add
+        """
+        if isinstance(docs, Doc):
+            raise ValueError(Errors.E179.format(key=key))
+        if docs is None or not isinstance(docs, List):
+            raise ValueError(Errors.E948.format(name="PhraseMatcher", arg_type=type(docs)))
+        if on_match is not None and not hasattr(on_match, "__call__"):
+            raise ValueError(Errors.E171.format(name="PhraseMatcher", arg_type=type(on_match)))
+
+        _ = self.vocab[key]
+        specs = []
+
+        for doc in docs:
+            if len(doc) == 0:
+                continue
+            if not isinstance(doc, Doc):
+                raise ValueError(Errors.E4000.format(type=type(doc)))
+
+            attrs = (TAG, POS, MORPH, LEMMA, DEP)
+            has_annotation = {attr: doc.has_annotation(attr) for attr in attrs}
+            for attr in attrs:
+                if self.attr == attr and not has_annotation[attr]:
+                    if attr == TAG:
+                        pipe = "tagger"
+                    elif attr in (POS, MORPH):
+                        pipe = "morphologizer or tagger+attribute_ruler"
+                    elif attr == LEMMA:
+                        pipe = "lemmatizer"
+                    elif attr == DEP:
+                        pipe = "parser"
+                    error_msg = Errors.E155.format(pipe=pipe, attr=self.vocab.strings.as_string(attr))
+                    raise ValueError(error_msg)
+            if self._validate and any(has_annotation.values()) \
+                    and self.attr not in attrs:
+                string_attr = self.vocab.strings[self.attr]
+                warnings.warn(Warnings.W012.format(key=key, attr=string_attr))
+            specs.append(self._convert_to_array(doc))
+
+        self._add_from_arrays(key, specs, on_match=on_match)
+
     def __call__(self, object doclike, *, as_spans=False):
         """Find all sequences matching the supplied patterns on the `Doc`.
 
         doclike (Doc or Span): The document to match over.
         as_spans (bool): Return Span objects with labels instead of (match_id,
             start, end) tuples.
         RETURNS (list): A list of `(match_id, start, end)` tuples,
@@ -341,15 +352,15 @@
         return [Token.get_struct_attr(&doc.c[i], self.attr) for i in range(len(doc))]
 
 
 def unpickle_matcher(vocab, docs, callbacks, attr):
     matcher = PhraseMatcher(vocab, attr=attr)
     for key, specs in docs.items():
         callback = callbacks.get(key, None)
-        matcher.add(key, specs, on_match=callback)
+        matcher._add_from_arrays(key, specs, on_match=callback)
     return matcher
 
 
 cdef SpanC make_spanstruct(attr_t label, int start, int end) nogil:
     cdef SpanC spanc
     spanc.label = label
     spanc.start = start
```

### Comparing `spacy-3.6.0.dev0/spacy/matcher/polyleven.c` & `spacy-4.0.0.dev0/spacy/matcher/polyleven.c`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/ml/_character_embed.py` & `spacy-4.0.0.dev0/spacy/ml/character_embed.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/ml/callbacks.py` & `spacy-4.0.0.dev0/spacy/ml/callbacks.py`

 * *Files 2% similar despite different names*

```diff
@@ -19,14 +19,15 @@
 DEFAULT_NVTX_ANNOTATABLE_PIPE_METHODS = [
     "pipe",
     "predict",
     "set_annotations",
     "update",
     "rehearse",
     "get_loss",
+    "get_teacher_student_loss",
     "initialize",
     "begin_update",
     "finish_update",
     "update",
 ]
```

### Comparing `spacy-3.6.0.dev0/spacy/ml/extract_ngrams.py` & `spacy-4.0.0.dev0/spacy/ml/extract_ngrams.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/ml/extract_spans.py` & `spacy-4.0.0.dev0/spacy/ml/extract_spans.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import List, Tuple, Callable
+from typing import Tuple, Callable
 from thinc.api import Model, to_numpy
 from thinc.types import Ragged, Ints1d
 
 from ..util import registry
 
 
 @registry.layers("spacy.extract_spans.v1")
@@ -48,19 +48,19 @@
 
 def _get_span_indices(ops, spans: Ragged, lengths: Ints1d) -> Ints1d:
     """Construct a flat array that has the indices we want to extract from the
     source data. For instance, if we want the spans (5, 9), (8, 10) the
     indices will be [5, 6, 7, 8, 8, 9].
     """
     spans, lengths = _ensure_cpu(spans, lengths)
-    indices: List[int] = []
+    indices = []
     offset = 0
     for i, length in enumerate(lengths):
         spans_i = spans[i].dataXd + offset
         for j in range(spans_i.shape[0]):
-            indices.extend(range(spans_i[j, 0], spans_i[j, 1]))  # type: ignore[arg-type, call-overload]
+            indices.append(ops.xp.arange(spans_i[j, 0], spans_i[j, 1]))  # type: ignore[call-overload, index]
         offset += length
-    return ops.asarray1i(indices)
+    return ops.flatten(indices, dtype="i", ndim_if_empty=1)
 
 
 def _ensure_cpu(spans: Ragged, lengths: Ints1d) -> Tuple[Ragged, Ints1d]:
     return Ragged(to_numpy(spans.dataXd), to_numpy(spans.lengths)), to_numpy(lengths)
```

### Comparing `spacy-3.6.0.dev0/spacy/ml/featureextractor.py` & `spacy-4.0.0.dev0/spacy/ml/featureextractor.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/ml/models/entity_linker.py` & `spacy-4.0.0.dev0/spacy/ml/models/entity_linker.py`

 * *Files 20% similar despite different names*

```diff
@@ -85,22 +85,14 @@
         kb = InMemoryLookupKB(vocab, entity_vector_length=1)
         kb.from_disk(kb_path)
         return kb
 
     return kb_from_file
 
 
-@registry.misc("spacy.EmptyKB.v2")
-def empty_kb_for_config() -> Callable[[Vocab, int], KnowledgeBase]:
-    def empty_kb_factory(vocab: Vocab, entity_vector_length: int):
-        return InMemoryLookupKB(vocab=vocab, entity_vector_length=entity_vector_length)
-
-    return empty_kb_factory
-
-
 @registry.misc("spacy.EmptyKB.v1")
 def empty_kb(
     entity_vector_length: int,
 ) -> Callable[[Vocab], KnowledgeBase]:
     def empty_kb_factory(vocab: Vocab):
         return InMemoryLookupKB(vocab=vocab, entity_vector_length=entity_vector_length)
```

### Comparing `spacy-3.6.0.dev0/spacy/ml/models/multi_task.py` & `spacy-4.0.0.dev0/spacy/ml/models/multi_task.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,18 +1,17 @@
 from typing import Any, Optional, Iterable, Tuple, List, Callable, TYPE_CHECKING, cast
-from thinc.types import Floats2d, Ints1d
+from thinc.types import Floats2d
 from thinc.api import chain, Maxout, LayerNorm, Softmax, Linear, zero_init, Model
 from thinc.api import MultiSoftmax, list2array
 from thinc.api import to_categorical, CosineDistance, L2Distance
 from thinc.loss import Loss
 
 from ...util import registry, OOV_RANK
 from ...errors import Errors
-from ...attrs import ID, ORTH
-from ...vectors import Mode as VectorsMode
+from ...attrs import ID
 
 import numpy
 from functools import partial
 
 if TYPE_CHECKING:
     # This lets us add type hints for mypy etc. without causing circular imports
     from ...vocab import Vocab  # noqa: F401
@@ -64,31 +63,22 @@
     return create_characters_objective
 
 
 def get_vectors_loss(ops, docs, prediction, distance):
     """Compute a loss based on a distance between the documents' vectors and
     the prediction.
     """
-    vocab = docs[0].vocab
-    if vocab.vectors.mode == VectorsMode.default:
-        # The simplest way to implement this would be to vstack the
-        # token.vector values, but that's a bit inefficient, especially on GPU.
-        # Instead we fetch the index into the vectors table for each of our
-        # tokens, and look them up all at once. This prevents data copying.
-        ids = ops.flatten([doc.to_array(ID).ravel() for doc in docs])
-        target = docs[0].vocab.vectors.data[ids]
-        target[ids == OOV_RANK] = 0
-        d_target, loss = distance(prediction, target)
-    elif vocab.vectors.mode == VectorsMode.floret:
-        keys = ops.flatten([cast(Ints1d, doc.to_array(ORTH)) for doc in docs])
-        target = vocab.vectors.get_batch(keys)
-        target = ops.as_contig(target)
-        d_target, loss = distance(prediction, target)
-    else:
-        raise ValueError(Errors.E850.format(mode=vocab.vectors.mode))
+    # The simplest way to implement this would be to vstack the
+    # token.vector values, but that's a bit inefficient, especially on GPU.
+    # Instead we fetch the index into the vectors table for each of our tokens,
+    # and look them up all at once. This prevents data copying.
+    ids = ops.flatten([doc.to_array(ID).ravel() for doc in docs])
+    target = docs[0].vocab.vectors.data[ids]
+    target[ids == OOV_RANK] = 0
+    d_target, loss = distance(prediction, target)
     return loss, d_target
 
 
 def get_characters_loss(ops, docs, prediction, nr_char):
     """Compute a loss based on a number of characters predicted from the docs."""
     target_ids = numpy.vstack([doc.to_utf8_array(nr_char=nr_char) for doc in docs])
     target_ids = target_ids.reshape((-1,))
```

### Comparing `spacy-3.6.0.dev0/spacy/ml/models/spancat.py` & `spacy-4.0.0.dev0/spacy/ml/models/spancat.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/ml/models/tagger.py` & `spacy-4.0.0.dev0/spacy/ml/models/tagger.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/ml/models/textcat.py` & `spacy-4.0.0.dev0/spacy/ml/models/textcat.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/ml/models/tok2vec.py` & `spacy-4.0.0.dev0/spacy/ml/models/tok2vec.py`

 * *Files 1% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 from thinc.api import chain, clone, concatenate, with_array, with_padded
 from thinc.api import Model, noop, list2ragged, ragged2list, HashEmbed
 from thinc.api import expand_window, residual, Maxout, Mish, PyTorchLSTM
 
 from ...tokens import Doc
 from ...util import registry
 from ...errors import Errors
-from ...ml import _character_embed
+from ...ml import character_embed
 from ..staticvectors import StaticVectors
 from ..featureextractor import FeatureExtractor
 from ...pipeline.tok2vec import Tok2VecListener
 from ...attrs import intify_attr
 
 
 @registry.architectures("spacy.Tok2VecListener.v1")
@@ -222,15 +222,15 @@
     include_static_vectors (bool): Whether to also use static word vectors.
         Requires a vectors table to be loaded in the Doc objects' vocab.
     """
     feature = intify_attr(feature)
     if feature is None:
         raise ValueError(Errors.E911.format(feat=feature))
     char_embed = chain(
-        _character_embed.CharacterEmbed(nM=nM, nC=nC),
+        character_embed.CharacterEmbed(nM=nM, nC=nC),
         cast(Model[List[Floats2d], Ragged], list2ragged()),
     )
     feature_extractor: Model[List[Doc], Ragged] = chain(
         FeatureExtractor([feature]),
         cast(Model[List[Ints2d], Ragged], list2ragged()),
         with_array(HashEmbed(nO=width, nV=rows, column=0, seed=5)),  # type: ignore[misc]
     )
```

### Comparing `spacy-3.6.0.dev0/spacy/ml/staticvectors.py` & `spacy-4.0.0.dev0/spacy/ml/staticvectors.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/pipe_analysis.py` & `spacy-4.0.0.dev0/spacy/pipe_analysis.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/__init__.py` & `spacy-4.0.0.dev0/spacy/pipeline/__init__.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,13 +1,12 @@
-from .attributeruler import AttributeRuler
+from .attribute_ruler import AttributeRuler
 from .dep_parser import DependencyParser
 from .edit_tree_lemmatizer import EditTreeLemmatizer
 from .entity_linker import EntityLinker
 from .ner import EntityRecognizer
-from .entityruler import EntityRuler
 from .lemmatizer import Lemmatizer
 from .morphologizer import Morphologizer
 from .pipe import Pipe
 from .trainable_pipe import TrainablePipe
 from .senter import SentenceRecognizer
 from .sentencizer import Sentencizer
 from .tagger import Tagger
@@ -19,15 +18,14 @@
 from .functions import merge_entities, merge_noun_chunks, merge_subtokens
 
 __all__ = [
     "AttributeRuler",
     "DependencyParser",
     "EntityLinker",
     "EntityRecognizer",
-    "EntityRuler",
     "Morphologizer",
     "Lemmatizer",
     "MultiLabel_TextCategorizer",
     "Pipe",
     "SentenceRecognizer",
     "Sentencizer",
     "SpanCategorizer",
```

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/_edit_tree_internals/edit_trees.pxd` & `spacy-4.0.0.dev0/spacy/pipeline/_edit_tree_internals/edit_trees.pxd`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/_edit_tree_internals/edit_trees.pyx` & `spacy-4.0.0.dev0/spacy/pipeline/_edit_tree_internals/edit_trees.pyx`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/_edit_tree_internals/schemas.py` & `spacy-4.0.0.dev0/spacy/pipeline/_edit_tree_internals/schemas.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/_parser_internals/_beam_utils.pyx` & `spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/_beam_utils.pyx`

 * *Files 2% similar despite different names*

```diff
@@ -1,37 +1,37 @@
 # cython: infer_types=True
 # cython: profile=True
 cimport numpy as np
 import numpy
 from cpython.ref cimport PyObject, Py_XDECREF
-from thinc.extra.search cimport Beam
-from thinc.extra.search import MaxViolation
-from thinc.extra.search cimport MaxViolation
 
 from ...typedefs cimport hash_t, class_t
 from .transition_system cimport TransitionSystem, Transition
 from ...errors import Errors
+from .batch cimport Batch
+from .search cimport Beam, MaxViolation
+from .search import MaxViolation
 from .stateclass cimport StateC, StateClass
 
 
-# These are passed as callbacks to thinc.search.Beam
+# These are passed as callbacks to .search.Beam
 cdef int transition_state(void* _dest, void* _src, class_t clas, void* _moves) except -1:
     dest = <StateC*>_dest
     src = <StateC*>_src
     moves = <const Transition*>_moves
     dest.clone(src)
     moves[clas].do(dest, moves[clas].label)
 
 
 cdef int check_final_state(void* _state, void* extra_args) except -1:
     state = <StateC*>_state
     return state.is_final()
 
 
-cdef class BeamBatch(object):
+cdef class BeamBatch(Batch):
     cdef public TransitionSystem moves
     cdef public object states
     cdef public object docs
     cdef public object golds
     cdef public object beams
 
     def __init__(self, TransitionSystem moves, states, golds,
```

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/_parser_internals/_state.pxd` & `spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/_state.pxd`

 * *Files 10% similar despite different names*

```diff
@@ -2,15 +2,14 @@
 from libc.string cimport memcpy, memset
 from libc.stdlib cimport calloc, free
 from libc.stdint cimport uint32_t, uint64_t
 cimport libcpp
 from libcpp.unordered_map cimport unordered_map
 from libcpp.vector cimport vector
 from libcpp.set cimport set
-from cpython.exc cimport PyErr_CheckSignals, PyErr_SetFromErrno
 from murmurhash.mrmr cimport hash64
 
 from ...vocab cimport EMPTY_LEXEME
 from ...structs cimport TokenC, SpanC
 from ...lexeme cimport Lexeme
 from ...attrs cimport IS_SPACE
 from ...typedefs cimport attr_t
@@ -22,47 +21,50 @@
 cdef struct ArcC:
     int head
     int child
     attr_t label
 
 
 cdef cppclass StateC:
-    int* _heads
+    vector[int] _heads
     const TokenC* _sent
     vector[int] _stack
     vector[int] _rebuffer
     vector[SpanC] _ents
     unordered_map[int, vector[ArcC]] _left_arcs
     unordered_map[int, vector[ArcC]] _right_arcs
     vector[libcpp.bool] _unshiftable
+    vector[int] history
     set[int] _sent_starts
     TokenC _empty_token
     int length
     int offset
     int _b_i
 
-    __init__(const TokenC* sent, int length) nogil:
+    __init__(const TokenC* sent, int length) nogil except +:
+        this._heads.resize(length, -1)
+        this._unshiftable.resize(length, False)
+
+        # Reserve memory ahead of time to minimize allocations during parsing.
+        # The initial capacity set here ideally reflects the expected average-case/majority usage.
+        cdef int init_capacity = 32
+        this._stack.reserve(init_capacity)
+        this._rebuffer.reserve(init_capacity)
+        this._ents.reserve(init_capacity)
+        this._left_arcs.reserve(init_capacity)
+        this._right_arcs.reserve(init_capacity)
+        this.history.reserve(init_capacity)
+
         this._sent = sent
-        this._heads = <int*>calloc(length, sizeof(int))
-        if not (this._sent and this._heads):
-            with gil:
-                PyErr_SetFromErrno(MemoryError)
-                PyErr_CheckSignals()
         this.offset = 0
         this.length = length
         this._b_i = 0
-        for i in range(length):
-            this._heads[i] = -1
-            this._unshiftable.push_back(0)
         memset(&this._empty_token, 0, sizeof(TokenC))
         this._empty_token.lex = &EMPTY_LEXEME
 
-    __dealloc__():
-        free(this._heads)
-
     void set_context_tokens(int* ids, int n) nogil:
         cdef int i, j
         if n == 1:
             if this.B(0) >= 0:
                 ids[0] = this.B(0)
             else:
                 ids[0] = -1
@@ -127,27 +129,28 @@
         for i in range(n):
             if ids[i] >= 0:
                 ids[i] += this.offset
             else:
                 ids[i] = -1
 
     int S(int i) nogil const:
-        if i >= this._stack.size():
-            return -1
-        elif i < 0:
+        cdef int stack_size = this._stack.size()
+        if i >= stack_size or i < 0:
             return -1
-        return this._stack.at(this._stack.size() - (i+1))
+        else:
+            return this._stack[stack_size - (i+1)]
 
     int B(int i) nogil const:
+        cdef int buf_size = this._rebuffer.size()
         if i < 0:
             return -1
-        elif i < this._rebuffer.size():
-            return this._rebuffer.at(this._rebuffer.size() - (i+1))
+        elif i < buf_size:
+            return this._rebuffer[buf_size - (i+1)]
         else:
-            b_i = this._b_i + (i - this._rebuffer.size())
+            b_i = this._b_i + (i - buf_size)
             if b_i >= this.length:
                 return -1
             else:
                 return b_i
 
     const TokenC* B_(int i) nogil const:
         return this.safe_get(this.B(i))
@@ -238,15 +241,15 @@
             return 0
 
     int is_sent_start(int word) nogil const:
         if word < 0 or word >= this.length:
             return 0
         elif this._sent[word].sent_start == 1:
             return 1
-        elif this._sent_starts.count(word) >= 1:
+        elif this._sent_starts.const_find(word) != this._sent_starts.const_end():
             return 1
         else:
             return 0
 
     void set_sent_start(int word, int value) nogil:
         if value >= 1:
             this._sent_starts.insert(word)
@@ -323,15 +326,15 @@
         this._rebuffer.push_back(s0)
         this._stack.pop_back()
 
     int is_unshiftable(int item) nogil const:
         if item >= this._unshiftable.size():
             return 0
         else:
-            return this._unshiftable.at(item)
+            return this._unshiftable[item]
 
     void set_reshiftable(int item) nogil:
         if item < this._unshiftable.size():
             this._unshiftable[item] = 0
 
     void add_arc(int head, int child, attr_t label) nogil:
         if this.has_head(child):
@@ -343,28 +346,31 @@
         if head > child:
             this._left_arcs[arc.head].push_back(arc)
         else:
             this._right_arcs[arc.head].push_back(arc)
         this._heads[child] = head
 
     void map_del_arc(unordered_map[int, vector[ArcC]]* heads_arcs, int h_i, int c_i) nogil:
+        cdef vector[ArcC]* arcs
+        cdef ArcC* arc
+
         arcs_it = heads_arcs.find(h_i)
         if arcs_it == heads_arcs.end():
             return
 
         arcs = &deref(arcs_it).second
         if arcs.size() == 0:
             return
 
-        arc = arcs.back()
+        arc = &arcs.back()
         if arc.head == h_i and arc.child == c_i:
             arcs.pop_back()
         else:
             for i in range(arcs.size()-1):
-                arc = arcs.at(i)
+                arc = &deref(arcs)[i]
                 if arc.head == h_i and arc.child == c_i:
                     arc.head = -1
                     arc.child = -1
                     arc.label = 0
                     break
 
     void del_arc(int h_i, int c_i) nogil:
@@ -396,14 +402,15 @@
     void clone(const StateC* src) nogil:
         this.length = src.length
         this._sent = src._sent
         this._stack = src._stack
         this._rebuffer = src._rebuffer
         this._sent_starts = src._sent_starts
         this._unshiftable = src._unshiftable
-        memcpy(this._heads, src._heads, this.length * sizeof(this._heads[0]))
+        this._heads = src._heads
         this._ents = src._ents
         this._left_arcs = src._left_arcs
         this._right_arcs = src._right_arcs
         this._b_i = src._b_i
         this.offset = src.offset
         this._empty_token = src._empty_token
+        this.history = src.history
```

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/_parser_internals/arc_eager.pyx` & `spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/arc_eager.pyx`

 * *Files 2% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 from ...tokens.doc cimport Doc, set_children_from_heads
 from ...tokens.token cimport MISSING_DEP
 from ...training import split_bilu_label
 from ...training.example cimport Example
 from .stateclass cimport StateClass
 from ._state cimport StateC, ArcC
 from ...errors import Errors
-from thinc.extra.search cimport Beam
+from .search cimport Beam
 
 cdef weight_t MIN_SCORE = -90000
 cdef attr_t SUBTOK_LABEL = hash_string('subtok')
 
 DEF NON_MONOTONIC = True
 
 cdef enum:
@@ -769,14 +769,16 @@
 
     cdef get_arcs(self, StateC* state):
         cdef vector[ArcC] arcs
         state.get_arcs(&arcs)
         return list(arcs)
 
     def has_gold(self, Example eg, start=0, end=None):
+        if end is not None and end < 0:
+            end = None
         for word in eg.y[start:end]:
             if word.dep != 0:
                 return True
         else:
             return False
 
     cdef int set_valid(self, int* output, const StateC* st) nogil:
@@ -854,14 +856,15 @@
                     if _debug:
                         example = _debug
                         debug_log.append(" ".join((
                             self.get_class_name(i),
                             state.print_state()
                         )))
                     action.do(state.c, action.label)
+                    state.c.history.push_back(i)
                     break
             else:
                 failed = False
                 break
         if failed and _debug not in (False, None):
             example = _debug
             print("Actions")
```

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/_parser_internals/ner.pyx` & `spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/ner.pyx`

 * *Files 3% similar despite different names*

```diff
@@ -1,24 +1,26 @@
 import os
 import random
 from libc.stdint cimport int32_t
+from libcpp.memory cimport shared_ptr
+from libcpp.vector cimport vector
 from cymem.cymem cimport Pool
 
 from collections import Counter
-from thinc.extra.search cimport Beam
 
 from ...tokens.doc cimport Doc
 from ...tokens.span import Span
 from ...tokens.span cimport Span
 from ...typedefs cimport weight_t, attr_t
 from ...lexeme cimport Lexeme
 from ...attrs cimport IS_SPACE
 from ...structs cimport TokenC, SpanC
 from ...training import split_bilu_label
 from ...training.example cimport Example
+from .search cimport Beam
 from .stateclass cimport StateClass
 from ._state cimport StateC
 from .transition_system cimport Transition, do_func_t
 
 from ...errors import Errors
 
 
@@ -39,17 +41,15 @@
 MOVE_NAMES[LAST] = 'L'
 MOVE_NAMES[UNIT] = 'U'
 MOVE_NAMES[OUT] = 'O'
 
 
 cdef struct GoldNERStateC:
     Transition* ner
-    SpanC* negs
-    int32_t length
-    int32_t nr_neg
+    vector[shared_ptr[SpanC]] negs
 
 
 cdef class BiluoGold:
     cdef Pool mem
     cdef GoldNERStateC c
 
     def __init__(self, BiluoPushDown moves, StateClass stcls, Example example, neg_key):
@@ -74,31 +74,29 @@
             example.y.spans.get(neg_key, []),
             allow_overlap=True
         )
     else:
         negs = []
     assert example.x.length > 0
     gs.ner = <Transition*>mem.alloc(example.x.length, sizeof(Transition))
-    gs.negs = <SpanC*>mem.alloc(len(negs), sizeof(SpanC))
-    gs.nr_neg = len(negs)
     ner_ents, ner_tags = example.get_aligned_ents_and_ner()
     for i, ner_tag in enumerate(ner_tags):
         gs.ner[i] = moves.lookup_transition(ner_tag)
 
     # Prevent conflicting spans in the data. For NER, spans are equal if they have the same offsets and label.
     neg_span_triples = {(neg_ent.start_char, neg_ent.end_char, neg_ent.label) for neg_ent in negs}
     for pos_span in ner_ents:
         if (pos_span.start_char, pos_span.end_char, pos_span.label) in neg_span_triples:
             raise ValueError(Errors.E868.format(span=(pos_span.start_char, pos_span.end_char, pos_span.label_)))
 
     # In order to handle negative samples, we need to maintain the full
     # (start, end, label) triple. If we break it down to the 'isnt B-LOC'
     # thing, we'll get blocked if there's an incorrect prefix.
-    for i, neg in enumerate(negs):
-        gs.negs[i] = neg.c
+    for neg in negs:
+        gs.negs.push_back(neg.c)
     return gs
 
 
 cdef void update_gold_state(GoldNERStateC* gs, const StateC* state) except *:
     # We don't need to update each time, unlike the parser.
     pass
 
@@ -154,15 +152,15 @@
 
     def get_doc_labels(self, doc):
         labels = set()
         for token in doc:
             if token.ent_type:
                 labels.add(token.ent_type_)
         return labels
-    
+
     def move_name(self, int move, attr_t label):
         if move == OUT:
             return 'O'
         elif move == MISSING:
             return 'M'
         else:
             return MOVE_NAMES[move] + '-' + self.strings[label]
@@ -304,14 +302,16 @@
         end = y_span.end
         neg_key = self.neg_key
         if neg_key is not None:
             # If we have any negative samples, count that as having annotation.
             for span in eg.y.spans.get(neg_key, []):
                 if span.start >= start and span.end <= end:
                     return True
+        if end is not None and end < 0:
+            end = None
         for word in eg.y[start:end]:
             if word.ent_iob != 0:
                 return True
         else:
             return False
 
     def get_cost(self, StateClass stcls, gold, int i):
@@ -407,14 +407,16 @@
     cdef weight_t cost(const StateC* s, const void* _gold, attr_t label) nogil:
         gold = <GoldNERStateC*>_gold
         b0 = s.B(0)
         cdef int cost = 0
         cdef int g_act = gold.ner[b0].move
         cdef attr_t g_tag = gold.ner[b0].label
 
+        cdef shared_ptr[SpanC] span
+
         if g_act == MISSING:
             pass
         elif g_act == BEGIN:
             # B, Gold B --> Label match
             cost += label != g_tag
         else:
             # B, Gold I --> False (P)
@@ -424,16 +426,16 @@
             cost += 1
         if s.buffer_length() < 3:
             # Handle negatives. In general we can't really do much to block
             # B, because we don't know whether the whole entity is going to
             # be correct or not. However, we can at least tell whether we're
             # going to be opening an entity where there's only one possible
             # L.
-            for span in gold.negs[:gold.nr_neg]:
-                if span.label == label and span.start == b0:
+            for span in gold.negs:
+                if span.get().label == label and span.get().start == b0:
                     cost += 1
                     break
         return cost
 
 
 cdef class In:
     @staticmethod
@@ -570,16 +572,17 @@
             # L, Gold U --> True
             pass
         else:
             cost += 1
         # If we have negative-example entities, integrate them into the objective,
         # by marking actions that close an entity that we know is incorrect
         # as costly.
-        for span in gold.negs[:gold.nr_neg]:
-            if span.label == label and (span.end-1) == b0 and span.start == ent_start:
+        cdef shared_ptr[SpanC] span
+        for span in gold.negs:
+            if span.get().label == label and (span.get().end-1) == b0 and span.get().start == ent_start:
                 cost += 1
                 break
         return cost
 
 
 cdef class Unit:
     @staticmethod
@@ -635,20 +638,21 @@
             # U, Gold L --> False
             # U, Gold O --> False
             cost += 1
         # If we have negative-example entities, integrate them into the objective.
         # This is fairly straight-forward for U- entities, as we have a single
         # action
         cdef int b0 = s.B(0)
-        for span in gold.negs[:gold.nr_neg]:
-            if span.label == label and span.start == b0 and span.end == (b0+1):
+        cdef shared_ptr[SpanC] span
+        for span in gold.negs:
+            if span.get().label == label and span.get().start == b0 and span.get().end == (b0+1):
                 cost += 1
                 break
         return cost
- 
+
 
 
 cdef class Out:
     @staticmethod
     cdef bint is_valid(const StateC* st, attr_t label) nogil:
         cdef int preset_ent_iob = st.B_(0).ent_iob
         if st.entity_is_open():
```

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/_parser_internals/nonproj.pyx` & `spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/nonproj.pyx`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/_parser_internals/stateclass.pxd` & `spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/stateclass.pxd`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/_parser_internals/stateclass.pyx` & `spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/stateclass.pyx`

 * *Files 2% similar despite different names*

```diff
@@ -17,14 +17,18 @@
             self.doc = None
 
     def __dealloc__(self):
         if self._borrowed != 1:
             del self.c
 
     @property
+    def history(self):
+        return list(self.c.history)
+
+    @property
     def stack(self):
         return [self.S(i) for i in range(self.c.stack_depth())]
 
     @property
     def queue(self):
         return [self.B(i) for i in range(self.c.buffer_length())]
 
@@ -172,7 +176,10 @@
         self.c.open_ent(label)
 
     def close_ent(self):
         self.c.close_ent()
 
     def clone(self, StateClass src):
         self.c.clone(src.c)
+
+    def set_context_tokens(self, int[:, :] output, int row, int n_feats):
+        self.c.set_context_tokens(&output[row, 0], n_feats)
```

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/_parser_internals/transition_system.pxd` & `spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/transition_system.pxd`

 * *Files 4% similar despite different names*

```diff
@@ -49,7 +49,14 @@
 
     cdef Transition init_transition(self, int clas, int move, attr_t label) except *
 
     cdef int set_valid(self, int* output, const StateC* st) nogil
 
     cdef int set_costs(self, int* is_valid, weight_t* costs,
                        const StateC* state, gold) except -1
+
+
+cdef void c_apply_actions(TransitionSystem moves, StateC** states, const int* actions,
+    int batch_size) nogil
+
+cdef void c_transition_batch(TransitionSystem moves, StateC** states, const float* scores,
+        int nr_class, int batch_size) nogil
```

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/_parser_internals/transition_system.pyx` & `spacy-4.0.0.dev0/spacy/pipeline/_parser_internals/transition_system.pyx`

 * *Files 12% similar despite different names*

```diff
@@ -1,19 +1,22 @@
 # cython: infer_types=True
 from __future__ import print_function
 from cymem.cymem cimport Pool
+from libc.stdlib cimport calloc, free
+from libcpp.vector cimport vector
 
 from collections import Counter
 import srsly
 
 from . cimport _beam_utils
 from ...typedefs cimport weight_t, attr_t
 from ...tokens.doc cimport Doc
 from ...structs cimport TokenC
 from .stateclass cimport StateClass
+from ._parser_utils cimport arg_max_if_valid
 
 from ...errors import Errors
 from ... import util
 
 
 cdef weight_t MIN_SCORE = -90000
 
@@ -69,26 +72,39 @@
         offset = 0
         for doc in docs:
             state = StateClass(doc, offset=offset)
             states.append(state)
             offset += len(doc)
         return states
 
+    def follow_history(self, doc, history):
+        cdef int clas
+        cdef StateClass state = StateClass(doc)
+        for clas in history:
+            action = self.c[clas]
+            action.do(state.c, action.label)
+            state.c.history.push_back(clas)
+        return state
+
     def get_oracle_sequence(self, Example example, _debug=False):
+        if not self.has_gold(example):
+            return []
         states, golds, _ = self.init_gold_batch([example])
         if not states:
             return []
         state = states[0]
         gold = golds[0]
         if _debug:
             return self.get_oracle_sequence_from_state(state, gold, _debug=example)
         else:
             return self.get_oracle_sequence_from_state(state, gold)
 
     def get_oracle_sequence_from_state(self, StateClass state, gold, _debug=None):
+        if state.is_final():
+            return []
         cdef Pool mem = Pool()
         # n_moves should not be zero at this point, but make sure to avoid zero-length mem alloc
         assert self.n_moves > 0
         costs = <float*>mem.alloc(self.n_moves, sizeof(float))
         is_valid = <int*>mem.alloc(self.n_moves, sizeof(int))
 
         history = []
@@ -106,14 +122,15 @@
                         debug_log.append(" ".join((
                             self.get_class_name(i),
                             "S0=", (example.x[s0].text if s0 >= 0 else "__"),
                             "B0=", (example.x[b0].text if b0 >= 0 else "__"),
                             "S0 head?", str(state.has_head(state.S(0))),
                         )))
                     action.do(state.c, action.label)
+                    state.c.history.push_back(i)
                     break
             else:
                 if _debug:
                     example = _debug
                     print("Actions")
                     for i in range(self.n_moves):
                         print(self.get_class_name(i))
@@ -133,14 +150,36 @@
         return history
 
     def apply_transition(self, StateClass state, name):
         if not self.is_valid(state, name):
             raise ValueError(Errors.E170.format(name=name))
         action = self.lookup_transition(name)
         action.do(state.c, action.label)
+        state.c.history.push_back(action.clas)
+
+    def apply_actions(self, states, const int[::1] actions):
+        assert len(states) == actions.shape[0]
+        cdef StateClass state
+        cdef vector[StateC*] c_states
+        c_states.resize(len(states))
+        cdef int i
+        for (i, state) in enumerate(states):
+            c_states[i] = state.c
+        c_apply_actions(self, &c_states[0], &actions[0], actions.shape[0])
+        return [state for state in states if not state.c.is_final()]
+
+    def transition_states(self, states, float[:, ::1] scores):
+        assert len(states) == scores.shape[0]
+        cdef StateClass state
+        cdef float* c_scores = &scores[0, 0]
+        cdef vector[StateC*] c_states
+        for state in states:
+            c_states.push_back(state.c)
+        c_transition_batch(self, &c_states[0], c_scores, scores.shape[1], scores.shape[0])
+        return [state for state in states if not state.c.is_final()]
 
     cdef Transition lookup_transition(self, object name) except *:
         raise NotImplementedError
 
     cdef Transition init_transition(self, int clas, int move, attr_t label) except *:
         raise NotImplementedError
 
@@ -246,7 +285,39 @@
             labels.update(srsly.json_loads(msg['moves']))
         if 'strings' not in exclude:
             self.strings.from_bytes(msg['strings'])
         if 'cfg' not in exclude and 'cfg' in msg:
             self.cfg.update(msg['cfg'])
         self.initialize_actions(labels)
         return self
+
+
+cdef void c_apply_actions(TransitionSystem moves, StateC** states, const int* actions,
+    int batch_size) nogil:
+        cdef int i
+        cdef Transition action
+        cdef StateC* state
+        for i in range(batch_size):
+            state = states[i]
+            action = moves.c[actions[i]]
+            action.do(state, action.label)
+            state.history.push_back(action.clas)
+
+
+cdef void c_transition_batch(TransitionSystem moves, StateC** states, const float* scores,
+    int nr_class, int batch_size) nogil:
+    is_valid = <int*>calloc(moves.n_moves, sizeof(int))
+    cdef int i, guess
+    cdef Transition action
+    for i in range(batch_size):
+        moves.set_valid(is_valid, states[i])
+        guess = arg_max_if_valid(&scores[i*nr_class], is_valid, nr_class)
+        if guess == -1:
+            # This shouldn't happen, but it's hard to raise an error here,
+            # and we don't want to infinite loop. So, force to end state.
+            states[i].force_final()
+        else:
+            action = moves.c[guess]
+            action.do(states[i], action.label)
+            states[i].history.push_back(guess)
+    free(is_valid)
+
```

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/attributeruler.py` & `spacy-4.0.0.dev0/spacy/pipeline/attribute_ruler.py`

 * *Files 0% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 from ..errors import Errors
 from ..training import Example
 from ..language import Language
 from ..matcher import Matcher
 from ..scorer import Scorer
 from ..symbols import IDS
 from ..tokens import Doc, Span
-from ..tokens._retokenize import normalize_token_attrs, set_token_attrs
+from ..tokens.retokenizer import normalize_token_attrs, set_token_attrs
 from ..vocab import Vocab
 from ..util import SimpleFrozenList, registry
 from .. import util
 
 
 MatcherPatternType = List[Dict[Union[int, str], Any]]
 AttributeRulerPatternType = Dict[str, Union[MatcherPatternType, Dict, int]]
```

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/dep_parser.pyx` & `spacy-4.0.0.dev0/spacy/pipeline/dep_parser.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,33 +1,32 @@
 # cython: infer_types=True, profile=True, binding=True
 from collections import defaultdict
 from typing import Optional, Iterable, Callable
 from thinc.api import Model, Config
 
 from ._parser_internals.transition_system import TransitionSystem
-from .transition_parser cimport Parser
-from ._parser_internals.arc_eager cimport ArcEager
+from .transition_parser import Parser
+from ._parser_internals.arc_eager import ArcEager
 
 from .functions import merge_subtokens
 from ..language import Language
 from ._parser_internals import nonproj
 from ._parser_internals.nonproj import DELIMITER
 from ..scorer import Scorer
 from ..training import remove_bilu_prefix
 from ..util import registry
 
 
 default_model_config = """
 [model]
-@architectures = "spacy.TransitionBasedParser.v2"
+@architectures = "spacy.TransitionBasedParser.v3"
 state_type = "parser"
 extra_state_tokens = false
 hidden_width = 64
 maxout_pieces = 2
-use_upper = true
 
 [model.tok2vec]
 @architectures = "spacy.HashEmbedCNN.v2"
 pretrained_vectors = null
 width = 96
 depth = 4
 embed_size = 2000
@@ -119,14 +118,15 @@
         beam_update_prob=0.0,
         # At some point in the future we can try to implement support for
         # partial annotations, perhaps only in the beam objective.
         incorrect_spans_key=None,
         scorer=scorer,
     )
 
+
 @Language.factory(
     "beam_parser",
     assigns=["token.dep", "token.head", "token.is_sent_start", "doc.sents"],
     default_config={
         "beam_width": 8,
         "beam_density": 0.01,
         "beam_update_prob": 0.5,
@@ -224,40 +224,45 @@
 
     examples (Iterable[Example]): The examples to score.
     RETURNS (Dict[str, Any]): The scores, produced by Scorer.score_spans
         and Scorer.score_deps.
 
     DOCS: https://spacy.io/api/dependencyparser#score
     """
+
     def has_sents(doc):
         return doc.has_annotation("SENT_START")
 
     def dep_getter(token, attr):
         dep = getattr(token, attr)
         dep = token.vocab.strings.as_string(dep).lower()
         return dep
+
     results = {}
-    results.update(Scorer.score_spans(examples, "sents", has_annotation=has_sents, **kwargs))
+    results.update(
+        Scorer.score_spans(examples, "sents", has_annotation=has_sents, **kwargs)
+    )
     kwargs.setdefault("getter", dep_getter)
     kwargs.setdefault("ignore_labels", ("p", "punct"))
     results.update(Scorer.score_deps(examples, "dep", **kwargs))
     del results["sents_per_type"]
     return results
 
 
 @registry.scorers("spacy.parser_scorer.v1")
 def make_parser_scorer():
     return parser_score
 
 
-cdef class DependencyParser(Parser):
+class DependencyParser(Parser):
     """Pipeline component for dependency parsing.
 
     DOCS: https://spacy.io/api/dependencyparser
     """
+
     TransitionSystem = ArcEager
 
     def __init__(
         self,
         vocab,
         model,
         name="parser",
@@ -269,16 +274,15 @@
         beam_width=1,
         beam_density=0.0,
         beam_update_prob=0.0,
         multitasks=tuple(),
         incorrect_spans_key=None,
         scorer=parser_score,
     ):
-        """Create a DependencyParser.
-        """
+        """Create a DependencyParser."""
         super().__init__(
             vocab,
             model,
             name,
             moves,
             update_with_oracle_cut_size=update_with_oracle_cut_size,
             min_action_freq=min_action_freq,
```

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/edit_tree_lemmatizer.py` & `spacy-4.0.0.dev0/spacy/pipeline/edit_tree_lemmatizer.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,31 +1,31 @@
-from typing import cast, Any, Callable, Dict, Iterable, List, Optional
+from typing import cast, Any, Callable, Dict, Iterable, List, Optional, Union
 from typing import Tuple
 from collections import Counter
 from itertools import islice
 import numpy as np
 
 import srsly
-from thinc.api import Config, Model, SequenceCategoricalCrossentropy, NumpyOps
-from thinc.types import Floats2d, Ints2d
+from thinc.api import Config, Model
+from thinc.types import ArrayXd, Floats2d, Ints1d
+from thinc.legacy import LegacySequenceCategoricalCrossentropy
 
 from ._edit_tree_internals.edit_trees import EditTrees
 from ._edit_tree_internals.schemas import validate_edit_tree
 from .lemmatizer import lemmatizer_score
 from .trainable_pipe import TrainablePipe
 from ..errors import Errors
 from ..language import Language
 from ..tokens import Doc
 from ..training import Example, validate_examples, validate_get_examples
 from ..vocab import Vocab
 from .. import util
 
 
-# The cutoff value of *top_k* above which an alternative method is used to process guesses.
-TOP_K_GUARDRAIL = 20
+ActivationsT = Dict[str, Union[List[Floats2d], List[Ints1d]]]
 
 
 default_model_config = """
 [model]
 @architectures = "spacy.Tagger.v2"
 
 [model.tok2vec]
@@ -48,37 +48,40 @@
     default_config={
         "model": DEFAULT_EDIT_TREE_LEMMATIZER_MODEL,
         "backoff": "orth",
         "min_tree_freq": 3,
         "overwrite": False,
         "top_k": 1,
         "scorer": {"@scorers": "spacy.lemmatizer_scorer.v1"},
+        "save_activations": False,
     },
     default_score_weights={"lemma_acc": 1.0},
 )
 def make_edit_tree_lemmatizer(
     nlp: Language,
     name: str,
     model: Model,
     backoff: Optional[str],
     min_tree_freq: int,
     overwrite: bool,
     top_k: int,
     scorer: Optional[Callable],
+    save_activations: bool,
 ):
     """Construct an EditTreeLemmatizer component."""
     return EditTreeLemmatizer(
         nlp.vocab,
         model,
         name,
         backoff=backoff,
         min_tree_freq=min_tree_freq,
         overwrite=overwrite,
         top_k=top_k,
         scorer=scorer,
+        save_activations=save_activations,
     )
 
 
 class EditTreeLemmatizer(TrainablePipe):
     """
     Lemmatizer that lemmatizes each word using a predicted edit tree.
     """
@@ -90,136 +93,120 @@
         name: str = "trainable_lemmatizer",
         *,
         backoff: Optional[str] = "orth",
         min_tree_freq: int = 3,
         overwrite: bool = False,
         top_k: int = 1,
         scorer: Optional[Callable] = lemmatizer_score,
+        save_activations: bool = False,
     ):
         """
         Construct an edit tree lemmatizer.
 
         backoff (Optional[str]): backoff to use when the predicted edit trees
             are not applicable. Must be an attribute of Token or None (leave the
             lemma unset).
         min_tree_freq (int): prune trees that are applied less than this
             frequency in the training data.
         overwrite (bool): overwrite existing lemma annotations.
         top_k (int): try to apply at most the k most probable edit trees.
+        save_activations (bool): save model activations in Doc when annotating.
         """
         self.vocab = vocab
         self.model = model
         self.name = name
         self.backoff = backoff
         self.min_tree_freq = min_tree_freq
         self.overwrite = overwrite
         self.top_k = top_k
 
         self.trees = EditTrees(self.vocab.strings)
         self.tree2label: Dict[int, int] = {}
 
         self.cfg: Dict[str, Any] = {"labels": []}
         self.scorer = scorer
-        self.numpy_ops = NumpyOps()
+        self.save_activations = save_activations
 
     def get_loss(
         self, examples: Iterable[Example], scores: List[Floats2d]
     ) -> Tuple[float, List[Floats2d]]:
         validate_examples(examples, "EditTreeLemmatizer.get_loss")
-        loss_func = SequenceCategoricalCrossentropy(normalize=False, missing_value=-1)
+        loss_func = LegacySequenceCategoricalCrossentropy(
+            normalize=False, missing_value=-1
+        )
 
         truths = []
         for eg in examples:
             eg_truths = []
             for (predicted, gold_lemma) in zip(
                 eg.predicted, eg.get_aligned("LEMMA", as_string=True)
             ):
-                if gold_lemma is None or gold_lemma == "":
+                if gold_lemma is None:
                     label = -1
                 else:
                     tree_id = self.trees.add(predicted.text, gold_lemma)
                     label = self.tree2label.get(tree_id, 0)
                 eg_truths.append(label)
 
             truths.append(eg_truths)
 
         d_scores, loss = loss_func(scores, truths)
         if self.model.ops.xp.isnan(loss):
             raise ValueError(Errors.E910.format(name=self.name))
 
         return float(loss), d_scores
 
-    def predict(self, docs: Iterable[Doc]) -> List[Ints2d]:
-        if self.top_k == 1:
-            scores2guesses = self._scores2guesses_top_k_equals_1
-        elif self.top_k <= TOP_K_GUARDRAIL:
-            scores2guesses = self._scores2guesses_top_k_greater_1
-        else:
-            scores2guesses = self._scores2guesses_top_k_guardrail
-        # The behaviour of *_scores2guesses_top_k_greater_1()* is efficient for values
-        # of *top_k>1* that are likely to be useful when the edit tree lemmatizer is used
-        # for its principal purpose of lemmatizing tokens. However, the code could also
-        # be used for other purposes, and with very large values of *top_k* the method
-        # becomes inefficient. In such cases, *_scores2guesses_top_k_guardrail()* is used
-        # instead.
+    def get_teacher_student_loss(
+        self, teacher_scores: List[Floats2d], student_scores: List[Floats2d]
+    ) -> Tuple[float, List[Floats2d]]:
+        """Calculate the loss and its gradient for a batch of student
+        scores, relative to teacher scores.
+
+        teacher_scores: Scores representing the teacher model's predictions.
+        student_scores: Scores representing the student model's predictions.
+
+        RETURNS (Tuple[float, float]): The loss and the gradient.
+        
+        DOCS: https://spacy.io/api/edittreelemmatizer#get_teacher_student_loss
+        """
+        loss_func = LegacySequenceCategoricalCrossentropy(normalize=False)
+        d_scores, loss = loss_func(student_scores, teacher_scores)
+        if self.model.ops.xp.isnan(loss):
+            raise ValueError(Errors.E910.format(name=self.name))
+        return float(loss), d_scores
+
+    def predict(self, docs: Iterable[Doc]) -> ActivationsT:
         n_docs = len(list(docs))
         if not any(len(doc) for doc in docs):
             # Handle cases where there are no tokens in any docs.
             n_labels = len(self.cfg["labels"])
-            guesses: List[Ints2d] = [self.model.ops.alloc2i(0, n_labels) for _ in docs]
+            guesses: List[Ints1d] = [
+                self.model.ops.alloc((0,), dtype="i") for doc in docs
+            ]
+            scores: List[Floats2d] = [
+                self.model.ops.alloc((0, n_labels), dtype="i") for doc in docs
+            ]
             assert len(guesses) == n_docs
-            return guesses
+            return {"probabilities": scores, "tree_ids": guesses}
         scores = self.model.predict(docs)
         assert len(scores) == n_docs
-        guesses = scores2guesses(docs, scores)
+        guesses = self._scores2guesses(docs, scores)
         assert len(guesses) == n_docs
-        return guesses
-
-    def _scores2guesses_top_k_equals_1(self, docs, scores):
-        guesses = []
-        for doc, doc_scores in zip(docs, scores):
-            doc_guesses = doc_scores.argmax(axis=1)
-            doc_guesses = self.numpy_ops.asarray(doc_guesses)
-
-            doc_compat_guesses = []
-            for i, token in enumerate(doc):
-                tree_id = self.cfg["labels"][doc_guesses[i]]
-                if self.trees.apply(tree_id, token.text) is not None:
-                    doc_compat_guesses.append(tree_id)
-                else:
-                    doc_compat_guesses.append(-1)
-            guesses.append(np.array(doc_compat_guesses))
-
-        return guesses
+        return {"probabilities": scores, "tree_ids": guesses}
 
-    def _scores2guesses_top_k_greater_1(self, docs, scores):
+    def _scores2guesses(self, docs, scores):
         guesses = []
-        top_k = min(self.top_k, len(self.labels))
         for doc, doc_scores in zip(docs, scores):
-            doc_scores = self.numpy_ops.asarray(doc_scores)
-            doc_compat_guesses = []
-            for i, token in enumerate(doc):
-                for _ in range(top_k):
-                    candidate = int(doc_scores[i].argmax())
-                    candidate_tree_id = self.cfg["labels"][candidate]
-                    if self.trees.apply(candidate_tree_id, token.text) is not None:
-                        doc_compat_guesses.append(candidate_tree_id)
-                        break
-                    doc_scores[i, candidate] = np.finfo(np.float32).min
-                else:
-                    doc_compat_guesses.append(-1)
-            guesses.append(np.array(doc_compat_guesses))
+            if self.top_k == 1:
+                doc_guesses = doc_scores.argmax(axis=1).reshape(-1, 1)
+            else:
+                doc_guesses = np.argsort(doc_scores)[..., : -self.top_k - 1 : -1]
 
-        return guesses
-
-    def _scores2guesses_top_k_guardrail(self, docs, scores):
-        guesses = []
-        for doc, doc_scores in zip(docs, scores):
-            doc_guesses = np.argsort(doc_scores)[..., : -self.top_k - 1 : -1]
-            doc_guesses = self.numpy_ops.asarray(doc_guesses)
+            if not isinstance(doc_guesses, np.ndarray):
+                doc_guesses = doc_guesses.get()
 
             doc_compat_guesses = []
             for token, candidates in zip(doc, doc_guesses):
                 tree_id = -1
                 for candidate in candidates:
                     candidate_tree_id = self.cfg["labels"][candidate]
 
@@ -228,16 +215,21 @@
                         break
                 doc_compat_guesses.append(tree_id)
 
             guesses.append(np.array(doc_compat_guesses))
 
         return guesses
 
-    def set_annotations(self, docs: Iterable[Doc], batch_tree_ids):
+    def set_annotations(self, docs: Iterable[Doc], activations: ActivationsT):
+        batch_tree_ids = activations["tree_ids"]
         for i, doc in enumerate(docs):
+            if self.save_activations:
+                doc.activations[self.name] = {}
+                for act_name, acts in activations.items():
+                    doc.activations[self.name][act_name] = acts[i]
             doc_tree_ids = batch_tree_ids[i]
             if hasattr(doc_tree_ids, "get"):
                 doc_tree_ids = doc_tree_ids.get()
             for j, tree_id in enumerate(doc_tree_ids):
                 if self.overwrite or doc[j].lemma == 0:
                     # If no applicable tree could be found during prediction,
                     # the special identifier -1 is used. Otherwise the tree
```

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/entity_linker.py` & `spacy-4.0.0.dev0/spacy/pipeline/entity_linker.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,9 +1,11 @@
-from typing import Optional, Iterable, Callable, Dict, Union, List, Any
-from thinc.types import Floats2d
+from typing import Optional, Iterable, Callable, Dict, Sequence, Union, List, Any
+from typing import cast
+from numpy import dtype
+from thinc.types import Floats1d, Floats2d, Ints1d, Ragged
 from pathlib import Path
 from itertools import islice
 import srsly
 import random
 from thinc.api import CosineDistance, Model, Optimizer, Config
 from thinc.api import set_dropout_rate
 
@@ -17,14 +19,19 @@
 from ..vocab import Vocab
 from ..training import Example, validate_examples, validate_get_examples
 from ..errors import Errors
 from ..util import SimpleFrozenList, registry
 from .. import util
 from ..scorer import Scorer
 
+
+ActivationsT = Dict[str, Union[List[Ragged], List[str]]]
+
+KNOWLEDGE_BASE_IDS = "kb_ids"
+
 # See #9050
 BACKWARD_OVERWRITE = True
 
 default_model_config = """
 [model]
 @architectures = "spacy.EntityLinker.v2"
 
@@ -50,20 +57,20 @@
         "labels_discard": [],
         "n_sents": 0,
         "incl_prior": True,
         "incl_context": True,
         "entity_vector_length": 64,
         "get_candidates": {"@misc": "spacy.CandidateGenerator.v1"},
         "get_candidates_batch": {"@misc": "spacy.CandidateBatchGenerator.v1"},
-        "generate_empty_kb": {"@misc": "spacy.EmptyKB.v2"},
         "overwrite": True,
         "scorer": {"@scorers": "spacy.entity_linker_scorer.v1"},
         "use_gold_ents": True,
         "candidates_batch_size": 1,
         "threshold": None,
+        "save_activations": False,
     },
     default_score_weights={
         "nel_micro_f": 1.0,
         "nel_micro_r": None,
         "nel_micro_p": None,
     },
 )
@@ -77,20 +84,20 @@
     incl_prior: bool,
     incl_context: bool,
     entity_vector_length: int,
     get_candidates: Callable[[KnowledgeBase, Span], Iterable[Candidate]],
     get_candidates_batch: Callable[
         [KnowledgeBase, Iterable[Span]], Iterable[Iterable[Candidate]]
     ],
-    generate_empty_kb: Callable[[Vocab, int], KnowledgeBase],
     overwrite: bool,
     scorer: Optional[Callable],
     use_gold_ents: bool,
     candidates_batch_size: int,
     threshold: Optional[float] = None,
+    save_activations: bool,
 ):
     """Construct an EntityLinker component.
 
     model (Model[List[Doc], Floats2d]): A model that learns document vector
         representations. Given a batch of Doc objects, it should return a single
         array, with one row per item in the batch.
     labels_discard (Iterable[str]): NER labels that will automatically get a "NIL" prediction.
@@ -99,21 +106,21 @@
     incl_context (bool): Whether or not to include the local context in the model.
     entity_vector_length (int): Size of encoding vectors in the KB.
     get_candidates (Callable[[KnowledgeBase, Span], Iterable[Candidate]]): Function that
         produces a list of candidates, given a certain knowledge base and a textual mention.
     get_candidates_batch (
         Callable[[KnowledgeBase, Iterable[Span]], Iterable[Iterable[Candidate]]], Iterable[Candidate]]
         ): Function that produces a list of candidates, given a certain knowledge base and several textual mentions.
-    generate_empty_kb (Callable[[Vocab, int], KnowledgeBase]): Callable returning empty KnowledgeBase.
     scorer (Optional[Callable]): The scoring method.
     use_gold_ents (bool): Whether to copy entities from gold docs or not. If false, another
         component must provide entity annotations.
     candidates_batch_size (int): Size of batches for entity candidate generation.
     threshold (Optional[float]): Confidence threshold for entity predictions. If confidence is below the threshold,
         prediction is discarded. If None, predictions are not filtered by any threshold.
+    save_activations (bool): save model activations in Doc when annotating.
     """
 
     if not model.attrs.get("include_span_maker", False):
         # The only difference in arguments here is that use_gold_ents and threshold aren't available.
         return EntityLinker_v1(
             nlp.vocab,
             model,
@@ -134,20 +141,20 @@
         labels_discard=labels_discard,
         n_sents=n_sents,
         incl_prior=incl_prior,
         incl_context=incl_context,
         entity_vector_length=entity_vector_length,
         get_candidates=get_candidates,
         get_candidates_batch=get_candidates_batch,
-        generate_empty_kb=generate_empty_kb,
         overwrite=overwrite,
         scorer=scorer,
         use_gold_ents=use_gold_ents,
         candidates_batch_size=candidates_batch_size,
         threshold=threshold,
+        save_activations=save_activations,
     )
 
 
 def entity_linker_score(examples, **kwargs):
     return Scorer.score_links(examples, negative_labels=[EntityLinker.NIL], **kwargs)
 
 
@@ -175,20 +182,20 @@
         incl_prior: bool,
         incl_context: bool,
         entity_vector_length: int,
         get_candidates: Callable[[KnowledgeBase, Span], Iterable[Candidate]],
         get_candidates_batch: Callable[
             [KnowledgeBase, Iterable[Span]], Iterable[Iterable[Candidate]]
         ],
-        generate_empty_kb: Callable[[Vocab, int], KnowledgeBase],
         overwrite: bool = BACKWARD_OVERWRITE,
         scorer: Optional[Callable] = entity_linker_score,
         use_gold_ents: bool,
         candidates_batch_size: int,
         threshold: Optional[float] = None,
+        save_activations: bool = False,
     ) -> None:
         """Initialize an entity linker.
 
         vocab (Vocab): The shared vocabulary.
         model (thinc.api.Model): The Thinc Model powering the pipeline component.
         name (str): The component instance name, used to add entries to the
             losses during training.
@@ -199,15 +206,14 @@
         entity_vector_length (int): Size of encoding vectors in the KB.
         get_candidates (Callable[[KnowledgeBase, Span], Iterable[Candidate]]): Function that
             produces a list of candidates, given a certain knowledge base and a textual mention.
         get_candidates_batch (
             Callable[[KnowledgeBase, Iterable[Span]], Iterable[Iterable[Candidate]]],
             Iterable[Candidate]]
             ): Function that produces a list of candidates, given a certain knowledge base and several textual mentions.
-        generate_empty_kb (Callable[[Vocab, int], KnowledgeBase]): Callable returning empty KnowledgeBase.
         scorer (Optional[Callable]): The scoring method. Defaults to Scorer.score_links.
         use_gold_ents (bool): Whether to copy entities from gold docs or not. If false, another
             component must provide entity annotations.
         candidates_batch_size (int): Size of batches for entity candidate generation.
         threshold (Optional[float]): Confidence threshold for entity predictions. If confidence is below the
             threshold, prediction is discarded. If None, predictions are not filtered by any threshold.
         DOCS: https://spacy.io/api/entitylinker#init
@@ -222,27 +228,29 @@
                 )
             )
 
         self.vocab = vocab
         self.model = model
         self.name = name
         self.labels_discard = list(labels_discard)
-        # how many neighbour sentences to take into account
         self.n_sents = n_sents
         self.incl_prior = incl_prior
         self.incl_context = incl_context
         self.get_candidates = get_candidates
         self.get_candidates_batch = get_candidates_batch
         self.cfg: Dict[str, Any] = {"overwrite": overwrite}
         self.distance = CosineDistance(normalize=False)
-        self.kb = generate_empty_kb(self.vocab, entity_vector_length)
+        # how many neighbour sentences to take into account
+        # create an empty KB by default
+        self.kb = empty_kb(entity_vector_length)(self.vocab)
         self.scorer = scorer
         self.use_gold_ents = use_gold_ents
         self.candidates_batch_size = candidates_batch_size
         self.threshold = threshold
+        self.save_activations = save_activations
 
         if candidates_batch_size < 1:
             raise ValueError(Errors.E1044)
 
     def set_kb(self, kb_loader: Callable[[Vocab], KnowledgeBase]):
         """Define the KB of this pipe by providing a function that will
         create it using this object's vocab."""
@@ -251,15 +259,15 @@
 
         self.kb = kb_loader(self.vocab)  # type: ignore
 
     def validate_kb(self) -> None:
         # Raise an error if the knowledge base is not initialized.
         if self.kb is None:
             raise ValueError(Errors.E1018.format(name=self.name))
-        if hasattr(self.kb, "is_empty") and self.kb.is_empty():
+        if len(self.kb) == 0:
             raise ValueError(Errors.E139.format(name=self.name))
 
     def initialize(
         self,
         get_examples: Callable[[], Iterable[Example]],
         *,
         nlp: Optional[Language] = None,
@@ -423,34 +431,41 @@
         out = self.model.ops.alloc2f(*sentence_encodings.shape)
         out[keep_ents] = gradients
 
         loss = self.distance.get_loss(selected_encodings, entity_encodings)
         loss = loss / len(entity_encodings)
         return float(loss), out
 
-    def predict(self, docs: Iterable[Doc]) -> List[str]:
+    def predict(self, docs: Iterable[Doc]) -> ActivationsT:
         """Apply the pipeline's model to a batch of docs, without modifying them.
         Returns the KB IDs for each entity in each doc, including NIL if there is
         no prediction.
 
         docs (Iterable[Doc]): The documents to predict.
         RETURNS (List[str]): The models prediction for each document.
 
         DOCS: https://spacy.io/api/entitylinker#predict
         """
         self.validate_kb()
         entity_count = 0
         final_kb_ids: List[str] = []
-        xp = self.model.ops.xp
+        ops = self.model.ops
+        xp = ops.xp
+        docs_ents: List[Ragged] = []
+        docs_scores: List[Ragged] = []
         if not docs:
-            return final_kb_ids
+            return {KNOWLEDGE_BASE_IDS: final_kb_ids, "ents": docs_ents, "scores": docs_scores}
         if isinstance(docs, Doc):
             docs = [docs]
-        for i, doc in enumerate(docs):
+        for doc in docs:
+            doc_ents: List[Ints1d] = []
+            doc_scores: List[Floats1d] = []
             if len(doc) == 0:
+                docs_scores.append(Ragged(ops.alloc1f(0), ops.alloc1i(0)))
+                docs_ents.append(Ragged(xp.zeros(0, dtype="uint64"), ops.alloc1i(0)))
                 continue
             sentences = [s for s in doc.sents]
 
             # Loop over entities in batches.
             for ent_idx in range(0, len(doc.ents), self.candidates_batch_size):
                 ent_batch = doc.ents[ent_idx : ent_idx + self.candidates_batch_size]
 
@@ -470,48 +485,60 @@
                         self.get_candidates(self.kb, ent_batch[idx])
                         for idx in valid_ent_idx
                     ]
                 )
 
                 # Looping through each entity in batch (TODO: rewrite)
                 for j, ent in enumerate(ent_batch):
-                    assert hasattr(ent, "sents")
-                    sents = list(ent.sents)
-                    sent_indices = (
-                        sentences.index(sents[0]),
-                        sentences.index(sents[-1]),
-                    )
-                    assert sent_indices[1] >= sent_indices[0] >= 0
+                    sent_index = sentences.index(ent.sent)
+                    assert sent_index >= 0
 
                     if self.incl_context:
                         # get n_neighbour sentences, clipped to the length of the document
-                        start_sentence = max(0, sent_indices[0] - self.n_sents)
+                        start_sentence = max(0, sent_index - self.n_sents)
                         end_sentence = min(
-                            len(sentences) - 1, sent_indices[1] + self.n_sents
+                            len(sentences) - 1, sent_index + self.n_sents
                         )
                         start_token = sentences[start_sentence].start
                         end_token = sentences[end_sentence].end
                         sent_doc = doc[start_token:end_token].as_doc()
-
                         # currently, the context is the same for each entity in a sentence (should be refined)
                         sentence_encoding = self.model.predict([sent_doc])[0]
                         sentence_encoding_t = sentence_encoding.T
                         sentence_norm = xp.linalg.norm(sentence_encoding_t)
                     entity_count += 1
                     if ent.label_ in self.labels_discard:
                         # ignoring this entity - setting to NIL
                         final_kb_ids.append(self.NIL)
+                        self._add_activations(
+                            doc_scores=doc_scores,
+                            doc_ents=doc_ents,
+                            scores=[0.0],
+                            ents=[0],
+                        )
                     else:
                         candidates = list(batch_candidates[j])
                         if not candidates:
                             # no prediction possible for this entity - setting to NIL
                             final_kb_ids.append(self.NIL)
+                            self._add_activations(
+                                doc_scores=doc_scores,
+                                doc_ents=doc_ents,
+                                scores=[0.0],
+                                ents=[0],
+                            )
                         elif len(candidates) == 1 and self.threshold is None:
                             # shortcut for efficiency reasons: take the 1 candidate
                             final_kb_ids.append(candidates[0].entity_)
+                            self._add_activations(
+                                doc_scores=doc_scores,
+                                doc_ents=doc_ents,
+                                scores=[1.0],
+                                ents=[candidates[0].entity_],
+                            )
                         else:
                             random.shuffle(candidates)
                             # set all prior probabilities to 0 if incl_prior=False
                             prior_probs = xp.asarray([c.prior_prob for c in candidates])
                             if not self.incl_prior:
                                 prior_probs = xp.asarray([0.0 for _ in candidates])
                             scores = prior_probs
@@ -537,36 +564,56 @@
                                 scores = prior_probs + sims - (prior_probs * sims)
                             final_kb_ids.append(
                                 candidates[scores.argmax().item()].entity_
                                 if self.threshold is None
                                 or scores.max() >= self.threshold
                                 else EntityLinker.NIL
                             )
-
+                            self._add_activations(
+                                doc_scores=doc_scores,
+                                doc_ents=doc_ents,
+                                scores=scores,
+                                ents=[c.entity for c in candidates],
+                            )
+            self._add_doc_activations(
+                docs_scores=docs_scores,
+                docs_ents=docs_ents,
+                doc_scores=doc_scores,
+                doc_ents=doc_ents,
+            )
         if not (len(final_kb_ids) == entity_count):
             err = Errors.E147.format(
                 method="predict", msg="result variables not of equal length"
             )
             raise RuntimeError(err)
-        return final_kb_ids
+        return {KNOWLEDGE_BASE_IDS: final_kb_ids, "ents": docs_ents, "scores": docs_scores}
 
-    def set_annotations(self, docs: Iterable[Doc], kb_ids: List[str]) -> None:
+    def set_annotations(self, docs: Iterable[Doc], activations: ActivationsT) -> None:
         """Modify a batch of documents, using pre-computed scores.
 
         docs (Iterable[Doc]): The documents to modify.
-        kb_ids (List[str]): The IDs to set, produced by EntityLinker.predict.
+        activations (ActivationsT): The activations used for setting annotations, produced
+                                 by EntityLinker.predict.
 
         DOCS: https://spacy.io/api/entitylinker#set_annotations
         """
+        kb_ids = cast(List[str], activations[KNOWLEDGE_BASE_IDS])
         count_ents = len([ent for doc in docs for ent in doc.ents])
         if count_ents != len(kb_ids):
             raise ValueError(Errors.E148.format(ents=count_ents, ids=len(kb_ids)))
         i = 0
         overwrite = self.cfg["overwrite"]
-        for doc in docs:
+        for j, doc in enumerate(docs):
+            if self.save_activations:
+                doc.activations[self.name] = {}
+                for act_name, acts in activations.items():
+                    if act_name != KNOWLEDGE_BASE_IDS:
+                        # We only copy activations that are Ragged.
+                        doc.activations[self.name][act_name] = cast(Ragged, acts[j])
+
             for ent in doc.ents:
                 kb_id = kb_ids[i]
                 i += 1
                 for token in ent:
                     if token.ent_kb_id == 0 or overwrite:
                         token.ent_kb_id_ = kb_id
 
@@ -657,7 +704,36 @@
         return self
 
     def rehearse(self, examples, *, sgd=None, losses=None, **config):
         raise NotImplementedError
 
     def add_label(self, label):
         raise NotImplementedError
+
+    def _add_doc_activations(
+        self,
+        *,
+        docs_scores: List[Ragged],
+        docs_ents: List[Ragged],
+        doc_scores: List[Floats1d],
+        doc_ents: List[Ints1d],
+    ):
+        if not self.save_activations:
+            return
+        ops = self.model.ops
+        lengths = ops.asarray1i([s.shape[0] for s in doc_scores])
+        docs_scores.append(Ragged(ops.flatten(doc_scores), lengths))
+        docs_ents.append(Ragged(ops.flatten(doc_ents), lengths))
+
+    def _add_activations(
+        self,
+        *,
+        doc_scores: List[Floats1d],
+        doc_ents: List[Ints1d],
+        scores: Sequence[float],
+        ents: Sequence[int],
+    ):
+        if not self.save_activations:
+            return
+        ops = self.model.ops
+        doc_scores.append(ops.asarray1f(scores))
+        doc_ents.append(ops.asarray1i(ents, dtype="uint64"))
```

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/entityruler.py` & `spacy-4.0.0.dev0/spacy/pipeline/span_ruler.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,38 +1,39 @@
-from typing import Optional, Union, List, Dict, Tuple, Iterable, Any, Callable, Sequence
+from typing import Optional, Union, List, Dict, Tuple, Iterable, Any, Callable
+from typing import Sequence, Set, cast
 import warnings
-from collections import defaultdict
+from functools import partial
 from pathlib import Path
 import srsly
 
 from .pipe import Pipe
 from ..training import Example
 from ..language import Language
 from ..errors import Errors, Warnings
-from ..util import ensure_path, to_disk, from_disk, SimpleFrozenList, registry
+from ..util import ensure_path, SimpleFrozenList, registry
 from ..tokens import Doc, Span
+from ..scorer import Scorer, get_ner_prf
 from ..matcher import Matcher, PhraseMatcher
 from ..matcher.levenshtein import levenshtein_compare
-from ..scorer import get_ner_prf
+from .. import util
 
-
-DEFAULT_ENT_ID_SEP = "||"
 PatternType = Dict[str, Union[str, List[Dict[str, Any]]]]
+DEFAULT_SPANS_KEY = "ruler"
 
 
 @Language.factory(
     "entity_ruler",
-    assigns=["doc.ents", "token.ent_type", "token.ent_iob"],
+    assigns=["doc.ents"],
     default_config={
         "phrase_matcher_attr": None,
-        "matcher_fuzzy_compare": {"@misc": "spacy.levenshtein_compare.v1"},
         "validate": False,
         "overwrite_ents": False,
-        "ent_id_sep": DEFAULT_ENT_ID_SEP,
         "scorer": {"@scorers": "spacy.entity_ruler_scorer.v1"},
+        "ent_id_sep": "__unused__",
+        "matcher_fuzzy_compare": {"@misc": "spacy.levenshtein_compare.v1"},
     },
     default_score_weights={
         "ents_f": 1.0,
         "ents_p": 0.0,
         "ents_r": 0.0,
         "ents_per_type": None,
     },
@@ -40,502 +41,554 @@
 def make_entity_ruler(
     nlp: Language,
     name: str,
     phrase_matcher_attr: Optional[Union[int, str]],
     matcher_fuzzy_compare: Callable,
     validate: bool,
     overwrite_ents: bool,
-    ent_id_sep: str,
     scorer: Optional[Callable],
+    ent_id_sep: str,
 ):
-    return EntityRuler(
+    if overwrite_ents:
+        ents_filter = prioritize_new_ents_filter
+    else:
+        ents_filter = prioritize_existing_ents_filter
+    return SpanRuler(
         nlp,
         name,
+        spans_key=None,
+        spans_filter=None,
+        annotate_ents=True,
+        ents_filter=ents_filter,
         phrase_matcher_attr=phrase_matcher_attr,
         matcher_fuzzy_compare=matcher_fuzzy_compare,
         validate=validate,
-        overwrite_ents=overwrite_ents,
-        ent_id_sep=ent_id_sep,
+        overwrite=False,
         scorer=scorer,
     )
 
 
 def entity_ruler_score(examples, **kwargs):
     return get_ner_prf(examples)
 
 
 @registry.scorers("spacy.entity_ruler_scorer.v1")
 def make_entity_ruler_scorer():
     return entity_ruler_score
 
 
-class EntityRuler(Pipe):
-    """The EntityRuler lets you add spans to the `Doc.ents` using token-based
-    rules or exact phrase matches. It can be combined with the statistical
-    `EntityRecognizer` to boost accuracy, or used on its own to implement a
-    purely rule-based entity recognition system. After initialization, the
-    component is typically added to the pipeline using `nlp.add_pipe`.
+@Language.factory(
+    "span_ruler",
+    assigns=["doc.spans"],
+    default_config={
+        "spans_key": DEFAULT_SPANS_KEY,
+        "spans_filter": None,
+        "annotate_ents": False,
+        "ents_filter": {"@misc": "spacy.first_longest_spans_filter.v1"},
+        "phrase_matcher_attr": None,
+        "matcher_fuzzy_compare": {"@misc": "spacy.levenshtein_compare.v1"},
+        "validate": False,
+        "overwrite": True,
+        "scorer": {
+            "@scorers": "spacy.overlapping_labeled_spans_scorer.v1",
+            "spans_key": DEFAULT_SPANS_KEY,
+        },
+    },
+    default_score_weights={
+        f"spans_{DEFAULT_SPANS_KEY}_f": 1.0,
+        f"spans_{DEFAULT_SPANS_KEY}_p": 0.0,
+        f"spans_{DEFAULT_SPANS_KEY}_r": 0.0,
+        f"spans_{DEFAULT_SPANS_KEY}_per_type": None,
+    },
+)
+def make_span_ruler(
+    nlp: Language,
+    name: str,
+    spans_key: Optional[str],
+    spans_filter: Optional[Callable[[Iterable[Span], Iterable[Span]], Iterable[Span]]],
+    annotate_ents: bool,
+    ents_filter: Callable[[Iterable[Span], Iterable[Span]], Iterable[Span]],
+    phrase_matcher_attr: Optional[Union[int, str]],
+    matcher_fuzzy_compare: Callable,
+    validate: bool,
+    overwrite: bool,
+    scorer: Optional[Callable],
+):
+    return SpanRuler(
+        nlp,
+        name,
+        spans_key=spans_key,
+        spans_filter=spans_filter,
+        annotate_ents=annotate_ents,
+        ents_filter=ents_filter,
+        phrase_matcher_attr=phrase_matcher_attr,
+        matcher_fuzzy_compare=matcher_fuzzy_compare,
+        validate=validate,
+        overwrite=overwrite,
+        scorer=scorer,
+    )
+
+
+def prioritize_new_ents_filter(
+    entities: Iterable[Span], spans: Iterable[Span]
+) -> List[Span]:
+    """Merge entities and spans into one list without overlaps by allowing
+    spans to overwrite any entities that they overlap with. Intended to
+    replicate the overwrite_ents=True behavior from the v3 EntityRuler.
+
+    entities (Iterable[Span]): The entities, already filtered for overlaps.
+    spans (Iterable[Span]): The spans to merge, may contain overlaps.
+    RETURNS (List[Span]): Filtered list of non-overlapping spans.
+    """
+    get_sort_key = lambda span: (span.end - span.start, -span.start)
+    spans = sorted(spans, key=get_sort_key, reverse=True)
+    entities = list(entities)
+    new_entities = []
+    seen_tokens: Set[int] = set()
+    for span in spans:
+        start = span.start
+        end = span.end
+        if all(token.i not in seen_tokens for token in span):
+            new_entities.append(span)
+            entities = [e for e in entities if not (e.start < end and e.end > start)]
+            seen_tokens.update(range(start, end))
+    return entities + new_entities
+
+
+@registry.misc("spacy.prioritize_new_ents_filter.v1")
+def make_prioritize_new_ents_filter():
+    return prioritize_new_ents_filter
+
+
+def prioritize_existing_ents_filter(
+    entities: Iterable[Span], spans: Iterable[Span]
+) -> List[Span]:
+    """Merge entities and spans into one list without overlaps by prioritizing
+    existing entities. Intended to replicate the overwrite_ents=False behavior
+    from the v3 EntityRuler.
+
+    entities (Iterable[Span]): The entities, already filtered for overlaps.
+    spans (Iterable[Span]): The spans to merge, may contain overlaps.
+    RETURNS (List[Span]): Filtered list of non-overlapping spans.
+    """
+    get_sort_key = lambda span: (span.end - span.start, -span.start)
+    spans = sorted(spans, key=get_sort_key, reverse=True)
+    entities = list(entities)
+    new_entities = []
+    seen_tokens: Set[int] = set()
+    seen_tokens.update(*(range(ent.start, ent.end) for ent in entities))
+    for span in spans:
+        start = span.start
+        end = span.end
+        if all(token.i not in seen_tokens for token in span):
+            new_entities.append(span)
+            seen_tokens.update(range(start, end))
+    return entities + new_entities
+
+
+@registry.misc("spacy.prioritize_existing_ents_filter.v1")
+def make_preserve_existing_ents_filter():
+    return prioritize_existing_ents_filter
+
+
+def overlapping_labeled_spans_score(
+    examples: Iterable[Example], *, spans_key=DEFAULT_SPANS_KEY, **kwargs
+) -> Dict[str, Any]:
+    kwargs = dict(kwargs)
+    attr_prefix = f"spans_"
+    kwargs.setdefault("attr", f"{attr_prefix}{spans_key}")
+    kwargs.setdefault("allow_overlap", True)
+    kwargs.setdefault("labeled", True)
+    kwargs.setdefault(
+        "getter", lambda doc, key: doc.spans.get(key[len(attr_prefix) :], [])
+    )
+    kwargs.setdefault("has_annotation", lambda doc: spans_key in doc.spans)
+    return Scorer.score_spans(examples, **kwargs)
+
+
+@registry.scorers("spacy.overlapping_labeled_spans_scorer.v1")
+def make_overlapping_labeled_spans_scorer(spans_key: str = DEFAULT_SPANS_KEY):
+    return partial(overlapping_labeled_spans_score, spans_key=spans_key)
+
 
-    DOCS: https://spacy.io/api/entityruler
-    USAGE: https://spacy.io/usage/rule-based-matching#entityruler
+class SpanRuler(Pipe):
+    """The SpanRuler lets you add spans to the `Doc.spans` using token-based
+    rules or exact phrase matches.
+
+    DOCS: https://spacy.io/api/spanruler
+    USAGE: https://spacy.io/usage/rule-based-matching#spanruler
     """
 
     def __init__(
         self,
         nlp: Language,
-        name: str = "entity_ruler",
+        name: str = "span_ruler",
         *,
+        spans_key: Optional[str] = DEFAULT_SPANS_KEY,
+        spans_filter: Optional[
+            Callable[[Iterable[Span], Iterable[Span]], Iterable[Span]]
+        ] = None,
+        annotate_ents: bool = False,
+        ents_filter: Callable[
+            [Iterable[Span], Iterable[Span]], Iterable[Span]
+        ] = util.filter_chain_spans,
         phrase_matcher_attr: Optional[Union[int, str]] = None,
         matcher_fuzzy_compare: Callable = levenshtein_compare,
         validate: bool = False,
-        overwrite_ents: bool = False,
-        ent_id_sep: str = DEFAULT_ENT_ID_SEP,
-        patterns: Optional[List[PatternType]] = None,
-        scorer: Optional[Callable] = entity_ruler_score,
+        overwrite: bool = False,
+        scorer: Optional[Callable] = partial(
+            overlapping_labeled_spans_score, spans_key=DEFAULT_SPANS_KEY
+        ),
     ) -> None:
-        """Initialize the entity ruler. If patterns are supplied here, they
+        """Initialize the span ruler. If patterns are supplied here, they
         need to be a list of dictionaries with a `"label"` and `"pattern"`
         key. A pattern can either be a token pattern (list) or a phrase pattern
         (string). For example: `{'label': 'ORG', 'pattern': 'Apple'}`.
 
         nlp (Language): The shared nlp object to pass the vocab to the matchers
             and process phrase patterns.
         name (str): Instance name of the current pipeline component. Typically
             passed in automatically from the factory when the component is
-            added. Used to disable the current entity ruler while creating
+            added. Used to disable the current span ruler while creating
             phrase patterns with the nlp object.
-        phrase_matcher_attr (int / str): Token attribute to match on, passed
-            to the internal PhraseMatcher as `attr`.
+        spans_key (Optional[str]): The spans key to save the spans under. If
+            `None`, no spans are saved. Defaults to "ruler".
+        spans_filter (Optional[Callable[[Iterable[Span], Iterable[Span]], List[Span]]):
+            The optional method to filter spans before they are assigned to
+            doc.spans. Defaults to `None`.
+        annotate_ents (bool): Whether to save spans to doc.ents. Defaults to
+            `False`.
+        ents_filter (Callable[[Iterable[Span], Iterable[Span]], List[Span]]):
+            The method to filter spans before they are assigned to doc.ents.
+            Defaults to `util.filter_chain_spans`.
+        phrase_matcher_attr (Optional[Union[int, str]]): Token attribute to
+            match on, passed to the internal PhraseMatcher as `attr`. Defaults
+            to `None`.
         matcher_fuzzy_compare (Callable): The fuzzy comparison method for the
             internal Matcher. Defaults to
             spacy.matcher.levenshtein.levenshtein_compare.
         validate (bool): Whether patterns should be validated, passed to
-            Matcher and PhraseMatcher as `validate`
-        patterns (iterable): Optional patterns to load in.
-        overwrite_ents (bool): If existing entities are present, e.g. entities
-            added by the model, overwrite them by matches if necessary.
-        ent_id_sep (str): Separator used internally for entity IDs.
+            Matcher and PhraseMatcher as `validate`.
+        overwrite (bool): Whether to remove any existing spans under this spans
+            key if `spans_key` is set, and/or to remove any ents under `doc.ents` if
+            `annotate_ents` is set. Defaults to `True`.
         scorer (Optional[Callable]): The scoring method. Defaults to
-            spacy.scorer.get_ner_prf.
+            spacy.pipeline.span_ruler.overlapping_labeled_spans_score.
 
-        DOCS: https://spacy.io/api/entityruler#init
+        DOCS: https://spacy.io/api/spanruler#init
         """
         self.nlp = nlp
         self.name = name
-        self.overwrite = overwrite_ents
-        self.token_patterns = defaultdict(list)  # type: ignore
-        self.phrase_patterns = defaultdict(list)  # type: ignore
-        self._validate = validate
-        self.matcher_fuzzy_compare = matcher_fuzzy_compare
-        self.matcher = Matcher(
-            nlp.vocab, validate=validate, fuzzy_compare=self.matcher_fuzzy_compare
-        )
+        self.spans_key = spans_key
+        self.annotate_ents = annotate_ents
         self.phrase_matcher_attr = phrase_matcher_attr
-        self.phrase_matcher = PhraseMatcher(
-            nlp.vocab, attr=self.phrase_matcher_attr, validate=validate
-        )
-        self.ent_id_sep = ent_id_sep
-        self._ent_ids = defaultdict(tuple)  # type: ignore
-        if patterns is not None:
-            self.add_patterns(patterns)
+        self.validate = validate
+        self.overwrite = overwrite
+        self.spans_filter = spans_filter
+        self.ents_filter = ents_filter
         self.scorer = scorer
+        self.matcher_fuzzy_compare = matcher_fuzzy_compare
+        self._match_label_id_map: Dict[int, Dict[str, str]] = {}
+        self.clear()
 
     def __len__(self) -> int:
-        """The number of all patterns added to the entity ruler."""
-        n_token_patterns = sum(len(p) for p in self.token_patterns.values())
-        n_phrase_patterns = sum(len(p) for p in self.phrase_patterns.values())
-        return n_token_patterns + n_phrase_patterns
+        """The number of all labels added to the span ruler."""
+        return len(self._patterns)
 
     def __contains__(self, label: str) -> bool:
         """Whether a label is present in the patterns."""
-        return label in self.token_patterns or label in self.phrase_patterns
+        for label_id in self._match_label_id_map.values():
+            if label_id["label"] == label:
+                return True
+        return False
+
+    @property
+    def key(self) -> Optional[str]:
+        """Key of the doc.spans dict to save the spans under."""
+        return self.spans_key
 
     def __call__(self, doc: Doc) -> Doc:
         """Find matches in document and add them as entities.
 
         doc (Doc): The Doc object in the pipeline.
         RETURNS (Doc): The Doc with added entities, if available.
 
-        DOCS: https://spacy.io/api/entityruler#call
+        DOCS: https://spacy.io/api/spanruler#call
         """
         error_handler = self.get_error_handler()
         try:
             matches = self.match(doc)
             self.set_annotations(doc, matches)
             return doc
         except Exception as e:
             return error_handler(self.name, self, [doc], e)
 
     def match(self, doc: Doc):
         self._require_patterns()
         with warnings.catch_warnings():
             warnings.filterwarnings("ignore", message="\\[W036")
-            matches = list(self.matcher(doc)) + list(self.phrase_matcher(doc))
-
-        final_matches = set(
-            [(m_id, start, end) for m_id, start, end in matches if start != end]
+            matches = cast(
+                List[Tuple[int, int, int]],
+                list(self.matcher(doc)) + list(self.phrase_matcher(doc)),
+            )
+        deduplicated_matches = set(
+            Span(
+                doc,
+                start,
+                end,
+                label=self._match_label_id_map[m_id]["label"],
+                span_id=self._match_label_id_map[m_id]["id"],
+            )
+            for m_id, start, end in matches
+            if start != end
         )
-        get_sort_key = lambda m: (m[2] - m[1], -m[1])
-        final_matches = sorted(final_matches, key=get_sort_key, reverse=True)
-        return final_matches
+        return sorted(list(deduplicated_matches))
 
     def set_annotations(self, doc, matches):
         """Modify the document in place"""
-        entities = list(doc.ents)
-        new_entities = []
-        seen_tokens = set()
-        for match_id, start, end in matches:
-            if any(t.ent_type for t in doc[start:end]) and not self.overwrite:
-                continue
-            # check for end - 1 here because boundaries are inclusive
-            if start not in seen_tokens and end - 1 not in seen_tokens:
-                if match_id in self._ent_ids:
-                    label, ent_id = self._ent_ids[match_id]
-                    span = Span(doc, start, end, label=label, span_id=ent_id)
-                else:
-                    span = Span(doc, start, end, label=match_id)
-                new_entities.append(span)
-                entities = [
-                    e for e in entities if not (e.start < end and e.end > start)
-                ]
-                seen_tokens.update(range(start, end))
-        doc.ents = entities + new_entities
+        # set doc.spans if spans_key is set
+        if self.key:
+            spans = []
+            if self.key in doc.spans and not self.overwrite:
+                spans = doc.spans[self.key]
+            spans.extend(
+                self.spans_filter(spans, matches) if self.spans_filter else matches
+            )
+            doc.spans[self.key] = spans
+        # set doc.ents if annotate_ents is set
+        if self.annotate_ents:
+            spans = []
+            if not self.overwrite:
+                spans = list(doc.ents)
+            spans = self.ents_filter(spans, matches)
+            try:
+                doc.ents = sorted(spans)
+            except ValueError:
+                raise ValueError(Errors.E854)
 
     @property
     def labels(self) -> Tuple[str, ...]:
         """All labels present in the match patterns.
 
         RETURNS (set): The string labels.
 
-        DOCS: https://spacy.io/api/entityruler#labels
+        DOCS: https://spacy.io/api/spanruler#labels
         """
-        keys = set(self.token_patterns.keys())
-        keys.update(self.phrase_patterns.keys())
-        all_labels = set()
-
-        for l in keys:
-            if self.ent_id_sep in l:
-                label, _ = self._split_label(l)
-                all_labels.add(label)
-            else:
-                all_labels.add(l)
-        return tuple(sorted(all_labels))
+        return tuple(sorted(set([cast(str, p["label"]) for p in self._patterns])))
+
+    @property
+    def ids(self) -> Tuple[str, ...]:
+        """All IDs present in the match patterns.
+
+        RETURNS (set): The string IDs.
+
+        DOCS: https://spacy.io/api/spanruler#ids
+        """
+        return tuple(
+            sorted(set([cast(str, p.get("id")) for p in self._patterns]) - set([None]))
+        )
 
     def initialize(
         self,
         get_examples: Callable[[], Iterable[Example]],
         *,
         nlp: Optional[Language] = None,
         patterns: Optional[Sequence[PatternType]] = None,
     ):
         """Initialize the pipe for training.
 
         get_examples (Callable[[], Iterable[Example]]): Function that
             returns a representative sample of gold-standard Example objects.
         nlp (Language): The current nlp object the component is part of.
-        patterns Optional[Iterable[PatternType]]: The list of patterns.
+        patterns (Optional[Iterable[PatternType]]): The list of patterns.
 
-        DOCS: https://spacy.io/api/entityruler#initialize
+        DOCS: https://spacy.io/api/spanruler#initialize
         """
         self.clear()
         if patterns:
             self.add_patterns(patterns)  # type: ignore[arg-type]
 
     @property
-    def ent_ids(self) -> Tuple[Optional[str], ...]:
-        """All entity ids present in the match patterns `id` properties
-
-        RETURNS (set): The string entity ids.
-
-        DOCS: https://spacy.io/api/entityruler#ent_ids
-        """
-        keys = set(self.token_patterns.keys())
-        keys.update(self.phrase_patterns.keys())
-        all_ent_ids = set()
-
-        for l in keys:
-            if self.ent_id_sep in l:
-                _, ent_id = self._split_label(l)
-                all_ent_ids.add(ent_id)
-        return tuple(all_ent_ids)
-
-    @property
     def patterns(self) -> List[PatternType]:
-        """Get all patterns that were added to the entity ruler.
+        """Get all patterns that were added to the span ruler.
 
         RETURNS (list): The original patterns, one dictionary per pattern.
 
-        DOCS: https://spacy.io/api/entityruler#patterns
+        DOCS: https://spacy.io/api/spanruler#patterns
         """
-        all_patterns = []
-        for label, patterns in self.token_patterns.items():
-            for pattern in patterns:
-                ent_label, ent_id = self._split_label(label)
-                p = {"label": ent_label, "pattern": pattern}
-                if ent_id:
-                    p["id"] = ent_id
-                all_patterns.append(p)
-        for label, patterns in self.phrase_patterns.items():
-            for pattern in patterns:
-                ent_label, ent_id = self._split_label(label)
-                p = {"label": ent_label, "pattern": pattern.text}
-                if ent_id:
-                    p["id"] = ent_id
-                all_patterns.append(p)
-        return all_patterns
+        return self._patterns
 
     def add_patterns(self, patterns: List[PatternType]) -> None:
-        """Add patterns to the entity ruler. A pattern can either be a token
+        """Add patterns to the span ruler. A pattern can either be a token
         pattern (list of dicts) or a phrase pattern (string). For example:
         {'label': 'ORG', 'pattern': 'Apple'}
+        {'label': 'ORG', 'pattern': 'Apple', 'id': 'apple'}
         {'label': 'GPE', 'pattern': [{'lower': 'san'}, {'lower': 'francisco'}]}
 
         patterns (list): The patterns to add.
 
-        DOCS: https://spacy.io/api/entityruler#add_patterns
+        DOCS: https://spacy.io/api/spanruler#add_patterns
         """
 
-        # disable the nlp components after this one in case they hadn't been initialized / deserialised yet
+        # disable the nlp components after this one in case they haven't been
+        # initialized / deserialized yet
         try:
             current_index = -1
             for i, (name, pipe) in enumerate(self.nlp.pipeline):
                 if self == pipe:
                     current_index = i
                     break
             subsequent_pipes = [pipe for pipe in self.nlp.pipe_names[current_index:]]
         except ValueError:
             subsequent_pipes = []
         with self.nlp.select_pipes(disable=subsequent_pipes):
-            token_patterns = []
             phrase_pattern_labels = []
             phrase_pattern_texts = []
-            phrase_pattern_ids = []
             for entry in patterns:
+                p_label = cast(str, entry["label"])
+                p_id = cast(str, entry.get("id", ""))
+                label = repr((p_label, p_id))
+                self._match_label_id_map[self.nlp.vocab.strings.as_int(label)] = {
+                    "label": p_label,
+                    "id": p_id,
+                }
                 if isinstance(entry["pattern"], str):
-                    phrase_pattern_labels.append(entry["label"])
+                    phrase_pattern_labels.append(label)
                     phrase_pattern_texts.append(entry["pattern"])
-                    phrase_pattern_ids.append(entry.get("id"))
                 elif isinstance(entry["pattern"], list):
-                    token_patterns.append(entry)
-            phrase_patterns = []
-            for label, pattern, ent_id in zip(
+                    self.matcher.add(label, [entry["pattern"]])
+                else:
+                    raise ValueError(Errors.E097.format(pattern=entry["pattern"]))
+                self._patterns.append(entry)
+            for label, pattern in zip(
                 phrase_pattern_labels,
                 self.nlp.pipe(phrase_pattern_texts),
-                phrase_pattern_ids,
             ):
-                phrase_pattern = {"label": label, "pattern": pattern}
-                if ent_id:
-                    phrase_pattern["id"] = ent_id
-                phrase_patterns.append(phrase_pattern)
-            for entry in token_patterns + phrase_patterns:  # type: ignore[operator]
-                label = entry["label"]  # type: ignore
-                if "id" in entry:
-                    ent_label = label
-                    label = self._create_label(label, entry["id"])
-                    key = self.matcher._normalize_key(label)
-                    self._ent_ids[key] = (ent_label, entry["id"])
-                pattern = entry["pattern"]  # type: ignore
-                if isinstance(pattern, Doc):
-                    self.phrase_patterns[label].append(pattern)
-                    self.phrase_matcher.add(label, [pattern])  # type: ignore
-                elif isinstance(pattern, list):
-                    self.token_patterns[label].append(pattern)
-                    self.matcher.add(label, [pattern])
-                else:
-                    raise ValueError(Errors.E097.format(pattern=pattern))
+                self.phrase_matcher.add(label, [pattern])
 
     def clear(self) -> None:
-        """Reset all patterns."""
-        self.token_patterns = defaultdict(list)
-        self.phrase_patterns = defaultdict(list)
-        self._ent_ids = defaultdict(tuple)
-        self.matcher = Matcher(
+        """Reset all patterns.
+
+        RETURNS: None
+        DOCS: https://spacy.io/api/spanruler#clear
+        """
+        self._patterns: List[PatternType] = []
+        self.matcher: Matcher = Matcher(
             self.nlp.vocab,
-            validate=self._validate,
+            validate=self.validate,
             fuzzy_compare=self.matcher_fuzzy_compare,
         )
-        self.phrase_matcher = PhraseMatcher(
-            self.nlp.vocab, attr=self.phrase_matcher_attr, validate=self._validate
+        self.phrase_matcher: PhraseMatcher = PhraseMatcher(
+            self.nlp.vocab,
+            attr=self.phrase_matcher_attr,
+            validate=self.validate,
         )
 
-    def remove(self, ent_id: str) -> None:
-        """Remove a pattern by its ent_id if a pattern with this ent_id was added before
+    def remove(self, label: str) -> None:
+        """Remove a pattern by its label.
 
-        ent_id (str): id of the pattern to be removed
+        label (str): Label of the pattern to be removed.
         RETURNS: None
-        DOCS: https://spacy.io/api/entityruler#remove
+        DOCS: https://spacy.io/api/spanruler#remove
         """
-        label_id_pairs = [
-            (label, eid) for (label, eid) in self._ent_ids.values() if eid == ent_id
-        ]
-        if not label_id_pairs:
+        if label not in self:
             raise ValueError(
-                Errors.E1024.format(attr_type="ID", label=ent_id, component=self.name)
+                Errors.E1024.format(attr_type="label", label=label, component=self.name)
             )
-        created_labels = [
-            self._create_label(label, eid) for (label, eid) in label_id_pairs
-        ]
-        # remove the patterns from self.phrase_patterns
-        self.phrase_patterns = defaultdict(
-            list,
-            {
-                label: val
-                for (label, val) in self.phrase_patterns.items()
-                if label not in created_labels
-            },
-        )
-        # remove the patterns from self.token_pattern
-        self.token_patterns = defaultdict(
-            list,
-            {
-                label: val
-                for (label, val) in self.token_patterns.items()
-                if label not in created_labels
-            },
-        )
-        # remove the patterns from self.token_pattern
-        for label in created_labels:
-            if label in self.phrase_matcher:
-                self.phrase_matcher.remove(label)
-            else:
-                self.matcher.remove(label)
+        self._patterns = [p for p in self._patterns if p["label"] != label]
+        for m_label in self._match_label_id_map:
+            if self._match_label_id_map[m_label]["label"] == label:
+                m_label_str = self.nlp.vocab.strings.as_string(m_label)
+                if m_label_str in self.phrase_matcher:
+                    self.phrase_matcher.remove(m_label_str)
+                if m_label_str in self.matcher:
+                    self.matcher.remove(m_label_str)
+
+    def remove_by_id(self, pattern_id: str) -> None:
+        """Remove a pattern by its pattern ID.
+
+        pattern_id (str): ID of the pattern to be removed.
+        RETURNS: None
+        DOCS: https://spacy.io/api/spanruler#remove_by_id
+        """
+        orig_len = len(self)
+        self._patterns = [p for p in self._patterns if p.get("id") != pattern_id]
+        if orig_len == len(self):
+            raise ValueError(
+                Errors.E1024.format(
+                    attr_type="ID", label=pattern_id, component=self.name
+                )
+            )
+        for m_label in self._match_label_id_map:
+            if self._match_label_id_map[m_label]["id"] == pattern_id:
+                m_label_str = self.nlp.vocab.strings.as_string(m_label)
+                if m_label_str in self.phrase_matcher:
+                    self.phrase_matcher.remove(m_label_str)
+                if m_label_str in self.matcher:
+                    self.matcher.remove(m_label_str)
 
     def _require_patterns(self) -> None:
         """Raise a warning if this component has no patterns defined."""
         if len(self) == 0:
             warnings.warn(Warnings.W036.format(name=self.name))
 
-    def _split_label(self, label: str) -> Tuple[str, Optional[str]]:
-        """Split Entity label into ent_label and ent_id if it contains self.ent_id_sep
-
-        label (str): The value of label in a pattern entry
-        RETURNS (tuple): ent_label, ent_id
-        """
-        if self.ent_id_sep in label:
-            ent_label, ent_id = label.rsplit(self.ent_id_sep, 1)
-        else:
-            ent_label = label
-            ent_id = None  # type: ignore
-        return ent_label, ent_id
-
-    def _create_label(self, label: Any, ent_id: Any) -> str:
-        """Join Entity label with ent_id if the pattern has an `id` attribute
-        If ent_id is not a string, the label is returned as is.
-
-        label (str): The label to set for ent.label_
-        ent_id (str): The label
-        RETURNS (str): The ent_label joined with configured `ent_id_sep`
-        """
-        if isinstance(ent_id, str):
-            label = f"{label}{self.ent_id_sep}{ent_id}"
-        return label
-
     def from_bytes(
-        self, patterns_bytes: bytes, *, exclude: Iterable[str] = SimpleFrozenList()
-    ) -> "EntityRuler":
-        """Load the entity ruler from a bytestring.
+        self, bytes_data: bytes, *, exclude: Iterable[str] = SimpleFrozenList()
+    ) -> "SpanRuler":
+        """Load the span ruler from a bytestring.
 
-        patterns_bytes (bytes): The bytestring to load.
-        RETURNS (EntityRuler): The loaded entity ruler.
+        bytes_data (bytes): The bytestring to load.
+        RETURNS (SpanRuler): The loaded span ruler.
 
-        DOCS: https://spacy.io/api/entityruler#from_bytes
+        DOCS: https://spacy.io/api/spanruler#from_bytes
         """
-        cfg = srsly.msgpack_loads(patterns_bytes)
         self.clear()
-        if isinstance(cfg, dict):
-            self.add_patterns(cfg.get("patterns", cfg))
-            self.overwrite = cfg.get("overwrite", False)
-            self.phrase_matcher_attr = cfg.get("phrase_matcher_attr", None)
-            self.phrase_matcher = PhraseMatcher(
-                self.nlp.vocab,
-                attr=self.phrase_matcher_attr,
-            )
-            self.ent_id_sep = cfg.get("ent_id_sep", DEFAULT_ENT_ID_SEP)
-        else:
-            self.add_patterns(cfg)
+        deserializers = {
+            "patterns": lambda b: self.add_patterns(srsly.json_loads(b)),
+        }
+        util.from_bytes(bytes_data, deserializers, exclude)
         return self
 
     def to_bytes(self, *, exclude: Iterable[str] = SimpleFrozenList()) -> bytes:
-        """Serialize the entity ruler patterns to a bytestring.
+        """Serialize the span ruler to a bytestring.
 
         RETURNS (bytes): The serialized patterns.
 
-        DOCS: https://spacy.io/api/entityruler#to_bytes
+        DOCS: https://spacy.io/api/spanruler#to_bytes
         """
-        serial = {
-            "overwrite": self.overwrite,
-            "ent_id_sep": self.ent_id_sep,
-            "phrase_matcher_attr": self.phrase_matcher_attr,
-            "patterns": self.patterns,
+        serializers = {
+            "patterns": lambda: srsly.json_dumps(self.patterns),
         }
-        return srsly.msgpack_dumps(serial)
+        return util.to_bytes(serializers, exclude)
 
     def from_disk(
         self, path: Union[str, Path], *, exclude: Iterable[str] = SimpleFrozenList()
-    ) -> "EntityRuler":
-        """Load the entity ruler from a file. Expects a file containing
-        newline-delimited JSON (JSONL) with one entry per line.
+    ) -> "SpanRuler":
+        """Load the span ruler from a directory.
 
-        path (str / Path): The JSONL file to load.
-        RETURNS (EntityRuler): The loaded entity ruler.
+        path (Union[str, Path]): A path to a directory.
+        RETURNS (SpanRuler): The loaded span ruler.
 
-        DOCS: https://spacy.io/api/entityruler#from_disk
+        DOCS: https://spacy.io/api/spanruler#from_disk
         """
-        path = ensure_path(path)
         self.clear()
-        depr_patterns_path = path.with_suffix(".jsonl")
-        if path.suffix == ".jsonl":  # user provides a jsonl
-            if path.is_file:
-                patterns = srsly.read_jsonl(path)
-                self.add_patterns(patterns)
-            else:
-                raise ValueError(Errors.E1023.format(path=path))
-        elif depr_patterns_path.is_file():
-            patterns = srsly.read_jsonl(depr_patterns_path)
-            self.add_patterns(patterns)
-        elif path.is_dir():  # path is a valid directory
-            cfg = {}
-            deserializers_patterns = {
-                "patterns": lambda p: self.add_patterns(
-                    srsly.read_jsonl(p.with_suffix(".jsonl"))
-                )
-            }
-            deserializers_cfg = {"cfg": lambda p: cfg.update(srsly.read_json(p))}
-            from_disk(path, deserializers_cfg, {})
-            self.overwrite = cfg.get("overwrite", False)
-            self.phrase_matcher_attr = cfg.get("phrase_matcher_attr")
-            self.ent_id_sep = cfg.get("ent_id_sep", DEFAULT_ENT_ID_SEP)
-
-            self.phrase_matcher = PhraseMatcher(
-                self.nlp.vocab, attr=self.phrase_matcher_attr
-            )
-            from_disk(path, deserializers_patterns, {})
-        else:  # path is not a valid directory or file
-            raise ValueError(Errors.E146.format(path=path))
+        path = ensure_path(path)
+        deserializers = {
+            "patterns": lambda p: self.add_patterns(srsly.read_jsonl(p)),
+        }
+        util.from_disk(path, deserializers, {})
         return self
 
     def to_disk(
         self, path: Union[str, Path], *, exclude: Iterable[str] = SimpleFrozenList()
     ) -> None:
-        """Save the entity ruler patterns to a directory. The patterns will be
-        saved as newline-delimited JSON (JSONL).
+        """Save the span ruler patterns to a directory.
 
-        path (str / Path): The JSONL file to save.
+        path (Union[str, Path]): A path to a directory.
 
-        DOCS: https://spacy.io/api/entityruler#to_disk
+        DOCS: https://spacy.io/api/spanruler#to_disk
         """
         path = ensure_path(path)
-        cfg = {
-            "overwrite": self.overwrite,
-            "phrase_matcher_attr": self.phrase_matcher_attr,
-            "ent_id_sep": self.ent_id_sep,
-        }
         serializers = {
-            "patterns": lambda p: srsly.write_jsonl(
-                p.with_suffix(".jsonl"), self.patterns
-            ),
-            "cfg": lambda p: srsly.write_json(p, cfg),
+            "patterns": lambda p: srsly.write_jsonl(p, self.patterns),
         }
-        if path.suffix == ".jsonl":  # user wants to save only JSONL
-            srsly.write_jsonl(path, self.patterns)
-        else:
-            to_disk(path, serializers, {})
+        util.to_disk(path, serializers, {})
```

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/functions.py` & `spacy-4.0.0.dev0/spacy/pipeline/functions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/legacy/entity_linker.py` & `spacy-4.0.0.dev0/spacy/pipeline/legacy/entity_linker.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/lemmatizer.py` & `spacy-4.0.0.dev0/spacy/pipeline/lemmatizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/morphologizer.pyx` & `spacy-4.0.0.dev0/spacy/pipeline/morphologizer.pyx`

 * *Files 22% similar despite different names*

```diff
@@ -1,23 +1,25 @@
 # cython: infer_types=True, profile=True, binding=True
-from typing import Optional, Union, Dict, Callable
+from typing import Callable, Dict, Iterable, List, Optional, Union
 import srsly
-from thinc.api import SequenceCategoricalCrossentropy, Model, Config
+from thinc.api import Model, Config
+from thinc.legacy import LegacySequenceCategoricalCrossentropy
+from thinc.types import Floats2d, Ints1d
 from itertools import islice
 
 from ..tokens.doc cimport Doc
 from ..vocab cimport Vocab
 from ..morphology cimport Morphology
 
 from ..parts_of_speech import IDS as POS_IDS
 from ..symbols import POS
 from ..language import Language
 from ..errors import Errors
 from .pipe import deserialize_config
-from .tagger import Tagger
+from .tagger import ActivationsT, Tagger
 from .. import util
 from ..scorer import Scorer
 from ..training import validate_examples, validate_get_examples
 from ..util import registry
 
 # See #9050
 BACKWARD_OVERWRITE = True
@@ -48,28 +50,34 @@
 
 DEFAULT_MORPH_MODEL = Config().from_str(default_model_config)["model"]
 
 
 @Language.factory(
     "morphologizer",
     assigns=["token.morph", "token.pos"],
-    default_config={"model": DEFAULT_MORPH_MODEL, "overwrite": True, "extend": False,
-                    "scorer": {"@scorers": "spacy.morphologizer_scorer.v1"}, "label_smoothing": 0.0},
+    default_config={
+        "model": DEFAULT_MORPH_MODEL,
+        "overwrite": True,
+        "extend": False,
+        "scorer": {"@scorers": "spacy.morphologizer_scorer.v1"},
+        "save_activations": False,
+    },
     default_score_weights={"pos_acc": 0.5, "morph_acc": 0.5, "morph_per_feat": None},
 )
 def make_morphologizer(
     nlp: Language,
     model: Model,
     name: str,
     overwrite: bool,
     extend: bool,
-    label_smoothing: float,
     scorer: Optional[Callable],
+    save_activations: bool,
 ):
-    return Morphologizer(nlp.vocab, model, name, overwrite=overwrite, extend=extend, label_smoothing=label_smoothing, scorer=scorer)
+    return Morphologizer(nlp.vocab, model, name, overwrite=overwrite, extend=extend, scorer=scorer,
+                         save_activations=save_activations)
 
 
 def morphologizer_score(examples, **kwargs):
     def morph_key_getter(token, attr):
         return getattr(token, attr).key
 
     results = {}
@@ -92,26 +100,27 @@
         self,
         vocab: Vocab,
         model: Model,
         name: str = "morphologizer",
         *,
         overwrite: bool = BACKWARD_OVERWRITE,
         extend: bool = BACKWARD_EXTEND,
-        label_smoothing: float = 0.0,
         scorer: Optional[Callable] = morphologizer_score,
+        save_activations: bool = False,
     ):
         """Initialize a morphologizer.
 
         vocab (Vocab): The shared vocabulary.
         model (thinc.api.Model): The Thinc Model powering the pipeline component.
         name (str): The component instance name, used to add entries to the
             losses during training.
         scorer (Optional[Callable]): The scoring method. Defaults to
             Scorer.score_token_attr for the attributes "pos" and "morph" and
             Scorer.score_token_attr_per_feat for the attribute "morph".
+        save_activations (bool): save model activations in Doc when annotating.
 
         DOCS: https://spacy.io/api/morphologizer#init
         """
         self.vocab = vocab
         self.model = model
         self.name = name
         self._rehearsal_model = None
@@ -120,23 +129,23 @@
         # 1) labels_morph stores a mapping from morph+POS->morph
         # 2) labels_pos stores a mapping from morph+POS->POS
         cfg = {
             "labels_morph": {},
             "labels_pos": {},
             "overwrite": overwrite,
             "extend": extend,
-            "label_smoothing": label_smoothing,
         }
         self.cfg = dict(sorted(cfg.items()))
         self.scorer = scorer
+        self.save_activations = save_activations
 
     @property
     def labels(self):
-        """RETURNS (Tuple[str]): The labels currently added to the component."""
-        return tuple(self.cfg["labels_morph"].keys())
+        """RETURNS (Iterable[str]): The labels currently added to the component."""
+        return self.cfg["labels_morph"].keys()
 
     @property
     def label_data(self) -> Dict[str, Dict[str, Union[str, float, int, None]]]:
         """A dictionary with all labels data."""
         return {"morph": self.cfg["labels_morph"], "pos": self.cfg["labels_pos"]}
 
     def add_label(self, label):
@@ -151,15 +160,15 @@
             raise ValueError(Errors.E187)
         if label in self.labels:
             return 0
         self._allow_extra_label()
         # normalize label
         norm_label = self.vocab.morphology.normalize_features(label)
         # extract separate POS and morph tags
-        label_dict = Morphology.feats_to_dict(label)
+        label_dict = Morphology.feats_to_dict(label, sort_values=False)
         pos = label_dict.get(self.POS_FEAT, "")
         if self.POS_FEAT in label_dict:
             label_dict.pop(self.POS_FEAT)
         # normalize morph string and add to morphology table
         norm_morph = self.vocab.strings[self.vocab.morphology.add(label_dict)]
         # add label mappings
         if norm_label not in self.cfg["labels_morph"]:
@@ -189,15 +198,15 @@
                     pos = token.pos_
                     # if both are unset, annotation is missing, so do not add
                     # an empty label
                     if pos == "" and not token.has_morph():
                         continue
                     morph = str(token.morph)
                     # create and add the combined morph+POS label
-                    morph_dict = Morphology.feats_to_dict(morph)
+                    morph_dict = Morphology.feats_to_dict(morph, sort_values=False)
                     if pos:
                         morph_dict[self.POS_FEAT] = pos
                     norm_label = self.vocab.strings[self.vocab.morphology.add(morph_dict)]
                     # add label->morph and label->POS mappings
                     if norm_label not in self.cfg["labels_morph"]:
                         self.cfg["labels_morph"][norm_label] = morph
                         self.cfg["labels_pos"][norm_label] = POS_IDS[pos]
@@ -206,59 +215,67 @@
         doc_sample = []
         label_sample = []
         for example in islice(get_examples(), 10):
             gold_array = []
             for i, token in enumerate(example.reference):
                 pos = token.pos_
                 morph = str(token.morph)
-                morph_dict = Morphology.feats_to_dict(morph)
+                morph_dict = Morphology.feats_to_dict(morph, sort_values=False)
                 if pos:
                     morph_dict[self.POS_FEAT] = pos
                 norm_label = self.vocab.strings[self.vocab.morphology.add(morph_dict)]
                 gold_array.append([1.0 if label == norm_label else 0.0 for label in self.labels])
             doc_sample.append(example.x)
             label_sample.append(self.model.ops.asarray(gold_array, dtype="float32"))
         assert len(doc_sample) > 0, Errors.E923.format(name=self.name)
         assert len(label_sample) > 0, Errors.E923.format(name=self.name)
         self.model.initialize(X=doc_sample, Y=label_sample)
 
-    def set_annotations(self, docs, batch_tag_ids):
+    def set_annotations(self, docs: Iterable[Doc], activations: ActivationsT):
         """Modify a batch of documents, using pre-computed scores.
 
         docs (Iterable[Doc]): The documents to modify.
-        batch_tag_ids: The IDs to set, produced by Morphologizer.predict.
+        activations (ActivationsT): The activations used for setting annotations, produced by Morphologizer.predict.
 
         DOCS: https://spacy.io/api/morphologizer#set_annotations
         """
+        batch_tag_ids = activations["label_ids"]
         if isinstance(docs, Doc):
             docs = [docs]
         cdef Doc doc
         cdef Vocab vocab = self.vocab
         cdef bint overwrite = self.cfg["overwrite"]
         cdef bint extend = self.cfg["extend"]
-        labels = self.labels
+
+        # We require random access for the upcoming ops, so we need
+        # to allocate a compatible container out of the iterable.
+        labels = tuple(self.labels)
         for i, doc in enumerate(docs):
+            if self.save_activations:
+                doc.activations[self.name] = {}
+                for act_name, acts in activations.items():
+                    doc.activations[self.name][act_name] = acts[i]
             doc_tag_ids = batch_tag_ids[i]
             if hasattr(doc_tag_ids, "get"):
                 doc_tag_ids = doc_tag_ids.get()
             for j, tag_id in enumerate(doc_tag_ids):
-                morph = labels[tag_id]
+                morph = labels[int(tag_id)]
                 # set morph
                 if doc.c[j].morph == 0 or overwrite or extend:
                     if overwrite and extend:
                         # morphologizer morph overwrites any existing features
                         # while extending
-                        extended_morph = Morphology.feats_to_dict(self.vocab.strings[doc.c[j].morph])
-                        extended_morph.update(Morphology.feats_to_dict(self.cfg["labels_morph"].get(morph, 0)))
+                        extended_morph = Morphology.feats_to_dict(self.vocab.strings[doc.c[j].morph], sort_values=False)
+                        extended_morph.update(Morphology.feats_to_dict(self.cfg["labels_morph"].get(morph, 0), sort_values=False))
                         doc.c[j].morph = self.vocab.morphology.add(extended_morph)
                     elif extend:
                         # existing features are preserved and any new features
                         # are added
-                        extended_morph = Morphology.feats_to_dict(self.cfg["labels_morph"].get(morph, 0))
-                        extended_morph.update(Morphology.feats_to_dict(self.vocab.strings[doc.c[j].morph]))
+                        extended_morph = Morphology.feats_to_dict(self.cfg["labels_morph"].get(morph, 0), sort_values=False)
+                        extended_morph.update(Morphology.feats_to_dict(self.vocab.strings[doc.c[j].morph], sort_values=False))
                         doc.c[j].morph = self.vocab.morphology.add(extended_morph)
                     else:
                         # clobber
                         doc.c[j].morph = self.vocab.morphology.add(self.cfg["labels_morph"].get(morph, 0))
                 # set POS
                 if doc.c[j].pos == 0 or overwrite:
                     doc.c[j].pos = self.cfg["labels_pos"].get(morph, 0)
@@ -270,16 +287,15 @@
         examples (Iterable[Examples]): The batch of examples.
         scores: Scores representing the model's predictions.
         RETURNS (Tuple[float, float]): The loss and the gradient.
 
         DOCS: https://spacy.io/api/morphologizer#get_loss
         """
         validate_examples(examples, "Morphologizer.get_loss")
-        loss_func = SequenceCategoricalCrossentropy(names=self.labels, normalize=False,
-                                                    label_smoothing=self.cfg["label_smoothing"])
+        loss_func = LegacySequenceCategoricalCrossentropy(names=tuple(self.labels), normalize=False)
         truths = []
         for eg in examples:
             eg_truths = []
             pos_tags = eg.get_aligned("POS", as_string=True)
             morphs = eg.get_aligned("MORPH", as_string=True)
             for i in range(len(morphs)):
                 pos = pos_tags[i]
@@ -292,15 +308,15 @@
                     label = None
                 # If both are unset, the annotation is missing (empty morph
                 # converted from int is "_" rather than "")
                 elif pos == "" and morph == "":
                     label = None
                 # Otherwise, generate the combined label
                 else:
-                    label_dict = Morphology.feats_to_dict(morph)
+                    label_dict = Morphology.feats_to_dict(morph, sort_values=False)
                     if pos:
                         label_dict[self.POS_FEAT] = pos
                     label = self.vocab.strings[self.vocab.morphology.add(label_dict)]
                     # As a fail-safe, skip any unrecognized labels
                     if label not in self.labels:
                         label = None
                 eg_truths.append(label)
```

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/multitask.pyx` & `spacy-4.0.0.dev0/spacy/pipeline/senter.pyx`

 * *Files 26% similar despite different names*

```diff
@@ -1,221 +1,209 @@
 # cython: infer_types=True, profile=True, binding=True
-from typing import Optional
-import numpy
-from thinc.api import CosineDistance, to_categorical, Model, Config
-from thinc.api import set_dropout_rate
+from typing import Dict, Iterable, Optional, Callable, List, Union
+from itertools import islice
+
+import srsly
+from thinc.api import Model, Config
+from thinc.legacy import LegacySequenceCategoricalCrossentropy
+
+from thinc.types import Floats2d, Ints1d
 
 from ..tokens.doc cimport Doc
 
-from .trainable_pipe import TrainablePipe
-from .tagger import Tagger
-from ..training import validate_examples
+from .tagger import ActivationsT, Tagger
 from ..language import Language
-from ._parser_internals import nonproj
-from ..attrs import POS, ID
 from ..errors import Errors
+from ..scorer import Scorer
+from ..training import validate_examples, validate_get_examples
+from ..util import registry
+from .. import util
 
+# See #9050
+BACKWARD_OVERWRITE = False
 
 default_model_config = """
 [model]
-@architectures = "spacy.MultiTask.v1"
-maxout_pieces = 3
-token_vector_width = 96
+@architectures = "spacy.Tagger.v2"
 
 [model.tok2vec]
 @architectures = "spacy.HashEmbedCNN.v2"
 pretrained_vectors = null
-width = 96
-depth = 4
+width = 12
+depth = 1
 embed_size = 2000
 window_size = 1
 maxout_pieces = 2
 subword_features = true
 """
-DEFAULT_MT_MODEL = Config().from_str(default_model_config)["model"]
+DEFAULT_SENTER_MODEL = Config().from_str(default_model_config)["model"]
 
 
 @Language.factory(
-    "nn_labeller",
-    default_config={"labels": None, "target": "dep_tag_offset", "model": DEFAULT_MT_MODEL}
+    "senter",
+    assigns=["token.is_sent_start"],
+    default_config={
+        "model": DEFAULT_SENTER_MODEL,
+        "overwrite": False,
+        "scorer": {"@scorers": "spacy.senter_scorer.v1"},
+        "save_activations": False,
+    },
+    default_score_weights={"sents_f": 1.0, "sents_p": 0.0, "sents_r": 0.0},
 )
-def make_nn_labeller(nlp: Language, name: str, model: Model, labels: Optional[dict], target: str):
-    return MultitaskObjective(nlp.vocab, model, name)
+def make_senter(nlp: Language,
+                name: str,
+                model: Model,
+                overwrite: bool,
+                scorer: Optional[Callable],
+                save_activations: bool):
+    return SentenceRecognizer(nlp.vocab, model, name, overwrite=overwrite, scorer=scorer, save_activations=save_activations)
+
+
+def senter_score(examples, **kwargs):
+    def has_sents(doc):
+        return doc.has_annotation("SENT_START")
+
+    results = Scorer.score_spans(examples, "sents", has_annotation=has_sents, **kwargs)
+    del results["sents_per_type"]
+    return results
+
 
+@registry.scorers("spacy.senter_scorer.v1")
+def make_senter_scorer():
+    return senter_score
 
-class MultitaskObjective(Tagger):
-    """Experimental: Assist training of a parser or tagger, by training a
-    side-objective.
+
+class SentenceRecognizer(Tagger):
+    """Pipeline component for sentence segmentation.
+
+    DOCS: https://spacy.io/api/sentencerecognizer
     """
+    def __init__(
+        self,
+        vocab,
+        model,
+        name="senter",
+        *,
+        overwrite=BACKWARD_OVERWRITE,
+        scorer=senter_score,
+        save_activations: bool = False,
+    ):
+        """Initialize a sentence recognizer.
+
+        vocab (Vocab): The shared vocabulary.
+        model (thinc.api.Model): The Thinc Model powering the pipeline component.
+        name (str): The component instance name, used to add entries to the
+            losses during training.
+        scorer (Optional[Callable]): The scoring method. Defaults to
+            Scorer.score_spans for the attribute "sents".
+        save_activations (bool): save model activations in Doc when annotating.
 
-    def __init__(self, vocab, model, name="nn_labeller", *, target):
+        DOCS: https://spacy.io/api/sentencerecognizer#init
+        """
         self.vocab = vocab
         self.model = model
         self.name = name
-        if target == "dep":
-            self.make_label = self.make_dep
-        elif target == "tag":
-            self.make_label = self.make_tag
-        elif target == "ent":
-            self.make_label = self.make_ent
-        elif target == "dep_tag_offset":
-            self.make_label = self.make_dep_tag_offset
-        elif target == "ent_tag":
-            self.make_label = self.make_ent_tag
-        elif target == "sent_start":
-            self.make_label = self.make_sent_start
-        elif hasattr(target, "__call__"):
-            self.make_label = target
-        else:
-            raise ValueError(Errors.E016)
-        cfg = {"labels": {}, "target": target}
-        self.cfg = dict(cfg)
+        self._rehearsal_model = None
+        self.cfg = {"overwrite": overwrite}
+        self.scorer = scorer
+        self.save_activations = save_activations
 
     @property
     def labels(self):
-        return self.cfg.setdefault("labels", {})
+        """RETURNS (Tuple[str]): The labels."""
+        # labels are numbered by index internally, so this matches GoldParse
+        # and Example where the sentence-initial tag is 1 and other positions
+        # are 0
+        return tuple(["I", "S"])
 
-    @labels.setter
-    def labels(self, value):
-        self.cfg["labels"] = value
-
-    def set_annotations(self, docs, dep_ids):
-        pass
-
-    def initialize(self, get_examples, nlp=None, labels=None):
-        if not hasattr(get_examples, "__call__"):
-            err = Errors.E930.format(name="MultitaskObjective", obj=type(get_examples))
-            raise ValueError(err)
-        if labels is not None:
-            self.labels = labels
-        else:
-            for example in get_examples():
-                for token in example.y:
-                    label = self.make_label(token)
-                    if label is not None and label not in self.labels:
-                        self.labels[label] = len(self.labels)
-        self.model.initialize()   # TODO: fix initialization by defining X and Y
-
-    def predict(self, docs):
-        tokvecs = self.model.get_ref("tok2vec")(docs)
-        scores = self.model.get_ref("softmax")(tokvecs)
-        return tokvecs, scores
+    @property
+    def hide_labels(self):
+        return True
 
-    def get_loss(self, examples, scores):
-        cdef int idx = 0
-        correct = numpy.zeros((scores.shape[0],), dtype="i")
-        guesses = scores.argmax(axis=1)
-        docs = [eg.predicted for eg in examples]
-        for i, eg in enumerate(examples):
-            # Handles alignment for tokenization differences
-            doc_annots = eg.get_aligned()  # TODO
-            for j in range(len(eg.predicted)):
-                tok_annots = {key: values[j] for key, values in tok_annots.items()}
-                label = self.make_label(j, tok_annots)
-                if label is None or label not in self.labels:
-                    correct[idx] = guesses[idx]
-                else:
-                    correct[idx] = self.labels[label]
-                idx += 1
-        correct = self.model.ops.xp.array(correct, dtype="i")
-        d_scores = scores - to_categorical(correct, n_classes=scores.shape[1])
-        loss = (d_scores**2).sum()
-        return float(loss), d_scores
+    @property
+    def label_data(self):
+        return None
+
+    def set_annotations(self, docs: Iterable[Doc], activations: ActivationsT):
+        """Modify a batch of documents, using pre-computed scores.
 
-    @staticmethod
-    def make_dep(token):
-        return token.dep_
-
-    @staticmethod
-    def make_tag(token):
-        return token.tag_
-
-    @staticmethod
-    def make_ent(token):
-        if token.ent_iob_ == "O":
-            return "O"
-        else:
-            return token.ent_iob_ + "-" + token.ent_type_
-
-    @staticmethod
-    def make_dep_tag_offset(token):
-        dep = token.dep_
-        tag = token.tag_
-        offset = token.head.i - token.i
-        offset = min(offset, 2)
-        offset = max(offset, -2)
-        return f"{dep}-{tag}:{offset}"
-
-    @staticmethod
-    def make_ent_tag(token):
-        if token.ent_iob_ == "O":
-            ent = "O"
-        else:
-            ent = token.ent_iob_ + "-" + token.ent_type_
-        tag = token.tag_
-        return f"{tag}-{ent}"
-
-    @staticmethod
-    def make_sent_start(token):
-        """A multi-task objective for representing sentence boundaries,
-        using BILU scheme. (O is impossible)
+        docs (Iterable[Doc]): The documents to modify.
+        activations (ActivationsT): The activations used for setting annotations, produced by SentenceRecognizer.predict.
+
+        DOCS: https://spacy.io/api/sentencerecognizer#set_annotations
         """
-        if token.is_sent_start and token.is_sent_end:
-            return "U-SENT"
-        elif token.is_sent_start:
-            return "B-SENT"
-        else:
-            return "I-SENT"
+        batch_tag_ids = activations["label_ids"]
+        if isinstance(docs, Doc):
+            docs = [docs]
+        cdef Doc doc
+        cdef bint overwrite = self.cfg["overwrite"]
+        for i, doc in enumerate(docs):
+            if self.save_activations:
+                doc.activations[self.name] = {}
+                for act_name, acts in activations.items():
+                    doc.activations[self.name][act_name] = acts[i]
+            doc_tag_ids = batch_tag_ids[i]
+            if hasattr(doc_tag_ids, "get"):
+                doc_tag_ids = doc_tag_ids.get()
+            for j, tag_id in enumerate(doc_tag_ids):
+                if doc.c[j].sent_start == 0 or overwrite:
+                    if tag_id == 1:
+                        doc.c[j].sent_start = 1
+                    else:
+                        doc.c[j].sent_start = -1
 
+    def get_loss(self, examples, scores):
+        """Find the loss and gradient of loss for the batch of documents and
+        their predicted scores.
 
-class ClozeMultitask(TrainablePipe):
-    def __init__(self, vocab, model, **cfg):
-        self.vocab = vocab
-        self.model = model
-        self.cfg = cfg
-        self.distance = CosineDistance(ignore_zeros=True, normalize=False)  # TODO: in config
+        examples (Iterable[Examples]): The batch of examples.
+        scores: Scores representing the model's predictions.
+        RETURNS (Tuple[float, float]): The loss and the gradient.
+
+        DOCS: https://spacy.io/api/sentencerecognizer#get_loss
+        """
+        validate_examples(examples, "SentenceRecognizer.get_loss")
+        labels = self.labels
+        loss_func = LegacySequenceCategoricalCrossentropy(names=labels, normalize=False)
+        truths = []
+        for eg in examples:
+            eg_truth = []
+            for x in eg.get_aligned("SENT_START"):
+                if x is None:
+                    eg_truth.append(None)
+                elif x == 1:
+                    eg_truth.append(labels[1])
+                else:
+                    # anything other than 1: 0, -1, -1 as uint64
+                    eg_truth.append(labels[0])
+            truths.append(eg_truth)
+        d_scores, loss = loss_func(scores, truths)
+        if self.model.ops.xp.isnan(loss):
+            raise ValueError(Errors.E910.format(name=self.name))
+        return float(loss), d_scores
 
-    def set_annotations(self, docs, dep_ids):
-        pass
+    def initialize(self, get_examples, *, nlp=None):
+        """Initialize the pipe for training, using a representative set
+        of data examples.
+
+        get_examples (Callable[[], Iterable[Example]]): Function that
+            returns a representative sample of gold-standard Example objects.
+        nlp (Language): The current nlp object the component is part of.
 
-    def initialize(self, get_examples, nlp=None):
-        self.model.initialize()  # TODO: fix initialization by defining X and Y
-        X = self.model.ops.alloc((5, self.model.get_ref("tok2vec").get_dim("nO")))
-        self.model.output_layer.initialize(X)
-
-    def predict(self, docs):
-        tokvecs = self.model.get_ref("tok2vec")(docs)
-        vectors = self.model.get_ref("output_layer")(tokvecs)
-        return tokvecs, vectors
-
-    def get_loss(self, examples, vectors, prediction):
-        validate_examples(examples, "ClozeMultitask.get_loss")
-        # The simplest way to implement this would be to vstack the
-        # token.vector values, but that's a bit inefficient, especially on GPU.
-        # Instead we fetch the index into the vectors table for each of our tokens,
-        # and look them up all at once. This prevents data copying.
-        ids = self.model.ops.flatten([eg.predicted.to_array(ID).ravel() for eg in examples])
-        target = vectors[ids]
-        gradient = self.distance.get_grad(prediction, target)
-        loss = self.distance.get_loss(prediction, target)
-        return float(loss), gradient
-
-    def update(self, examples, *, drop=0., sgd=None, losses=None):
-        pass
-
-    def rehearse(self, examples, drop=0., sgd=None, losses=None):
-        if losses is not None and self.name not in losses:
-            losses[self.name] = 0.
-        set_dropout_rate(self.model, drop)
-        validate_examples(examples, "ClozeMultitask.rehearse")
-        docs = [eg.predicted for eg in examples]
-        predictions, bp_predictions = self.model.begin_update()
-        loss, d_predictions = self.get_loss(examples, self.vocab.vectors.data, predictions)
-        bp_predictions(d_predictions)
-        if sgd is not None:
-            self.finish_update(sgd)
-        if losses is not None:
-            losses[self.name] += loss
-        return losses
+        DOCS: https://spacy.io/api/sentencerecognizer#initialize
+        """
+        validate_get_examples(get_examples, "SentenceRecognizer.initialize")
+        util.check_lexeme_norms(self.vocab, "senter")
+        doc_sample = []
+        label_sample = []
+        assert self.labels, Errors.E924.format(name=self.name)
+        for example in islice(get_examples(), 10):
+            doc_sample.append(example.x)
+            gold_tags = example.get_aligned("SENT_START")
+            gold_array = [[1.0 if tag == gold_tag else 0.0 for tag in self.labels] for gold_tag in gold_tags]
+            label_sample.append(self.model.ops.asarray(gold_array, dtype="float32"))
+        assert len(doc_sample) > 0, Errors.E923.format(name=self.name)
+        assert len(label_sample) > 0, Errors.E923.format(name=self.name)
+        self.model.initialize(X=doc_sample, Y=label_sample)
 
-    def add_label(self, label):
+    def add_label(self, label, values=None):
         raise NotImplementedError
```

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/ner.pyx` & `spacy-4.0.0.dev0/spacy/pipeline/ner.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,29 +1,29 @@
 # cython: infer_types=True, profile=True, binding=True
 from collections import defaultdict
 from typing import Optional, Iterable, Callable
 from thinc.api import Model, Config
 
 from ._parser_internals.transition_system import TransitionSystem
-from .transition_parser cimport Parser
-from ._parser_internals.ner cimport BiluoPushDown
+from .transition_parser import Parser
+from ._parser_internals.ner import BiluoPushDown
 from ..language import Language
 from ..scorer import get_ner_prf, PRFScore
+from ..training import validate_examples
 from ..util import registry
 from ..training import remove_bilu_prefix
 
 
 default_model_config = """
 [model]
-@architectures = "spacy.TransitionBasedParser.v2"
+@architectures = "spacy.TransitionBasedParser.v3"
 state_type = "ner"
 extra_state_tokens = false
 hidden_width = 64
 maxout_pieces = 2
-use_upper = true
 
 [model.tok2vec]
 @architectures = "spacy.HashEmbedCNN.v2"
 pretrained_vectors = null
 width = 96
 depth = 4
 embed_size = 2000
@@ -40,16 +40,20 @@
     default_config={
         "moves": None,
         "update_with_oracle_cut_size": 100,
         "model": DEFAULT_NER_MODEL,
         "incorrect_spans_key": None,
         "scorer": {"@scorers": "spacy.ner_scorer.v1"},
     },
-    default_score_weights={"ents_f": 1.0, "ents_p": 0.0, "ents_r": 0.0, "ents_per_type": None},
-
+    default_score_weights={
+        "ents_f": 1.0,
+        "ents_p": 0.0,
+        "ents_r": 0.0,
+        "ents_per_type": None,
+    },
 )
 def make_ner(
     nlp: Language,
     name: str,
     model: Model,
     moves: Optional[TransitionSystem],
     update_with_oracle_cut_size: int,
@@ -94,28 +98,34 @@
         multitasks=[],
         beam_width=1,
         beam_density=0.0,
         beam_update_prob=0.0,
         scorer=scorer,
     )
 
+
 @Language.factory(
     "beam_ner",
     assigns=["doc.ents", "token.ent_iob", "token.ent_type"],
     default_config={
         "moves": None,
         "update_with_oracle_cut_size": 100,
         "model": DEFAULT_NER_MODEL,
         "beam_density": 0.01,
         "beam_update_prob": 0.5,
         "beam_width": 32,
         "incorrect_spans_key": None,
         "scorer": None,
     },
-    default_score_weights={"ents_f": 1.0, "ents_p": 0.0, "ents_r": 0.0, "ents_per_type": None},
+    default_score_weights={
+        "ents_f": 1.0,
+        "ents_p": 0.0,
+        "ents_r": 0.0,
+        "ents_per_type": None,
+    },
 )
 def make_beam_ner(
     nlp: Language,
     name: str,
     model: Model,
     moves: Optional[TransitionSystem],
     update_with_oracle_cut_size: int,
@@ -181,19 +191,20 @@
 
 
 @registry.scorers("spacy.ner_scorer.v1")
 def make_ner_scorer():
     return ner_score
 
 
-cdef class EntityRecognizer(Parser):
+class EntityRecognizer(Parser):
     """Pipeline component for named entity recognition.
 
     DOCS: https://spacy.io/api/entityrecognizer
     """
+
     TransitionSystem = BiluoPushDown
 
     def __init__(
         self,
         vocab,
         model,
         name="ner",
@@ -203,23 +214,22 @@
         beam_width=1,
         beam_density=0.0,
         beam_update_prob=0.0,
         multitasks=tuple(),
         incorrect_spans_key=None,
         scorer=ner_score,
     ):
-        """Create an EntityRecognizer.
-        """
+        """Create an EntityRecognizer."""
         super().__init__(
             vocab,
             model,
             name,
             moves,
             update_with_oracle_cut_size=update_with_oracle_cut_size,
-            min_action_freq=1,   # not relevant for NER
+            min_action_freq=1,  # not relevant for NER
             learn_tokens=False,  # not relevant for NER
             beam_width=beam_width,
             beam_density=beam_density,
             beam_update_prob=beam_update_prob,
             multitasks=multitasks,
             incorrect_spans_key=incorrect_spans_key,
             scorer=scorer,
```

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/pipe.pyi` & `spacy-4.0.0.dev0/spacy/pipeline/pipe.pyi`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/pipe.pyx` & `spacy-4.0.0.dev0/spacy/pipeline/pipe.pyx`

 * *Files 6% similar despite different names*

```diff
@@ -15,21 +15,14 @@
     an interface for pipeline components to implement.
     Trainable pipeline components like the EntityRecognizer or TextCategorizer
     should inherit from the subclass 'TrainablePipe'.
 
     DOCS: https://spacy.io/api/pipe
     """
 
-    @classmethod
-    def __init_subclass__(cls, **kwargs):
-        """Raise a warning if an inheriting class implements 'begin_training'
-         (from v2) instead of the new 'initialize' method (from v3)"""
-        if hasattr(cls, "begin_training"):
-            warnings.warn(Warnings.W088.format(name=cls.__name__))
-
     def __call__(self, Doc doc) -> Doc:
         """Apply the pipe to one document. The document is modified in place,
         and returned. This usually happens under the hood when the nlp object
         is called on a text and all components are applied to the Doc.
 
         doc (Doc): The Doc to process.
         RETURNS (Doc): The processed Doc.
@@ -91,14 +84,18 @@
                 scorer_kwargs["labels"] = self.labels
             # override with kwargs settings
             scorer_kwargs.update(kwargs)
             return self.scorer(examples, **scorer_kwargs)
         return {}
 
     @property
+    def is_distillable(self) -> bool:
+        return False
+
+    @property
     def is_trainable(self) -> bool:
         return False
 
     @property
     def labels(self) -> Tuple[str, ...]:
         return tuple()
```

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/sentencizer.pyx` & `spacy-4.0.0.dev0/spacy/pipeline/sentencizer.pyx`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/spancat.py` & `spacy-4.0.0.dev0/spacy/pipeline/spancat.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,10 +1,9 @@
-from typing import List, Dict, Callable, Tuple, Optional, Iterable, Any, cast, Union
-from dataclasses import dataclass
-from functools import partial
+from typing import List, Dict, Callable, Tuple, Optional, Iterable, Any, cast
+from typing import Union
 from thinc.api import Config, Model, get_current_ops, set_dropout_rate, Ops
 from thinc.api import Optimizer
 from thinc.types import Ragged, Ints2d, Floats2d
 
 import numpy
 
 from ..compat import Protocol, runtime_checkable
@@ -14,14 +13,17 @@
 from ..tokens import Doc, SpanGroup, Span
 from ..vocab import Vocab
 from ..training import Example, validate_examples
 from ..errors import Errors
 from ..util import registry
 
 
+ActivationsT = Dict[str, Union[Floats2d, Ragged]]
+
+
 spancat_default_config = """
 [model]
 @architectures = "spacy.SpanCategorizer.v1"
 scorer = {"@layers": "spacy.LinearLogistic.v1"}
 
 [model.reducer]
 @layers = spacy.mean_max_reducer.v1
@@ -41,88 +43,56 @@
 @architectures = "spacy.MaxoutWindowEncoder.v2"
 width = ${model.tok2vec.embed.width}
 window_size = 1
 maxout_pieces = 3
 depth = 4
 """
 
-spancat_singlelabel_default_config = """
-[model]
-@architectures = "spacy.SpanCategorizer.v1"
-scorer = {"@layers": "Softmax.v2"}
-
-[model.reducer]
-@layers = spacy.mean_max_reducer.v1
-hidden_size = 128
-
-[model.tok2vec]
-@architectures = "spacy.Tok2Vec.v2"
-[model.tok2vec.embed]
-@architectures = "spacy.MultiHashEmbed.v1"
-width = 96
-rows = [5000, 1000, 2500, 1000]
-attrs = ["NORM", "PREFIX", "SUFFIX", "SHAPE"]
-include_static_vectors = false
-
-[model.tok2vec.encode]
-@architectures = "spacy.MaxoutWindowEncoder.v2"
-width = ${model.tok2vec.embed.width}
-window_size = 1
-maxout_pieces = 3
-depth = 4
-"""
-
 DEFAULT_SPANCAT_MODEL = Config().from_str(spancat_default_config)["model"]
-DEFAULT_SPANCAT_SINGLELABEL_MODEL = Config().from_str(
-    spancat_singlelabel_default_config
-)["model"]
 
 
 @runtime_checkable
 class Suggester(Protocol):
     def __call__(self, docs: Iterable[Doc], *, ops: Optional[Ops] = None) -> Ragged:
         ...
 
 
-def ngram_suggester(
-    docs: Iterable[Doc], sizes: List[int], *, ops: Optional[Ops] = None
-) -> Ragged:
-    if ops is None:
-        ops = get_current_ops()
-    spans = []
-    lengths = []
-    for doc in docs:
-        starts = ops.xp.arange(len(doc), dtype="i")
-        starts = starts.reshape((-1, 1))
-        length = 0
-        for size in sizes:
-            if size <= len(doc):
-                starts_size = starts[: len(doc) - (size - 1)]
-                spans.append(ops.xp.hstack((starts_size, starts_size + size)))
-                length += spans[-1].shape[0]
-            if spans:
-                assert spans[-1].ndim == 2, spans[-1].shape
-        lengths.append(length)
-    lengths_array = ops.asarray1i(lengths)
-    if len(spans) > 0:
-        output = Ragged(ops.xp.vstack(spans), lengths_array)
-    else:
-        output = Ragged(ops.xp.zeros((0, 0), dtype="i"), lengths_array)
-
-    assert output.dataXd.ndim == 2
-    return output
-
-
 @registry.misc("spacy.ngram_suggester.v1")
 def build_ngram_suggester(sizes: List[int]) -> Suggester:
     """Suggest all spans of the given lengths. Spans are returned as a ragged
     array of integers. The array has two columns, indicating the start and end
     position."""
 
-    return partial(ngram_suggester, sizes=sizes)
+    def ngram_suggester(docs: Iterable[Doc], *, ops: Optional[Ops] = None) -> Ragged:
+        if ops is None:
+            ops = get_current_ops()
+        spans = []
+        lengths = []
+        for doc in docs:
+            starts = ops.xp.arange(len(doc), dtype="i")
+            starts = starts.reshape((-1, 1))
+            length = 0
+            for size in sizes:
+                if size <= len(doc):
+                    starts_size = starts[: len(doc) - (size - 1)]
+                    spans.append(ops.xp.hstack((starts_size, starts_size + size)))
+                    length += spans[-1].shape[0]
+                if spans:
+                    assert spans[-1].ndim == 2, spans[-1].shape
+            lengths.append(length)
+        lengths_array = ops.asarray1i(lengths)
+        if len(spans) > 0:
+            output = Ragged(ops.xp.vstack(spans), lengths_array)
+        else:
+            output = Ragged(ops.xp.zeros((0, 0), dtype="i"), lengths_array)
+
+        assert output.dataXd.ndim == 2
+        return output
+
+    return ngram_suggester
 
 
 @registry.misc("spacy.ngram_range_suggester.v1")
 def build_ngram_range_suggester(min_size: int, max_size: int) -> Suggester:
     """Suggest all spans of the given lengths between a given min and max value - both inclusive.
     Spans are returned as a ragged array of integers. The array has two columns,
     indicating the start and end position."""
@@ -136,35 +106,33 @@
     default_config={
         "threshold": 0.5,
         "spans_key": "sc",
         "max_positive": None,
         "model": DEFAULT_SPANCAT_MODEL,
         "suggester": {"@misc": "spacy.ngram_suggester.v1", "sizes": [1, 2, 3]},
         "scorer": {"@scorers": "spacy.spancat_scorer.v1"},
+        "save_activations": False,
     },
     default_score_weights={"spans_sc_f": 1.0, "spans_sc_p": 0.0, "spans_sc_r": 0.0},
 )
 def make_spancat(
     nlp: Language,
     name: str,
     suggester: Suggester,
     model: Model[Tuple[List[Doc], Ragged], Floats2d],
     spans_key: str,
     scorer: Optional[Callable],
     threshold: float,
     max_positive: Optional[int],
+    save_activations: bool,
 ) -> "SpanCategorizer":
-    """Create a SpanCategorizer component and configure it for multi-label
-    classification to be able to assign multiple labels for each span.
-    The span categorizer consists of two
+    """Create a SpanCategorizer component. The span categorizer consists of two
     parts: a suggester function that proposes candidate spans, and a labeller
     model that predicts one or more labels for each span.
 
-    name (str): The component instance name, used to add entries to the
-        losses during training.
     suggester (Callable[[Iterable[Doc], Optional[Ops]], Ragged]): A function that suggests spans.
         Spans are returned as a ragged array with two integer columns, for the
         start and end positions.
     model (Model[Tuple[List[Doc], Ragged], Floats2d]): A model instance that
         is given a list of documents and (start, end) indices representing
         candidate span offsets. The model predicts a probability for each category
         for each span.
@@ -175,92 +143,26 @@
         Scorer.score_spans for the Doc.spans[spans_key] with overlapping
         spans allowed.
     threshold (float): Minimum probability to consider a prediction positive.
         Spans with a positive prediction will be saved on the Doc. Defaults to
         0.5.
     max_positive (Optional[int]): Maximum number of labels to consider positive
         per span. Defaults to None, indicating no limit.
+        save_activations (bool): save model activations in Doc when annotating.
     """
     return SpanCategorizer(
         nlp.vocab,
-        model=model,
         suggester=suggester,
-        name=name,
+        model=model,
         spans_key=spans_key,
-        negative_weight=None,
-        allow_overlap=True,
-        max_positive=max_positive,
         threshold=threshold,
-        scorer=scorer,
-        add_negative_label=False,
-    )
-
-
-@Language.factory(
-    "spancat_singlelabel",
-    assigns=["doc.spans"],
-    default_config={
-        "spans_key": "sc",
-        "model": DEFAULT_SPANCAT_SINGLELABEL_MODEL,
-        "negative_weight": 1.0,
-        "suggester": {"@misc": "spacy.ngram_suggester.v1", "sizes": [1, 2, 3]},
-        "scorer": {"@scorers": "spacy.spancat_scorer.v1"},
-        "allow_overlap": True,
-    },
-    default_score_weights={"spans_sc_f": 1.0, "spans_sc_p": 0.0, "spans_sc_r": 0.0},
-)
-def make_spancat_singlelabel(
-    nlp: Language,
-    name: str,
-    suggester: Suggester,
-    model: Model[Tuple[List[Doc], Ragged], Floats2d],
-    spans_key: str,
-    negative_weight: float,
-    allow_overlap: bool,
-    scorer: Optional[Callable],
-) -> "SpanCategorizer":
-    """Create a SpanCategorizer component and configure it for multi-class
-    classification. With this configuration each span can get at most one
-    label. The span categorizer consists of two
-    parts: a suggester function that proposes candidate spans, and a labeller
-    model that predicts one or more labels for each span.
-
-    name (str): The component instance name, used to add entries to the
-        losses during training.
-    suggester (Callable[[Iterable[Doc], Optional[Ops]], Ragged]): A function that suggests spans.
-        Spans are returned as a ragged array with two integer columns, for the
-        start and end positions.
-    model (Model[Tuple[List[Doc], Ragged], Floats2d]): A model instance that
-        is given a list of documents and (start, end) indices representing
-        candidate span offsets. The model predicts a probability for each category
-        for each span.
-    spans_key (str): Key of the doc.spans dict to save the spans under. During
-        initialization and training, the component will look for spans on the
-        reference document under the same key.
-    scorer (Optional[Callable]): The scoring method. Defaults to
-        Scorer.score_spans for the Doc.spans[spans_key] with overlapping
-        spans allowed.
-    negative_weight (float): Multiplier for the loss terms.
-        Can be used to downweight the negative samples if there are too many.
-    allow_overlap (bool): If True the data is assumed to contain overlapping spans.
-        Otherwise it produces non-overlapping spans greedily prioritizing
-        higher assigned label scores.
-    """
-    return SpanCategorizer(
-        nlp.vocab,
-        model=model,
-        suggester=suggester,
+        max_positive=max_positive,
         name=name,
-        spans_key=spans_key,
-        negative_weight=negative_weight,
-        allow_overlap=allow_overlap,
-        max_positive=1,
-        add_negative_label=True,
-        threshold=None,
         scorer=scorer,
+        save_activations=save_activations,
     )
 
 
 def spancat_score(examples: Iterable[Example], **kwargs) -> Dict[str, Any]:
     kwargs = dict(kwargs)
     attr_prefix = "spans_"
     key = kwargs["spans_key"]
@@ -274,131 +176,74 @@
 
 
 @registry.scorers("spacy.spancat_scorer.v1")
 def make_spancat_scorer():
     return spancat_score
 
 
-@dataclass
-class _Intervals:
-    """
-    Helper class to avoid storing overlapping spans.
-    """
-
-    def __init__(self):
-        self.ranges = set()
-
-    def add(self, i, j):
-        for e in range(i, j):
-            self.ranges.add(e)
-
-    def __contains__(self, rang):
-        i, j = rang
-        for e in range(i, j):
-            if e in self.ranges:
-                return True
-        return False
-
-
 class SpanCategorizer(TrainablePipe):
     """Pipeline component to label spans of text.
 
     DOCS: https://spacy.io/api/spancategorizer
     """
 
     def __init__(
         self,
         vocab: Vocab,
         model: Model[Tuple[List[Doc], Ragged], Floats2d],
         suggester: Suggester,
         name: str = "spancat",
         *,
-        add_negative_label: bool = False,
         spans_key: str = "spans",
-        negative_weight: Optional[float] = 1.0,
-        allow_overlap: Optional[bool] = True,
+        threshold: float = 0.5,
         max_positive: Optional[int] = None,
-        threshold: Optional[float] = 0.5,
         scorer: Optional[Callable] = spancat_score,
+        save_activations: bool = False,
     ) -> None:
-        """Initialize the multi-label or multi-class span categorizer.
-
+        """Initialize the span categorizer.
         vocab (Vocab): The shared vocabulary.
         model (thinc.api.Model): The Thinc Model powering the pipeline component.
-            For multi-class classification (single label per span) we recommend
-            using a Softmax classifier as a the final layer, while for multi-label
-            classification (multiple possible labels per span) we recommend Logistic.
-        suggester (Callable[[Iterable[Doc], Optional[Ops]], Ragged]): A function that suggests spans.
-            Spans are returned as a ragged array with two integer columns, for the
-            start and end positions.
         name (str): The component instance name, used to add entries to the
             losses during training.
         spans_key (str): Key of the Doc.spans dict to save the spans under.
             During initialization and training, the component will look for
             spans on the reference document under the same key. Defaults to
             `"spans"`.
-        add_negative_label (bool): Learn to predict a special 'negative_label'
-            when a Span is not annotated.
-        threshold (Optional[float]): Minimum probability to consider a prediction
-            positive. Defaults to 0.5. Spans with a positive prediction will be saved
-            on the Doc.
+        threshold (float): Minimum probability to consider a prediction
+            positive. Spans with a positive prediction will be saved on the Doc.
+            Defaults to 0.5.
         max_positive (Optional[int]): Maximum number of labels to consider
             positive per span. Defaults to None, indicating no limit.
-        negative_weight (float): Multiplier for the loss terms.
-            Can be used to downweight the negative samples if there are too many
-            when add_negative_label is True. Otherwise its unused.
-        allow_overlap (bool): If True the data is assumed to contain overlapping spans.
-            Otherwise it produces non-overlapping spans greedily prioritizing
-            higher assigned label scores. Only used when max_positive is 1.
         scorer (Optional[Callable]): The scoring method. Defaults to
             Scorer.score_spans for the Doc.spans[spans_key] with overlapping
             spans allowed.
 
         DOCS: https://spacy.io/api/spancategorizer#init
         """
         self.cfg = {
             "labels": [],
             "spans_key": spans_key,
             "threshold": threshold,
             "max_positive": max_positive,
-            "negative_weight": negative_weight,
-            "allow_overlap": allow_overlap,
         }
         self.vocab = vocab
         self.suggester = suggester
         self.model = model
         self.name = name
         self.scorer = scorer
-        self.add_negative_label = add_negative_label
-        if not allow_overlap and max_positive is not None and max_positive > 1:
-            raise ValueError(Errors.E1051.format(max_positive=max_positive))
+        self.save_activations = save_activations
 
     @property
     def key(self) -> str:
         """Key of the doc.spans dict to save the spans under. During
         initialization and training, the component will look for spans on the
         reference document under the same key.
         """
         return str(self.cfg["spans_key"])
 
-    def _allow_extra_label(self) -> None:
-        """Raise an error if the component can not add any more labels."""
-        nO = None
-        if self.model.has_dim("nO"):
-            nO = self.model.get_dim("nO")
-        elif self.model.has_ref("output_layer") and self.model.get_ref(
-            "output_layer"
-        ).has_dim("nO"):
-            nO = self.model.get_ref("output_layer").get_dim("nO")
-        if nO is not None and nO == self._n_labels:
-            if not self.is_resizable:
-                raise ValueError(
-                    Errors.E922.format(name=self.name, nO=self.model.get_dim("nO"))
-                )
-
     def add_label(self, label: str) -> int:
         """Add a new label to the pipe.
 
         label (str): The label to add.
         RETURNS (int): 0 if label is already present, otherwise 1.
 
         DOCS: https://spacy.io/api/spancategorizer#add_label
@@ -424,49 +269,28 @@
     def label_data(self) -> List[str]:
         """RETURNS (List[str]): Information about the component's labels.
 
         DOCS: https://spacy.io/api/spancategorizer#label_data
         """
         return list(self.labels)
 
-    @property
-    def _label_map(self) -> Dict[str, int]:
-        """RETURNS (Dict[str, int]): The label map."""
-        return {label: i for i, label in enumerate(self.labels)}
-
-    @property
-    def _n_labels(self) -> int:
-        """RETURNS (int): Number of labels."""
-        if self.add_negative_label:
-            return len(self.labels) + 1
-        else:
-            return len(self.labels)
-
-    @property
-    def _negative_label_i(self) -> Union[int, None]:
-        """RETURNS (Union[int, None]): Index of the negative label."""
-        if self.add_negative_label:
-            return len(self.label_data)
-        else:
-            return None
-
-    def predict(self, docs: Iterable[Doc]):
+    def predict(self, docs: Iterable[Doc]) -> ActivationsT:
         """Apply the pipeline's model to a batch of docs, without modifying them.
 
         docs (Iterable[Doc]): The documents to predict.
         RETURNS: The models prediction for each document.
 
         DOCS: https://spacy.io/api/spancategorizer#predict
         """
         indices = self.suggester(docs, ops=self.model.ops)
         if indices.lengths.sum() == 0:
             scores = self.model.ops.alloc2f(0, 0)
         else:
             scores = self.model.predict((docs, indices))  # type: ignore
-        return indices, scores
+        return {"indices": indices, "scores": scores}
 
     def set_candidates(
         self, docs: Iterable[Doc], *, candidates_key: str = "candidates"
     ) -> None:
         """Use the spancat suggester to add a list of span candidates to a list of docs.
         This method is intended to be used for debugging purposes.
 
@@ -478,40 +302,40 @@
         suggester_output = self.suggester(docs, ops=self.model.ops)
 
         for candidates, doc in zip(suggester_output, docs):  # type: ignore
             doc.spans[candidates_key] = []
             for index in candidates.dataXd:
                 doc.spans[candidates_key].append(doc[index[0] : index[1]])
 
-    def set_annotations(self, docs: Iterable[Doc], indices_scores) -> None:
+    def set_annotations(self, docs: Iterable[Doc], activations: ActivationsT) -> None:
         """Modify a batch of Doc objects, using pre-computed scores.
 
         docs (Iterable[Doc]): The documents to modify.
-        scores: The scores to set, produced by SpanCategorizer.predict.
+        activations: ActivationsT: The activations, produced by SpanCategorizer.predict.
 
         DOCS: https://spacy.io/api/spancategorizer#set_annotations
         """
-        indices, scores = indices_scores
+        labels = self.labels
+
+        indices = activations["indices"]
+        assert isinstance(indices, Ragged)
+        scores = cast(Floats2d, activations["scores"])
+
         offset = 0
         for i, doc in enumerate(docs):
             indices_i = indices[i].dataXd
-            allow_overlap = cast(bool, self.cfg["allow_overlap"])
-            if self.cfg["max_positive"] == 1:
-                doc.spans[self.key] = self._make_span_group_singlelabel(
-                    doc,
-                    indices_i,
-                    scores[offset : offset + indices.lengths[i]],
-                    allow_overlap,
-                )
-            else:
-                doc.spans[self.key] = self._make_span_group_multilabel(
-                    doc,
-                    indices_i,
-                    scores[offset : offset + indices.lengths[i]],
-                )
+            if self.save_activations:
+                doc.activations[self.name] = {}
+                doc.activations[self.name]["indices"] = indices_i
+                doc.activations[self.name]["scores"] = scores[
+                    offset : offset + indices.lengths[i]
+                ]
+            doc.spans[self.key] = self._make_span_group(
+                doc, indices_i, scores[offset : offset + indices.lengths[i]], labels  # type: ignore[arg-type]
+            )
             offset += indices.lengths[i]
 
     def update(
         self,
         examples: Iterable[Example],
         *,
         drop: float = 0.0,
@@ -563,19 +387,17 @@
 
         DOCS: https://spacy.io/api/spancategorizer#get_loss
         """
         spans, scores = spans_scores
         spans = Ragged(
             self.model.ops.to_numpy(spans.data), self.model.ops.to_numpy(spans.lengths)
         )
+        label_map = {label: i for i, label in enumerate(self.labels)}
         target = numpy.zeros(scores.shape, dtype=scores.dtype)
-        if self.add_negative_label:
-            negative_spans = numpy.ones((scores.shape[0]))
         offset = 0
-        label_map = self._label_map
         for i, eg in enumerate(examples):
             # Map (start, end) offset of spans to the row in the d_scores array,
             # so that we can adjust the gradient for predictions that were
             # in the gold standard.
             spans_index = {}
             spans_i = spans[i].dataXd
             for j in range(spans.lengths[i]):
@@ -584,36 +406,26 @@
                 spans_index[(start, end)] = offset + j
             for gold_span in self._get_aligned_spans(eg):
                 key = (gold_span.start, gold_span.end)
                 if key in spans_index:
                     row = spans_index[key]
                     k = label_map[gold_span.label_]
                     target[row, k] = 1.0
-                    if self.add_negative_label:
-                        # delete negative label target.
-                        negative_spans[row] = 0.0
             # The target is a flat array for all docs. Track the position
             # we're at within the flat array.
             offset += spans.lengths[i]
         target = self.model.ops.asarray(target, dtype="f")  # type: ignore
-        if self.add_negative_label:
-            negative_samples = numpy.nonzero(negative_spans)[0]
-            target[negative_samples, self._negative_label_i] = 1.0  # type: ignore
         # The target will have the values 0 (for untrue predictions) or 1
         # (for true predictions).
         # The scores should be in the range [0, 1].
         # If the prediction is 0.9 and it's true, the gradient
         # will be -0.1 (0.9 - 1.0).
         # If the prediction is 0.9 and it's false, the gradient will be
         # 0.9 (0.9 - 0.0)
         d_scores = scores - target
-        if self.add_negative_label:
-            neg_weight = cast(float, self.cfg["negative_weight"])
-            if neg_weight != 1.0:
-                d_scores[negative_samples] *= neg_weight
         loss = float((d_scores**2).sum())
         return loss, d_scores
 
     def initialize(
         self,
         get_examples: Callable[[], Iterable[Example]],
         *,
@@ -642,116 +454,49 @@
                     self.add_label(span.label_)
             if len(subbatch) < 10:
                 subbatch.append(eg)
         self._require_labels()
         if subbatch:
             docs = [eg.x for eg in subbatch]
             spans = build_ngram_suggester(sizes=[1])(docs)
-            Y = self.model.ops.alloc2f(spans.dataXd.shape[0], self._n_labels)
+            Y = self.model.ops.alloc2f(spans.dataXd.shape[0], len(self.labels))
             self.model.initialize(X=(docs, spans), Y=Y)
         else:
             self.model.initialize()
 
     def _validate_categories(self, examples: Iterable[Example]):
         # TODO
         pass
 
     def _get_aligned_spans(self, eg: Example):
         return eg.get_aligned_spans_y2x(
             eg.reference.spans.get(self.key, []), allow_overlap=True
         )
 
-    def _make_span_group_multilabel(
-        self,
-        doc: Doc,
-        indices: Ints2d,
-        scores: Floats2d,
+    def _make_span_group(
+        self, doc: Doc, indices: Ints2d, scores: Floats2d, labels: List[str]
     ) -> SpanGroup:
-        """Find the top-k labels for each span (k=max_positive)."""
         spans = SpanGroup(doc, name=self.key)
-        if scores.size == 0:
-            return spans
-        scores = self.model.ops.to_numpy(scores)
-        indices = self.model.ops.to_numpy(indices)
-        threshold = self.cfg["threshold"]
         max_positive = self.cfg["max_positive"]
+        threshold = self.cfg["threshold"]
 
         keeps = scores >= threshold
+        ranked = (scores * -1).argsort()  # type: ignore
         if max_positive is not None:
             assert isinstance(max_positive, int)
-            if self.add_negative_label:
-                negative_scores = numpy.copy(scores[:, self._negative_label_i])
-                scores[:, self._negative_label_i] = -numpy.inf
-                ranked = (scores * -1).argsort()  # type: ignore
-                scores[:, self._negative_label_i] = negative_scores
-            else:
-                ranked = (scores * -1).argsort()  # type: ignore
             span_filter = ranked[:, max_positive:]
             for i, row in enumerate(span_filter):
                 keeps[i, row] = False
+        spans.attrs["scores"] = scores[keeps].flatten()
 
-        attrs_scores = []
-        for i in range(indices.shape[0]):
-            start = indices[i, 0]
-            end = indices[i, 1]
-            for j, keep in enumerate(keeps[i]):
-                if keep:
-                    if j != self._negative_label_i:
-                        spans.append(Span(doc, start, end, label=self.labels[j]))
-                        attrs_scores.append(scores[i, j])
-        spans.attrs["scores"] = numpy.array(attrs_scores)
-        return spans
-
-    def _make_span_group_singlelabel(
-        self,
-        doc: Doc,
-        indices: Ints2d,
-        scores: Floats2d,
-        allow_overlap: bool = True,
-    ) -> SpanGroup:
-        """Find the argmax label for each span."""
-        # Handle cases when there are zero suggestions
-        if scores.size == 0:
-            return SpanGroup(doc, name=self.key)
-        scores = self.model.ops.to_numpy(scores)
         indices = self.model.ops.to_numpy(indices)
-        predicted = scores.argmax(axis=1)
-        argmax_scores = numpy.take_along_axis(
-            scores, numpy.expand_dims(predicted, 1), axis=1
-        )
-        keeps = numpy.ones(predicted.shape, dtype=bool)
-        # Remove samples where the negative label is the argmax.
-        if self.add_negative_label:
-            keeps = numpy.logical_and(keeps, predicted != self._negative_label_i)
-        # Filter samples according to threshold.
-        threshold = self.cfg["threshold"]
-        if threshold is not None:
-            keeps = numpy.logical_and(keeps, (argmax_scores >= threshold).squeeze())
-        # Sort spans according to argmax probability
-        if not allow_overlap:
-            # Get the probabilities
-            sort_idx = (argmax_scores.squeeze() * -1).argsort()
-            argmax_scores = argmax_scores[sort_idx]
-            predicted = predicted[sort_idx]
-            indices = indices[sort_idx]
-            keeps = keeps[sort_idx]
-        seen = _Intervals()
-        spans = SpanGroup(doc, name=self.key)
-        attrs_scores = []
-        for i in range(indices.shape[0]):
-            if not keeps[i]:
-                continue
+        keeps = self.model.ops.to_numpy(keeps)
 
-            label = predicted[i]
+        for i in range(indices.shape[0]):
             start = indices[i, 0]
             end = indices[i, 1]
 
-            if not allow_overlap:
-                if (start, end) in seen:
-                    continue
-                else:
-                    seen.add(start, end)
-            attrs_scores.append(argmax_scores[i])
-            spans.append(Span(doc, start, end, label=self.labels[label]))
+            for j, keep in enumerate(keeps[i]):
+                if keep:
+                    spans.append(Span(doc, start, end, label=labels[j]))
 
-        spans.attrs["scores"] = numpy.array(attrs_scores)
         return spans
```

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/tagger.pyx` & `spacy-4.0.0.dev0/spacy/pipeline/tagger.pyx`

 * *Files 6% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 # cython: infer_types=True, profile=True, binding=True
-from typing import Callable, Optional
+from typing import Callable, Dict, Iterable, List, Optional, Union
+from typing import Tuple
 import numpy
 import srsly
-from thinc.api import Model, set_dropout_rate, SequenceCategoricalCrossentropy, Config
-from thinc.types import Floats2d
+from thinc.api import Model, set_dropout_rate, Config
+from thinc.legacy import LegacySequenceCategoricalCrossentropy
+from thinc.types import Floats2d, Ints1d
 import warnings
 from itertools import islice
 
 from ..tokens.doc cimport Doc
 from ..morphology cimport Morphology
 from ..vocab cimport Vocab
 
@@ -18,14 +20,17 @@
 from ..parts_of_speech import X
 from ..errors import Errors, Warnings
 from ..scorer import Scorer
 from ..training import validate_examples, validate_get_examples
 from ..util import registry
 from .. import util
 
+
+ActivationsT = Dict[str, Union[List[Floats2d], List[Ints1d]]]
+
 # See #9050
 BACKWARD_OVERWRITE = False
 
 default_model_config = """
 [model]
 @architectures = "spacy.Tagger.v2"
 
@@ -41,34 +46,41 @@
 """
 DEFAULT_TAGGER_MODEL = Config().from_str(default_model_config)["model"]
 
 
 @Language.factory(
     "tagger",
     assigns=["token.tag"],
-    default_config={"model": DEFAULT_TAGGER_MODEL, "overwrite": False, "scorer": {"@scorers": "spacy.tagger_scorer.v1"}, "neg_prefix": "!", "label_smoothing": 0.0},
+    default_config={
+        "model": DEFAULT_TAGGER_MODEL,
+        "overwrite": False,
+        "scorer": {"@scorers": "spacy.tagger_scorer.v1"},
+        "neg_prefix": "!",
+        "save_activations": False,
+    },
     default_score_weights={"tag_acc": 1.0},
 )
 def make_tagger(
     nlp: Language,
     name: str,
     model: Model,
     overwrite: bool,
     scorer: Optional[Callable],
     neg_prefix: str,
-    label_smoothing: float,
+    save_activations: bool,
 ):
     """Construct a part-of-speech tagger component.
 
     model (Model[List[Doc], List[Floats2d]]): A model instance that predicts
         the tag probabilities. The output vectors should match the number of tags
         in size, and be normalized as probabilities (all scores between 0 and 1,
         with the rows summing to 1).
     """
-    return Tagger(nlp.vocab, model, name, overwrite=overwrite, scorer=scorer, neg_prefix=neg_prefix, label_smoothing=label_smoothing)
+    return Tagger(nlp.vocab, model, name, overwrite=overwrite, scorer=scorer, neg_prefix=neg_prefix,
+                  save_activations=save_activations)
 
 
 def tagger_score(examples, **kwargs):
     return Scorer.score_token_attr(examples, "tag", **kwargs)
 
 
 @registry.scorers("spacy.tagger_scorer.v1")
@@ -86,34 +98,36 @@
         vocab,
         model,
         name="tagger",
         *,
         overwrite=BACKWARD_OVERWRITE,
         scorer=tagger_score,
         neg_prefix="!",
-        label_smoothing=0.0,
+        save_activations: bool = False,
     ):
         """Initialize a part-of-speech tagger.
 
         vocab (Vocab): The shared vocabulary.
         model (thinc.api.Model): The Thinc Model powering the pipeline component.
         name (str): The component instance name, used to add entries to the
             losses during training.
         scorer (Optional[Callable]): The scoring method. Defaults to
             Scorer.score_token_attr for the attribute "tag".
+        save_activations (bool): save model activations in Doc when annotating.
 
         DOCS: https://spacy.io/api/tagger#init
         """
         self.vocab = vocab
         self.model = model
         self.name = name
         self._rehearsal_model = None
-        cfg = {"labels": [], "overwrite": overwrite, "neg_prefix": neg_prefix, "label_smoothing": label_smoothing}
+        cfg = {"labels": [], "overwrite": overwrite, "neg_prefix": neg_prefix}
         self.cfg = dict(sorted(cfg.items()))
         self.scorer = scorer
+        self.save_activations = save_activations
 
     @property
     def labels(self):
         """The labels currently added to the component. Note that even for a
         blank component, this will always include the built-in coarse-grained
         part-of-speech tags by default.
 
@@ -124,58 +138,63 @@
         return tuple(self.cfg["labels"])
 
     @property
     def label_data(self):
         """Data about the labels currently added to the component."""
         return tuple(self.cfg["labels"])
 
-    def predict(self, docs):
+    def predict(self, docs) -> ActivationsT:
         """Apply the pipeline's model to a batch of docs, without modifying them.
 
         docs (Iterable[Doc]): The documents to predict.
         RETURNS: The models prediction for each document.
 
         DOCS: https://spacy.io/api/tagger#predict
         """
         if not any(len(doc) for doc in docs):
             # Handle cases where there are no tokens in any docs.
             n_labels = len(self.labels)
             guesses = [self.model.ops.alloc((0, n_labels)) for doc in docs]
             assert len(guesses) == len(docs)
-            return guesses
+            return {"probabilities": guesses, "label_ids": guesses}
         scores = self.model.predict(docs)
         assert len(scores) == len(docs), (len(scores), len(docs))
         guesses = self._scores2guesses(scores)
         assert len(guesses) == len(docs)
-        return guesses
+        return {"probabilities": scores, "label_ids": guesses}
 
     def _scores2guesses(self, scores):
         guesses = []
         for doc_scores in scores:
             doc_guesses = doc_scores.argmax(axis=1)
             if not isinstance(doc_guesses, numpy.ndarray):
                 doc_guesses = doc_guesses.get()
             guesses.append(doc_guesses)
         return guesses
 
-    def set_annotations(self, docs, batch_tag_ids):
+    def set_annotations(self, docs: Iterable[Doc], activations: ActivationsT):
         """Modify a batch of documents, using pre-computed scores.
 
         docs (Iterable[Doc]): The documents to modify.
-        batch_tag_ids: The IDs to set, produced by Tagger.predict.
+        activations (ActivationsT): The activations used for setting annotations, produced by Tagger.predict.
 
         DOCS: https://spacy.io/api/tagger#set_annotations
         """
+        batch_tag_ids = activations["label_ids"]
         if isinstance(docs, Doc):
             docs = [docs]
         cdef Doc doc
         cdef Vocab vocab = self.vocab
         cdef bint overwrite = self.cfg["overwrite"]
         labels = self.labels
         for i, doc in enumerate(docs):
+            if self.save_activations:
+                doc.activations[self.name] = {}
+                for act_name, acts in activations.items():
+                    doc.activations[self.name][act_name] = acts[i]
             doc_tag_ids = batch_tag_ids[i]
             if hasattr(doc_tag_ids, "get"):
                 doc_tag_ids = doc_tag_ids.get()
             for j, tag_id in enumerate(doc_tag_ids):
                 if doc.c[j].tag == 0 or overwrite:
                     doc.c[j].tag = self.vocab.strings[labels[tag_id]]
 
@@ -223,46 +242,65 @@
         sgd (thinc.api.Optimizer): The optimizer.
         losses (Dict[str, float]): Optional record of the loss during training.
             Updated using the component name as the key.
         RETURNS (Dict[str, float]): The updated losses dictionary.
 
         DOCS: https://spacy.io/api/tagger#rehearse
         """
-        loss_func = SequenceCategoricalCrossentropy()
         if losses is None:
             losses = {}
         losses.setdefault(self.name, 0.0)
         validate_examples(examples, "Tagger.rehearse")
         docs = [eg.predicted for eg in examples]
         if self._rehearsal_model is None:
             return losses
         if not any(len(doc) for doc in docs):
             # Handle cases where there are no tokens in any docs.
             return losses
         set_dropout_rate(self.model, drop)
         tag_scores, bp_tag_scores = self.model.begin_update(docs)
         tutor_tag_scores, _ = self._rehearsal_model.begin_update(docs)
-        grads, loss = loss_func(tag_scores, tutor_tag_scores)
+        loss, grads = self.get_teacher_student_loss(tutor_tag_scores, tag_scores)
         bp_tag_scores(grads)
-        self.finish_update(sgd)
+        if sgd is not None:
+            self.finish_update(sgd)
         losses[self.name] += loss
         return losses
 
+    def get_teacher_student_loss(
+        self, teacher_scores: List[Floats2d], student_scores: List[Floats2d]
+    ) -> Tuple[float, List[Floats2d]]:
+        """Calculate the loss and its gradient for a batch of student
+        scores, relative to teacher scores.
+
+        teacher_scores: Scores representing the teacher model's predictions.
+        student_scores: Scores representing the student model's predictions.
+
+        RETURNS (Tuple[float, float]): The loss and the gradient.
+        
+        DOCS: https://spacy.io/api/tagger#get_teacher_student_loss
+        """
+        loss_func = LegacySequenceCategoricalCrossentropy(normalize=False)
+        d_scores, loss = loss_func(student_scores, teacher_scores)
+        if self.model.ops.xp.isnan(loss):
+            raise ValueError(Errors.E910.format(name=self.name))
+        return float(loss), d_scores
+
     def get_loss(self, examples, scores):
         """Find the loss and gradient of loss for the batch of documents and
         their predicted scores.
 
         examples (Iterable[Examples]): The batch of examples.
         scores: Scores representing the model's predictions.
         RETURNS (Tuple[float, float]): The loss and the gradient.
 
         DOCS: https://spacy.io/api/tagger#get_loss
         """
         validate_examples(examples, "Tagger.get_loss")
-        loss_func = SequenceCategoricalCrossentropy(names=self.labels, normalize=False, neg_prefix=self.cfg["neg_prefix"], label_smoothing=self.cfg["label_smoothing"])
+        loss_func = LegacySequenceCategoricalCrossentropy(names=self.labels, normalize=False, neg_prefix=self.cfg["neg_prefix"])
         # Convert empty tag "" to missing value None so that both misaligned
         # tokens and tokens with missing annotation have the default missing
         # value None.
         truths = []
         for eg in examples:
             eg_truths = [tag if tag is not "" else None for tag in eg.get_aligned("TAG", as_string=True)]
             truths.append(eg_truths)
```

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/textcat.py` & `spacy-4.0.0.dev0/spacy/pipeline/textcat.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import Iterable, Tuple, Optional, Dict, List, Callable, Any
+from typing import Iterable, Tuple, Optional, Dict, List, Callable, Any, Union
 from thinc.api import get_array_module, Model, Optimizer, set_dropout_rate, Config
 from thinc.types import Floats2d
 import numpy
 from itertools import islice
 
 from .trainable_pipe import TrainablePipe
 from ..language import Language
@@ -10,14 +10,17 @@
 from ..errors import Errors
 from ..scorer import Scorer
 from ..tokens import Doc
 from ..util import registry
 from ..vocab import Vocab
 
 
+ActivationsT = Dict[str, Floats2d]
+
+
 single_label_default_config = """
 [model]
 @architectures = "spacy.TextCatEnsemble.v2"
 
 [model.tok2vec]
 @architectures = "spacy.Tok2Vec.v2"
 
@@ -71,14 +74,15 @@
 @Language.factory(
     "textcat",
     assigns=["doc.cats"],
     default_config={
         "threshold": 0.0,
         "model": DEFAULT_SINGLE_TEXTCAT_MODEL,
         "scorer": {"@scorers": "spacy.textcat_scorer.v2"},
+        "save_activations": False,
     },
     default_score_weights={
         "cats_score": 1.0,
         "cats_score_desc": None,
         "cats_micro_p": None,
         "cats_micro_r": None,
         "cats_micro_f": None,
@@ -91,25 +95,34 @@
 )
 def make_textcat(
     nlp: Language,
     name: str,
     model: Model[List[Doc], List[Floats2d]],
     threshold: float,
     scorer: Optional[Callable],
+    save_activations: bool,
 ) -> "TextCategorizer":
     """Create a TextCategorizer component. The text categorizer predicts categories
     over a whole document. It can learn one or more labels, and the labels are considered
     to be mutually exclusive (i.e. one true label per doc).
 
     model (Model[List[Doc], List[Floats2d]]): A model instance that predicts
         scores for each category.
     threshold (float): Cutoff to consider a prediction "positive".
     scorer (Optional[Callable]): The scoring method.
+    save_activations (bool): save model activations in Doc when annotating.
     """
-    return TextCategorizer(nlp.vocab, model, name, threshold=threshold, scorer=scorer)
+    return TextCategorizer(
+        nlp.vocab,
+        model,
+        name,
+        threshold=threshold,
+        scorer=scorer,
+        save_activations=save_activations,
+    )
 
 
 def textcat_score(examples: Iterable[Example], **kwargs) -> Dict[str, Any]:
     return Scorer.score_cats(
         examples,
         "cats",
         multi_label=False,
@@ -132,14 +145,15 @@
         self,
         vocab: Vocab,
         model: Model,
         name: str = "textcat",
         *,
         threshold: float,
         scorer: Optional[Callable] = textcat_score,
+        save_activations: bool = False,
     ) -> None:
         """Initialize a text categorizer for single-label classification.
 
         vocab (Vocab): The shared vocabulary.
         model (thinc.api.Model): The Thinc Model powering the pipeline component.
         name (str): The component instance name, used to add entries to the
             losses during training.
@@ -157,14 +171,15 @@
         cfg: Dict[str, Any] = {
             "labels": [],
             "threshold": threshold,
             "positive_label": None,
         }
         self.cfg = dict(cfg)
         self.scorer = scorer
+        self.save_activations = save_activations
 
     @property
     def support_missing_values(self):
         # There are no missing values as the textcat should always
         # predict exactly one label. All other labels are 0.0
         # Subclasses may override this property to change internal behaviour.
         return False
@@ -181,43 +196,47 @@
     def label_data(self) -> List[str]:
         """RETURNS (List[str]): Information about the component's labels.
 
         DOCS: https://spacy.io/api/textcategorizer#label_data
         """
         return self.labels  # type: ignore[return-value]
 
-    def predict(self, docs: Iterable[Doc]):
+    def predict(self, docs: Iterable[Doc]) -> ActivationsT:
         """Apply the pipeline's model to a batch of docs, without modifying them.
 
         docs (Iterable[Doc]): The documents to predict.
         RETURNS: The models prediction for each document.
 
         DOCS: https://spacy.io/api/textcategorizer#predict
         """
         if not any(len(doc) for doc in docs):
             # Handle cases where there are no tokens in any docs.
             tensors = [doc.tensor for doc in docs]
             xp = self.model.ops.xp
             scores = xp.zeros((len(list(docs)), len(self.labels)))
-            return scores
+            return {"probabilities": scores}
         scores = self.model.predict(docs)
         scores = self.model.ops.asarray(scores)
-        return scores
+        return {"probabilities": scores}
 
-    def set_annotations(self, docs: Iterable[Doc], scores) -> None:
+    def set_annotations(self, docs: Iterable[Doc], activations: ActivationsT) -> None:
         """Modify a batch of Doc objects, using pre-computed scores.
 
         docs (Iterable[Doc]): The documents to modify.
         scores: The scores to set, produced by TextCategorizer.predict.
 
         DOCS: https://spacy.io/api/textcategorizer#set_annotations
         """
+        probs = activations["probabilities"]
         for i, doc in enumerate(docs):
+            if self.save_activations:
+                doc.activations[self.name] = {}
+                doc.activations[self.name]["probabilities"] = probs[i]
             for j, label in enumerate(self.labels):
-                doc.cats[label] = float(scores[i, j])
+                doc.cats[label] = float(probs[i, j])
 
     def update(
         self,
         examples: Iterable[Example],
         *,
         drop: float = 0.0,
         sgd: Optional[Optimizer] = None,
```

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/textcat_multilabel.py` & `spacy-4.0.0.dev0/spacy/pipeline/textcat_multilabel.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import Iterable, Optional, Dict, List, Callable, Any
+from typing import Iterable, Optional, Dict, List, Callable, Any, Union
 from thinc.types import Floats2d
 from thinc.api import Model, Config
 
 from itertools import islice
 
 from ..language import Language
 from ..training import Example, validate_get_examples
@@ -71,14 +71,15 @@
 @Language.factory(
     "textcat_multilabel",
     assigns=["doc.cats"],
     default_config={
         "threshold": 0.5,
         "model": DEFAULT_MULTI_TEXTCAT_MODEL,
         "scorer": {"@scorers": "spacy.textcat_multilabel_scorer.v2"},
+        "save_activations": False,
     },
     default_score_weights={
         "cats_score": 1.0,
         "cats_score_desc": None,
         "cats_micro_p": None,
         "cats_micro_r": None,
         "cats_micro_f": None,
@@ -91,27 +92,33 @@
 )
 def make_multilabel_textcat(
     nlp: Language,
     name: str,
     model: Model[List[Doc], List[Floats2d]],
     threshold: float,
     scorer: Optional[Callable],
+    save_activations: bool,
 ) -> "MultiLabel_TextCategorizer":
-    """Create a MultiLabel_TextCategorizer component. The text categorizer predicts categories
+    """Create a TextCategorizer component. The text categorizer predicts categories
     over a whole document. It can learn one or more labels, and the labels are considered
     to be non-mutually exclusive, which means that there can be zero or more labels
     per doc).
 
     model (Model[List[Doc], List[Floats2d]]): A model instance that predicts
         scores for each category.
     threshold (float): Cutoff to consider a prediction "positive".
     scorer (Optional[Callable]): The scoring method.
     """
     return MultiLabel_TextCategorizer(
-        nlp.vocab, model, name, threshold=threshold, scorer=scorer
+        nlp.vocab,
+        model,
+        name,
+        threshold=threshold,
+        scorer=scorer,
+        save_activations=save_activations,
     )
 
 
 def textcat_multilabel_score(examples: Iterable[Example], **kwargs) -> Dict[str, Any]:
     return Scorer.score_cats(
         examples,
         "cats",
@@ -135,33 +142,36 @@
         self,
         vocab: Vocab,
         model: Model,
         name: str = "textcat_multilabel",
         *,
         threshold: float,
         scorer: Optional[Callable] = textcat_multilabel_score,
+        save_activations: bool = False,
     ) -> None:
         """Initialize a text categorizer for multi-label classification.
 
         vocab (Vocab): The shared vocabulary.
         model (thinc.api.Model): The Thinc Model powering the pipeline component.
         name (str): The component instance name, used to add entries to the
             losses during training.
         threshold (float): Cutoff to consider a prediction "positive".
         scorer (Optional[Callable]): The scoring method.
+        save_activations (bool): save model activations in Doc when annotating.
 
         DOCS: https://spacy.io/api/textcategorizer#init
         """
         self.vocab = vocab
         self.model = model
         self.name = name
         self._rehearsal_model = None
         cfg = {"labels": [], "threshold": threshold}
         self.cfg = dict(cfg)
         self.scorer = scorer
+        self.save_activations = save_activations
 
     @property
     def support_missing_values(self):
         return True
 
     def initialize(  # type: ignore[override]
         self,
```

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/tok2vec.py` & `spacy-4.0.0.dev0/spacy/pipeline/tok2vec.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/trainable_pipe.pyx` & `spacy-4.0.0.dev0/spacy/pipeline/trainable_pipe.pyx`

 * *Files 15% similar despite different names*

```diff
@@ -1,16 +1,17 @@
 # cython: infer_types=True, profile=True, binding=True
 from typing import Iterable, Iterator, Optional, Dict, Tuple, Callable
 import srsly
 from thinc.api import set_dropout_rate, Model, Optimizer
+import warnings
 
 from ..tokens.doc cimport Doc
 
-from ..training import validate_examples
-from ..errors import Errors
+from ..training import validate_examples, validate_distillation_examples
+from ..errors import Errors, Warnings
 from .pipe import Pipe, deserialize_config
 from .. import util
 from ..vocab import Vocab
 from ..language import Language
 from ..training import Example
 
 
@@ -51,14 +52,61 @@
         try:
             scores = self.predict([doc])
             self.set_annotations([doc], scores)
             return doc
         except Exception as e:
             error_handler(self.name, self, [doc], e)
 
+
+    def distill(self,
+               teacher_pipe: Optional["TrainablePipe"],
+               examples: Iterable["Example"],
+               *,
+               drop: float=0.0,
+               sgd: Optional[Optimizer]=None,
+               losses: Optional[Dict[str, float]]=None) -> Dict[str, float]:
+        """Train a pipe (the student) on the predictions of another pipe
+        (the teacher). The student is typically trained on the probability
+        distribution of the teacher, but details may differ per pipe.
+
+        teacher_pipe (Optional[TrainablePipe]): The teacher pipe to learn
+            from.
+        examples (Iterable[Example]): Distillation examples. The reference
+            and predicted docs must have the same number of tokens and the
+            same orthography.
+        drop (float): dropout rate.
+        sgd (Optional[Optimizer]): An optimizer. Will be created via
+            create_optimizer if not set.
+        losses (Optional[Dict[str, float]]): Optional record of loss during
+            distillation.
+        RETURNS: The updated losses dictionary.
+        
+        DOCS: https://spacy.io/api/pipe#distill
+        """
+        # By default we require a teacher pipe, but there are downstream
+        # implementations that don't require a pipe.
+        if teacher_pipe is None:
+            raise ValueError(Errors.E4002.format(name=self.name))
+        if losses is None:
+            losses = {}
+        losses.setdefault(self.name, 0.0)
+        validate_distillation_examples(examples, "TrainablePipe.distill")
+        set_dropout_rate(self.model, drop)
+        for node in teacher_pipe.model.walk():
+            if node.name == "softmax":
+                node.attrs["softmax_normalize"] = True
+        teacher_scores = teacher_pipe.model.predict([eg.reference for eg in examples])
+        student_scores, bp_student_scores = self.model.begin_update([eg.predicted for eg in examples])
+        loss, d_scores = self.get_teacher_student_loss(teacher_scores, student_scores)
+        bp_student_scores(d_scores)
+        if sgd is not None:
+            self.finish_update(sgd)
+        losses[self.name] += loss
+        return losses
+
     def pipe(self, stream: Iterable[Doc], *, batch_size: int=128) -> Iterator[Doc]:
         """Apply the pipe to a stream of documents. This usually happens under
         the hood when the nlp object is called on a text and all components are
         applied to the Doc.
 
         stream (Iterable[Doc]): A stream of documents.
         batch_size (int): The number of documents to buffer.
@@ -164,14 +212,27 @@
         scores: Scores representing the model's predictions.
         RETURNS (Tuple[float, float]): The loss and the gradient.
 
         DOCS: https://spacy.io/api/pipe#get_loss
         """
         raise NotImplementedError(Errors.E931.format(parent="TrainablePipe", method="get_loss", name=self.name))
 
+    def get_teacher_student_loss(self, teacher_scores, student_scores):
+        """Calculate the loss and its gradient for a batch of student
+        scores, relative to teacher scores.
+
+        teacher_scores: Scores representing the teacher model's predictions.
+        student_scores: Scores representing the student model's predictions.
+
+        RETURNS (Tuple[float, float]): The loss and the gradient.
+        
+        DOCS: https://spacy.io/api/pipe#get_teacher_student_loss
+        """
+        raise NotImplementedError(Errors.E931.format(parent="TrainablePipe", method="get_teacher_student_loss", name=self.name))
+
     def create_optimizer(self) -> Optimizer:
         """Create an optimizer for the pipeline component.
 
         RETURNS (thinc.api.Optimizer): The optimizer.
 
         DOCS: https://spacy.io/api/pipe#create_optimizer
         """
@@ -201,14 +262,22 @@
         RETURNS (int): 0 if label is already present, otherwise 1.
 
         DOCS: https://spacy.io/api/pipe#add_label
         """
         raise NotImplementedError(Errors.E931.format(parent="Pipe", method="add_label", name=self.name))
 
     @property
+    def is_distillable(self) -> bool:
+        # Normally a pipe overrides `get_teacher_student_loss` to implement
+        # distillation. In more exceptional cases, a pipe can provide its
+        # own `distill` implementation. If neither of these methods is
+        # overridden, the pipe does not implement distillation.
+        return not (self.__class__.distill is TrainablePipe.distill and self.__class__.get_teacher_student_loss is TrainablePipe.get_teacher_student_loss)
+
+    @property
     def is_trainable(self) -> bool:
         return True
 
     @property
     def is_resizable(self) -> bool:
         return getattr(self, "model", None) and "resize_output" in self.model.attrs
 
@@ -338,7 +407,15 @@
         deserialize = {}
         if hasattr(self, "cfg") and self.cfg is not None:
             deserialize["cfg"] = lambda p: self.cfg.update(deserialize_config(p))
         deserialize["vocab"] = lambda p: self.vocab.from_disk(p, exclude=exclude)
         deserialize["model"] = load_model
         util.from_disk(path, deserialize, exclude)
         return self
+
+    @property
+    def save_activations(self):
+        return self._save_activations
+
+    @save_activations.setter
+    def save_activations(self, save_activations: bool):
+        self._save_activations = save_activations
```

### Comparing `spacy-3.6.0.dev0/spacy/pipeline/transition_parser.pyx` & `spacy-4.0.0.dev0/spacy/pipeline/transition_parser.pyx`

 * *Files 17% similar despite different names*

```diff
@@ -1,43 +1,49 @@
 # cython: infer_types=True, cdivision=True, boundscheck=False, binding=True
 from __future__ import print_function
+from typing import Dict, Iterable, List, Optional, Tuple
 from cymem.cymem cimport Pool
 cimport numpy as np
 from itertools import islice
 from libcpp.vector cimport vector
 from libc.string cimport memset, memcpy
 from libc.stdlib cimport calloc, free
 import random
+import contextlib
 
 import srsly
-from thinc.api import get_ops, set_dropout_rate, CupyOps, NumpyOps
-from thinc.extra.search cimport Beam
+from thinc.api import get_ops, set_dropout_rate, CupyOps, NumpyOps, Optimizer
+from thinc.api import chain, softmax_activation, use_ops, get_array_module
+from thinc.legacy import LegacySequenceCategoricalCrossentropy
+from thinc.types import Floats2d, Ints1d
 import numpy.random
 import numpy
 import warnings
 
-from ._parser_internals.stateclass cimport StateClass
-from ..ml.parser_model cimport alloc_activations, free_activations
-from ..ml.parser_model cimport predict_states, arg_max_if_valid
-from ..ml.parser_model cimport WeightsC, ActivationsC, SizesC, cpu_log_loss
-from ..ml.parser_model cimport get_c_weights, get_c_sizes
+from ..ml.tb_framework import TransitionModelInputs
+from ._parser_internals.stateclass cimport StateC, StateClass
+from ._parser_internals.search cimport Beam
 from ..tokens.doc cimport Doc
-from .trainable_pipe import TrainablePipe
+from .trainable_pipe cimport TrainablePipe
 from ._parser_internals cimport _beam_utils
 from ._parser_internals import _beam_utils
+from ..vocab cimport Vocab
+from ._parser_internals.transition_system cimport Transition, TransitionSystem
+from ..typedefs cimport weight_t
 
 from ..training import validate_examples, validate_get_examples
+from ..training import validate_distillation_examples
 from ..errors import Errors, Warnings
 from .. import util
 
 
 NUMPY_OPS = NumpyOps()
 
 
-cdef class Parser(TrainablePipe):
+class Parser(TrainablePipe):
     """
     Base class of the DependencyParser and EntityRecognizer.
     """
 
     def __init__(
         self,
         Vocab vocab,
@@ -119,25 +125,27 @@
         self.cfg = cfg
         self._multitasks = []
         for multitask in cfg["multitasks"]:
             self.add_multitask_objective(multitask)
 
         self._rehearsal_model = None
         self.scorer = scorer
+        self._cpu_ops = get_ops("cpu") if isinstance(self.model.ops, CupyOps) else self.model.ops
 
     def __getnewargs_ex__(self):
         """This allows pickling the Parser and its keyword-only init arguments"""
         args = (self.vocab, self.model, self.name, self.moves)
         return args, self.cfg
 
     @property
     def move_names(self):
         names = []
+        cdef TransitionSystem moves = self.moves
         for i in range(self.moves.n_moves):
-            name = self.moves.move_name(self.moves.c[i].move, self.moves.c[i].label)
+            name = self.moves.move_name(moves.c[i].move, moves.c[i].label)
             # Explicitly removing the internal "U-" token used for blocking entities
             if name != "U-":
                 names.append(name)
         return names
 
     @property
     def labels(self):
@@ -198,14 +206,126 @@
                 self._rehearsal_model, self.moves.n_moves
             )
 
     def add_multitask_objective(self, target):
         # Defined in subclasses, to avoid circular import
         raise NotImplementedError
 
+    def distill(self,
+               teacher_pipe: Optional[TrainablePipe],
+               examples: Iterable["Example"],
+               *,
+               drop: float=0.0,
+               sgd: Optional[Optimizer]=None,
+               losses: Optional[Dict[str, float]]=None):
+        """Train a pipe (the student) on the predictions of another pipe
+        (the teacher). The student is trained on the transition probabilities
+        of the teacher.
+
+        teacher_pipe (Optional[TrainablePipe]): The teacher pipe to learn
+            from.
+        examples (Iterable[Example]): Distillation examples. The reference
+            and predicted docs must have the same number of tokens and the
+            same orthography.
+        drop (float): dropout rate.
+        sgd (Optional[Optimizer]): An optimizer. Will be created via
+            create_optimizer if not set.
+        losses (Optional[Dict[str, float]]): Optional record of loss during
+            distillation.
+        RETURNS: The updated losses dictionary.
+        
+        DOCS: https://spacy.io/api/dependencyparser#distill
+        """
+        if teacher_pipe is None:
+            raise ValueError(Errors.E4002.format(name=self.name))
+        if losses is None:
+            losses = {}
+        losses.setdefault(self.name, 0.0)
+
+        validate_distillation_examples(examples, "TransitionParser.distill")
+
+        set_dropout_rate(self.model, drop)
+
+        student_docs = [eg.predicted for eg in examples]
+
+        max_moves = self.cfg["update_with_oracle_cut_size"]
+        if max_moves >= 1:
+            # Chop sequences into lengths of this many words, to make the
+            # batch uniform length. Since we do not have a gold standard
+            # sequence, we use the teacher's predictions as the gold
+            # standard.
+            max_moves = int(random.uniform(max_moves // 2, max_moves * 2))
+            states = self._init_batch(teacher_pipe, student_docs, max_moves)
+        else:
+            states = self.moves.init_batch(student_docs)
+
+        # We distill as follows: 1. we first let the student predict transition
+        # sequences (and the corresponding transition probabilities); (2) we
+        # let the teacher follow the student's predicted transition sequences
+        # to obtain the teacher's transition probabilities; (3) we compute the
+        # gradients of the student's transition distributions relative to the
+        # teacher's distributions.
+
+        student_inputs = TransitionModelInputs(docs=student_docs, moves=self.moves,
+            max_moves=max_moves)
+        (student_states, student_scores), backprop_scores = self.model.begin_update(student_inputs)
+        actions = states2actions(student_states)
+        teacher_inputs = TransitionModelInputs(docs=[eg.reference for eg in examples],
+            moves=self.moves, actions=actions)
+        (_, teacher_scores) = teacher_pipe.model.predict(teacher_inputs)
+
+        loss, d_scores = self.get_teacher_student_loss(teacher_scores, student_scores)
+        backprop_scores((student_states, d_scores))
+
+        if sgd is not None:
+            self.finish_update(sgd)
+
+        losses[self.name] += loss
+
+        return losses
+
+
+    def get_teacher_student_loss(
+        self, teacher_scores: List[Floats2d], student_scores: List[Floats2d],
+        normalize: bool=False,
+    ) -> Tuple[float, List[Floats2d]]:
+        """Calculate the loss and its gradient for a batch of student
+        scores, relative to teacher scores.
+
+        teacher_scores: Scores representing the teacher model's predictions.
+        student_scores: Scores representing the student model's predictions.
+
+        RETURNS (Tuple[float, float]): The loss and the gradient.
+        
+        DOCS: https://spacy.io/api/dependencyparser#get_teacher_student_loss
+        """
+
+        # We can't easily hook up a softmax layer in the parsing model, since
+        # the get_loss does additional masking. So, we could apply softmax
+        # manually here and use Thinc's cross-entropy loss. But it's a bit
+        # suboptimal, since we can have a lot of states that would result in
+        # many kernel launches. Futhermore the parsing model's backprop expects
+        # a XP array, so we'd have to concat the softmaxes anyway. So, like
+        # the get_loss implementation, we'll compute the loss and gradients
+        # ourselves.
+
+        teacher_scores = self.model.ops.softmax(self.model.ops.xp.vstack(teacher_scores),
+            axis=-1, inplace=True)
+        student_scores = self.model.ops.softmax(self.model.ops.xp.vstack(student_scores),
+            axis=-1, inplace=True)
+
+        assert teacher_scores.shape == student_scores.shape
+
+        d_scores = student_scores - teacher_scores
+        if normalize:
+            d_scores /= d_scores.shape[0]
+        loss = (d_scores**2).sum() / d_scores.size
+
+        return float(loss), d_scores
+
     def init_multitask_objectives(self, get_examples, pipeline, **cfg):
         """Setup models for secondary objectives, to benefit from multi-task
         learning. This method is intended to be overridden by subclasses.
 
         For instance, the dependency parser can benefit from sharing
         an input representation with a label prediction model. These auxiliary
         models are discarded after training.
@@ -218,17 +338,14 @@
             yield
 
     def pipe(self, docs, *, int batch_size=256):
         """Process a stream of documents.
 
         stream: The sequence of documents to process.
         batch_size (int): Number of documents to accumulate into a working set.
-        error_handler (Callable[[str, List[Doc], Exception], Any]): Function that
-            deals with a failing batch of documents. The default function just reraises
-            the exception.
 
         YIELDS (Doc): Documents, in order.
         """
         cdef Doc doc
         error_handler = self.get_error_handler()
         for batch in util.minibatch(docs, size=batch_size):
             batch_in_order = list(batch)
@@ -242,302 +359,198 @@
             except Exception as e:
                 error_handler(self.name, self, batch_in_order, e)
 
 
     def predict(self, docs):
         if isinstance(docs, Doc):
             docs = [docs]
+        self._ensure_labels_are_added(docs)
         if not any(len(doc) for doc in docs):
             result = self.moves.init_batch(docs)
             return result
-        if self.cfg["beam_width"] == 1:
-            return self.greedy_parse(docs, drop=0.0)
-        else:
-            return self.beam_parse(
-                docs,
-                drop=0.0,
-                beam_width=self.cfg["beam_width"],
-                beam_density=self.cfg["beam_density"]
-            )
+        with _change_attrs(self.model, beam_width=self.cfg["beam_width"], beam_density=self.cfg["beam_density"]):
+            inputs = TransitionModelInputs(docs=docs, moves=self.moves)
+            states_or_beams, _ = self.model.predict(inputs)
+        return states_or_beams
 
     def greedy_parse(self, docs, drop=0.):
-        cdef vector[StateC*] states
-        cdef StateClass state
-        ops = self.model.ops
-        cdef CBlas cblas
-        if isinstance(ops, CupyOps):
-            cblas = NUMPY_OPS.cblas()
-        else:
-            cblas = ops.cblas()
+        self._resize()
         self._ensure_labels_are_added(docs)
-        set_dropout_rate(self.model, drop)
-        batch = self.moves.init_batch(docs)
-        model = self.model.predict(docs)
-        weights = get_c_weights(model)
-        for state in batch:
-            if not state.is_final():
-                states.push_back(state.c)
-        sizes = get_c_sizes(model, states.size())
-        with nogil:
-            self._parseC(cblas, &states[0], weights, sizes)
-        model.clear_memory()
-        del model
-        return batch
+        with _change_attrs(self.model, beam_width=1):
+            inputs = TransitionModelInputs(docs=docs, moves=self.moves)
+            states, _ = self.model.predict(inputs)
+        return states
 
     def beam_parse(self, docs, int beam_width, float drop=0., beam_density=0.):
-        cdef Beam beam
-        cdef Doc doc
         self._ensure_labels_are_added(docs)
-        batch = _beam_utils.BeamBatch(
-            self.moves,
-            self.moves.init_batch(docs),
-            None,
-            beam_width,
-            density=beam_density
-        )
-        model = self.model.predict(docs)
-        while not batch.is_done:
-            states = batch.get_unfinished_states()
-            if not states:
-                break
-            scores = model.predict(states)
-            batch.advance(scores)
-        model.clear_memory()
-        del model
-        return list(batch)
-
-    cdef void _parseC(self, CBlas cblas, StateC** states,
-            WeightsC weights, SizesC sizes) nogil:
-        cdef int i, j
-        cdef vector[StateC*] unfinished
-        cdef ActivationsC activations = alloc_activations(sizes)
-        while sizes.states >= 1:
-            predict_states(cblas, &activations, states, &weights, sizes)
-            # Validate actions, argmax, take action.
-            self.c_transition_batch(states,
-                activations.scores, sizes.classes, sizes.states)
-            for i in range(sizes.states):
-                if not states[i].is_final():
-                    unfinished.push_back(states[i])
-            for i in range(unfinished.size()):
-                states[i] = unfinished[i]
-            sizes.states = unfinished.size()
-            unfinished.clear()
-        free_activations(&activations)
+        with _change_attrs(self.model, beam_width=self.cfg["beam_width"], beam_density=self.cfg["beam_density"]):
+            inputs = TransitionModelInputs(docs=docs, moves=self.moves)
+            beams, _ = self.model.predict(inputs)
+        return beams
 
     def set_annotations(self, docs, states_or_beams):
         cdef StateClass state
         cdef Beam beam
         cdef Doc doc
         states = _beam_utils.collect_states(states_or_beams, docs)
         for i, (state, doc) in enumerate(zip(states, docs)):
             self.moves.set_annotations(state, doc)
             for hook in self.postprocesses:
                 hook(doc)
 
-    def transition_states(self, states, float[:, ::1] scores):
-        cdef StateClass state
-        cdef float* c_scores = &scores[0, 0]
-        cdef vector[StateC*] c_states
-        for state in states:
-            c_states.push_back(state.c)
-        self.c_transition_batch(&c_states[0], c_scores, scores.shape[1], scores.shape[0])
-        return [state for state in states if not state.c.is_final()]
-
-    cdef void c_transition_batch(self, StateC** states, const float* scores,
-            int nr_class, int batch_size) nogil:
-        # n_moves should not be zero at this point, but make sure to avoid zero-length mem alloc
-        with gil:
-            assert self.moves.n_moves > 0, Errors.E924.format(name=self.name)
-        is_valid = <int*>calloc(self.moves.n_moves, sizeof(int))
-        cdef int i, guess
-        cdef Transition action
-        for i in range(batch_size):
-            self.moves.set_valid(is_valid, states[i])
-            guess = arg_max_if_valid(&scores[i*nr_class], is_valid, nr_class)
-            if guess == -1:
-                # This shouldn't happen, but it's hard to raise an error here,
-                # and we don't want to infinite loop. So, force to end state.
-                states[i].force_final()
-            else:
-                action = self.moves.c[guess]
-                action.do(states[i], action.label)
-        free(is_valid)
-
     def update(self, examples, *, drop=0., sgd=None, losses=None):
         cdef StateClass state
         if losses is None:
             losses = {}
         losses.setdefault(self.name, 0.)
         validate_examples(examples, "Parser.update")
         self._ensure_labels_are_added(
             [eg.x for eg in examples] + [eg.y for eg in examples]
         )
         for multitask in self._multitasks:
             multitask.update(examples, drop=drop, sgd=sgd)
+        # We need to take care to act on the whole batch, because we might be
+        # getting vectors via a listener.
         n_examples = len([eg for eg in examples if self.moves.has_gold(eg)])
         if n_examples == 0:
             return losses
         set_dropout_rate(self.model, drop)
-        # The probability we use beam update, instead of falling back to
-        # a greedy update
-        beam_update_prob = self.cfg["beam_update_prob"]
-        if self.cfg['beam_width'] >= 2 and numpy.random.random() < beam_update_prob:
-            return self.update_beam(
-                examples,
-                beam_width=self.cfg["beam_width"],
-                sgd=sgd,
-                losses=losses,
-                beam_density=self.cfg["beam_density"]
-            )
+        docs = [eg.x for eg in examples if len(eg.x)]
+
         max_moves = self.cfg["update_with_oracle_cut_size"]
         if max_moves >= 1:
             # Chop sequences into lengths of this many words, to make the
             # batch uniform length.
-            max_moves = int(random.uniform(max_moves // 2, max_moves * 2))
-            states, golds, _ = self._init_gold_batch(
+            max_moves = int(random.uniform(max(max_moves // 2, 1), max_moves * 2))
+            init_states, gold_states, _ = self._init_gold_batch(
                 examples,
                 max_length=max_moves
             )
         else:
-            states, golds, _ = self.moves.init_gold_batch(examples)
-        if not states:
-            return losses
-        model, backprop_tok2vec = self.model.begin_update([eg.x for eg in examples])
- 
-        all_states = list(states)
-        states_golds = list(zip(states, golds))
-        n_moves = 0
-        while states_golds:
-            states, golds = zip(*states_golds)
-            scores, backprop = model.begin_update(states)
-            d_scores = self.get_batch_loss(states, golds, scores, losses)
-            # Note that the gradient isn't normalized by the batch size
-            # here, because our "samples" are really the states...But we
-            # can't normalize by the number of states either, as then we'd
-            # be getting smaller gradients for states in long sequences.
-            backprop(d_scores)
-            # Follow the predicted action
-            self.transition_states(states, scores)
-            states_golds = [(s, g) for (s, g) in zip(states, golds) if not s.is_final()]
-            if max_moves >= 1 and n_moves >= max_moves:
-                break
-            n_moves += 1
+            init_states, gold_states, _ = self.moves.init_gold_batch(examples)
 
-        backprop_tok2vec(golds)
+        inputs = TransitionModelInputs(docs=docs, moves=self.moves,
+            max_moves=max_moves, states=[state.copy() for state in init_states])
+        (pred_states, scores), backprop_scores = self.model.begin_update(inputs)
+        if sum(s.shape[0] for s in scores) == 0:
+            return losses
+        d_scores = self.get_loss((gold_states, init_states, pred_states, scores),
+            examples, max_moves)
+        backprop_scores((pred_states, d_scores))
         if sgd not in (None, False):
             self.finish_update(sgd)
+        losses[self.name] += float((d_scores**2).sum())
         # Ugh, this is annoying. If we're working on GPU, we want to free the
         # memory ASAP. It seems that Python doesn't necessarily get around to
         # removing these in time if we don't explicitly delete? It's confusing.
-        del backprop
-        del backprop_tok2vec
-        model.clear_memory()
-        del model
+        del backprop_scores
         return losses
 
+    def get_loss(self, states_scores, examples, max_moves):
+        gold_states, init_states, pred_states, scores = states_scores
+        scores = self.model.ops.xp.vstack(scores)
+        costs = self._get_costs_from_histories(
+            examples,
+            gold_states,
+            init_states,
+            [list(state.history) for state in pred_states],
+            max_moves
+        )
+        xp = get_array_module(scores)
+        best_costs = costs.min(axis=1, keepdims=True)
+        gscores = scores.copy()
+        min_score = scores.min() - 1000
+        assert costs.shape == scores.shape, (costs.shape, scores.shape)
+        gscores[costs > best_costs] = min_score
+        max_ = scores.max(axis=1, keepdims=True)
+        gmax = gscores.max(axis=1, keepdims=True)
+        exp_scores = xp.exp(scores - max_)
+        exp_gscores = xp.exp(gscores - gmax)
+        Z = exp_scores.sum(axis=1, keepdims=True)
+        gZ = exp_gscores.sum(axis=1, keepdims=True)
+        d_scores = exp_scores / Z
+        d_scores -= (costs <= best_costs) * (exp_gscores / gZ)
+        return d_scores
+
+    def _get_costs_from_histories(self, examples, gold_states, init_states, histories, max_moves):
+        cdef TransitionSystem moves = self.moves
+        cdef StateClass state
+        cdef int clas
+        cdef int nF = self.model.get_dim("nF")
+        cdef int nO = moves.n_moves
+        cdef int nS = sum([len(history) for history in histories])
+        cdef Pool mem = Pool()
+        cdef np.ndarray costs_i
+        is_valid = <int*>mem.alloc(nO, sizeof(int))
+        batch = list(zip(init_states, histories, gold_states))
+        n_moves = 0
+        output = []
+        while batch:
+            costs = numpy.zeros((len(batch), nO), dtype="f")
+            for i, (state, history, gold) in enumerate(batch):
+                costs_i = costs[i]
+                clas = history.pop(0)
+                moves.set_costs(is_valid, <weight_t*>costs_i.data, state.c, gold)
+                action = moves.c[clas]
+                action.do(state.c, action.label)
+                state.c.history.push_back(clas)
+            output.append(costs)
+            batch = [(s, h, g) for s, h, g in batch if len(h) != 0]
+            if n_moves >= max_moves >= 1:
+                break
+            n_moves += 1
+
+        return self.model.ops.xp.vstack(output)
+
     def rehearse(self, examples, sgd=None, losses=None, **cfg):
         """Perform a "rehearsal" update, to prevent catastrophic forgetting."""
         if losses is None:
             losses = {}
         for multitask in self._multitasks:
             if hasattr(multitask, 'rehearse'):
                 multitask.rehearse(examples, losses=losses, sgd=sgd)
         if self._rehearsal_model is None:
             return None
-        losses.setdefault(self.name, 0.)
+        losses.setdefault(self.name, 0.0)
         validate_examples(examples, "Parser.rehearse")
         docs = [eg.predicted for eg in examples]
-        states = self.moves.init_batch(docs)
         # This is pretty dirty, but the NER can resize itself in init_batch,
         # if labels are missing. We therefore have to check whether we need to
         # expand our model output.
         self._resize()
         # Prepare the stepwise model, and get the callback for finishing the batch
         set_dropout_rate(self._rehearsal_model, 0.0)
         set_dropout_rate(self.model, 0.0)
-        tutor, _ = self._rehearsal_model.begin_update(docs)
-        model, backprop_tok2vec = self.model.begin_update(docs)
-        n_scores = 0.
-        loss = 0.
-        while states:
-            targets, _ = tutor.begin_update(states)
-            guesses, backprop = model.begin_update(states)
-            d_scores = (guesses - targets) / targets.shape[0]
-            # If all weights for an output are 0 in the original model, don't
-            # supervise that output. This allows us to add classes.
-            loss += (d_scores**2).sum()
-            backprop(d_scores)
-            # Follow the predicted action
-            self.transition_states(states, guesses)
-            states = [state for state in states if not state.is_final()]
-            n_scores += d_scores.size
-        # Do the backprop
-        backprop_tok2vec(docs)
+        student_inputs = TransitionModelInputs(docs=docs, moves=self.moves)
+        (student_states, student_scores), backprop_scores = self.model.begin_update(student_inputs)
+        actions = states2actions(student_states)
+        teacher_inputs = TransitionModelInputs(docs=docs, moves=self.moves, actions=actions)
+        _, teacher_scores = self._rehearsal_model.predict(teacher_inputs)
+
+        loss, d_scores = self.get_teacher_student_loss(teacher_scores, student_scores, normalize=True)
+
+        teacher_scores = self.model.ops.xp.vstack(teacher_scores)
+        student_scores = self.model.ops.xp.vstack(student_scores)
+        assert teacher_scores.shape == student_scores.shape
+
+        d_scores = (student_scores - teacher_scores) / teacher_scores.shape[0]
+        # If all weights for an output are 0 in the original model, don't
+        # supervise that output. This allows us to add classes.
+        loss = (d_scores**2).sum() / d_scores.size
+        backprop_scores((student_states, d_scores))
+
         if sgd is not None:
             self.finish_update(sgd)
-        losses[self.name] += loss / n_scores
-        del backprop
-        del backprop_tok2vec
-        model.clear_memory()
-        tutor.clear_memory()
-        del model
-        del tutor
+        losses[self.name] += loss
+
         return losses
 
     def update_beam(self, examples, *, beam_width,
             drop=0., sgd=None, losses=None, beam_density=0.0):
-        states, golds, _ = self.moves.init_gold_batch(examples)
-        if not states:
-            return losses
-        # Prepare the stepwise model, and get the callback for finishing the batch
-        model, backprop_tok2vec = self.model.begin_update(
-            [eg.predicted for eg in examples])
-        loss = _beam_utils.update_beam(
-            self.moves,
-            states,
-            golds,
-            model,
-            beam_width,
-            beam_density=beam_density,
-        )
-        losses[self.name] += loss
-        backprop_tok2vec(golds)
-        if sgd is not None:
-            self.finish_update(sgd)
-
-    def get_batch_loss(self, states, golds, float[:, ::1] scores, losses):
-        cdef StateClass state
-        cdef Pool mem = Pool()
-        cdef int i
-
-        # n_moves should not be zero at this point, but make sure to avoid zero-length mem alloc
-        assert self.moves.n_moves > 0, Errors.E924.format(name=self.name)
-
-        is_valid = <int*>mem.alloc(self.moves.n_moves, sizeof(int))
-        costs = <float*>mem.alloc(self.moves.n_moves, sizeof(float))
-        cdef np.ndarray d_scores = numpy.zeros((len(states), self.moves.n_moves),
-                                        dtype='f', order='C')
-        c_d_scores = <float*>d_scores.data
-        unseen_classes = self.model.attrs["unseen_classes"]
-        for i, (state, gold) in enumerate(zip(states, golds)):
-            memset(is_valid, 0, self.moves.n_moves * sizeof(int))
-            memset(costs, 0, self.moves.n_moves * sizeof(float))
-            self.moves.set_costs(is_valid, costs, state.c, gold)
-            for j in range(self.moves.n_moves):
-                if costs[j] <= 0.0 and j in unseen_classes:
-                    unseen_classes.remove(j)
-            cpu_log_loss(c_d_scores,
-                costs, is_valid, &scores[i, 0], d_scores.shape[1])
-            c_d_scores += d_scores.shape[1]
-        # Note that we don't normalize this. See comment in update() for why.
-        if losses is not None:
-            losses.setdefault(self.name, 0.)
-            losses[self.name] += (d_scores**2).sum()
-        return d_scores
+        raise NotImplementedError
 
     def set_output(self, nO):
         self.model.attrs["resize_output"](self.model, nO)
 
     def initialize(self, get_examples, nlp=None, labels=None):
         validate_get_examples(get_examples, "Parser.initialize")
         util.check_lexeme_norms(self.vocab, "parser or NER")
@@ -568,15 +581,15 @@
                     doc_sample = list(component.pipe(doc_sample, batch_size=8))
                 else:
                     doc_sample = [component(doc) for doc in doc_sample]
         if not doc_sample:
             for example in islice(get_examples(), 10):
                 doc_sample.append(example.predicted)
         assert len(doc_sample) > 0, Errors.E923.format(name=self.name)
-        self.model.initialize(doc_sample)
+        self.model.initialize((doc_sample, self.moves))
         if nlp is not None:
             self.init_multitask_objectives(get_examples, nlp.pipeline)
 
     def to_disk(self, path, exclude=tuple()):
         serializers = {
             "model": lambda p: (self.model.to_disk(p) if self.model is not True else True),
             "vocab": lambda p: self.vocab.to_disk(p, exclude=exclude),
@@ -625,48 +638,122 @@
             if 'model' in msg:
                 try:
                     self.model.from_bytes(msg['model'])
                 except AttributeError:
                     raise ValueError(Errors.E149) from None
         return self
 
-    def _init_gold_batch(self, examples, max_length):
-        """Make a square batch, of length equal to the shortest transition
+    def _init_batch(self, teacher_step_model, docs, max_length):
+        """Make a square batch of length equal to the shortest transition
         sequence or a cap. A long
         doc will get multiple states. Let's say we have a doc of length 2*N,
         where N is the shortest doc. We'll make two states, one representing
-        long_doc[:N], and another representing long_doc[N:]."""
+        long_doc[:N], and another representing long_doc[N:]. In contrast to
+        _init_gold_batch, this version uses a teacher model to generate the
+        cut sequences."""
+        cdef:
+            StateClass start_state
+            StateClass state
+            Transition action
+        all_states = self.moves.init_batch(docs)
+        states = []
+        to_cut = []
+        for state, doc in zip(all_states, docs):
+            if not state.is_final():
+                if len(doc) < max_length:
+                    states.append(state)
+                else:
+                    to_cut.append(state)
+        while to_cut:
+            states.extend(state.copy() for state in to_cut)
+            # Move states forward max_length actions.
+            length = 0
+            while to_cut and length < max_length:
+                teacher_scores = teacher_step_model.predict(to_cut)
+                self.transition_states(to_cut, teacher_scores)
+                # States that are completed do not need further cutting.
+                to_cut = [state for state in to_cut if not state.is_final()]
+                length += 1
+        return states
+
+
+    def _init_gold_batch(self, examples, max_length):
+        """Make a square batch, of length equal to the shortest transition
+        sequence or a cap. A long doc will get multiple states. Let's say we
+        have a doc of length 2*N, where N is the shortest doc. We'll make
+        two states, one representing long_doc[:N], and another representing
+        long_doc[N:]."""
         cdef:
             StateClass start_state
             StateClass state
             Transition action
-        all_states = self.moves.init_batch([eg.predicted for eg in examples])
+            TransitionSystem moves = self.moves
+        all_states = moves.init_batch([eg.predicted for eg in examples])
         states = []
         golds = []
         to_cut = []
         for state, eg in zip(all_states, examples):
-            if self.moves.has_gold(eg) and not state.is_final():
-                gold = self.moves.init_gold(state, eg)
+            if moves.has_gold(eg) and not state.is_final():
+                gold = moves.init_gold(state, eg)
                 if len(eg.x) < max_length:
                     states.append(state)
                     golds.append(gold)
                 else:
-                    oracle_actions = self.moves.get_oracle_sequence_from_state(
+                    oracle_actions = moves.get_oracle_sequence_from_state(
                         state.copy(), gold)
                     to_cut.append((eg, state, gold, oracle_actions))
         if not to_cut:
             return states, golds, 0
         cdef int clas
         for eg, state, gold, oracle_actions in to_cut:
             for i in range(0, len(oracle_actions), max_length):
                 start_state = state.copy()
                 for clas in oracle_actions[i:i+max_length]:
-                    action = self.moves.c[clas]
+                    action = moves.c[clas]
                     action.do(state.c, action.label)
                     if state.is_final():
                         break
-                if self.moves.has_gold(eg, start_state.B(0), state.B(0)):
+                if moves.has_gold(eg, start_state.B(0), state.B(0)):
                     states.append(start_state)
                     golds.append(gold)
                 if state.is_final():
                     break
         return states, golds, max_length
+
+
+@contextlib.contextmanager
+def _change_attrs(model, **kwargs):
+    """Temporarily modify a thinc model's attributes."""
+    unset = object()
+    old_attrs = {}
+    for key, value in kwargs.items():
+        old_attrs[key] = model.attrs.get(key, unset)
+        model.attrs[key] = value
+    yield model
+    for key, value in old_attrs.items():
+        if value is unset:
+            model.attrs.pop(key)
+        else:
+            model.attrs[key] = value
+
+
+def states2actions(states: List[StateClass]) -> List[Ints1d]:
+    cdef int step
+    cdef StateClass state
+    cdef StateC* c_state
+    actions = []
+    while True:
+        step = len(actions)
+
+        step_actions = []
+        for state in states:
+            c_state = state.c
+            if step < c_state.history.size():
+                step_actions.append(c_state.history[step])
+
+        # We are done if we have exhausted all histories.
+        if len(step_actions) == 0:
+            break
+
+        actions.append(numpy.array(step_actions, dtype="i"))
+
+    return actions
```

### Comparing `spacy-3.6.0.dev0/spacy/schemas.py` & `spacy-4.0.0.dev0/spacy/schemas.py`

 * *Files 1% similar despite different names*

```diff
@@ -140,15 +140,15 @@
 
 
 # Matcher token patterns
 
 
 def validate_token_pattern(obj: list) -> List[str]:
     # Try to convert non-string keys (e.g. {ORTH: "foo"} -> {"ORTH": "foo"})
-    get_key = lambda k: NAMES[k] if isinstance(k, int) and k < len(NAMES) else k
+    get_key = lambda k: NAMES[k] if isinstance(k, int) and k in NAMES else k
     if isinstance(obj, list):
         converted = []
         for pattern in obj:
             if isinstance(pattern, dict):
                 pattern = {get_key(k): v for k, v in pattern.items()}
             converted.append(pattern)
         obj = converted
@@ -159,41 +159,23 @@
     REGEX: Optional[Union[StrictStr, "TokenPatternString"]] = Field(None, alias="regex")
     IN: Optional[List[StrictStr]] = Field(None, alias="in")
     NOT_IN: Optional[List[StrictStr]] = Field(None, alias="not_in")
     IS_SUBSET: Optional[List[StrictStr]] = Field(None, alias="is_subset")
     IS_SUPERSET: Optional[List[StrictStr]] = Field(None, alias="is_superset")
     INTERSECTS: Optional[List[StrictStr]] = Field(None, alias="intersects")
     FUZZY: Optional[Union[StrictStr, "TokenPatternString"]] = Field(None, alias="fuzzy")
-    FUZZY1: Optional[Union[StrictStr, "TokenPatternString"]] = Field(
-        None, alias="fuzzy1"
-    )
-    FUZZY2: Optional[Union[StrictStr, "TokenPatternString"]] = Field(
-        None, alias="fuzzy2"
-    )
-    FUZZY3: Optional[Union[StrictStr, "TokenPatternString"]] = Field(
-        None, alias="fuzzy3"
-    )
-    FUZZY4: Optional[Union[StrictStr, "TokenPatternString"]] = Field(
-        None, alias="fuzzy4"
-    )
-    FUZZY5: Optional[Union[StrictStr, "TokenPatternString"]] = Field(
-        None, alias="fuzzy5"
-    )
-    FUZZY6: Optional[Union[StrictStr, "TokenPatternString"]] = Field(
-        None, alias="fuzzy6"
-    )
-    FUZZY7: Optional[Union[StrictStr, "TokenPatternString"]] = Field(
-        None, alias="fuzzy7"
-    )
-    FUZZY8: Optional[Union[StrictStr, "TokenPatternString"]] = Field(
-        None, alias="fuzzy8"
-    )
-    FUZZY9: Optional[Union[StrictStr, "TokenPatternString"]] = Field(
-        None, alias="fuzzy9"
-    )
+    FUZZY1: Optional[Union[StrictStr, "TokenPatternString"]] = Field(None, alias="fuzzy1")
+    FUZZY2: Optional[Union[StrictStr, "TokenPatternString"]] = Field(None, alias="fuzzy2")
+    FUZZY3: Optional[Union[StrictStr, "TokenPatternString"]] = Field(None, alias="fuzzy3")
+    FUZZY4: Optional[Union[StrictStr, "TokenPatternString"]] = Field(None, alias="fuzzy4")
+    FUZZY5: Optional[Union[StrictStr, "TokenPatternString"]] = Field(None, alias="fuzzy5")
+    FUZZY6: Optional[Union[StrictStr, "TokenPatternString"]] = Field(None, alias="fuzzy6")
+    FUZZY7: Optional[Union[StrictStr, "TokenPatternString"]] = Field(None, alias="fuzzy7")
+    FUZZY8: Optional[Union[StrictStr, "TokenPatternString"]] = Field(None, alias="fuzzy8")
+    FUZZY9: Optional[Union[StrictStr, "TokenPatternString"]] = Field(None, alias="fuzzy9")
 
     class Config:
         extra = "forbid"
         allow_population_by_field_name = True  # allow alias and field name
 
     @validator("*", pre=True, each_item=True, allow_reuse=True)
     def raise_for_none(cls, v):
```

### Comparing `spacy-3.6.0.dev0/spacy/scorer.py` & `spacy-4.0.0.dev0/spacy/scorer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/strings.pyi` & `spacy-4.0.0.dev0/spacy/strings.pyi`

 * *Files 20% similar despite different names*

```diff
@@ -1,25 +1,26 @@
-from typing import Optional, Iterable, Iterator, Union, Any, overload
+from typing import List, Optional, Iterable, Iterator, Union, Any, Tuple, overload
 from pathlib import Path
 
-def get_string_id(key: Union[str, int]) -> int: ...
-
 class StringStore:
-    def __init__(
-        self, strings: Optional[Iterable[str]] = ..., freeze: bool = ...
-    ) -> None: ...
+    def __init__(self, strings: Optional[Iterable[str]]) -> None: ...
     @overload
-    def __getitem__(self, string_or_id: Union[bytes, str]) -> int: ...
+    def __getitem__(self, string_or_hash: str) -> int: ...
     @overload
-    def __getitem__(self, string_or_id: int) -> str: ...
-    def as_int(self, key: Union[bytes, str, int]) -> int: ...
-    def as_string(self, key: Union[bytes, str, int]) -> str: ...
+    def __getitem__(self, string_or_hash: int) -> str: ...
+    def as_int(self, string_or_hash: Union[str, int]) -> int: ...
+    def as_string(self, string_or_hash: Union[str, int]) -> str: ...
     def add(self, string: str) -> int: ...
+    def items(self) -> List[Tuple[str, int]]: ...
+    def keys(self) -> List[str]: ...
+    def values(self) -> List[int]: ...
     def __len__(self) -> int: ...
-    def __contains__(self, string: str) -> bool: ...
+    def __contains__(self, string_or_hash: Union[str, int]) -> bool: ...
     def __iter__(self) -> Iterator[str]: ...
     def __reduce__(self) -> Any: ...
     def to_disk(self, path: Union[str, Path]) -> None: ...
     def from_disk(self, path: Union[str, Path]) -> StringStore: ...
     def to_bytes(self, **kwargs: Any) -> bytes: ...
     def from_bytes(self, bytes_data: bytes, **kwargs: Any) -> StringStore: ...
     def _reset_and_load(self, strings: Iterable[str]) -> None: ...
+
+def get_string_id(string_or_hash: Union[str, int]) -> int: ...
```

### Comparing `spacy-3.6.0.dev0/spacy/strings.pyx` & `spacy-4.0.0.dev0/spacy/strings.pyx`

 * *Files 10% similar despite different names*

```diff
@@ -1,253 +1,161 @@
 # cython: infer_types=True
+from typing import Optional, Union, Iterable, Tuple, Callable, Any, List, Iterator
 cimport cython
 from libc.string cimport memcpy
 from libcpp.set cimport set
 from libc.stdint cimport uint32_t
-from murmurhash.mrmr cimport hash64, hash32
+from murmurhash.mrmr cimport hash64
 
 import srsly
 
 from .typedefs cimport hash_t
 
 from .symbols import IDS as SYMBOLS_BY_STR
 from .symbols import NAMES as SYMBOLS_BY_INT
 from .errors import Errors
 from . import util
 
-# Not particularly elegant, but this is faster than `isinstance(key, numbers.Integral)`
-cdef inline bint _try_coerce_to_hash(object key, hash_t* out_hash):
-    try:
-        out_hash[0] = key
-        return True
-    except:
-        return False
-
-def get_string_id(key):
-    """Get a string ID, handling the reserved symbols correctly. If the key is
-    already an ID, return it.
-
-    This function optimises for convenience over performance, so shouldn't be
-    used in tight loops.
-    """
-    cdef hash_t str_hash    
-    if isinstance(key, str):
-        if len(key) == 0:
-            return 0
-
-        symbol = SYMBOLS_BY_STR.get(key, None)
-        if symbol is not None:
-            return symbol
-        else:
-            chars = key.encode("utf8")
-            return hash_utf8(chars, len(chars))
-    elif _try_coerce_to_hash(key, &str_hash):
-        # Coerce the integral key to the expected primitive hash type.
-        # This ensures that custom/overloaded "primitive" data types
-        # such as those implemented by numpy are not inadvertently used 
-        # downsteam (as these are internally implemented as custom PyObjects 
-        # whose comparison operators can incur a significant overhead).
-        return str_hash
-    else:
-        # TODO: Raise an error instead
-        return key
-
-
-cpdef hash_t hash_string(str string) except 0:
-    chars = string.encode("utf8")
-    return hash_utf8(chars, len(chars))
-
-
-cdef hash_t hash_utf8(char* utf8_string, int length) nogil:
-    return hash64(utf8_string, length, 1)
-
-
-cdef uint32_t hash32_utf8(char* utf8_string, int length) nogil:
-    return hash32(utf8_string, length, 1)
-
-
-cdef str decode_Utf8Str(const Utf8Str* string):
-    cdef int i, length
-    if string.s[0] < sizeof(string.s) and string.s[0] != 0:
-        return string.s[1:string.s[0]+1].decode("utf8")
-    elif string.p[0] < 255:
-        return string.p[1:string.p[0]+1].decode("utf8")
-    else:
-        i = 0
-        length = 0
-        while string.p[i] == 255:
-            i += 1
-            length += 255
-        length += string.p[i]
-        i += 1
-        return string.p[i:length + i].decode("utf8")
-
-
-cdef Utf8Str* _allocate(Pool mem, const unsigned char* chars, uint32_t length) except *:
-    cdef int n_length_bytes
-    cdef int i
-    cdef Utf8Str* string = <Utf8Str*>mem.alloc(1, sizeof(Utf8Str))
-    cdef uint32_t ulength = length
-    if length < sizeof(string.s):
-        string.s[0] = <unsigned char>length
-        memcpy(&string.s[1], chars, length)
-        return string
-    elif length < 255:
-        string.p = <unsigned char*>mem.alloc(length + 1, sizeof(unsigned char))
-        string.p[0] = length
-        memcpy(&string.p[1], chars, length)
-        return string
-    else:
-        i = 0
-        n_length_bytes = (length // 255) + 1
-        string.p = <unsigned char*>mem.alloc(length + n_length_bytes, sizeof(unsigned char))
-        for i in range(n_length_bytes-1):
-            string.p[i] = 255
-        string.p[n_length_bytes-1] = length % 255
-        memcpy(&string.p[n_length_bytes], chars, length)
-        return string
-
 
 cdef class StringStore:
-    """Look up strings by 64-bit hashes.
+    """Look up strings by 64-bit hashes. Implicitly handles reserved symbols.
 
     DOCS: https://spacy.io/api/stringstore
     """
-    def __init__(self, strings=None, freeze=False):
+    def __init__(self, strings: Optional[Iterable[str]] = None):
         """Create the StringStore.
 
         strings (iterable): A sequence of unicode strings to add to the store.
         """
         self.mem = Pool()
         self._map = PreshMap()
         if strings is not None:
             for string in strings:
                 self.add(string)
 
-    def __getitem__(self, object string_or_id):
-        """Retrieve a string from a given hash, or vice versa.
+    def __getitem__(self, string_or_hash: Union[str, int]) -> Union[str, int]:
+        """Retrieve a string from a given hash. If a string
+        is passed as the input, add it to the store and return
+        its hash.
 
-        string_or_id (bytes, str or uint64): The value to encode.
-        Returns (str / uint64): The value to be retrieved.
+        string_or_hash (int / str): The hash value to lookup or the string to store.
+        RETURNS (str / int): The stored string or the hash of the newly added string.
         """
-        cdef hash_t str_hash
-        cdef Utf8Str* utf8str = NULL
-
-        if isinstance(string_or_id, str):
-            if len(string_or_id) == 0:
-                return 0
-
-            # Return early if the string is found in the symbols LUT.
-            symbol = SYMBOLS_BY_STR.get(string_or_id, None)
-            if symbol is not None:
-                return symbol
-            else:
-                return hash_string(string_or_id)
-        elif isinstance(string_or_id, bytes):
-            return hash_utf8(string_or_id, len(string_or_id))
-        elif _try_coerce_to_hash(string_or_id, &str_hash):
-            if str_hash == 0:
-                return ""
-            elif str_hash < len(SYMBOLS_BY_INT):
-                return SYMBOLS_BY_INT[str_hash]
-            else:
-                utf8str = <Utf8Str*>self._map.get(str_hash)
+        if isinstance(string_or_hash, str):
+            return self.add(string_or_hash)
         else:
-            # TODO: Raise an error instead
-            utf8str = <Utf8Str*>self._map.get(string_or_id)
+            return self._get_interned_str(string_or_hash)
 
-        if utf8str is NULL:
-            raise KeyError(Errors.E018.format(hash_value=string_or_id))
-        else:
-            return decode_Utf8Str(utf8str)
+    def __contains__(self, string_or_hash: Union[str, int]) -> bool:
+        """Check whether a string or a hash is in the store.
 
-    def as_int(self, key):
-        """If key is an int, return it; otherwise, get the int value."""
-        if not isinstance(key, str):
-            return key
+        string (str / int): The string/hash to check.
+        RETURNS (bool): Whether the store contains the string.
+        """
+        cdef hash_t str_hash = get_string_id(string_or_hash)
+        if str_hash in SYMBOLS_BY_INT:
+            return True
         else:
-            return self[key]
+            return self._map.get(str_hash) is not NULL
 
-    def as_string(self, key):
-        """If key is a string, return it; otherwise, get the string value."""
-        if isinstance(key, str):
-            return key
-        else:
-            return self[key]
+    def __iter__(self) -> Iterator[str]:
+        """Iterate over the strings in the store in insertion order.
+
+        RETURNS: An iterable collection of strings.
+        """
+        return iter(self.keys())
+
+    def __reduce__(self):
+        strings = list(self)
+        return (StringStore, (strings,), None, None, None)
+
+    def __len__(self) -> int:
+        """The number of strings in the store.
+
+        RETURNS (int): The number of strings in the store.
+        """
+        return self._keys.size()
 
-    def add(self, string):
+    def add(self, string: str) -> int:
         """Add a string to the StringStore.
 
         string (str): The string to add.
         RETURNS (uint64): The string's hash value.
         """
-        cdef hash_t str_hash
-        if isinstance(string, str):
-            if string in SYMBOLS_BY_STR:
-                return SYMBOLS_BY_STR[string]
-
-            string = string.encode("utf8")
-            str_hash = hash_utf8(string, len(string))
-            self._intern_utf8(string, len(string), &str_hash)
-        elif isinstance(string, bytes):
-            if string in SYMBOLS_BY_STR:
-                return SYMBOLS_BY_STR[string]
-            str_hash = hash_utf8(string, len(string))
-            self._intern_utf8(string, len(string), &str_hash)
-        else:
+        if not isinstance(string, str):
             raise TypeError(Errors.E017.format(value_type=type(string)))
-        return str_hash
 
-    def __len__(self):
-        """The number of strings in the store.
+        if string in SYMBOLS_BY_STR:
+            return SYMBOLS_BY_STR[string]
+        else:
+            return self._intern_str(string)
 
-        RETURNS (int): The number of strings in the store.
+    def as_int(self, string_or_hash: Union[str, int]) -> str:
+        """If a hash value is passed as the input, return it as-is. If the input
+        is a string, return its corresponding hash.
+
+        string_or_hash (str / int): The string to hash or a hash value.
+        RETURNS (int): The hash of the string or the input hash value.
         """
-        return self.keys.size()
+        if isinstance(string_or_hash, int):
+            return string_or_hash
+        else:
+            return get_string_id(string_or_hash)
 
-    def __contains__(self, string_or_id not None):
-        """Check whether a string or ID is in the store.
+    def as_string(self, string_or_hash: Union[str, int]) -> str:
+        """If a string is passed as the input, return it as-is. If the input
+        is a hash value, return its corresponding string.
 
-        string_or_id (str or int): The string to check.
-        RETURNS (bool): Whether the store contains the string.
+        string_or_hash (str / int): The hash value to lookup or a string.
+        RETURNS (str): The stored string or the input string.
         """
-        cdef hash_t str_hash
-        if isinstance(string_or_id, str):
-            if len(string_or_id) == 0:
-                return True
-            elif string_or_id in SYMBOLS_BY_STR:
-                return True
-            str_hash = hash_string(string_or_id)
-        elif _try_coerce_to_hash(string_or_id, &str_hash):
-            pass
+        if isinstance(string_or_hash, str):
+            return string_or_hash
         else:
-            # TODO: Raise an error instead
-            return self._map.get(string_or_id) is not NULL
+            return self._get_interned_str(string_or_hash)
 
-        if str_hash < len(SYMBOLS_BY_INT):
-            return True
-        else:
-            return self._map.get(str_hash) is not NULL
+    def items(self) -> List[Tuple[str, int]]:
+        """Iterate over the stored strings and their hashes in insertion order.
+
+        RETURNS: A list of string-hash pairs.
+        """
+        # Even though we internally store the hashes as keys and the strings as
+        # values, we invert the order in the public API to keep it consistent with
+        # the implementation of the `__iter__` method (where we wish to iterate over
+        # the strings in the store).
+        cdef int i
+        pairs = [None] * self._keys.size()
+        for i in range(self._keys.size()):
+            str_hash = self._keys[i]
+            utf8str = <Utf8Str*>self._map.get(str_hash)
+            pairs[i] = (self._decode_str_repr(utf8str), str_hash)
+        return pairs
 
-    def __iter__(self):
-        """Iterate over the strings in the store, in order.
+    def keys(self) -> List[str]:
+        """Iterate over the stored strings in insertion order.
 
-        YIELDS (str): A string in the store.
+        RETURNS: A list of strings.
         """
         cdef int i
-        cdef hash_t key
-        for i in range(self.keys.size()):
-            key = self.keys[i]
-            utf8str = <Utf8Str*>self._map.get(key)
-            yield decode_Utf8Str(utf8str)
-        # TODO: Iterate OOV here?
+        strings = [None] * self._keys.size()
+        for i in range(self._keys.size()):
+            utf8str = <Utf8Str*>self._map.get(self._keys[i])
+            strings[i] = self._decode_str_repr(utf8str)
+        return strings
 
-    def __reduce__(self):
-        strings = list(self)
-        return (StringStore, (strings,), None, None, None)
+    def values(self) -> List[int]:
+        """Iterate over the stored strings hashes in insertion order.
+
+        RETURNS: A list of string hashs.
+        """
+        cdef int i
+        hashes = [None] * self._keys.size()
+        for i in range(self._keys.size()):
+            hashes[i] = self._keys[i]
+        return hashes
 
     def to_disk(self, path):
         """Save the current state to a directory.
 
         path (str / Path): A path to a directory, which will be created if
             it doesn't exist. Paths may be either strings or Path-like objects.
         """
@@ -290,28 +198,126 @@
         for word in prev:
             self.add(word)
         return self
 
     def _reset_and_load(self, strings):
         self.mem = Pool()
         self._map = PreshMap()
-        self.keys.clear()
+        self._keys.clear()
         for string in strings:
             self.add(string)
 
-    cdef const Utf8Str* intern_unicode(self, str py_string):
-        # 0 means missing, but we don't bother offsetting the index.
-        cdef bytes byte_string = py_string.encode("utf8")
-        return self._intern_utf8(byte_string, len(byte_string), NULL)
+    def _get_interned_str(self, hash_value: int) -> str:
+        cdef hash_t str_hash
+        if not _try_coerce_to_hash(hash_value, &str_hash):
+            raise TypeError(Errors.E4001.format(expected_types="'int'", received_type=type(hash_value)))
+
+        # Handle reserved symbols and empty strings correctly.
+        if str_hash == 0:
+            return ""
+
+        symbol = SYMBOLS_BY_INT.get(str_hash)
+        if symbol is not None:
+            return symbol
 
-    @cython.final
-    cdef const Utf8Str* _intern_utf8(self, char* utf8_string, int length, hash_t* precalculated_hash):
+        utf8str = <Utf8Str*>self._map.get(str_hash)
+        if utf8str is NULL:
+            raise KeyError(Errors.E018.format(hash_value=str_hash))
+        else:
+            return self._decode_str_repr(utf8str)
+
+    cdef hash_t _intern_str(self, str string):
         # TODO: This function's API/behaviour is an unholy mess...
         # 0 means missing, but we don't bother offsetting the index.
-        cdef hash_t key = precalculated_hash[0] if precalculated_hash is not NULL else hash_utf8(utf8_string, length)
+        chars = string.encode('utf-8')
+        cdef hash_t key = hash64(<unsigned char*>chars, len(chars), 1)
         cdef Utf8Str* value = <Utf8Str*>self._map.get(key)
         if value is not NULL:
-            return value
-        value = _allocate(self.mem, <unsigned char*>utf8_string, length)
+            return key
+
+        value = self._allocate_str_repr(<unsigned char*>chars, len(chars))
         self._map.set(key, value)
-        self.keys.push_back(key)
-        return value
+        self._keys.push_back(key)
+        return key
+
+    cdef Utf8Str* _allocate_str_repr(self, const unsigned char* chars, uint32_t length) except *:
+        cdef int n_length_bytes
+        cdef int i
+        cdef Utf8Str* string = <Utf8Str*>self.mem.alloc(1, sizeof(Utf8Str))
+        cdef uint32_t ulength = length
+        if length < sizeof(string.s):
+            string.s[0] = <unsigned char>length
+            memcpy(&string.s[1], chars, length)
+            return string
+        elif length < 255:
+            string.p = <unsigned char*>self.mem.alloc(length + 1, sizeof(unsigned char))
+            string.p[0] = length
+            memcpy(&string.p[1], chars, length)
+            return string
+        else:
+            i = 0
+            n_length_bytes = (length // 255) + 1
+            string.p = <unsigned char*>self.mem.alloc(length + n_length_bytes, sizeof(unsigned char))
+            for i in range(n_length_bytes-1):
+                string.p[i] = 255
+            string.p[n_length_bytes-1] = length % 255
+            memcpy(&string.p[n_length_bytes], chars, length)
+            return string
+
+    cdef str _decode_str_repr(self, const Utf8Str* string):
+        cdef int i, length
+        if string.s[0] < sizeof(string.s) and string.s[0] != 0:
+            return string.s[1:string.s[0]+1].decode('utf-8')
+        elif string.p[0] < 255:
+            return string.p[1:string.p[0]+1].decode('utf-8')
+        else:
+            i = 0
+            length = 0
+            while string.p[i] == 255:
+                i += 1
+                length += 255
+            length += string.p[i]
+            i += 1
+            return string.p[i:length + i].decode('utf-8')
+
+
+cpdef hash_t hash_string(object string) except -1:
+    if not isinstance(string, str):
+        raise TypeError(Errors.E4001.format(expected_types="'str'", received_type=type(string)))
+
+    # Handle reserved symbols and empty strings correctly.
+    if len(string) == 0:
+        return 0
+
+    symbol = SYMBOLS_BY_STR.get(string)
+    if symbol is not None:
+        return symbol
+
+    chars = string.encode('utf-8')
+    return hash64(<unsigned char*>chars, len(chars), 1)
+
+
+cpdef hash_t get_string_id(object string_or_hash) except -1:
+    cdef hash_t str_hash
+
+    try:
+        return hash_string(string_or_hash)
+    except:
+        if _try_coerce_to_hash(string_or_hash, &str_hash):
+            # Coerce the integral key to the expected primitive hash type.
+            # This ensures that custom/overloaded "primitive" data types
+            # such as those implemented by numpy are not inadvertently used
+            # downsteam (as these are internally implemented as custom PyObjects
+            # whose comparison operators can incur a significant overhead).
+            return str_hash
+        else:
+            raise TypeError(Errors.E4001.format(expected_types="'str','int'", received_type=type(string_or_hash)))
+
+
+# Not particularly elegant, but this is faster than `isinstance(key, numbers.Integral)`
+cdef inline bint _try_coerce_to_hash(object key, hash_t* out_hash):
+    try:
+        out_hash[0] = key
+        return True
+    except:
+        return False
+
```

### Comparing `spacy-3.6.0.dev0/spacy/structs.pxd` & `spacy-4.0.0.dev0/spacy/structs.pxd`

 * *Files 10% similar despite different names*

```diff
@@ -54,22 +54,14 @@
     int sent_start
     int ent_iob
     attr_t ent_type # TODO: Is there a better way to do this? Multiple sources of truth..
     attr_t ent_kb_id
     hash_t ent_id
 
 
-cdef struct MorphAnalysisC:
-    hash_t key
-    int length
-
-    attr_t* fields
-    attr_t* features
-
-
 # Internal struct, for storage and disambiguation of entities.
 cdef struct KBEntryC:
 
     # The hash of this entry's unique ID/name in the kB
     hash_t entity_hash
 
     # Allows retrieval of the entity vector, as an index into a vectors table of the KB.
```

### Comparing `spacy-3.6.0.dev0/spacy/symbols.pxd` & `spacy-4.0.0.dev0/spacy/symbols.pxd`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,10 @@
+# DO NOT EDIT! The symbols are frozen as of spaCy v3.0.0.
 cdef enum symbol_t:
-    NIL
+    NIL = 0
     IS_ALPHA
     IS_ASCII
     IS_DIGIT
     IS_LOWER
     IS_PUNCT
     IS_SPACE
     IS_TITLE
@@ -61,15 +62,15 @@
     FLAG58
     FLAG59
     FLAG60
     FLAG61
     FLAG62
     FLAG63
 
-    ID
+    ID = 64
     ORTH
     LOWER
     NORM
     SHAPE
     PREFIX
     SUFFIX
 
@@ -381,15 +382,15 @@
     DEPRECATED271
     DEPRECATED272
     DEPRECATED273
     DEPRECATED274
     DEPRECATED275
     DEPRECATED276
 
-    PERSON
+    PERSON = 380
     NORP
     FACILITY
     ORG
     GPE
     LOC
     PRODUCT
     EVENT
@@ -401,15 +402,15 @@
     TIME
     PERCENT
     MONEY
     QUANTITY
     ORDINAL
     CARDINAL
 
-    acomp
+    acomp = 398
     advcl
     advmod
     agent
     amod
     appos
     attr
     aux
@@ -454,16 +455,16 @@
     prt
     punct
     quantmod
     relcl
     rcmod
     root
     xcomp
-
     acl
 
-    ENT_KB_ID
+    ENT_KB_ID = 452
     MORPH
     ENT_ID
 
     IDX
-    _
+    _ = 456
+    # DO NOT ADD ANY NEW SYMBOLS!
```

### Comparing `spacy-3.6.0.dev0/spacy/symbols.pyx` & `spacy-4.0.0.dev0/spacy/symbols.pyx`

 * *Files 0% similar despite different names*

```diff
@@ -465,16 +465,12 @@
     "acl": acl,
     "LAW": LAW,
     "MORPH": MORPH,
     "_": _,
 }
 
 
-def sort_nums(x):
-    return x[1]
-
-
-NAMES = [it[0] for it in sorted(IDS.items(), key=sort_nums)]
+NAMES = {v: k for k, v in IDS.items()}
 # Unfortunate hack here, to work around problem with long cpdef enum
 # (which is generating an enormous amount of C++ in Cython 0.24+)
 # We keep the enum cdef, and just make sure the names are available to Python
 locals().update(IDS)
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/conftest.py` & `spacy-4.0.0.dev0/spacy/tests/conftest.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,14 @@
 import pytest
 from spacy.util import get_lang_class
+import functools
 from hypothesis import settings
+import inspect
+import importlib
+import sys
 
 # Functionally disable deadline settings for tests
 # to prevent spurious test failures in CI builds.
 settings.register_profile("no_deadlines", deadline=2 * 60 * 1000)  # in ms
 settings.load_profile("no_deadlines")
 
 
@@ -43,14 +47,41 @@
             issue_refs = [mark.args[0] for mark in item.iter_markers(name="issue")]
             if not any([ref in issue_nos for ref in issue_refs]):
                 pytest.skip(f"not referencing specified issues: {issue_nos}")
         else:
             pytest.skip("not referencing any issues")
 
 
+# Decorator for Cython-built tests
+# https://shwina.github.io/cython-testing/
+def cytest(func):
+    """
+    Wraps `func` in a plain Python function.
+    """
+
+    @functools.wraps(func)
+    def wrapped(*args, **kwargs):
+        bound = inspect.signature(func).bind(*args, **kwargs)
+        return func(*bound.args, **bound.kwargs)
+
+    return wrapped
+
+
+def register_cython_tests(cython_mod_name: str, test_mod_name: str):
+    """
+    Registers all callables with name `test_*` in Cython module `cython_mod_name`
+    as attributes in module `test_mod_name`, making them discoverable by pytest.
+    """
+    cython_mod = importlib.import_module(cython_mod_name)
+    for name in dir(cython_mod):
+        item = getattr(cython_mod, name)
+        if callable(item) and name.startswith("test_"):
+            setattr(sys.modules[test_mod_name], name, item)
+
+
 # Fixtures for language tokenizers (languages sorted alphabetically)
 
 
 @pytest.fixture(scope="module")
 def tokenizer():
     return get_lang_class("xx")().tokenizer
 
@@ -235,15 +266,15 @@
 @pytest.fixture(scope="session")
 def hsb_tokenizer():
     return get_lang_class("hsb")().tokenizer
 
 
 @pytest.fixture(scope="session")
 def ko_tokenizer():
-    pytest.importorskip("natto")
+    pytest.importorskip("mecab_ko")
     return get_lang_class("ko")().tokenizer
 
 
 @pytest.fixture(scope="session")
 def ko_tokenizer_tokenizer():
     config = {
         "nlp": {
@@ -258,14 +289,28 @@
 
 @pytest.fixture(scope="module")
 def la_tokenizer():
     return get_lang_class("la")().tokenizer
 
 
 @pytest.fixture(scope="session")
+def ko_tokenizer_natto():
+    pytest.importorskip("natto")
+    config = {
+        "nlp": {
+            "tokenizer": {
+                "@tokenizers": "spacy.KoreanNattoTokenizer.v1",
+            }
+        }
+    }
+    nlp = get_lang_class("ko").from_config(config)
+    return nlp.tokenizer
+
+
+@pytest.fixture(scope="session")
 def lb_tokenizer():
     return get_lang_class("lb")().tokenizer
 
 
 @pytest.fixture(scope="session")
 def lg_tokenizer():
     return get_lang_class("lg")().tokenizer
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/doc/test_add_entities.py` & `spacy-4.0.0.dev0/spacy/tests/doc/test_add_entities.py`

 * *Files 18% similar despite different names*

```diff
@@ -41,14 +41,41 @@
     ner.initialize(lambda: [_ner_example(ner)])
     ner(doc)
     orig_iobs = [t.ent_iob_ for t in doc]
     doc.ents = list(doc.ents)
     assert [t.ent_iob_ for t in doc] == orig_iobs
 
 
+def test_ents_clear(en_vocab):
+    """Ensure that removing entities clears token attributes"""
+    text = ["Louisiana", "Office", "of", "Conservation"]
+    doc = Doc(en_vocab, words=text)
+    entity = Span(doc, 0, 4, label=391, span_id="TEST")
+    doc.ents = [entity]
+    doc.ents = []
+    for token in doc:
+        assert token.ent_iob == 2
+        assert token.ent_type == 0
+        assert token.ent_id == 0
+        assert token.ent_kb_id == 0
+    doc.ents = [entity]
+    doc.set_ents([], default="missing")
+    for token in doc:
+        assert token.ent_iob == 0
+        assert token.ent_type == 0
+        assert token.ent_id == 0
+        assert token.ent_kb_id == 0
+    doc.set_ents([], default="blocked")
+    for token in doc:
+        assert token.ent_iob == 3
+        assert token.ent_type == 0
+        assert token.ent_id == 0
+        assert token.ent_kb_id == 0
+
+
 def test_add_overlapping_entities(en_vocab):
     text = ["Louisiana", "Office", "of", "Conservation"]
     doc = Doc(en_vocab, words=text)
     entity = Span(doc, 0, 4, label=391)
     doc.ents = [entity]
 
     new_entity = Span(doc, 0, 1, label=392)
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/doc/test_array.py` & `spacy-4.0.0.dev0/spacy/tests/doc/test_array.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/doc/test_creation.py` & `spacy-4.0.0.dev0/spacy/tests/doc/test_creation.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/doc/test_doc_api.py` & `spacy-4.0.0.dev0/spacy/tests/doc/test_doc_api.py`

 * *Files 12% similar despite different names*

```diff
@@ -376,17 +376,15 @@
     new_tokens = Doc(tokens.vocab).from_bytes(
         tokens.to_bytes(exclude=["tensor"]), exclude=["tensor"]
     )
     assert tokens.text == new_tokens.text
     assert [t.text for t in tokens] == [t.text for t in new_tokens]
     assert [t.orth for t in tokens] == [t.orth for t in new_tokens]
 
-    new_tokens = Doc(tokens.vocab).from_bytes(
-        tokens.to_bytes(exclude=["sentiment"]), exclude=["sentiment"]
-    )
+    new_tokens = Doc(tokens.vocab).from_bytes(tokens.to_bytes())
     assert tokens.text == new_tokens.text
     assert [t.text for t in tokens] == [t.text for t in new_tokens]
     assert [t.orth for t in tokens] == [t.orth for t in new_tokens]
 
     def inner_func(d1, d2):
         return "hello!"
 
@@ -986,7 +984,16 @@
     doc = en_tokenizer("Some text about Colombia and the Czech Republic")
     doc.spans.setdefault("key1")
     assert len(doc.spans["key1"]) == 0
     doc.spans.setdefault("key2", default=[doc[0:1]])
     assert len(doc.spans["key2"]) == 1
     doc.spans.setdefault("key3", default=SpanGroup(doc, spans=[doc[0:1], doc[1:2]]))
     assert len(doc.spans["key3"]) == 2
+
+
+def test_doc_sentiment_from_bytes_v3_to_v4():
+    """Test if a doc with sentiment attribute created in v3.x works with '.from_bytes' in v4.x without throwing errors. The sentiment attribute was removed in v4"""
+    doc_bytes = b"\x89\xa4text\xa5happy\xaaarray_head\x9fGQACKOLMN\xcd\x01\xc4\xcd\x01\xc6I\xcd\x01\xc5JP\xaaarray_body\x85\xc4\x02nd\xc3\xc4\x04type\xa3<u8\xc4\x04kind\xc4\x00\xc4\x05shape\x92\x01\x0f\xc4\x04data\xc4x\x05\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\xa4\x9a\xd3\x17\xca\xf0b\x03\xa4\x9a\xd3\x17\xca\xf0b\x03\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\xa9sentiment\xcb?\xf0\x00\x00\x00\x00\x00\x00\xa6tensor\x85\xc4\x02nd\xc3\xc4\x04type\xa3<f4\xc4\x04kind\xc4\x00\xc4\x05shape\x91\x00\xc4\x04data\xc4\x00\xa4cats\x80\xa5spans\xc4\x01\x90\xa7strings\x92\xa0\xa5happy\xb2has_unknown_spaces\xc2"
+    doc = Doc(Vocab()).from_bytes(doc_bytes)
+    assert doc.text == "happy"
+    with pytest.raises(AttributeError):
+        doc.sentiment == 1.0
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/doc/test_graph.py` & `spacy-4.0.0.dev0/spacy/tests/doc/test_graph.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/doc/test_json_doc_conversion.py` & `spacy-4.0.0.dev0/spacy/tests/doc/test_json_doc_conversion.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/doc/test_morphanalysis.py` & `spacy-4.0.0.dev0/spacy/tests/doc/test_morphanalysis.py`

 * *Files 4% similar despite different names*

```diff
@@ -29,16 +29,14 @@
     assert i_has[0].morph.key == i_has[0].morph.key
     assert i_has[0].morph.key != i_has[1].morph.key
 
 
 def test_morph_props(i_has):
     assert i_has[0].morph.get("PronType") == ["prs"]
     assert i_has[1].morph.get("PronType") == []
-    assert i_has[1].morph.get("AsdfType", ["asdf"]) == ["asdf"]
-    assert i_has[1].morph.get("AsdfType", default=["asdf", "qwer"]) == ["asdf", "qwer"]
 
 
 def test_morph_iter(i_has):
     assert set(i_has[0].morph) == set(["PronType=prs"])
     assert set(i_has[1].morph) == set(
         ["Number=sing", "Person=three", "Tense=pres", "VerbForm=fin"]
     )
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/doc/test_pickle_doc.py` & `spacy-4.0.0.dev0/spacy/tests/doc/test_pickle_doc.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/doc/test_retokenize_merge.py` & `spacy-4.0.0.dev0/spacy/tests/doc/test_retokenize_merge.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/doc/test_retokenize_split.py` & `spacy-4.0.0.dev0/spacy/tests/doc/test_retokenize_split.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/doc/test_span.py` & `spacy-4.0.0.dev0/spacy/tests/doc/test_span.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 import pytest
 import numpy
 from numpy.testing import assert_array_equal
 
 from spacy.attrs import ORTH, LENGTH
 from spacy.lang.en import English
-from spacy.tokens import Doc, Span, Token
+from spacy.tokens import Doc, Span, SpanGroup, Token
 from spacy.vocab import Vocab
 from spacy.util import filter_spans
 from thinc.api import get_current_ops
 
 from ..util import add_vecs_to_vocab
 from .test_underscore import clean_underscore  # noqa: F401
 
@@ -159,24 +159,24 @@
     span = sents[i_sent].char_span(i, j)
     if not text:
         assert not span
     else:
         assert span.text == text
 
 
-def test_char_span_attributes(doc):
-    label = "LABEL"
-    kb_id = "KB_ID"
-    span_id = "SPAN_ID"
-    span1 = doc.char_span(20, 45, label=label, kb_id=kb_id, span_id=span_id)
-    span2 = doc[1:].char_span(15, 40, label=label, kb_id=kb_id, span_id=span_id)
-    assert span1.text == span2.text
-    assert span1.label_ == span2.label_ == label
-    assert span1.kb_id_ == span2.kb_id_ == kb_id
-    assert span1.id_ == span2.id_ == span_id
+@pytest.mark.issue(9556)
+def test_modify_span_group(doc):
+    group = SpanGroup(doc, spans=doc.ents)
+    for span in group:
+        span.start = 0
+        span.label = doc.vocab.strings["TEST"]
+
+    # Span changes must be reflected in the span group
+    assert group[0].start == 0
+    assert group[0].label == doc.vocab.strings["TEST"]
 
 
 def test_spans_sent_spans(doc):
     sents = list(doc.sents)
     assert sents[0].start == 0
     assert sents[0].end == 5
     assert len(sents) == 3
@@ -301,39 +301,14 @@
     span2 = doc[2:]
     with pytest.warns(UserWarning):
         assert span1.similarity(span2) == 1.0
         assert span1.similarity(doc) == 0.0
         assert span1[:1].similarity(doc.vocab["a"]) == 1.0
 
 
-def test_spans_default_sentiment(en_tokenizer):
-    """Test span.sentiment property's default averaging behaviour"""
-    text = "good stuff bad stuff"
-    tokens = en_tokenizer(text)
-    tokens.vocab[tokens[0].text].sentiment = 3.0
-    tokens.vocab[tokens[2].text].sentiment = -2.0
-    doc = Doc(tokens.vocab, words=[t.text for t in tokens])
-    assert doc[:2].sentiment == 3.0 / 2
-    assert doc[-2:].sentiment == -2.0 / 2
-    assert doc[:-1].sentiment == (3.0 + -2) / 3.0
-
-
-def test_spans_override_sentiment(en_tokenizer):
-    """Test span.sentiment property's default averaging behaviour"""
-    text = "good stuff bad stuff"
-    tokens = en_tokenizer(text)
-    tokens.vocab[tokens[0].text].sentiment = 3.0
-    tokens.vocab[tokens[2].text].sentiment = -2.0
-    doc = Doc(tokens.vocab, words=[t.text for t in tokens])
-    doc.user_span_hooks["sentiment"] = lambda span: 10.0
-    assert doc[:2].sentiment == 10.0
-    assert doc[-2:].sentiment == 10.0
-    assert doc[:-1].sentiment == 10.0
-
-
 def test_spans_are_hashable(en_tokenizer):
     """Test spans can be hashed."""
     text = "good stuff bad stuff"
     tokens = en_tokenizer(text)
     span1 = tokens[:2]
     span2 = tokens[2:4]
     assert hash(span1) != hash(span2)
@@ -375,22 +350,14 @@
 
     # unsupported alignment mode
     with pytest.raises(ValueError):
         span2 = doc.char_span(
             span1.start_char + 1, span1.end_char, label="GPE", alignment_mode="unk"
         )
 
-    # Span.char_span + alignment mode "contract"
-    span2 = doc[0:2].char_span(
-        span1.start_char - 3, span1.end_char, label="GPE", alignment_mode="contract"
-    )
-    assert span1.start_char == span2.start_char
-    assert span1.end_char == span2.end_char
-    assert span2.label_ == "GPE"
-
 
 def test_span_to_array(doc):
     span = doc[1:-2]
     arr = span.to_array([ORTH, LENGTH])
     assert arr.shape == (len(span), 2)
     assert arr[0, 0] == span[0].orth
     assert arr[0, 1] == len(span[0])
@@ -698,36 +665,25 @@
     # add a new span to the original doc
     doc.spans["test"].append(doc[3:4])
     assert len(doc.spans["test"]) == 3
     # check that the copy spans were not modified and this is an isolated doc
     assert len(doc_copy.spans["test"]) == 2
 
 
-def test_for_partial_ent_sents():
-    """Spans may be associated with multiple sentences. These .sents should always be complete, not partial, sentences,
-    which this tests for.
-    """
-    doc = Doc(
-        English().vocab,
-        words=["Mahler's", "Symphony", "No.", "8", "was", "beautiful."],
-        sent_starts=[1, 0, 0, 1, 0, 0],
-    )
-    doc.set_ents([Span(doc, 1, 4, "WORK")])
-    # The specified entity is associated with both sentences in this doc, so we expect all sentences in the doc to be
-    # equal to the sentences referenced in ent.sents.
-    for doc_sent, ent_sent in zip(doc.sents, doc.ents[0].sents):
-        assert doc_sent == ent_sent
-
-
-def test_for_no_ent_sents():
-    """Span.sents() should set .sents correctly, even if Span in question is trailing and doesn't form a full
-    sentence.
-    """
-    doc = Doc(
-        English().vocab,
-        words=["This", "is", "a", "test.", "ENTITY"],
-        sent_starts=[1, 0, 0, 0, 1],
-    )
-    doc.set_ents([Span(doc, 4, 5, "WORK")])
-    sents = list(doc.ents[0].sents)
-    assert len(sents) == 1
-    assert str(sents[0]) == str(doc.ents[0].sent) == "ENTITY"
+@pytest.mark.issue(11113)
+def test_span_ent_id(en_tokenizer):
+    doc = en_tokenizer("a b c d")
+    doc.ents = [Span(doc, 1, 3, label="A", span_id="ID0")]
+    span = doc.ents[0]
+    assert doc[1].ent_id_ == "ID0"
+
+    # setting Span.id sets Token.ent_id
+    span.id_ = "ID1"
+    doc.ents = [span]
+    assert doc.ents[0].ent_id_ == "ID1"
+    assert doc[1].ent_id_ == "ID1"
+
+    # Span.ent_id is an alias of Span.id
+    span.ent_id_ = "ID2"
+    doc.ents = [span]
+    assert doc.ents[0].ent_id_ == "ID2"
+    assert doc[1].ent_id_ == "ID2"
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/doc/test_span_group.py` & `spacy-4.0.0.dev0/spacy/tests/doc/test_span_group.py`

 * *Files 1% similar despite different names*

```diff
@@ -98,16 +98,18 @@
     span_group = doc.spans["SPANS"]
 
     index = 5
     span = span_group[index]
     span.label_ = "NEW LABEL"
     span.kb_id = doc.vocab.strings["KB_ID"]
 
-    assert span_group[index].label != span.label
-    assert span_group[index].kb_id != span.kb_id
+    # Indexing a span group returns a span in which C
+    # data is shared.
+    assert span_group[index].label == span.label
+    assert span_group[index].kb_id == span.kb_id
 
     span_group[index] = span
     assert span_group[index].start == span.start
     assert span_group[index].end == span.end
     assert span_group[index].label == span.label
     assert span_group[index].kb_id == span.kb_id
     assert span_group[index] == span
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/doc/test_token_api.py` & `spacy-4.0.0.dev0/spacy/tests/doc/test_token_api.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/af/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/af/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/af/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/af/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/am/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/am/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/ar/test_exceptions.py` & `spacy-4.0.0.dev0/spacy/tests/lang/ar/test_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/ar/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/ar/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/bn/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/bn/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/ca/test_exception.py` & `spacy-4.0.0.dev0/spacy/tests/lang/ca/test_exception.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/ca/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/ca/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/da/test_exceptions.py` & `spacy-4.0.0.dev0/spacy/tests/lang/da/test_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/da/test_noun_chunks.py` & `spacy-4.0.0.dev0/spacy/tests/lang/da/test_noun_chunks.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/da/test_prefix_suffix_infix.py` & `spacy-4.0.0.dev0/spacy/tests/lang/da/test_prefix_suffix_infix.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/da/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/da/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/de/test_exceptions.py` & `spacy-4.0.0.dev0/spacy/tests/lang/de/test_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/de/test_parser.py` & `spacy-4.0.0.dev0/spacy/tests/lang/de/test_parser.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/de/test_prefix_suffix_infix.py` & `spacy-4.0.0.dev0/spacy/tests/lang/de/test_prefix_suffix_infix.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/de/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/de/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/dsb/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/dsb/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/dsb/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/dsb/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/el/test_exception.py` & `spacy-4.0.0.dev0/spacy/tests/lang/el/test_exception.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/el/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/el/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/en/test_customized_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/en/test_customized_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/en/test_exceptions.py` & `spacy-4.0.0.dev0/spacy/tests/lang/en/test_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/en/test_indices.py` & `spacy-4.0.0.dev0/spacy/tests/lang/en/test_indices.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/en/test_noun_chunks.py` & `spacy-4.0.0.dev0/spacy/tests/lang/en/test_noun_chunks.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/en/test_parser.py` & `spacy-4.0.0.dev0/spacy/tests/lang/en/test_parser.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/en/test_prefix_suffix_infix.py` & `spacy-4.0.0.dev0/spacy/tests/lang/en/test_prefix_suffix_infix.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/en/test_punct.py` & `spacy-4.0.0.dev0/spacy/tests/lang/en/test_punct.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/en/test_sbd.py` & `spacy-4.0.0.dev0/spacy/tests/lang/en/test_sbd.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/en/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/en/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/en/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/en/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/es/test_exception.py` & `spacy-4.0.0.dev0/spacy/tests/lang/es/test_exception.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/es/test_noun_chunks.py` & `spacy-4.0.0.dev0/spacy/tests/lang/es/test_noun_chunks.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/es/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/es/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/et/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/et/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/et/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/et/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/fi/test_noun_chunks.py` & `spacy-4.0.0.dev0/spacy/tests/lang/fi/test_noun_chunks.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/fi/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/fi/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/fi/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/fi/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/fr/test_exceptions.py` & `spacy-4.0.0.dev0/spacy/tests/lang/fr/test_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/fr/test_noun_chunks.py` & `spacy-4.0.0.dev0/spacy/tests/lang/fr/test_noun_chunks.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/fr/test_prefix_suffix_infix.py` & `spacy-4.0.0.dev0/spacy/tests/lang/fr/test_prefix_suffix_infix.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/fr/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/fr/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/ga/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/ga/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/grc/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/grc/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/grc/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/grc/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/gu/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/gu/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/he/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/he/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/hi/test_lex_attrs.py` & `spacy-4.0.0.dev0/spacy/tests/lang/hi/test_lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/hr/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/hr/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/hr/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/hr/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/hsb/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/hsb/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/hsb/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/hsb/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/hu/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/hu/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/hy/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/hy/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/id/test_prefix_suffix_infix.py` & `spacy-4.0.0.dev0/spacy/tests/lang/id/test_prefix_suffix_infix.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/is/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/is/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/is/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/is/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/it/test_noun_chunks.py` & `spacy-4.0.0.dev0/spacy/tests/lang/it/test_noun_chunks.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/ja/test_lemmatization.py` & `spacy-4.0.0.dev0/spacy/tests/lang/ja/test_lemmatization.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/ja/test_serialize.py` & `spacy-4.0.0.dev0/spacy/tests/lang/ja/test_serialize.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/ja/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/ja/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/ko/test_serialize.py` & `spacy-4.0.0.dev0/spacy/tests/lang/th/test_serialize.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,24 +1,24 @@
 import pickle
 
-from spacy.lang.ko import Korean
+from spacy.lang.th import Thai
 from ...util import make_tempdir
 
 
-def test_ko_tokenizer_serialize(ko_tokenizer):
-    tokenizer_bytes = ko_tokenizer.to_bytes()
-    nlp = Korean()
+def test_th_tokenizer_serialize(th_tokenizer):
+    tokenizer_bytes = th_tokenizer.to_bytes()
+    nlp = Thai()
     nlp.tokenizer.from_bytes(tokenizer_bytes)
     assert tokenizer_bytes == nlp.tokenizer.to_bytes()
 
     with make_tempdir() as d:
         file_path = d / "tokenizer"
-        ko_tokenizer.to_disk(file_path)
-        nlp = Korean()
+        th_tokenizer.to_disk(file_path)
+        nlp = Thai()
         nlp.tokenizer.from_disk(file_path)
         assert tokenizer_bytes == nlp.tokenizer.to_bytes()
 
 
-def test_ko_tokenizer_pickle(ko_tokenizer):
-    b = pickle.dumps(ko_tokenizer)
-    ko_tokenizer_re = pickle.loads(b)
-    assert ko_tokenizer.to_bytes() == ko_tokenizer_re.to_bytes()
+def test_th_tokenizer_pickle(th_tokenizer):
+    b = pickle.dumps(th_tokenizer)
+    th_tokenizer_re = pickle.loads(b)
+    assert th_tokenizer.to_bytes() == th_tokenizer_re.to_bytes()
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/ky/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/ky/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/la/test_noun_chunks.py` & `spacy-4.0.0.dev0/spacy/tests/lang/sv/test_noun_chunks.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,52 +1,48 @@
 import pytest
 from spacy.tokens import Doc
 
 
-def test_noun_chunks_is_parsed(la_tokenizer):
-    """Test that noun_chunks raises Value Error for 'la' language if Doc is not parsed.
-    To check this test, we're constructing a Doc
-    with a new Vocab here and forcing is_parsed to 'False'
-    to make sure the noun chunks don't run.
-    """
-    doc = la_tokenizer("Haec est sententia.")
+def test_noun_chunks_is_parsed_sv(sv_tokenizer):
+    """Test that noun_chunks raises Value Error for 'sv' language if Doc is not parsed."""
+    doc = sv_tokenizer("Studenten lste den bsta boken")
     with pytest.raises(ValueError):
         list(doc.noun_chunks)
 
 
-LA_NP_TEST_EXAMPLES = [
+SV_NP_TEST_EXAMPLES = [
     (
-        "Haec narrantur a poetis de Perseo.",
-        ["DET", "VERB", "ADP", "NOUN", "ADP", "PROPN", "PUNCT"],
-        ["nsubj:pass", "ROOT", "case", "obl", "case", "obl", "punct"],
-        [1, 0, -1, -1, -3, -1, -5],
-        ["poetis", "Perseo"],
+        "En student lste en bok",  # A student read a book
+        ["DET", "NOUN", "VERB", "DET", "NOUN"],
+        ["det", "nsubj", "ROOT", "det", "dobj"],
+        [1, 2, 2, 4, 2],
+        ["En student", "en bok"],
     ),
     (
-        "Perseus autem in sinu matris dormiebat.",
-        ["NOUN", "ADV", "ADP", "NOUN", "NOUN", "VERB", "PUNCT"],
-        ["nsubj", "discourse", "case", "obl", "nmod", "ROOT", "punct"],
-        [5, 4, 3, -1, -1, 0, -1],
-        ["Perseus", "sinu matris"],
+        "Studenten lste den bsta boken.",  # The student read the best book
+        ["NOUN", "VERB", "DET", "ADJ", "NOUN", "PUNCT"],
+        ["nsubj", "ROOT", "det", "amod", "dobj", "punct"],
+        [1, 1, 4, 4, 1, 1],
+        ["Studenten", "den bsta boken"],
+    ),
+    (
+        "De samvetslsa skurkarna hade stulit de strsta juvelerna p sndagen",  # The remorseless crooks had stolen the largest jewels that sunday
+        ["DET", "ADJ", "NOUN", "VERB", "VERB", "DET", "ADJ", "NOUN", "ADP", "NOUN"],
+        ["det", "amod", "nsubj", "aux", "root", "det", "amod", "dobj", "case", "nmod"],
+        [2, 2, 4, 4, 4, 7, 7, 4, 9, 4],
+        ["De samvetslsa skurkarna", "de strsta juvelerna", "p sndagen"],
     ),
 ]
 
 
 @pytest.mark.parametrize(
-    "text,pos,deps,heads,expected_noun_chunks", LA_NP_TEST_EXAMPLES
+    "text,pos,deps,heads,expected_noun_chunks", SV_NP_TEST_EXAMPLES
 )
-def test_la_noun_chunks(la_tokenizer, text, pos, deps, heads, expected_noun_chunks):
-    tokens = la_tokenizer(text)
-
+def test_sv_noun_chunks(sv_tokenizer, text, pos, deps, heads, expected_noun_chunks):
+    tokens = sv_tokenizer(text)
     assert len(heads) == len(pos)
-    doc = Doc(
-        tokens.vocab,
-        words=[t.text for t in tokens],
-        heads=[head + i for i, head in enumerate(heads)],
-        deps=deps,
-        pos=pos,
-    )
-
+    words = [t.text for t in tokens]
+    doc = Doc(tokens.vocab, words=words, heads=heads, deps=deps, pos=pos)
     noun_chunks = list(doc.noun_chunks)
     assert len(noun_chunks) == len(expected_noun_chunks)
     for i, np in enumerate(noun_chunks):
         assert np.text == expected_noun_chunks[i]
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/la/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/la/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/lb/test_exceptions.py` & `spacy-4.0.0.dev0/spacy/tests/lang/lb/test_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/lb/test_prefix_suffix_infix.py` & `spacy-4.0.0.dev0/spacy/tests/lang/lb/test_prefix_suffix_infix.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/lb/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/lb/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/lt/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/lt/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/lv/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/lv/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/lv/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/lv/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/mk/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/mk/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/ml/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/ml/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/nb/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/nb/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/ne/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/ne/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/nl/test_noun_chunks.py` & `spacy-4.0.0.dev0/spacy/tests/lang/nl/test_noun_chunks.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/nl/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/nl/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/pl/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/pl/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/pl/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/pl/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/pt/test_noun_chunks.py` & `spacy-4.0.0.dev0/spacy/tests/lang/pt/test_noun_chunks.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/ro/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/ro/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/ru/test_lemmatizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/ru/test_lemmatizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/ru/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/ru/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/sa/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/sa/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/sk/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/sk/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/sl/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/sl/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/sl/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/sl/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/sq/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/sq/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/sq/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/sq/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/sr/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/sr/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/sv/test_exceptions.py` & `spacy-4.0.0.dev0/spacy/tests/lang/sv/test_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/sv/test_lex_attrs.py` & `spacy-4.0.0.dev0/spacy/tests/lang/sv/test_lex_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/sv/test_prefix_suffix_infix.py` & `spacy-4.0.0.dev0/spacy/tests/lang/sv/test_prefix_suffix_infix.py`

 * *Files 15% similar despite different names*

```diff
@@ -28,14 +28,7 @@
     assert tokens[2].text == text.split(",")[1]
 
 
 @pytest.mark.parametrize("text", ["svart...Gul", "svart...gul"])
 def test_tokenizer_splits_ellipsis_infix(sv_tokenizer, text):
     tokens = sv_tokenizer(text)
     assert len(tokens) == 3
-
-
-@pytest.mark.issue(12311)
-@pytest.mark.parametrize("text", ["99:e", "c:a", "EU:s", "Maj:t"])
-def test_sv_tokenizer_handles_colon(sv_tokenizer, text):
-    tokens = sv_tokenizer(text)
-    assert len(tokens) == 1
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/sv/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/sv/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/sv/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/sv/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/ta/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/ta/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/ta/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/ta/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/test_attrs.py` & `spacy-4.0.0.dev0/spacy/tests/lang/test_attrs.py`

 * *Files 10% similar despite different names*

```diff
@@ -22,22 +22,14 @@
 
 @pytest.mark.parametrize("text", ["dog"])
 def test_attrs_idempotence(text):
     int_attrs = intify_attrs({"lemma": text, "is_alpha": True}, strings_map={text: 10})
     assert intify_attrs(int_attrs) == {LEMMA: 10, IS_ALPHA: True}
 
 
-@pytest.mark.parametrize("text", ["dog"])
-def test_attrs_do_deprecated(text):
-    int_attrs = intify_attrs(
-        {"F": text, "is_alpha": True}, strings_map={text: 10}, _do_deprecated=True
-    )
-    assert int_attrs == {ORTH: 10, IS_ALPHA: True}
-
-
 def test_attrs_ent_iob_intify():
     int_attrs = intify_attrs({"ENT_IOB": ""})
     assert int_attrs == {ENT_IOB: 0}
 
     int_attrs = intify_attrs({"ENT_IOB": "I"})
     assert int_attrs == {ENT_IOB: 1}
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/test_initialize.py` & `spacy-4.0.0.dev0/spacy/tests/lang/test_initialize.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/test_lemmatizers.py` & `spacy-4.0.0.dev0/spacy/tests/lang/test_lemmatizers.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/ti/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/ti/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/tl/test_punct.py` & `spacy-4.0.0.dev0/spacy/tests/lang/tl/test_punct.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/tl/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/tl/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/tr/test_parser.py` & `spacy-4.0.0.dev0/spacy/tests/lang/tr/test_parser.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/tr/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/tr/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/tr/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/tr/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/tt/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/tt/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/uk/test_lemmatizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/uk/test_lemmatizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/uk/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/uk/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/vi/test_serialize.py` & `spacy-4.0.0.dev0/spacy/tests/lang/vi/test_serialize.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/vi/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/vi/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/xx/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/xx/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/xx/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/xx/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/yo/test_text.py` & `spacy-4.0.0.dev0/spacy/tests/lang/yo/test_text.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/zh/test_serialize.py` & `spacy-4.0.0.dev0/spacy/tests/lang/zh/test_serialize.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/lang/zh/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/lang/zh/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/matcher/test_dependency_matcher.py` & `spacy-4.0.0.dev0/spacy/tests/matcher/test_dependency_matcher.py`

 * *Files 6% similar despite different names*

```diff
@@ -312,40 +312,24 @@
         ("brown", "brown", "$-", 0),
         ("the", "brown", "$++", 1),
         ("brown", "the", "$++", 0),
         ("brown", "brown", "$++", 0),
         ("the", "brown", "$--", 0),
         ("brown", "the", "$--", 1),
         ("brown", "brown", "$--", 0),
-        ("over", "jumped", "<+", 0),
-        ("quick", "fox", "<+", 0),
-        ("the", "quick", "<+", 0),
-        ("brown", "fox", "<+", 1),
         ("quick", "fox", "<++", 1),
         ("quick", "over", "<++", 0),
         ("over", "jumped", "<++", 0),
         ("the", "fox", "<++", 2),
-        ("brown", "fox", "<-", 0),
-        ("fox", "over", "<-", 0),
-        ("the", "over", "<-", 0),
-        ("over", "jumped", "<-", 1),
         ("brown", "fox", "<--", 0),
         ("fox", "jumped", "<--", 0),
         ("fox", "over", "<--", 1),
-        ("fox", "brown", ">+", 0),
-        ("over", "fox", ">+", 0),
-        ("over", "the", ">+", 0),
-        ("jumped", "over", ">+", 1),
         ("jumped", "over", ">++", 1),
         ("fox", "lazy", ">++", 0),
         ("over", "the", ">++", 0),
-        ("jumped", "over", ">-", 0),
-        ("fox", "quick", ">-", 0),
-        ("brown", "quick", ">-", 0),
-        ("fox", "brown", ">-", 1),
         ("brown", "fox", ">--", 0),
         ("fox", "brown", ">--", 1),
         ("jumped", "fox", ">--", 1),
         ("fox", "the", ">--", 2),
     ],
 )
 def test_dependency_matcher_ops(en_vocab, doc, left, right, op, num_matches):
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/matcher/test_levenshtein.py` & `spacy-4.0.0.dev0/spacy/tests/matcher/test_levenshtein.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/matcher/test_matcher_api.py` & `spacy-4.0.0.dev0/spacy/tests/matcher/test_matcher_api.py`

 * *Files 1% similar despite different names*

```diff
@@ -46,26 +46,23 @@
     text = "Wow  This is really cool!  "
     doc = Doc(en_vocab, words=text.split(" "))
     pos_emoji = ["", "", "", "", "", ""]
     pos_patterns = [[{"ORTH": emoji}] for emoji in pos_emoji]
 
     def label_sentiment(matcher, doc, i, matches):
         match_id, start, end = matches[i]
-        if doc.vocab.strings[match_id] == "HAPPY":
-            doc.sentiment += 0.1
         span = doc[start:end]
         with doc.retokenize() as retokenizer:
             retokenizer.merge(span)
         token = doc[start]
         token.vocab[token.text].norm_ = "happy emoji"
 
     matcher = Matcher(en_vocab)
     matcher.add("HAPPY", pos_patterns, on_match=label_sentiment)
     matcher(doc)
-    assert doc.sentiment != 0
     assert doc[1].norm_ == "happy emoji"
 
 
 def test_matcher_len_contains(matcher):
     assert len(matcher) == 3
     matcher.add("TEST", [[{"ORTH": "test"}]])
     assert "TEST" in matcher
@@ -789,17 +786,24 @@
 
 
 def test_matcher_span(matcher):
     text = "JavaScript is good but Java is better"
     doc = Doc(matcher.vocab, words=text.split())
     span_js = doc[:3]
     span_java = doc[4:]
-    assert len(matcher(doc)) == 2
-    assert len(matcher(span_js)) == 1
-    assert len(matcher(span_java)) == 1
+    doc_matches = matcher(doc)
+    span_js_matches = matcher(span_js)
+    span_java_matches = matcher(span_java)
+    assert len(doc_matches) == 2
+    assert len(span_js_matches) == 1
+    assert len(span_java_matches) == 1
+
+    # match offsets always refer to the doc
+    assert doc_matches[0] == span_js_matches[0]
+    assert doc_matches[1] == span_java_matches[0]
 
 
 def test_matcher_as_spans(matcher):
     """Test the new as_spans=True API."""
     text = "JavaScript is good but Java is better"
     doc = Doc(matcher.vocab, words=text.split())
     matches = matcher(doc, as_spans=True)
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/matcher/test_matcher_logic.py` & `spacy-4.0.0.dev0/spacy/tests/matcher/test_matcher_logic.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/matcher/test_pattern_validation.py` & `spacy-4.0.0.dev0/spacy/tests/matcher/test_pattern_validation.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/matcher/test_phrase_matcher.py` & `spacy-4.0.0.dev0/spacy/tests/matcher/test_phrase_matcher.py`

 * *Files 2% similar despite different names*

```diff
@@ -83,30 +83,31 @@
     assert isinstance(matcher.vocab, Vocab)
     matcher = PhraseMatcher(Vocab())
     assert isinstance(matcher.vocab, Vocab)
 
 
 @pytest.mark.issue(4651)
 def test_issue4651_with_phrase_matcher_attr():
-    """Test that the EntityRuler PhraseMatcher is deserialized correctly using
-    the method from_disk when the EntityRuler argument phrase_matcher_attr is
+    """Test that the entity_ruler PhraseMatcher is deserialized correctly using
+    the method from_disk when the entity_ruler argument phrase_matcher_attr is
     specified.
     """
     text = "Spacy is a python library for nlp"
     nlp = English()
     patterns = [{"label": "PYTHON_LIB", "pattern": "spacy", "id": "spaCy"}]
-    ruler = nlp.add_pipe("entity_ruler", config={"phrase_matcher_attr": "LOWER"})
+    config = {"phrase_matcher_attr": "LOWER"}
+    ruler = nlp.add_pipe("entity_ruler", config=config)
     ruler.add_patterns(patterns)
     doc = nlp(text)
     res = [(ent.text, ent.label_, ent.ent_id_) for ent in doc.ents]
     nlp_reloaded = English()
     with make_tempdir() as d:
         file_path = d / "entityruler"
         ruler.to_disk(file_path)
-        nlp_reloaded.add_pipe("entity_ruler").from_disk(file_path)
+        nlp_reloaded.add_pipe("entity_ruler", config=config).from_disk(file_path)
     doc_reloaded = nlp_reloaded(text)
     res_reloaded = [(ent.text, ent.label_, ent.ent_id_) for ent in doc_reloaded.ents]
     assert res == res_reloaded
 
 
 @pytest.mark.issue(6839)
 def test_issue6839(en_vocab):
@@ -194,36 +195,14 @@
 def test_phrase_matcher_contains(en_vocab):
     matcher = PhraseMatcher(en_vocab)
     matcher.add("TEST", [Doc(en_vocab, words=["test"])])
     assert "TEST" in matcher
     assert "TEST2" not in matcher
 
 
-def test_phrase_matcher_add_new_api(en_vocab):
-    doc = Doc(en_vocab, words=["a", "b"])
-    patterns = [Doc(en_vocab, words=["a"]), Doc(en_vocab, words=["a", "b"])]
-    matcher = PhraseMatcher(en_vocab)
-    matcher.add("OLD_API", None, *patterns)
-    assert len(matcher(doc)) == 2
-    matcher = PhraseMatcher(en_vocab)
-    on_match = Mock()
-    matcher.add("OLD_API_CALLBACK", on_match, *patterns)
-    assert len(matcher(doc)) == 2
-    assert on_match.call_count == 2
-    # New API: add(key: str, patterns: List[List[dict]], on_match: Callable)
-    matcher = PhraseMatcher(en_vocab)
-    matcher.add("NEW_API", patterns)
-    assert len(matcher(doc)) == 2
-    matcher = PhraseMatcher(en_vocab)
-    on_match = Mock()
-    matcher.add("NEW_API_CALLBACK", patterns, on_match=on_match)
-    assert len(matcher(doc)) == 2
-    assert on_match.call_count == 2
-
-
 def test_phrase_matcher_repeated_add(en_vocab):
     matcher = PhraseMatcher(en_vocab)
     # match ID only gets added once
     matcher.add("TEST", [Doc(en_vocab, words=["like"])])
     matcher.add("TEST", [Doc(en_vocab, words=["like"])])
     matcher.add("TEST", [Doc(en_vocab, words=["like"])])
     matcher.add("TEST", [Doc(en_vocab, words=["like"])])
@@ -464,14 +443,21 @@
     with pytest.warns(DeprecationWarning) as record:
         for _ in matcher.pipe([doc]):
             pass
         assert record.list
         assert "spaCy v3.0" in str(record.list[0].message)
 
 
+def test_phrase_matcher_non_doc(en_vocab):
+    matcher = PhraseMatcher(en_vocab)
+    doc = Doc(en_vocab, words=["hello", "world"])
+    with pytest.raises(ValueError):
+        matcher.add("TEST", [doc, "junk"])
+
+
 @pytest.mark.parametrize("attr", ["SENT_START", "IS_SENT_START"])
 def test_phrase_matcher_sent_start(en_vocab, attr):
     _ = PhraseMatcher(en_vocab, attr=attr)  # noqa: F841
 
 
 def test_span_in_phrasematcher(en_vocab):
     """Ensure that PhraseMatcher accepts Span and Doc as input"""
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/morphology/test_morph_converters.py` & `spacy-4.0.0.dev0/spacy/tests/morphology/test_morph_converters.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/morphology/test_morph_features.py` & `spacy-4.0.0.dev0/spacy/tests/morphology/test_morph_features.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/morphology/test_morph_pickle.py` & `spacy-4.0.0.dev0/spacy/tests/morphology/test_morph_pickle.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/package/requirements.txt` & `spacy-4.0.0.dev0/spacy/tests/package/requirements.txt`

 * *Files 13% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 # Our libraries
 spacy-legacy>=3.0.11,<3.1.0
 spacy-loggers>=1.0.0,<2.0.0
 cymem>=2.0.2,<2.1.0
 preshed>=3.0.2,<3.1.0
-thinc>=8.1.8,<8.2.0
+thinc>=9.0.0.dev2,<9.1.0
 ml_datasets>=0.2.0,<0.3.0
 murmurhash>=0.28.0,<1.1.0
 wasabi>=0.9.1,<1.2.0
 srsly>=2.4.3,<3.0.0
 catalogue>=2.0.6,<2.1.0
 typer>=0.3.0,<0.8.0
 pathy>=0.10.0
@@ -18,23 +18,23 @@
 tqdm>=4.38.0,<5.0.0
 pydantic>=1.7.4,!=1.8,!=1.8.1,<1.11.0
 jinja2
 langcodes>=3.2.0,<4.0.0
 # Official Python utilities
 setuptools
 packaging>=20.0
-typing_extensions>=3.7.4.1,<4.5.0; python_version < "3.8"
+typing_extensions>=3.7.4.1,<4.2.0; python_version < "3.8"
 # Development dependencies
 pre-commit>=2.13.0
 cython>=0.25,<3.0
 pytest>=5.2.0,!=7.1.0
 pytest-timeout>=1.3.0,<2.0.0
 mock>=2.0.0,<3.0.0
 flake8>=3.8.0,<6.0.0
 hypothesis>=3.27.0,<7.0.0
-mypy>=0.990,<1.1.0; platform_machine != "aarch64" and python_version >= "3.7"
+mypy>=0.990,<0.1000; platform_machine != "aarch64" and python_version >= "3.7"
 types-dataclasses>=0.1.3; python_version < "3.7"
 types-mock>=0.1.1
 types-setuptools>=57.0.0
 types-requests
 types-setuptools>=57.0.0
-black==22.3.0
+black>=22.0,<23.0
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/package/setup.cfg` & `spacy-4.0.0.dev0/spacy/tests/package/setup.cfg`

 * *Files 10% similar despite different names*

```diff
@@ -28,30 +28,22 @@
     Release notes = https://github.com/explosion/spaCy/releases
     Source = https://github.com/explosion/spaCy
 
 [options]
 zip_safe = false
 include_package_data = true
 python_requires = >=3.6
-setup_requires =
-    cython>=0.25,<3.0
-    numpy>=1.15.0
-    # We also need our Cython packages here to compile against
-    cymem>=2.0.2,<2.1.0
-    preshed>=3.0.2,<3.1.0
-    murmurhash>=0.28.0,<1.1.0
-    thinc>=8.1.8,<8.2.0
 install_requires =
     # Our libraries
     spacy-legacy>=3.0.11,<3.1.0
     spacy-loggers>=1.0.0,<2.0.0
     murmurhash>=0.28.0,<1.1.0
     cymem>=2.0.2,<2.1.0
     preshed>=3.0.2,<3.1.0
-    thinc>=8.1.8,<8.2.0
+    thinc>=9.0.0.dev2,<9.1.0
     wasabi>=0.9.1,<1.2.0
     srsly>=2.4.3,<3.0.0
     catalogue>=2.0.6,<2.1.0
     # Third-party dependencies
     typer>=0.3.0,<0.8.0
     pathy>=0.10.0
     smart-open>=5.2.1,<7.0.0
@@ -59,72 +51,72 @@
     numpy>=1.15.0
     requests>=2.13.0,<3.0.0
     pydantic>=1.7.4,!=1.8,!=1.8.1,<1.11.0
     jinja2
     # Official Python utilities
     setuptools
     packaging>=20.0
-    typing_extensions>=3.7.4.1,<4.5.0; python_version < "3.8"
+    typing_extensions>=3.7.4,<4.2.0; python_version < "3.8"
     langcodes>=3.2.0,<4.0.0
 
 [options.entry_points]
 console_scripts =
     spacy = spacy.cli:setup_cli
 
 [options.extras_require]
 lookups =
     spacy_lookups_data>=1.0.3,<1.1.0
 transformers =
     spacy_transformers>=1.1.2,<1.3.0
 ray =
     spacy_ray>=0.1.0,<1.0.0
 cuda =
-    cupy>=5.0.0b4,<13.0.0
+    cupy>=5.0.0b4,<12.0.0
 cuda80 =
-    cupy-cuda80>=5.0.0b4,<13.0.0
+    cupy-cuda80>=5.0.0b4,<12.0.0
 cuda90 =
-    cupy-cuda90>=5.0.0b4,<13.0.0
+    cupy-cuda90>=5.0.0b4,<12.0.0
 cuda91 =
-    cupy-cuda91>=5.0.0b4,<13.0.0
+    cupy-cuda91>=5.0.0b4,<12.0.0
 cuda92 =
-    cupy-cuda92>=5.0.0b4,<13.0.0
+    cupy-cuda92>=5.0.0b4,<12.0.0
 cuda100 =
-    cupy-cuda100>=5.0.0b4,<13.0.0
+    cupy-cuda100>=5.0.0b4,<12.0.0
 cuda101 =
-    cupy-cuda101>=5.0.0b4,<13.0.0
+    cupy-cuda101>=5.0.0b4,<12.0.0
 cuda102 =
-    cupy-cuda102>=5.0.0b4,<13.0.0
+    cupy-cuda102>=5.0.0b4,<12.0.0
 cuda110 =
-    cupy-cuda110>=5.0.0b4,<13.0.0
+    cupy-cuda110>=5.0.0b4,<12.0.0
 cuda111 =
-    cupy-cuda111>=5.0.0b4,<13.0.0
+    cupy-cuda111>=5.0.0b4,<12.0.0
 cuda112 =
-    cupy-cuda112>=5.0.0b4,<13.0.0
+    cupy-cuda112>=5.0.0b4,<12.0.0
 cuda113 =
-    cupy-cuda113>=5.0.0b4,<13.0.0
+    cupy-cuda113>=5.0.0b4,<12.0.0
 cuda114 =
-    cupy-cuda114>=5.0.0b4,<13.0.0
+    cupy-cuda114>=5.0.0b4,<12.0.0
 cuda115 =
-    cupy-cuda115>=5.0.0b4,<13.0.0
+    cupy-cuda115>=5.0.0b4,<12.0.0
 cuda116 =
-    cupy-cuda116>=5.0.0b4,<13.0.0
+    cupy-cuda116>=5.0.0b4,<12.0.0
 cuda117 =
-    cupy-cuda117>=5.0.0b4,<13.0.0
+    cupy-cuda117>=5.0.0b4,<12.0.0
 cuda11x =
-    cupy-cuda11x>=11.0.0,<13.0.0
+    cupy-cuda11x>=11.0.0,<12.0.0
 cuda-autodetect =
-    cupy-wheel>=11.0.0,<13.0.0
+    cupy-wheel>=11.0.0,<12.0.0
 apple =
     thinc-apple-ops>=0.1.0.dev0,<1.0.0
 # Language tokenizers with external dependencies
 ja =
     sudachipy>=0.5.2,!=0.6.1
     sudachidict_core>=20211220
 ko =
-    natto-py>=0.9.0
+    mecab-ko>=1.0.0
 th =
     pythainlp>=2.0
 
 [bdist_wheel]
 universal = false
 
 [sdist]
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/package/test_requirements.py` & `spacy-4.0.0.dev0/spacy/tests/package/test_requirements.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 import re
 from pathlib import Path
 
 
 def test_build_dependencies():
     # Check that library requirements are pinned exactly the same across different setup files.
-    # TODO: correct checks for numpy rather than ignoring
     libs_ignore_requirements = [
+        "cython",
         "pytest",
         "pytest-timeout",
         "mock",
         "flake8",
         "hypothesis",
         "pre-commit",
         "black",
@@ -18,15 +18,15 @@
         "types-mock",
         "types-requests",
         "types-setuptools",
     ]
     # ignore language-specific packages that shouldn't be installed by all
     libs_ignore_setup = [
         "fugashi",
-        "natto-py",
+        "mecab-ko",
         "pythainlp",
         "sudachipy",
         "sudachidict_core",
         "spacy-pkuseg",
         "thinc-apple-ops",
     ]
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/parser/test_add_label.py` & `spacy-4.0.0.dev0/spacy/tests/parser/test_add_label.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/parser/test_arc_eager_oracle.py` & `spacy-4.0.0.dev0/spacy/tests/parser/test_arc_eager_oracle.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/parser/test_ner.py` & `spacy-4.0.0.dev0/spacy/tests/parser/test_ner.py`

 * *Files 2% similar despite different names*

```diff
@@ -5,23 +5,24 @@
 
 from spacy.attrs import ENT_IOB
 from spacy import util, registry
 from spacy.lang.en import English
 from spacy.lang.it import Italian
 from spacy.language import Language
 from spacy.lookups import Lookups
-from spacy.pipeline import EntityRecognizer
-from spacy.pipeline.ner import DEFAULT_NER_MODEL
 from spacy.pipeline._parser_internals.ner import BiluoPushDown
 from spacy.training import Example, iob_to_biluo, split_bilu_label
 from spacy.tokens import Doc, Span
 from spacy.vocab import Vocab
+from thinc.api import fix_random_seed
 import logging
 
 from ..util import make_tempdir
+from ...pipeline import EntityRecognizer
+from ...pipeline.ner import DEFAULT_NER_MODEL
 
 TRAIN_DATA = [
     ("Who is Shaka Khan?", {"entities": [(7, 17, "PERSON")]}),
     ("I like London and Berlin.", {"entities": [(7, 13, "LOC"), (18, 24, "LOC")]}),
 ]
 
 
@@ -408,15 +409,15 @@
 
     nlp = English()
     train_examples = []
     for t in train_data:
         train_examples.append(Example.from_dict(nlp.make_doc(t[0]), t[1]))
     ner = nlp.add_pipe("ner", last=True)
     ner.add_label("PERSON")
-    nlp.initialize()
+    nlp.initialize(get_examples=lambda: train_examples)
     for itn in range(2):
         losses = {}
         batches = util.minibatch(train_examples, size=8)
         for batch in batches:
             nlp.update(batch, losses=losses)
 
 
@@ -535,19 +536,19 @@
     doc = nlp("This is Antti L Korhonen speaking in Finland")
     expected_iobs = ["O", "O", "B", "B", "B", "O", "O", "O"]
     expected_types = ["", "", "", "", "", "", "", ""]
     assert [token.ent_iob_ for token in doc] == expected_iobs
     assert [token.ent_type_ for token in doc] == expected_types
 
 
-@pytest.mark.parametrize("use_upper", [True, False])
-def test_overfitting_IO(use_upper):
+def test_overfitting_IO():
+    fix_random_seed(1)
     # Simple test to try and quickly overfit the NER component
     nlp = English()
-    ner = nlp.add_pipe("ner", config={"model": {"use_upper": use_upper}})
+    ner = nlp.add_pipe("ner", config={"model": {}})
     train_examples = []
     for text, annotations in TRAIN_DATA:
         train_examples.append(Example.from_dict(nlp.make_doc(text), annotations))
         for ent in annotations.get("entities"):
             ner.add_label(ent[2])
     optimizer = nlp.initialize()
 
@@ -571,15 +572,14 @@
         doc2 = nlp2(test_text)
         ents2 = doc2.ents
         assert len(ents2) == 1
         assert ents2[0].text == "London"
         assert ents2[0].label_ == "LOC"
         # Ensure that the predictions are still the same, even after adding a new label
         ner2 = nlp2.get_pipe("ner")
-        assert ner2.model.attrs["has_upper"] == use_upper
         ner2.add_label("RANDOM_NEW_LABEL")
         doc3 = nlp2(test_text)
         ents3 = doc3.ents
         assert len(ents3) == 1
         assert ents3[0].text == "London"
         assert ents3[0].label_ == "LOC"
 
@@ -613,14 +613,60 @@
     assert ents[0].kb_id == 1234
     # ent added by ner has kb_id == 0
     assert ents[1].text == "London"
     assert ents[1].label_ == "LOC"
     assert ents[1].kb_id == 0
 
 
+def test_is_distillable():
+    nlp = English()
+    ner = nlp.add_pipe("ner")
+    assert ner.is_distillable
+
+
+def test_distill():
+    teacher = English()
+    teacher_ner = teacher.add_pipe("ner")
+    train_examples = []
+    for text, annotations in TRAIN_DATA:
+        train_examples.append(Example.from_dict(teacher.make_doc(text), annotations))
+        for ent in annotations.get("entities"):
+            teacher_ner.add_label(ent[2])
+
+    optimizer = teacher.initialize(get_examples=lambda: train_examples)
+
+    for i in range(50):
+        losses = {}
+        teacher.update(train_examples, sgd=optimizer, losses=losses)
+    assert losses["ner"] < 0.00001
+
+    student = English()
+    student_ner = student.add_pipe("ner")
+    student_ner.initialize(
+        get_examples=lambda: train_examples, labels=teacher_ner.label_data
+    )
+
+    distill_examples = [
+        Example.from_dict(teacher.make_doc(t[0]), {}) for t in TRAIN_DATA
+    ]
+
+    for i in range(100):
+        losses = {}
+        student_ner.distill(teacher_ner, distill_examples, sgd=optimizer, losses=losses)
+    assert losses["ner"] < 0.0001
+
+    # test the trained model
+    test_text = "I like London."
+    doc = student(test_text)
+    ents = doc.ents
+    assert len(ents) == 1
+    assert ents[0].text == "London"
+    assert ents[0].label_ == "LOC"
+
+
 def test_beam_ner_scores():
     # Test that we can get confidence values out of the beam_ner pipe
     beam_width = 16
     beam_density = 0.0001
     nlp = English()
     config = {
         "beam_width": beam_width,
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/parser/test_neural_parser.py` & `spacy-4.0.0.dev0/spacy/tests/parser/test_neural_parser.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/parser/test_nn_beam.py` & `spacy-4.0.0.dev0/spacy/tests/parser/test_nn_beam.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/parser/test_nonproj.py` & `spacy-4.0.0.dev0/spacy/tests/parser/test_nonproj.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/parser/test_parse.py` & `spacy-4.0.0.dev0/spacy/tests/parser/test_parse.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,22 +1,26 @@
+import itertools
 import pytest
+import numpy
 from numpy.testing import assert_equal
 from thinc.api import Adam
 
 from spacy import registry, util
 from spacy.attrs import DEP, NORM
 from spacy.lang.en import English
-from spacy.tokens import Doc
 from spacy.training import Example
+from spacy.tokens import Doc
 from spacy.vocab import Vocab
-from spacy.pipeline import DependencyParser
-from spacy.pipeline.dep_parser import DEFAULT_PARSER_MODEL
-from spacy.pipeline.tok2vec import DEFAULT_TOK2VEC_MODEL
+from spacy import util, registry
+from thinc.api import fix_random_seed
 
+from ...pipeline import DependencyParser
+from ...pipeline.dep_parser import DEFAULT_PARSER_MODEL
 from ..util import apply_transition_sequence, make_tempdir
+from ...pipeline.tok2vec import DEFAULT_TOK2VEC_MODEL
 
 TRAIN_DATA = [
     (
         "They trade mortgage-backed securities.",
         {
             "heads": [1, 1, 4, 4, 5, 1, 1],
             "deps": ["nsubj", "ROOT", "compound", "punct", "nmod", "dobj", "punct"],
@@ -55,14 +59,16 @@
         {
             "heads": [1, 1, 1, None],
             "deps": ["nsubj", "ROOT", "dobj", None],
         },
     ),
 ]
 
+PARSERS = ["parser"]  # TODO: Test beam_parser when ready
+
 eps = 0.1
 
 
 @pytest.fixture
 def vocab():
     return Vocab(lex_attr_getters={NORM: lambda s: s})
 
@@ -167,14 +173,65 @@
     doc = Doc(en_vocab, words=words, heads=[0], deps=["ROOT"])
     assert len(doc) == 1
     with en_parser.step_through(doc) as _:  # noqa: F841
         pass
     assert doc[0].dep != 0
 
 
+def test_parser_apply_actions(en_vocab, en_parser):
+    words = ["I", "ate", "pizza"]
+    words2 = ["Eat", "more", "pizza", "!"]
+    doc1 = Doc(en_vocab, words=words)
+    doc2 = Doc(en_vocab, words=words2)
+    docs = [doc1, doc2]
+
+    moves = en_parser.moves
+    moves.add_action(0, "")
+    moves.add_action(1, "")
+    moves.add_action(2, "nsubj")
+    moves.add_action(3, "obj")
+    moves.add_action(2, "amod")
+
+    actions = [
+        numpy.array([0, 0], dtype="i"),
+        numpy.array([2, 0], dtype="i"),
+        numpy.array([0, 4], dtype="i"),
+        numpy.array([3, 3], dtype="i"),
+        numpy.array([1, 1], dtype="i"),
+        numpy.array([1, 1], dtype="i"),
+        numpy.array([0], dtype="i"),
+        numpy.array([1], dtype="i"),
+    ]
+
+    states = moves.init_batch(docs)
+    active_states = states
+
+    for step_actions in actions:
+        active_states = moves.apply_actions(active_states, step_actions)
+
+    assert len(active_states) == 0
+
+    for (state, doc) in zip(states, docs):
+        moves.set_annotations(state, doc)
+
+    assert docs[0][0].head.i == 1
+    assert docs[0][0].dep_ == "nsubj"
+    assert docs[0][1].head.i == 1
+    assert docs[0][1].dep_ == "ROOT"
+    assert docs[0][2].head.i == 1
+    assert docs[0][2].dep_ == "obj"
+
+    assert docs[1][0].head.i == 0
+    assert docs[1][0].dep_ == "ROOT"
+    assert docs[1][1].head.i == 2
+    assert docs[1][1].dep_ == "amod"
+    assert docs[1][2].head.i == 0
+    assert docs[1][2].dep_ == "obj"
+
+
 @pytest.mark.skip(
     reason="The step_through API was removed (but should be brought back)"
 )
 def test_parser_initial(en_vocab, en_parser):
     words = ["I", "ate", "the", "pizza", "with", "anchovies", "."]
     transition = ["L-nsubj", "S", "L-det"]
     doc = Doc(en_vocab, words=words)
@@ -315,15 +372,15 @@
     }
     cfg = {"model": DEFAULT_PARSER_MODEL}
     model = registry.resolve(cfg, validate=True)["model"]
     DependencyParser(en_vocab, model, **config)
     DependencyParser(en_vocab, model)
 
 
-@pytest.mark.parametrize("pipe_name", ["parser", "beam_parser"])
+@pytest.mark.parametrize("pipe_name", PARSERS)
 def test_incomplete_data(pipe_name):
     # Test that the parser works with incomplete information
     nlp = English()
     parser = nlp.add_pipe(pipe_name)
     train_examples = []
     for text, annotations in PARTIAL_DATA:
         train_examples.append(Example.from_dict(nlp.make_doc(text), annotations))
@@ -341,19 +398,23 @@
     doc = nlp(test_text)
     assert doc[0].dep_ == "nsubj"
     assert doc[2].dep_ == "dobj"
     assert doc[0].head.i == 1
     assert doc[2].head.i == 1
 
 
-@pytest.mark.parametrize("pipe_name", ["parser", "beam_parser"])
-def test_overfitting_IO(pipe_name):
+@pytest.mark.parametrize(
+    "pipe_name,max_moves", itertools.product(PARSERS, [0, 1, 5, 100])
+)
+def test_overfitting_IO(pipe_name, max_moves):
+    fix_random_seed(0)
     # Simple test to try and quickly overfit the dependency parser (normal or beam)
     nlp = English()
     parser = nlp.add_pipe(pipe_name)
+    parser.cfg["update_with_oracle_cut_size"] = max_moves
     train_examples = []
     for text, annotations in TRAIN_DATA:
         train_examples.append(Example.from_dict(nlp.make_doc(text), annotations))
         for dep in annotations.get("deps", []):
             parser.add_label(dep)
     optimizer = nlp.initialize()
     # run overfitting
@@ -392,24 +453,75 @@
     batch_deps_1 = [doc.to_array([DEP]) for doc in nlp.pipe(texts)]
     batch_deps_2 = [doc.to_array([DEP]) for doc in nlp.pipe(texts)]
     no_batch_deps = [doc.to_array([DEP]) for doc in [nlp(text) for text in texts]]
     assert_equal(batch_deps_1, batch_deps_2)
     assert_equal(batch_deps_1, no_batch_deps)
 
 
+def test_is_distillable():
+    nlp = English()
+    parser = nlp.add_pipe("parser")
+    assert parser.is_distillable
+
+
+def test_distill():
+    teacher = English()
+    teacher_parser = teacher.add_pipe("parser")
+    train_examples = []
+    for text, annotations in TRAIN_DATA:
+        train_examples.append(Example.from_dict(teacher.make_doc(text), annotations))
+        for dep in annotations.get("deps", []):
+            teacher_parser.add_label(dep)
+
+    optimizer = teacher.initialize(get_examples=lambda: train_examples)
+
+    for i in range(200):
+        losses = {}
+        teacher.update(train_examples, sgd=optimizer, losses=losses)
+    assert losses["parser"] < 0.0001
+
+    student = English()
+    student_parser = student.add_pipe("parser")
+    student_parser.initialize(
+        get_examples=lambda: train_examples, labels=teacher_parser.label_data
+    )
+
+    distill_examples = [
+        Example.from_dict(teacher.make_doc(t[0]), {}) for t in TRAIN_DATA
+    ]
+
+    for i in range(200):
+        losses = {}
+        student_parser.distill(
+            teacher_parser, distill_examples, sgd=optimizer, losses=losses
+        )
+    assert losses["parser"] < 0.0001
+
+    test_text = "I like securities."
+    doc = student(test_text)
+    assert doc[0].dep_ == "nsubj"
+    assert doc[2].dep_ == "dobj"
+    assert doc[3].dep_ == "punct"
+    assert doc[0].head.i == 1
+    assert doc[2].head.i == 1
+    assert doc[3].head.i == 1
+
+
 # fmt: off
 @pytest.mark.slow
 @pytest.mark.parametrize("pipe_name", ["parser", "beam_parser"])
 @pytest.mark.parametrize(
     "parser_config",
     [
-        # TransitionBasedParser V1
-        ({"@architectures": "spacy.TransitionBasedParser.v1", "tok2vec": DEFAULT_TOK2VEC_MODEL, "state_type": "parser", "extra_state_tokens": False, "hidden_width": 64, "maxout_pieces": 2, "use_upper": True}),
-        # TransitionBasedParser V2
+        # TODO: re-enable after we have a spacy-legacy release for v4. See
+        # https://github.com/explosion/spacy-legacy/pull/36
+        #({"@architectures": "spacy.TransitionBasedParser.v1", "tok2vec": DEFAULT_TOK2VEC_MODEL, "state_type": "parser", "extra_state_tokens": False, "hidden_width": 64, "maxout_pieces": 2, "use_upper": True}),
         ({"@architectures": "spacy.TransitionBasedParser.v2", "tok2vec": DEFAULT_TOK2VEC_MODEL, "state_type": "parser", "extra_state_tokens": False, "hidden_width": 64, "maxout_pieces": 2, "use_upper": True}),
+        ({"@architectures": "spacy.TransitionBasedParser.v2", "tok2vec": DEFAULT_TOK2VEC_MODEL, "state_type": "parser", "extra_state_tokens": False, "hidden_width": 64, "maxout_pieces": 2, "use_upper": False}),
+        ({"@architectures": "spacy.TransitionBasedParser.v3", "tok2vec": DEFAULT_TOK2VEC_MODEL, "state_type": "parser", "extra_state_tokens": False, "hidden_width": 64, "maxout_pieces": 2}),
     ],
 )
 # fmt: on
 def test_parser_configs(pipe_name, parser_config):
     pipe_config = {"model": parser_config}
     nlp = English()
     parser = nlp.add_pipe(pipe_name, config=pipe_config)
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/parser/test_parse_navigate.py` & `spacy-4.0.0.dev0/spacy/tests/parser/test_parse_navigate.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/parser/test_preset_sbd.py` & `spacy-4.0.0.dev0/spacy/tests/parser/test_preset_sbd.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/parser/test_space_attachment.py` & `spacy-4.0.0.dev0/spacy/tests/parser/test_space_attachment.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/parser/test_state.py` & `spacy-4.0.0.dev0/spacy/tests/parser/test_state.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/pipeline/test_analysis.py` & `spacy-4.0.0.dev0/spacy/tests/pipeline/test_analysis.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/pipeline/test_annotates_on_update.py` & `spacy-4.0.0.dev0/spacy/tests/pipeline/test_annotates_on_update.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/pipeline/test_attributeruler.py` & `spacy-4.0.0.dev0/spacy/tests/pipeline/test_attributeruler.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/pipeline/test_edit_tree_lemmatizer.py` & `spacy-4.0.0.dev0/spacy/tests/pipeline/test_edit_tree_lemmatizer.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,15 +1,17 @@
+from typing import cast
 import pickle
 import pytest
 from hypothesis import given
 import hypothesis.strategies as st
 from spacy import util
 from spacy.lang.en import English
 from spacy.language import Language
 from spacy.pipeline._edit_tree_internals.edit_trees import EditTrees
+from spacy.pipeline.trainable_pipe import TrainablePipe
 from spacy.training import Example
 from spacy.strings import StringStore
 from spacy.util import make_tempdir
 
 
 TRAIN_DATA = [
     ("She likes green eggs", {"lemmas": ["she", "like", "green", "egg"]}),
@@ -97,38 +99,36 @@
                 "suffix_tree": 4294967295,
             },
         ],
         "labels": (1, 3, 4, 6),
     }
 
 
-@pytest.mark.parametrize("top_k", (1, 5, 30))
-def test_no_data(top_k):
+def test_no_data():
     # Test that the lemmatizer provides a nice error when there's no tagging data / labels
     TEXTCAT_DATA = [
         ("I'm so happy.", {"cats": {"POSITIVE": 1.0, "NEGATIVE": 0.0}}),
         ("I'm so angry", {"cats": {"POSITIVE": 0.0, "NEGATIVE": 1.0}}),
     ]
     nlp = English()
-    nlp.add_pipe("trainable_lemmatizer", config={"top_k": top_k})
+    nlp.add_pipe("trainable_lemmatizer")
     nlp.add_pipe("textcat")
 
     train_examples = []
     for t in TEXTCAT_DATA:
         train_examples.append(Example.from_dict(nlp.make_doc(t[0]), t[1]))
 
     with pytest.raises(ValueError):
         nlp.initialize(get_examples=lambda: train_examples)
 
 
-@pytest.mark.parametrize("top_k", (1, 5, 30))
-def test_incomplete_data(top_k):
+def test_incomplete_data():
     # Test that the lemmatizer works with incomplete information
     nlp = English()
-    lemmatizer = nlp.add_pipe("trainable_lemmatizer", config={"top_k": top_k})
+    lemmatizer = nlp.add_pipe("trainable_lemmatizer")
     lemmatizer.min_tree_freq = 1
     train_examples = []
     for t in PARTIAL_DATA:
         train_examples.append(Example.from_dict(nlp.make_doc(t[0]), t[1]))
     optimizer = nlp.initialize(get_examples=lambda: train_examples)
     for i in range(50):
         losses = {}
@@ -137,33 +137,18 @@
 
     # test the trained model
     test_text = "She likes blue eggs"
     doc = nlp(test_text)
     assert doc[1].lemma_ == "like"
     assert doc[2].lemma_ == "blue"
 
-    # Check that incomplete annotations are ignored.
-    scores, _ = lemmatizer.model([eg.predicted for eg in train_examples], is_train=True)
-    _, dX = lemmatizer.get_loss(train_examples, scores)
-    xp = lemmatizer.model.ops.xp
-
-    # Missing annotations.
-    assert xp.count_nonzero(dX[0][0]) == 0
-    assert xp.count_nonzero(dX[0][3]) == 0
-    assert xp.count_nonzero(dX[1][0]) == 0
-    assert xp.count_nonzero(dX[1][3]) == 0
-
-    # Misaligned annotations.
-    assert xp.count_nonzero(dX[1][1]) == 0
 
-
-@pytest.mark.parametrize("top_k", (1, 5, 30))
-def test_overfitting_IO(top_k):
+def test_overfitting_IO():
     nlp = English()
-    lemmatizer = nlp.add_pipe("trainable_lemmatizer", config={"top_k": top_k})
+    lemmatizer = nlp.add_pipe("trainable_lemmatizer")
     lemmatizer.min_tree_freq = 1
     train_examples = []
     for t in TRAIN_DATA:
         train_examples.append(Example.from_dict(nlp.make_doc(t[0]), t[1]))
 
     optimizer = nlp.initialize(get_examples=lambda: train_examples)
 
@@ -188,15 +173,15 @@
         assert doc2[1].lemma_ == "like"
         assert doc2[2].lemma_ == "blue"
         assert doc2[3].lemma_ == "egg"
 
     # Check model after a {to,from}_bytes roundtrip
     nlp_bytes = nlp.to_bytes()
     nlp3 = English()
-    nlp3.add_pipe("trainable_lemmatizer", config={"top_k": top_k})
+    nlp3.add_pipe("trainable_lemmatizer")
     nlp3.from_bytes(nlp_bytes)
     doc3 = nlp3(test_text)
     assert doc3[0].lemma_ == "she"
     assert doc3[1].lemma_ == "like"
     assert doc3[2].lemma_ == "blue"
     assert doc3[3].lemma_ == "egg"
 
@@ -206,14 +191,61 @@
     doc4 = nlp4(test_text)
     assert doc4[0].lemma_ == "she"
     assert doc4[1].lemma_ == "like"
     assert doc4[2].lemma_ == "blue"
     assert doc4[3].lemma_ == "egg"
 
 
+def test_is_distillable():
+    nlp = English()
+    lemmatizer = nlp.add_pipe("trainable_lemmatizer")
+    assert lemmatizer.is_distillable
+
+
+def test_distill():
+    teacher = English()
+    teacher_lemmatizer = teacher.add_pipe("trainable_lemmatizer")
+    teacher_lemmatizer.min_tree_freq = 1
+    train_examples = []
+    for t in TRAIN_DATA:
+        train_examples.append(Example.from_dict(teacher.make_doc(t[0]), t[1]))
+
+    optimizer = teacher.initialize(get_examples=lambda: train_examples)
+
+    for i in range(50):
+        losses = {}
+        teacher.update(train_examples, sgd=optimizer, losses=losses)
+    assert losses["trainable_lemmatizer"] < 0.00001
+
+    student = English()
+    student_lemmatizer = student.add_pipe("trainable_lemmatizer")
+    student_lemmatizer.min_tree_freq = 1
+    student_lemmatizer.initialize(
+        get_examples=lambda: train_examples, labels=teacher_lemmatizer.label_data
+    )
+
+    distill_examples = [
+        Example.from_dict(teacher.make_doc(t[0]), {}) for t in TRAIN_DATA
+    ]
+
+    for i in range(50):
+        losses = {}
+        student_lemmatizer.distill(
+            teacher_lemmatizer, distill_examples, sgd=optimizer, losses=losses
+        )
+    assert losses["trainable_lemmatizer"] < 0.00001
+
+    test_text = "She likes blue eggs"
+    doc = student(test_text)
+    assert doc[0].lemma_ == "she"
+    assert doc[1].lemma_ == "like"
+    assert doc[2].lemma_ == "blue"
+    assert doc[3].lemma_ == "egg"
+
+
 def test_lemmatizer_requires_labels():
     nlp = English()
     nlp.add_pipe("trainable_lemmatizer")
     with pytest.raises(ValueError):
         nlp.initialize()
 
 
@@ -326,7 +358,30 @@
 
 def test_empty_strings():
     strings = StringStore()
     trees = EditTrees(strings)
     no_change = trees.add("xyz", "xyz")
     empty = trees.add("", "")
     assert no_change == empty
+
+
+def test_save_activations():
+    nlp = English()
+    lemmatizer = cast(TrainablePipe, nlp.add_pipe("trainable_lemmatizer"))
+    lemmatizer.min_tree_freq = 1
+    train_examples = []
+    for t in TRAIN_DATA:
+        train_examples.append(Example.from_dict(nlp.make_doc(t[0]), t[1]))
+    nlp.initialize(get_examples=lambda: train_examples)
+    nO = lemmatizer.model.get_dim("nO")
+
+    doc = nlp("This is a test.")
+    assert "trainable_lemmatizer" not in doc.activations
+
+    lemmatizer.save_activations = True
+    doc = nlp("This is a test.")
+    assert list(doc.activations["trainable_lemmatizer"].keys()) == [
+        "probabilities",
+        "tree_ids",
+    ]
+    assert doc.activations["trainable_lemmatizer"]["probabilities"].shape == (5, nO)
+    assert doc.activations["trainable_lemmatizer"]["tree_ids"].shape == (5,)
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/pipeline/test_entity_linker.py` & `spacy-4.0.0.dev0/spacy/tests/pipeline/test_entity_linker.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,20 +1,21 @@
-from typing import Callable, Iterable, Dict, Any, Tuple
+from typing import Callable, Iterable, Dict, Any, cast
 
 import pytest
 from numpy.testing import assert_equal
+from thinc.types import Ragged
 
-from spacy import registry, util, Language
+from spacy import registry, util
 from spacy.attrs import ENT_KB_ID
 from spacy.compat import pickle
 from spacy.kb import Candidate, InMemoryLookupKB, get_candidates, KnowledgeBase
 from spacy.lang.en import English
 from spacy.ml import load_kb
 from spacy.ml.models.entity_linker import build_span_maker
-from spacy.pipeline import EntityLinker
+from spacy.pipeline import EntityLinker, TrainablePipe
 from spacy.pipeline.legacy import EntityLinker_v1
 from spacy.pipeline.tok2vec import DEFAULT_TOK2VEC_MODEL
 from spacy.scorer import Scorer
 from spacy.tests.util import make_tempdir
 from spacy.tokens import Span, Doc
 from spacy.training import Example
 from spacy.util import ensure_path
@@ -104,31 +105,26 @@
     sent0 = sentences[0]
     ent = doc.ents[0]
     assert ent.start < sent0.end < ent.end
     assert sentences.index(ent.sent) == 0
 
 
 @pytest.mark.issue(7065)
-@pytest.mark.parametrize("entity_in_first_sentence", [True, False])
-def test_sentence_crossing_ents(entity_in_first_sentence: bool):
-    """Tests if NEL crashes if entities cross sentence boundaries and the first associated sentence doesn't have an
-    entity.
-    entity_in_prior_sentence (bool): Whether to include an entity in the first sentence associated with the
-    sentence-crossing entity.
-    """
+def test_issue7065_b():
     # Test that the NEL doesn't crash when an entity crosses a sentence boundary
     nlp = English()
     vector_length = 3
+    nlp.add_pipe("sentencizer")
     text = "Mahler 's Symphony No. 8 was beautiful."
-    entities = [(10, 24, "WORK")]
-    links = {(10, 24): {"Q7304": 0.0, "Q270853": 1.0}}
-    if entity_in_first_sentence:
-        entities.append((0, 6, "PERSON"))
-        links[(0, 6)] = {"Q7304": 1.0, "Q270853": 0.0}
-    sent_starts = [1, -1, 0, 0, 0, 1, 0, 0, 0]
+    entities = [(0, 6, "PERSON"), (10, 24, "WORK")]
+    links = {
+        (0, 6): {"Q7304": 1.0, "Q270853": 0.0},
+        (10, 24): {"Q7304": 0.0, "Q270853": 1.0},
+    }
+    sent_starts = [1, -1, 0, 0, 0, 0, 0, 0, 0]
     doc = nlp(text)
     example = Example.from_dict(
         doc, {"entities": entities, "links": links, "sent_starts": sent_starts}
     )
     train_examples = [example]
 
     def create_kb(vocab):
@@ -146,22 +142,39 @@
             entities=["Q7304"],
             probabilities=[1.0],
         )
         return mykb
 
     # Create the Entity Linker component and add it to the pipeline
     entity_linker = nlp.add_pipe("entity_linker", last=True)
-    entity_linker.set_kb(create_kb)  # type: ignore
+    entity_linker.set_kb(create_kb)
     # train the NEL pipe
     optimizer = nlp.initialize(get_examples=lambda: train_examples)
     for i in range(2):
-        nlp.update(train_examples, sgd=optimizer)
+        losses = {}
+        nlp.update(train_examples, sgd=optimizer, losses=losses)
 
-    # This shouldn't crash.
-    entity_linker.predict([example.reference])  # type: ignore
+    # Add a custom rule-based component to mimick NER
+    patterns = [
+        {"label": "PERSON", "pattern": [{"LOWER": "mahler"}]},
+        {
+            "label": "WORK",
+            "pattern": [
+                {"LOWER": "symphony"},
+                {"LOWER": "no"},
+                {"LOWER": "."},
+                {"LOWER": "8"},
+            ],
+        },
+    ]
+    ruler = nlp.add_pipe("entity_ruler", before="entity_linker")
+    ruler.add_patterns(patterns)
+    # test the trained model - this should not throw E148
+    doc = nlp(text)
+    assert doc
 
 
 def test_no_entities():
     # Test that having no entities doesn't crash the model
     TRAIN_DATA = [
         (
             "The sky is blue.",
@@ -337,17 +350,14 @@
         mykb.add_entity(entity="Q2", freq=5, entity_vector=[2])
 
 
 def test_kb_default(nlp):
     """Test that the default (empty) KB is loaded upon construction"""
     entity_linker = nlp.add_pipe("entity_linker", config={})
     assert len(entity_linker.kb) == 0
-    with pytest.raises(ValueError, match="E139"):
-        # this raises an error because the KB is empty
-        entity_linker.validate_kb()
     assert entity_linker.kb.get_size_entities() == 0
     assert entity_linker.kb.get_size_aliases() == 0
     # 64 is the default value from pipeline.entity_linker
     assert entity_linker.kb.entity_vector_length == 64
 
 
 def test_kb_custom_length(nlp):
@@ -1190,14 +1200,77 @@
     ruler.add_patterns([{"label": "PERSON", "pattern": [{"LOWER": "mahler"}]}])  # type: ignore
     doc = nlp(text)
 
     assert len(doc.ents) == 1
     assert doc.ents[0].kb_id_ == entity_id if meet_threshold else EntityLinker.NIL
 
 
+def test_save_activations():
+    nlp = English()
+    vector_length = 3
+    assert "Q2146908" not in nlp.vocab.strings
+
+    # Convert the texts to docs to make sure we have doc.ents set for the training examples
+    train_examples = []
+    for text, annotation in TRAIN_DATA:
+        doc = nlp(text)
+        train_examples.append(Example.from_dict(doc, annotation))
+
+    def create_kb(vocab):
+        # create artificial KB - assign same prior weight to the two russ cochran's
+        # Q2146908 (Russ Cochran): American golfer
+        # Q7381115 (Russ Cochran): publisher
+        mykb = InMemoryLookupKB(vocab, entity_vector_length=vector_length)
+        mykb.add_entity(entity="Q2146908", freq=12, entity_vector=[6, -4, 3])
+        mykb.add_entity(entity="Q7381115", freq=12, entity_vector=[9, 1, -7])
+        mykb.add_alias(
+            alias="Russ Cochran",
+            entities=["Q2146908", "Q7381115"],
+            probabilities=[0.5, 0.5],
+        )
+        return mykb
+
+    # Create the Entity Linker component and add it to the pipeline
+    entity_linker = cast(TrainablePipe, nlp.add_pipe("entity_linker", last=True))
+    assert isinstance(entity_linker, EntityLinker)
+    entity_linker.set_kb(create_kb)
+    assert "Q2146908" in entity_linker.vocab.strings
+    assert "Q2146908" in entity_linker.kb.vocab.strings
+
+    # initialize the NEL pipe
+    nlp.initialize(get_examples=lambda: train_examples)
+
+    nO = entity_linker.model.get_dim("nO")
+
+    nlp.add_pipe("sentencizer", first=True)
+    patterns = [
+        {"label": "PERSON", "pattern": [{"LOWER": "russ"}, {"LOWER": "cochran"}]},
+        {"label": "ORG", "pattern": [{"LOWER": "ec"}, {"LOWER": "comics"}]},
+    ]
+    ruler = nlp.add_pipe("entity_ruler", before="entity_linker")
+    ruler.add_patterns(patterns)
+
+    doc = nlp("Russ Cochran was a publisher")
+    assert "entity_linker" not in doc.activations
+
+    entity_linker.save_activations = True
+    doc = nlp("Russ Cochran was a publisher")
+    assert set(doc.activations["entity_linker"].keys()) == {"ents", "scores"}
+    ents = doc.activations["entity_linker"]["ents"]
+    assert isinstance(ents, Ragged)
+    assert ents.data.shape == (2, 1)
+    assert ents.data.dtype == "uint64"
+    assert ents.lengths.shape == (1,)
+    scores = doc.activations["entity_linker"]["scores"]
+    assert isinstance(scores, Ragged)
+    assert scores.data.shape == (2, 1)
+    assert scores.data.dtype == "float32"
+    assert scores.lengths.shape == (1,)
+
+
 def test_span_maker_forward_with_empty():
     """The forward pass of the span maker may have a doc with no entities."""
     nlp = English()
     doc1 = nlp("a b c")
     ent = doc1[0:1]
     ent.label_ = "X"
     doc1.ents = [ent]
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/pipeline/test_entity_ruler.py` & `spacy-4.0.0.dev0/spacy/tests/pipeline/test_entity_ruler.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,23 +1,21 @@
 import pytest
 
 from spacy import registry
 from spacy.tokens import Doc, Span
 from spacy.language import Language
 from spacy.lang.en import English
-from spacy.pipeline import EntityRuler, EntityRecognizer, merge_entities
+from spacy.pipeline import EntityRecognizer, merge_entities
 from spacy.pipeline import SpanRuler
 from spacy.pipeline.ner import DEFAULT_NER_MODEL
 from spacy.errors import MatchPatternError
 from spacy.tests.util import make_tempdir
 
 from thinc.api import NumpyOps, get_current_ops
 
-ENTITY_RULERS = ["entity_ruler", "future_entity_ruler"]
-
 
 @pytest.fixture
 def nlp():
     return Language()
 
 
 @pytest.fixture
@@ -36,21 +34,20 @@
 @Language.component("add_ent")
 def add_ent_component(doc):
     doc.ents = [Span(doc, 0, 3, label="ORG")]
     return doc
 
 
 @pytest.mark.issue(3345)
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_issue3345(entity_ruler_factory):
+def test_issue3345():
     """Test case where preset entity crosses sentence boundary."""
     nlp = English()
     doc = Doc(nlp.vocab, words=["I", "live", "in", "New", "York"])
     doc[4].is_sent_start = True
-    ruler = nlp.add_pipe(entity_ruler_factory, name="entity_ruler")
+    ruler = nlp.add_pipe("entity_ruler")
     ruler.add_patterns([{"label": "GPE", "pattern": "New York"}])
     cfg = {"model": DEFAULT_NER_MODEL}
     model = registry.resolve(cfg, validate=True)["model"]
     ner = EntityRecognizer(doc.vocab, model)
     # Add the OUT action. I wouldn't have thought this would be necessary...
     ner.moves.add_action(5, "")
     ner.add_label("GPE")
@@ -61,23 +58,22 @@
     ner.moves.apply_transition(state, "O")
     ner.moves.apply_transition(state, "O")
     # Check that B-GPE is valid.
     assert ner.moves.is_valid(state, "B-GPE")
 
 
 @pytest.mark.issue(4849)
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_issue4849(entity_ruler_factory):
+def test_issue4849():
     nlp = English()
     patterns = [
         {"label": "PERSON", "pattern": "joe biden", "id": "joe-biden"},
         {"label": "PERSON", "pattern": "bernie sanders", "id": "bernie-sanders"},
     ]
     ruler = nlp.add_pipe(
-        entity_ruler_factory,
+        "entity_ruler",
         name="entity_ruler",
         config={"phrase_matcher_attr": "LOWER"},
     )
     ruler.add_patterns(patterns)
     text = """
     The left is starting to take aim at Democratic front-runner Joe Biden.
     Sen. Bernie Sanders joined in her criticism: "There is no 'middle ground' when it comes to climate policy."
@@ -92,19 +88,18 @@
         count_ents = 0
         for doc in nlp.pipe([text], n_process=2):
             count_ents += len([ent for ent in doc.ents if ent.ent_id > 0])
         assert count_ents == 2
 
 
 @pytest.mark.issue(5918)
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_issue5918(entity_ruler_factory):
+def test_issue5918():
     # Test edge case when merging entities.
     nlp = English()
-    ruler = nlp.add_pipe(entity_ruler_factory, name="entity_ruler")
+    ruler = nlp.add_pipe("entity_ruler")
     patterns = [
         {"label": "ORG", "pattern": "Digicon Inc"},
         {"label": "ORG", "pattern": "Rotan Mosle Inc's"},
         {"label": "ORG", "pattern": "Rotan Mosle Technology Partners Ltd"},
     ]
     ruler.add_patterns(patterns)
 
@@ -121,18 +116,17 @@
     # with pytest.warns(UserWarning):
     #     doc[29].head = doc[33]
     doc = merge_entities(doc)
     assert len(doc.ents) == 3
 
 
 @pytest.mark.issue(8168)
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_issue8168(entity_ruler_factory):
+def test_issue8168():
     nlp = English()
-    ruler = nlp.add_pipe(entity_ruler_factory, name="entity_ruler")
+    ruler = nlp.add_pipe("entity_ruler")
     patterns = [
         {"label": "ORG", "pattern": "Apple"},
         {
             "label": "GPE",
             "pattern": [{"LOWER": "san"}, {"LOWER": "francisco"}],
             "id": "san-francisco",
         },
@@ -144,212 +138,190 @@
     ]
     ruler.add_patterns(patterns)
     doc = nlp("San Francisco San Fran")
     assert all(t.ent_id_ == "san-francisco" for t in doc)
 
 
 @pytest.mark.issue(8216)
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_entity_ruler_fix8216(nlp, patterns, entity_ruler_factory):
+def test_entity_ruler_fix8216(nlp, patterns):
     """Test that patterns don't get added excessively."""
-    ruler = nlp.add_pipe(
-        entity_ruler_factory, name="entity_ruler", config={"validate": True}
-    )
+    ruler = nlp.add_pipe("entity_ruler", config={"validate": True})
     ruler.add_patterns(patterns)
     pattern_count = sum(len(mm) for mm in ruler.matcher._patterns.values())
     assert pattern_count > 0
     ruler.add_patterns([])
     after_count = sum(len(mm) for mm in ruler.matcher._patterns.values())
     assert after_count == pattern_count
 
 
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_entity_ruler_init(nlp, patterns, entity_ruler_factory):
-    ruler = nlp.add_pipe(entity_ruler_factory, name="entity_ruler")
+def test_entity_ruler_init(nlp, patterns):
+    ruler = nlp.add_pipe("entity_ruler")
     ruler.add_patterns(patterns)
     assert len(ruler) == len(patterns)
     assert len(ruler.labels) == 4
     assert "HELLO" in ruler
     assert "BYE" in ruler
     nlp.remove_pipe("entity_ruler")
-    ruler = nlp.add_pipe(entity_ruler_factory, name="entity_ruler")
+    ruler = nlp.add_pipe("entity_ruler")
     ruler.add_patterns(patterns)
     doc = nlp("hello world bye bye")
     assert len(doc.ents) == 2
     assert doc.ents[0].label_ == "HELLO"
     assert doc.ents[1].label_ == "BYE"
 
 
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_entity_ruler_no_patterns_warns(nlp, entity_ruler_factory):
-    ruler = nlp.add_pipe(entity_ruler_factory, name="entity_ruler")
+def test_entity_ruler_no_patterns_warns(nlp):
+    ruler = nlp.add_pipe("entity_ruler")
     assert len(ruler) == 0
     assert len(ruler.labels) == 0
     nlp.remove_pipe("entity_ruler")
-    nlp.add_pipe(entity_ruler_factory, name="entity_ruler")
+    nlp.add_pipe("entity_ruler")
     assert nlp.pipe_names == ["entity_ruler"]
     with pytest.warns(UserWarning):
         doc = nlp("hello world bye bye")
     assert len(doc.ents) == 0
 
 
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_entity_ruler_init_patterns(nlp, patterns, entity_ruler_factory):
+def test_entity_ruler_init_patterns(nlp, patterns):
     # initialize with patterns
-    ruler = nlp.add_pipe(entity_ruler_factory, name="entity_ruler")
+    ruler = nlp.add_pipe("entity_ruler")
     assert len(ruler.labels) == 0
     ruler.initialize(lambda: [], patterns=patterns)
     assert len(ruler.labels) == 4
     doc = nlp("hello world bye bye")
     assert doc.ents[0].label_ == "HELLO"
     assert doc.ents[1].label_ == "BYE"
     nlp.remove_pipe("entity_ruler")
     # initialize with patterns from misc registry
     nlp.config["initialize"]["components"]["entity_ruler"] = {
         "patterns": {"@misc": "entity_ruler_patterns"}
     }
-    ruler = nlp.add_pipe(entity_ruler_factory, name="entity_ruler")
+    ruler = nlp.add_pipe("entity_ruler")
     assert len(ruler.labels) == 0
     nlp.initialize()
     assert len(ruler.labels) == 4
     doc = nlp("hello world bye bye")
     assert doc.ents[0].label_ == "HELLO"
     assert doc.ents[1].label_ == "BYE"
 
 
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_entity_ruler_init_clear(nlp, patterns, entity_ruler_factory):
+def test_entity_ruler_init_clear(nlp, patterns):
     """Test that initialization clears patterns."""
-    ruler = nlp.add_pipe(entity_ruler_factory, name="entity_ruler")
+    ruler = nlp.add_pipe("entity_ruler")
     ruler.add_patterns(patterns)
     assert len(ruler.labels) == 4
     ruler.initialize(lambda: [])
     assert len(ruler.labels) == 0
 
 
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_entity_ruler_clear(nlp, patterns, entity_ruler_factory):
+def test_entity_ruler_clear(nlp, patterns):
     """Test that initialization clears patterns."""
-    ruler = nlp.add_pipe(entity_ruler_factory, name="entity_ruler")
+    ruler = nlp.add_pipe("entity_ruler")
     ruler.add_patterns(patterns)
     assert len(ruler.labels) == 4
     doc = nlp("hello world")
     assert len(doc.ents) == 1
     ruler.clear()
     assert len(ruler.labels) == 0
     with pytest.warns(UserWarning):
         doc = nlp("hello world")
     assert len(doc.ents) == 0
 
 
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_entity_ruler_existing(nlp, patterns, entity_ruler_factory):
-    ruler = nlp.add_pipe(entity_ruler_factory, name="entity_ruler")
+def test_entity_ruler_existing(nlp, patterns):
+    ruler = nlp.add_pipe("entity_ruler")
     ruler.add_patterns(patterns)
     nlp.add_pipe("add_ent", before="entity_ruler")
     doc = nlp("OH HELLO WORLD bye bye")
     assert len(doc.ents) == 2
     assert doc.ents[0].label_ == "ORG"
     assert doc.ents[1].label_ == "BYE"
 
 
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_entity_ruler_existing_overwrite(nlp, patterns, entity_ruler_factory):
-    ruler = nlp.add_pipe(
-        entity_ruler_factory, name="entity_ruler", config={"overwrite_ents": True}
-    )
+def test_entity_ruler_existing_overwrite(nlp, patterns):
+    ruler = nlp.add_pipe("entity_ruler", config={"overwrite_ents": True})
     ruler.add_patterns(patterns)
     nlp.add_pipe("add_ent", before="entity_ruler")
     doc = nlp("OH HELLO WORLD bye bye")
     assert len(doc.ents) == 2
     assert doc.ents[0].label_ == "HELLO"
     assert doc.ents[0].text == "HELLO"
     assert doc.ents[1].label_ == "BYE"
 
 
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_entity_ruler_existing_complex(nlp, patterns, entity_ruler_factory):
-    ruler = nlp.add_pipe(
-        entity_ruler_factory, name="entity_ruler", config={"overwrite_ents": True}
-    )
+def test_entity_ruler_existing_complex(nlp, patterns):
+    ruler = nlp.add_pipe("entity_ruler", config={"overwrite_ents": True})
     ruler.add_patterns(patterns)
     nlp.add_pipe("add_ent", before="entity_ruler")
     doc = nlp("foo foo bye bye")
     assert len(doc.ents) == 2
     assert doc.ents[0].label_ == "COMPLEX"
     assert doc.ents[1].label_ == "BYE"
     assert len(doc.ents[0]) == 2
     assert len(doc.ents[1]) == 2
 
 
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_entity_ruler_entity_id(nlp, patterns, entity_ruler_factory):
-    ruler = nlp.add_pipe(
-        entity_ruler_factory, name="entity_ruler", config={"overwrite_ents": True}
-    )
+def test_entity_ruler_entity_id(nlp, patterns):
+    ruler = nlp.add_pipe("entity_ruler", config={"overwrite_ents": True})
     ruler.add_patterns(patterns)
     doc = nlp("Apple is a technology company")
     assert len(doc.ents) == 1
     assert doc.ents[0].label_ == "TECH_ORG"
     assert doc.ents[0].ent_id_ == "a1"
 
 
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_entity_ruler_cfg_ent_id_sep(nlp, patterns, entity_ruler_factory):
+def test_entity_ruler_cfg_ent_id_sep(nlp, patterns):
     config = {"overwrite_ents": True, "ent_id_sep": "**"}
-    ruler = nlp.add_pipe(entity_ruler_factory, name="entity_ruler", config=config)
+    ruler = nlp.add_pipe("entity_ruler", config=config)
     ruler.add_patterns(patterns)
     doc = nlp("Apple is a technology company")
-    if isinstance(ruler, EntityRuler):
-        assert "TECH_ORG**a1" in ruler.phrase_patterns
     assert len(doc.ents) == 1
     assert doc.ents[0].label_ == "TECH_ORG"
     assert doc.ents[0].ent_id_ == "a1"
 
 
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_entity_ruler_serialize_bytes(nlp, patterns, entity_ruler_factory):
-    ruler = EntityRuler(nlp, patterns=patterns)
+def test_entity_ruler_serialize_bytes(nlp, patterns):
+    ruler = nlp.add_pipe("entity_ruler")
+    ruler.add_patterns(patterns)
     assert len(ruler) == len(patterns)
     assert len(ruler.labels) == 4
     ruler_bytes = ruler.to_bytes()
-    new_ruler = EntityRuler(nlp)
+    new_ruler = nlp.add_pipe("entity_ruler", name="new_ruler")
     assert len(new_ruler) == 0
     assert len(new_ruler.labels) == 0
     new_ruler = new_ruler.from_bytes(ruler_bytes)
     assert len(new_ruler) == len(patterns)
     assert len(new_ruler.labels) == 4
     assert len(new_ruler.patterns) == len(ruler.patterns)
     for pattern in ruler.patterns:
         assert pattern in new_ruler.patterns
     assert sorted(new_ruler.labels) == sorted(ruler.labels)
 
 
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_entity_ruler_serialize_phrase_matcher_attr_bytes(
-    nlp, patterns, entity_ruler_factory
-):
-    ruler = EntityRuler(nlp, phrase_matcher_attr="LOWER", patterns=patterns)
+def test_entity_ruler_serialize_phrase_matcher_attr_bytes(nlp, patterns):
+    ruler = nlp.add_pipe("entity_ruler", config={"phrase_matcher_attr": "LOWER"})
+    ruler.add_patterns(patterns)
     assert len(ruler) == len(patterns)
     assert len(ruler.labels) == 4
     ruler_bytes = ruler.to_bytes()
-    new_ruler = EntityRuler(nlp)
+    new_ruler = nlp.add_pipe(
+        "entity_ruler", name="new_ruler", config={"phrase_matcher_attr": "LOWER"}
+    )
     assert len(new_ruler) == 0
     assert len(new_ruler.labels) == 0
-    assert new_ruler.phrase_matcher_attr is None
     new_ruler = new_ruler.from_bytes(ruler_bytes)
     assert len(new_ruler) == len(patterns)
     assert len(new_ruler.labels) == 4
-    assert new_ruler.phrase_matcher_attr == "LOWER"
 
 
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_entity_ruler_validate(nlp, entity_ruler_factory):
-    ruler = nlp.add_pipe(entity_ruler_factory, name="entity_ruler")
-    validated_ruler = EntityRuler(nlp, validate=True)
+def test_entity_ruler_validate(nlp):
+    ruler = nlp.add_pipe("entity_ruler")
+    validated_ruler = nlp.add_pipe(
+        "entity_ruler", name="validated_ruler", config={"validate": True}
+    )
 
     valid_pattern = {"label": "HELLO", "pattern": [{"LOWER": "HELLO"}]}
     invalid_pattern = {"label": "HELLO", "pattern": [{"ASDF": "HELLO"}]}
 
     # invalid pattern raises error without validate
     with pytest.raises(ValueError):
         ruler.add_patterns([invalid_pattern])
@@ -358,217 +330,182 @@
     validated_ruler.add_patterns([valid_pattern])
 
     # invalid pattern raises error with validate
     with pytest.raises(MatchPatternError):
         validated_ruler.add_patterns([invalid_pattern])
 
 
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_entity_ruler_properties(nlp, patterns, entity_ruler_factory):
-    ruler = EntityRuler(nlp, patterns=patterns, overwrite_ents=True)
+def test_entity_ruler_properties(nlp, patterns):
+    ruler = nlp.add_pipe("entity_ruler", config={"overwrite_ents": True})
+    ruler.add_patterns(patterns)
     assert sorted(ruler.labels) == sorted(["HELLO", "BYE", "COMPLEX", "TECH_ORG"])
-    assert sorted(ruler.ent_ids) == ["a1", "a2"]
+    assert sorted(ruler.ids) == ["a1", "a2"]
 
 
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_entity_ruler_overlapping_spans(nlp, entity_ruler_factory):
-    ruler = nlp.add_pipe(entity_ruler_factory, name="entity_ruler")
+def test_entity_ruler_overlapping_spans(nlp):
+    ruler = nlp.add_pipe("entity_ruler")
     patterns = [
         {"label": "FOOBAR", "pattern": "foo bar"},
         {"label": "BARBAZ", "pattern": "bar baz"},
     ]
     ruler.add_patterns(patterns)
     doc = nlp("foo bar baz")
     assert len(doc.ents) == 1
     assert doc.ents[0].label_ == "FOOBAR"
 
 
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_entity_ruler_fuzzy_pipe(nlp, entity_ruler_factory):
-    ruler = nlp.add_pipe(entity_ruler_factory, name="entity_ruler")
+def test_entity_ruler_fuzzy_pipe(nlp):
+    ruler = nlp.add_pipe("entity_ruler")
     patterns = [{"label": "HELLO", "pattern": [{"LOWER": {"FUZZY": "hello"}}]}]
     ruler.add_patterns(patterns)
     doc = nlp("helloo")
     assert len(doc.ents) == 1
     assert doc.ents[0].label_ == "HELLO"
 
 
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_entity_ruler_fuzzy(nlp, entity_ruler_factory):
-    ruler = nlp.add_pipe(entity_ruler_factory, name="entity_ruler")
+def test_entity_ruler_fuzzy(nlp):
+    ruler = nlp.add_pipe("entity_ruler")
     patterns = [{"label": "HELLO", "pattern": [{"LOWER": {"FUZZY": "hello"}}]}]
     ruler.add_patterns(patterns)
     doc = nlp("helloo")
     assert len(doc.ents) == 1
     assert doc.ents[0].label_ == "HELLO"
 
 
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_entity_ruler_fuzzy_disabled(nlp, entity_ruler_factory):
+def test_entity_ruler_fuzzy_disabled(nlp):
     @registry.misc("test_fuzzy_compare_disabled")
     def make_test_fuzzy_compare_disabled():
         return lambda x, y, z: False
 
     ruler = nlp.add_pipe(
-        entity_ruler_factory,
-        name="entity_ruler",
+        "entity_ruler",
         config={"matcher_fuzzy_compare": {"@misc": "test_fuzzy_compare_disabled"}},
     )
     patterns = [{"label": "HELLO", "pattern": [{"LOWER": {"FUZZY": "hello"}}]}]
     ruler.add_patterns(patterns)
     doc = nlp("helloo")
     assert len(doc.ents) == 0
 
 
 @pytest.mark.parametrize("n_process", [1, 2])
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_entity_ruler_multiprocessing(nlp, n_process, entity_ruler_factory):
+def test_entity_ruler_multiprocessing(nlp, n_process):
     if isinstance(get_current_ops, NumpyOps) or n_process < 2:
         texts = ["I enjoy eating Pizza Hut pizza."]
 
         patterns = [{"label": "FASTFOOD", "pattern": "Pizza Hut", "id": "1234"}]
 
-        ruler = nlp.add_pipe(entity_ruler_factory, name="entity_ruler")
+        ruler = nlp.add_pipe("entity_ruler")
         ruler.add_patterns(patterns)
 
         for doc in nlp.pipe(texts, n_process=2):
             for ent in doc.ents:
                 assert ent.ent_id_ == "1234"
 
 
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_entity_ruler_serialize_jsonl(nlp, patterns, entity_ruler_factory):
-    ruler = nlp.add_pipe(entity_ruler_factory, name="entity_ruler")
+def test_entity_ruler_serialize_jsonl(nlp, patterns):
+    ruler = nlp.add_pipe("entity_ruler")
     ruler.add_patterns(patterns)
     with make_tempdir() as d:
         ruler.to_disk(d / "test_ruler.jsonl")
         ruler.from_disk(d / "test_ruler.jsonl")  # read from an existing jsonl file
         with pytest.raises(ValueError):
             ruler.from_disk(d / "non_existing.jsonl")  # read from a bad jsonl file
 
 
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_entity_ruler_serialize_dir(nlp, patterns, entity_ruler_factory):
-    ruler = nlp.add_pipe(entity_ruler_factory, name="entity_ruler")
+def test_entity_ruler_serialize_dir(nlp, patterns):
+    ruler = nlp.add_pipe("entity_ruler")
     ruler.add_patterns(patterns)
     with make_tempdir() as d:
         ruler.to_disk(d / "test_ruler")
         ruler.from_disk(d / "test_ruler")  # read from an existing directory
         with pytest.raises(ValueError):
             ruler.from_disk(d / "non_existing_dir")  # read from a bad directory
 
 
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_entity_ruler_remove_basic(nlp, entity_ruler_factory):
-    ruler = nlp.add_pipe(entity_ruler_factory, name="entity_ruler")
+def test_entity_ruler_remove_basic(nlp):
+    ruler = nlp.add_pipe("entity_ruler")
     patterns = [
         {"label": "PERSON", "pattern": "Dina", "id": "dina"},
         {"label": "ORG", "pattern": "ACME", "id": "acme"},
         {"label": "ORG", "pattern": "ACM"},
     ]
     ruler.add_patterns(patterns)
     doc = nlp("Dina went to school")
     assert len(ruler.patterns) == 3
     assert len(doc.ents) == 1
-    if isinstance(ruler, EntityRuler):
-        assert "PERSON||dina" in ruler.phrase_matcher
     assert doc.ents[0].label_ == "PERSON"
     assert doc.ents[0].text == "Dina"
-    if isinstance(ruler, EntityRuler):
-        ruler.remove("dina")
-    else:
-        ruler.remove_by_id("dina")
+    ruler.remove_by_id("dina")
     doc = nlp("Dina went to school")
     assert len(doc.ents) == 0
-    if isinstance(ruler, EntityRuler):
-        assert "PERSON||dina" not in ruler.phrase_matcher
     assert len(ruler.patterns) == 2
 
 
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_entity_ruler_remove_same_id_multiple_patterns(nlp, entity_ruler_factory):
-    ruler = nlp.add_pipe(entity_ruler_factory, name="entity_ruler")
+def test_entity_ruler_remove_same_id_multiple_patterns(nlp):
+    ruler = nlp.add_pipe("entity_ruler")
     patterns = [
         {"label": "PERSON", "pattern": "Dina", "id": "dina"},
         {"label": "ORG", "pattern": "DinaCorp", "id": "dina"},
         {"label": "ORG", "pattern": "ACME", "id": "acme"},
     ]
     ruler.add_patterns(patterns)
     doc = nlp("Dina founded DinaCorp and ACME.")
     assert len(ruler.patterns) == 3
-    if isinstance(ruler, EntityRuler):
-        assert "PERSON||dina" in ruler.phrase_matcher
-        assert "ORG||dina" in ruler.phrase_matcher
     assert len(doc.ents) == 3
-    if isinstance(ruler, EntityRuler):
-        ruler.remove("dina")
-    else:
-        ruler.remove_by_id("dina")
+    ruler.remove_by_id("dina")
     doc = nlp("Dina founded DinaCorp and ACME.")
     assert len(ruler.patterns) == 1
-    if isinstance(ruler, EntityRuler):
-        assert "PERSON||dina" not in ruler.phrase_matcher
-        assert "ORG||dina" not in ruler.phrase_matcher
     assert len(doc.ents) == 1
 
 
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_entity_ruler_remove_nonexisting_pattern(nlp, entity_ruler_factory):
-    ruler = nlp.add_pipe(entity_ruler_factory, name="entity_ruler")
+def test_entity_ruler_remove_nonexisting_pattern(nlp):
+    ruler = nlp.add_pipe("entity_ruler")
     patterns = [
         {"label": "PERSON", "pattern": "Dina", "id": "dina"},
         {"label": "ORG", "pattern": "ACME", "id": "acme"},
         {"label": "ORG", "pattern": "ACM"},
     ]
     ruler.add_patterns(patterns)
     assert len(ruler.patterns) == 3
     with pytest.raises(ValueError):
         ruler.remove("nepattern")
     if isinstance(ruler, SpanRuler):
         with pytest.raises(ValueError):
             ruler.remove_by_id("nepattern")
 
 
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_entity_ruler_remove_several_patterns(nlp, entity_ruler_factory):
-    ruler = nlp.add_pipe(entity_ruler_factory, name="entity_ruler")
+def test_entity_ruler_remove_several_patterns(nlp):
+    ruler = nlp.add_pipe("entity_ruler")
     patterns = [
         {"label": "PERSON", "pattern": "Dina", "id": "dina"},
         {"label": "ORG", "pattern": "ACME", "id": "acme"},
         {"label": "ORG", "pattern": "ACM"},
     ]
     ruler.add_patterns(patterns)
     doc = nlp("Dina founded her company ACME.")
     assert len(ruler.patterns) == 3
     assert len(doc.ents) == 2
     assert doc.ents[0].label_ == "PERSON"
     assert doc.ents[0].text == "Dina"
     assert doc.ents[1].label_ == "ORG"
     assert doc.ents[1].text == "ACME"
-    if isinstance(ruler, EntityRuler):
-        ruler.remove("dina")
-    else:
-        ruler.remove_by_id("dina")
+    ruler.remove_by_id("dina")
     doc = nlp("Dina founded her company ACME")
     assert len(ruler.patterns) == 2
     assert len(doc.ents) == 1
     assert doc.ents[0].label_ == "ORG"
     assert doc.ents[0].text == "ACME"
-    if isinstance(ruler, EntityRuler):
-        ruler.remove("acme")
-    else:
-        ruler.remove_by_id("acme")
+    ruler.remove_by_id("acme")
     doc = nlp("Dina founded her company ACME")
     assert len(ruler.patterns) == 1
     assert len(doc.ents) == 0
 
 
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_entity_ruler_remove_patterns_in_a_row(nlp, entity_ruler_factory):
-    ruler = nlp.add_pipe(entity_ruler_factory, name="entity_ruler")
+def test_entity_ruler_remove_patterns_in_a_row(nlp):
+    ruler = nlp.add_pipe("entity_ruler")
     patterns = [
         {"label": "PERSON", "pattern": "Dina", "id": "dina"},
         {"label": "ORG", "pattern": "ACME", "id": "acme"},
         {"label": "DATE", "pattern": "her birthday", "id": "bday"},
         {"label": "ORG", "pattern": "ACM"},
     ]
     ruler.add_patterns(patterns)
@@ -576,59 +513,43 @@
     assert len(doc.ents) == 3
     assert doc.ents[0].label_ == "PERSON"
     assert doc.ents[0].text == "Dina"
     assert doc.ents[1].label_ == "ORG"
     assert doc.ents[1].text == "ACME"
     assert doc.ents[2].label_ == "DATE"
     assert doc.ents[2].text == "her birthday"
-    if isinstance(ruler, EntityRuler):
-        ruler.remove("dina")
-        ruler.remove("acme")
-        ruler.remove("bday")
-    else:
-        ruler.remove_by_id("dina")
-        ruler.remove_by_id("acme")
-        ruler.remove_by_id("bday")
+    ruler.remove_by_id("dina")
+    ruler.remove_by_id("acme")
+    ruler.remove_by_id("bday")
     doc = nlp("Dina went to school")
     assert len(doc.ents) == 0
 
 
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_entity_ruler_remove_all_patterns(nlp, entity_ruler_factory):
-    ruler = nlp.add_pipe(entity_ruler_factory, name="entity_ruler")
+def test_entity_ruler_remove_all_patterns(nlp):
+    ruler = nlp.add_pipe("entity_ruler")
     patterns = [
         {"label": "PERSON", "pattern": "Dina", "id": "dina"},
         {"label": "ORG", "pattern": "ACME", "id": "acme"},
         {"label": "DATE", "pattern": "her birthday", "id": "bday"},
     ]
     ruler.add_patterns(patterns)
     assert len(ruler.patterns) == 3
-    if isinstance(ruler, EntityRuler):
-        ruler.remove("dina")
-    else:
-        ruler.remove_by_id("dina")
+    ruler.remove_by_id("dina")
     assert len(ruler.patterns) == 2
-    if isinstance(ruler, EntityRuler):
-        ruler.remove("acme")
-    else:
-        ruler.remove_by_id("acme")
+    ruler.remove_by_id("acme")
     assert len(ruler.patterns) == 1
-    if isinstance(ruler, EntityRuler):
-        ruler.remove("bday")
-    else:
-        ruler.remove_by_id("bday")
+    ruler.remove_by_id("bday")
     assert len(ruler.patterns) == 0
     with pytest.warns(UserWarning):
         doc = nlp("Dina founded her company ACME on her birthday")
         assert len(doc.ents) == 0
 
 
-@pytest.mark.parametrize("entity_ruler_factory", ENTITY_RULERS)
-def test_entity_ruler_remove_and_add(nlp, entity_ruler_factory):
-    ruler = nlp.add_pipe(entity_ruler_factory, name="entity_ruler")
+def test_entity_ruler_remove_and_add(nlp):
+    ruler = nlp.add_pipe("entity_ruler")
     patterns = [{"label": "DATE", "pattern": "last time"}]
     ruler.add_patterns(patterns)
     doc = ruler(
         nlp.make_doc("I saw him last time we met, this time he brought some flowers")
     )
     assert len(ruler.patterns) == 1
     assert len(doc.ents) == 1
@@ -641,18 +562,15 @@
     )
     assert len(ruler.patterns) == 2
     assert len(doc.ents) == 2
     assert doc.ents[0].label_ == "DATE"
     assert doc.ents[0].text == "last time"
     assert doc.ents[1].label_ == "DATE"
     assert doc.ents[1].text == "this time"
-    if isinstance(ruler, EntityRuler):
-        ruler.remove("ttime")
-    else:
-        ruler.remove_by_id("ttime")
+    ruler.remove_by_id("ttime")
     doc = ruler(
         nlp.make_doc("I saw him last time we met, this time he brought some flowers")
     )
     assert len(ruler.patterns) == 1
     assert len(doc.ents) == 1
     assert doc.ents[0].label_ == "DATE"
     assert doc.ents[0].text == "last time"
@@ -667,18 +585,15 @@
     doc = ruler(
         nlp.make_doc(
             "I saw him last time we met, this time he brought some flowers, another time some chocolate."
         )
     )
     assert len(ruler.patterns) == 3
     assert len(doc.ents) == 3
-    if isinstance(ruler, EntityRuler):
-        ruler.remove("ttime")
-    else:
-        ruler.remove_by_id("ttime")
+    ruler.remove_by_id("ttime")
     doc = ruler(
         nlp.make_doc(
             "I saw him last time we met, this time he brought some flowers, another time some chocolate."
         )
     )
     assert len(ruler.patterns) == 1
     assert len(doc.ents) == 1
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/pipeline/test_functions.py` & `spacy-4.0.0.dev0/spacy/tests/pipeline/test_functions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/pipeline/test_initialize.py` & `spacy-4.0.0.dev0/spacy/tests/pipeline/test_initialize.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/pipeline/test_lemmatizer.py` & `spacy-4.0.0.dev0/spacy/tests/pipeline/test_lemmatizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/pipeline/test_models.py` & `spacy-4.0.0.dev0/spacy/tests/pipeline/test_models.py`

 * *Files 0% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 from numpy.testing import assert_almost_equal
 from spacy.vocab import Vocab
 from thinc.api import Model, data_validation, get_current_ops
 from thinc.types import Array2d, Ragged
 
 from spacy.lang.en import English
 from spacy.ml import FeatureExtractor, StaticVectors
-from spacy.ml._character_embed import CharacterEmbed
+from spacy.ml.character_embed import CharacterEmbed
 from spacy.tokens import Doc
 
 
 OPS = get_current_ops()
 
 texts = ["These are 4 words", "Here just three"]
 l0 = [[1, 2], [3, 4], [5, 6], [7, 8]]
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/pipeline/test_morphologizer.py` & `spacy-4.0.0.dev0/spacy/tests/pipeline/test_morphologizer.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,69 +1,43 @@
+from typing import cast
 import pytest
-from numpy.testing import assert_equal, assert_almost_equal
-
-from thinc.api import get_current_ops
+from numpy.testing import assert_equal
 
 from spacy import util
 from spacy.training import Example
 from spacy.lang.en import English
 from spacy.language import Language
 from spacy.tests.util import make_tempdir
 from spacy.morphology import Morphology
+from spacy.pipeline import TrainablePipe
 from spacy.attrs import MORPH
 from spacy.tokens import Doc
 
 
 def test_label_types():
     nlp = Language()
     morphologizer = nlp.add_pipe("morphologizer")
     morphologizer.add_label("Feat=A")
     with pytest.raises(ValueError):
         morphologizer.add_label(9)
 
 
-TAGS = ["Feat=N", "Feat=V", "Feat=J"]
-
 TRAIN_DATA = [
     (
         "I like green eggs",
         {
             "morphs": ["Feat=N", "Feat=V", "Feat=J", "Feat=N"],
             "pos": ["NOUN", "VERB", "ADJ", "NOUN"],
         },
     ),
     # test combinations of morph+POS
     ("Eat blue ham", {"morphs": ["Feat=V", "", ""], "pos": ["", "ADJ", ""]}),
 ]
 
 
-def test_label_smoothing():
-    nlp = Language()
-    morph_no_ls = nlp.add_pipe("morphologizer", "no_label_smoothing")
-    morph_ls = nlp.add_pipe(
-        "morphologizer", "label_smoothing", config=dict(label_smoothing=0.05)
-    )
-    train_examples = []
-    losses = {}
-    for tag in TAGS:
-        morph_no_ls.add_label(tag)
-        morph_ls.add_label(tag)
-    for t in TRAIN_DATA:
-        train_examples.append(Example.from_dict(nlp.make_doc(t[0]), t[1]))
-
-    nlp.initialize(get_examples=lambda: train_examples)
-    tag_scores, bp_tag_scores = morph_ls.model.begin_update(
-        [eg.predicted for eg in train_examples]
-    )
-    ops = get_current_ops()
-    no_ls_grads = ops.to_numpy(morph_no_ls.get_loss(train_examples, tag_scores)[1][0])
-    ls_grads = ops.to_numpy(morph_ls.get_loss(train_examples, tag_scores)[1][0])
-    assert_almost_equal(ls_grads / no_ls_grads, 0.94285715)
-
-
 def test_no_label():
     nlp = Language()
     nlp.add_pipe("morphologizer")
     with pytest.raises(ValueError):
         nlp.initialize()
 
 
@@ -72,14 +46,20 @@
     nlp.add_pipe("morphologizer")
     train_examples = []
     for t in TRAIN_DATA:
         train_examples.append(Example.from_dict(nlp.make_doc(t[0]), t[1]))
     nlp.initialize(get_examples=lambda: train_examples)
 
 
+def test_is_distillable():
+    nlp = English()
+    morphologizer = nlp.add_pipe("morphologizer")
+    assert morphologizer.is_distillable
+
+
 def test_no_resize():
     nlp = Language()
     morphologizer = nlp.add_pipe("morphologizer")
     morphologizer.add_label("POS" + Morphology.FIELD_SEP + "NOUN")
     morphologizer.add_label("POS" + Morphology.FIELD_SEP + "VERB")
     nlp.initialize()
     # this throws an error because the morphologizer can't be resized after initialization
@@ -221,7 +201,29 @@
     # Test the trained model
     test_text = "I like blue ham"
     doc = nlp(test_text)
     gold_morphs = ["", "", "", ""]
     gold_pos_tags = ["NOUN", "NOUN", "NOUN", "NOUN"]
     assert [str(t.morph) for t in doc] == gold_morphs
     assert [t.pos_ for t in doc] == gold_pos_tags
+
+
+def test_save_activations():
+    nlp = English()
+    morphologizer = cast(TrainablePipe, nlp.add_pipe("morphologizer"))
+    train_examples = []
+    for inst in TRAIN_DATA:
+        train_examples.append(Example.from_dict(nlp.make_doc(inst[0]), inst[1]))
+    nlp.initialize(get_examples=lambda: train_examples)
+
+    doc = nlp("This is a test.")
+    assert "morphologizer" not in doc.activations
+
+    morphologizer.save_activations = True
+    doc = nlp("This is a test.")
+    assert "morphologizer" in doc.activations
+    assert set(doc.activations["morphologizer"].keys()) == {
+        "label_ids",
+        "probabilities",
+    }
+    assert doc.activations["morphologizer"]["probabilities"].shape == (5, 6)
+    assert doc.activations["morphologizer"]["label_ids"].shape == (5,)
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/pipeline/test_pipe_factories.py` & `spacy-4.0.0.dev0/spacy/tests/pipeline/test_pipe_factories.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/pipeline/test_pipe_methods.py` & `spacy-4.0.0.dev0/spacy/tests/pipeline/test_pipe_methods.py`

 * *Files 1% similar despite different names*

```diff
@@ -525,25 +525,14 @@
     pipe = nlp.add_pipe(pipe)
     assert getattr(pipe, "label_data", None) is None
     initialize = getattr(pipe, "initialize", None)
     if initialize is not None:
         assert "labels" not in get_arg_names(initialize)
 
 
-def test_warning_pipe_begin_training():
-    with pytest.warns(UserWarning, match="begin_training"):
-
-        class IncompatPipe(TrainablePipe):
-            def __init__(self):
-                ...
-
-            def begin_training(*args, **kwargs):
-                ...
-
-
 def test_pipe_methods_initialize():
     """Test that the [initialize] config reflects the components correctly."""
     nlp = Language()
     nlp.add_pipe("tagger")
     assert "tagger" not in nlp.config["initialize"]["components"]
     nlp.config["initialize"]["components"]["tagger"] = {"labels": ["hello"]}
     assert nlp.config["initialize"]["components"]["tagger"] == {"labels": ["hello"]}
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/pipeline/test_sentencizer.py` & `spacy-4.0.0.dev0/spacy/tests/pipeline/test_sentencizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/pipeline/test_senter.py` & `spacy-4.0.0.dev0/spacy/tests/pipeline/test_senter.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,18 +1,26 @@
+from typing import cast
 import pytest
 from numpy.testing import assert_equal
 from spacy.attrs import SENT_START
 
 from spacy import util
 from spacy.training import Example
 from spacy.lang.en import English
 from spacy.language import Language
+from spacy.pipeline import TrainablePipe
 from spacy.tests.util import make_tempdir
 
 
+def test_is_distillable():
+    nlp = English()
+    senter = nlp.add_pipe("senter")
+    assert senter.is_distillable
+
+
 def test_label_types():
     nlp = Language()
     senter = nlp.add_pipe("senter")
     with pytest.raises(NotImplementedError):
         senter.add_label("A")
 
 
@@ -97,7 +105,30 @@
     ]
     assert_equal(batch_deps_1, batch_deps_2)
     assert_equal(batch_deps_1, no_batch_deps)
 
     # test internal pipe labels vs. Language.pipe_labels with hidden labels
     assert nlp.get_pipe("senter").labels == ("I", "S")
     assert "senter" not in nlp.pipe_labels
+
+
+def test_save_activations():
+    # Test if activations are correctly added to Doc when requested.
+    nlp = English()
+    senter = cast(TrainablePipe, nlp.add_pipe("senter"))
+
+    train_examples = []
+    for t in TRAIN_DATA:
+        train_examples.append(Example.from_dict(nlp.make_doc(t[0]), t[1]))
+
+    nlp.initialize(get_examples=lambda: train_examples)
+    nO = senter.model.get_dim("nO")
+
+    doc = nlp("This is a test.")
+    assert "senter" not in doc.activations
+
+    senter.save_activations = True
+    doc = nlp("This is a test.")
+    assert "senter" in doc.activations
+    assert set(doc.activations["senter"].keys()) == {"label_ids", "probabilities"}
+    assert doc.activations["senter"]["probabilities"].shape == (5, nO)
+    assert doc.activations["senter"]["label_ids"].shape == (5,)
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/pipeline/test_span_ruler.py` & `spacy-4.0.0.dev0/spacy/tests/pipeline/test_span_ruler.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/pipeline/test_tagger.py` & `spacy-4.0.0.dev0/spacy/tests/pipeline/test_tagger.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,16 +1,18 @@
+from typing import cast
 import pytest
-from numpy.testing import assert_equal, assert_almost_equal
+from numpy.testing import assert_equal
 from spacy.attrs import TAG
 
 from spacy import util
 from spacy.training import Example
 from spacy.lang.en import English
 from spacy.language import Language
-from thinc.api import compounding, get_current_ops
+from spacy.pipeline import TrainablePipe
+from thinc.api import compounding
 
 from ..util import make_tempdir
 
 
 @pytest.mark.issue(4348)
 def test_issue4348():
     """Test that training the tagger with empty data, doesn't throw errors"""
@@ -18,15 +20,17 @@
     example = Example.from_dict(nlp.make_doc(""), {"tags": []})
     TRAIN_DATA = [example, example]
     tagger = nlp.add_pipe("tagger")
     tagger.add_label("A")
     optimizer = nlp.initialize()
     for i in range(5):
         losses = {}
-        batches = util.minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))
+        batches = util.minibatch(
+            TRAIN_DATA, size=compounding(4.0, 32.0, 1.001).to_generator()
+        )
         for batch in batches:
             nlp.update(batch, sgd=optimizer, losses=losses)
 
 
 def test_label_types():
     nlp = Language()
     tagger = nlp.add_pipe("tagger")
@@ -63,38 +67,14 @@
             "words": ["He", "hate", "s", "green", "eggs"],
             "tags": ["", "V", "S", "J", ""],
         },
     ),
 ]
 
 
-def test_label_smoothing():
-    nlp = Language()
-    tagger_no_ls = nlp.add_pipe("tagger", "no_label_smoothing")
-    tagger_ls = nlp.add_pipe(
-        "tagger", "label_smoothing", config=dict(label_smoothing=0.05)
-    )
-    train_examples = []
-    losses = {}
-    for tag in TAGS:
-        tagger_no_ls.add_label(tag)
-        tagger_ls.add_label(tag)
-    for t in TRAIN_DATA:
-        train_examples.append(Example.from_dict(nlp.make_doc(t[0]), t[1]))
-
-    nlp.initialize(get_examples=lambda: train_examples)
-    tag_scores, bp_tag_scores = tagger_ls.model.begin_update(
-        [eg.predicted for eg in train_examples]
-    )
-    ops = get_current_ops()
-    no_ls_grads = ops.to_numpy(tagger_no_ls.get_loss(train_examples, tag_scores)[1][0])
-    ls_grads = ops.to_numpy(tagger_ls.get_loss(train_examples, tag_scores)[1][0])
-    assert_almost_equal(ls_grads / no_ls_grads, 0.925)
-
-
 def test_no_label():
     nlp = Language()
     nlp.add_pipe("tagger")
     with pytest.raises(ValueError):
         nlp.initialize()
 
 
@@ -231,12 +211,78 @@
         nlp.update([neg_ex], sgd=optimizer, losses=losses)
 
     # test the "untrained" tag
     doc3 = nlp(test_text)
     assert doc3[0].tag_ != "N"
 
 
+def test_is_distillable():
+    nlp = English()
+    tagger = nlp.add_pipe("tagger")
+    assert tagger.is_distillable
+
+
+def test_distill():
+    teacher = English()
+    teacher_tagger = teacher.add_pipe("tagger")
+    train_examples = []
+    for t in TRAIN_DATA:
+        train_examples.append(Example.from_dict(teacher.make_doc(t[0]), t[1]))
+
+    optimizer = teacher.initialize(get_examples=lambda: train_examples)
+
+    for i in range(50):
+        losses = {}
+        teacher.update(train_examples, sgd=optimizer, losses=losses)
+    assert losses["tagger"] < 0.00001
+
+    student = English()
+    student_tagger = student.add_pipe("tagger")
+    student_tagger.min_tree_freq = 1
+    student_tagger.initialize(
+        get_examples=lambda: train_examples, labels=teacher_tagger.label_data
+    )
+
+    distill_examples = [
+        Example.from_dict(teacher.make_doc(t[0]), {}) for t in TRAIN_DATA
+    ]
+
+    for i in range(50):
+        losses = {}
+        student_tagger.distill(
+            teacher_tagger, distill_examples, sgd=optimizer, losses=losses
+        )
+    assert losses["tagger"] < 0.00001
+
+    test_text = "I like blue eggs"
+    doc = student(test_text)
+    assert doc[0].tag_ == "N"
+    assert doc[1].tag_ == "V"
+    assert doc[2].tag_ == "J"
+    assert doc[3].tag_ == "N"
+
+
+def test_save_activations():
+    # Test if activations are correctly added to Doc when requested.
+    nlp = English()
+    tagger = cast(TrainablePipe, nlp.add_pipe("tagger"))
+    train_examples = []
+    for t in TRAIN_DATA:
+        train_examples.append(Example.from_dict(nlp.make_doc(t[0]), t[1]))
+    nlp.initialize(get_examples=lambda: train_examples)
+
+    doc = nlp("This is a test.")
+    assert "tagger" not in doc.activations
+
+    tagger.save_activations = True
+    doc = nlp("This is a test.")
+    assert "tagger" in doc.activations
+    assert set(doc.activations["tagger"].keys()) == {"label_ids", "probabilities"}
+    assert doc.activations["tagger"]["probabilities"].shape == (5, len(TAGS))
+    assert doc.activations["tagger"]["label_ids"].shape == (5,)
+
+
 def test_tagger_requires_labels():
     nlp = English()
     nlp.add_pipe("tagger")
     with pytest.raises(ValueError):
         nlp.initialize()
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/pipeline/test_textcat.py` & `spacy-4.0.0.dev0/spacy/tests/pipeline/test_textcat.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,21 +1,22 @@
+from typing import cast
 import random
 
 import numpy.random
 import pytest
 from numpy.testing import assert_almost_equal
 from thinc.api import Config, compounding, fix_random_seed, get_current_ops
 from wasabi import msg
 
 import spacy
 from spacy import util
 from spacy.cli.evaluate import print_prf_per_type, print_textcats_auc_per_cat
 from spacy.lang.en import English
 from spacy.language import Language
-from spacy.pipeline import TextCategorizer
+from spacy.pipeline import TextCategorizer, TrainablePipe
 from spacy.pipeline.textcat import single_label_bow_config
 from spacy.pipeline.textcat import single_label_cnn_config
 from spacy.pipeline.textcat import single_label_default_config
 from spacy.pipeline.textcat_multilabel import multi_label_bow_config
 from spacy.pipeline.textcat_multilabel import multi_label_cnn_config
 from spacy.pipeline.textcat_multilabel import multi_label_default_config
 from spacy.pipeline.tok2vec import DEFAULT_TOK2VEC_MODEL
@@ -86,15 +87,17 @@
     for label in unique_classes:
         textcat.add_label(label)
     # training the network
     with nlp.select_pipes(enable="textcat"):
         optimizer = nlp.initialize()
         for i in range(3):
             losses = {}
-            batches = util.minibatch(train_data, size=compounding(4.0, 32.0, 1.001))
+            batches = util.minibatch(
+                train_data, size=compounding(4.0, 32.0, 1.001).to_generator()
+            )
 
             for batch in batches:
                 nlp.update(examples=batch, sgd=optimizer, drop=0.1, losses=losses)
 
 
 @pytest.mark.issue(4030)
 def test_issue4030():
@@ -123,15 +126,17 @@
     for label in unique_classes:
         textcat.add_label(label)
     # training the network
     with nlp.select_pipes(enable="textcat"):
         optimizer = nlp.initialize()
         for i in range(3):
             losses = {}
-            batches = util.minibatch(train_data, size=compounding(4.0, 32.0, 1.001))
+            batches = util.minibatch(
+                train_data, size=compounding(4.0, 32.0, 1.001).to_generator()
+            )
 
             for batch in batches:
                 nlp.update(examples=batch, sgd=optimizer, drop=0.1, losses=losses)
     # processing of an empty doc should result in 0.0 for all categories
     doc = nlp("")
     assert doc.cats["offensive"] == 0.0
     assert doc.cats["inoffensive"] == 0.0
@@ -281,15 +286,15 @@
 def test_issue9904():
     nlp = Language()
     textcat = nlp.add_pipe("textcat")
     get_examples = make_get_examples_single_label(nlp)
     nlp.initialize(get_examples)
 
     examples = get_examples()
-    scores = textcat.predict([eg.predicted for eg in examples])
+    scores = textcat.predict([eg.predicted for eg in examples])["probabilities"]
 
     loss = textcat.get_loss(examples, scores)[0]
     loss_double_bs = textcat.get_loss(examples * 2, scores.repeat(2, axis=0))[0]
     assert loss == pytest.approx(loss_double_bs)
 
 
 @pytest.mark.skip(reason="Test is flakey when run with others")
@@ -560,14 +565,20 @@
     nlp.initialize(get_examples=get_examples(nlp))
     with pytest.raises(TypeError):
         nlp.initialize(get_examples=lambda: None)
     with pytest.raises(TypeError):
         nlp.initialize(get_examples=get_examples())
 
 
+def test_is_distillable():
+    nlp = English()
+    textcat = nlp.add_pipe("textcat")
+    assert not textcat.is_distillable
+
+
 def test_overfitting_IO():
     # Simple test to try and quickly overfit the single-label textcat component - ensuring the ML models work correctly
     fix_random_seed(0)
     nlp = English()
     textcat = nlp.add_pipe("textcat")
 
     train_examples = []
@@ -893,14 +904,52 @@
     scores = nlp.evaluate(train_examples, scorer_cfg={"threshold": 1.0})
     assert scores["cats_f_per_type"]["POSITIVE"]["r"] == 0
 
     scores = nlp.evaluate(train_examples, scorer_cfg={"threshold": 0})
     assert scores["cats_f_per_type"]["POSITIVE"]["r"] == 1.0
 
 
+def test_save_activations():
+    nlp = English()
+    textcat = cast(TrainablePipe, nlp.add_pipe("textcat"))
+
+    train_examples = []
+    for text, annotations in TRAIN_DATA_SINGLE_LABEL:
+        train_examples.append(Example.from_dict(nlp.make_doc(text), annotations))
+    nlp.initialize(get_examples=lambda: train_examples)
+    nO = textcat.model.get_dim("nO")
+
+    doc = nlp("This is a test.")
+    assert "textcat" not in doc.activations
+
+    textcat.save_activations = True
+    doc = nlp("This is a test.")
+    assert list(doc.activations["textcat"].keys()) == ["probabilities"]
+    assert doc.activations["textcat"]["probabilities"].shape == (nO,)
+
+
+def test_save_activations_multi():
+    nlp = English()
+    textcat = cast(TrainablePipe, nlp.add_pipe("textcat_multilabel"))
+
+    train_examples = []
+    for text, annotations in TRAIN_DATA_MULTI_LABEL:
+        train_examples.append(Example.from_dict(nlp.make_doc(text), annotations))
+    nlp.initialize(get_examples=lambda: train_examples)
+    nO = textcat.model.get_dim("nO")
+
+    doc = nlp("This is a test.")
+    assert "textcat_multilabel" not in doc.activations
+
+    textcat.save_activations = True
+    doc = nlp("This is a test.")
+    assert list(doc.activations["textcat_multilabel"].keys()) == ["probabilities"]
+    assert doc.activations["textcat_multilabel"]["probabilities"].shape == (nO,)
+
+
 @pytest.mark.parametrize(
     "component_name,scorer",
     [
         ("textcat", "spacy.textcat_scorer.v1"),
         ("textcat_multilabel", "spacy.textcat_multilabel_scorer.v1"),
     ],
 )
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/pipeline/test_tok2vec.py` & `spacy-4.0.0.dev0/spacy/tests/pipeline/test_tok2vec.py`

 * *Files 0% similar despite different names*

```diff
@@ -378,15 +378,15 @@
     @architectures = "spacy.Tok2VecListener.v1"
     width = ${components.tok2vec.model.encode.width}
 
     [components.ner]
     factory = "ner"
 
     [components.ner.model]
-    @architectures = "spacy.TransitionBasedParser.v2"
+    @architectures = "spacy.TransitionBasedParser.v3"
 
     [components.ner.model.tok2vec]
     @architectures = "spacy.Tok2VecListener.v1"
     width = ${components.tok2vec.model.encode.width}
 
     [components.tok2vec]
     factory = "tok2vec"
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/serialize/test_resource_warning.py` & `spacy-4.0.0.dev0/spacy/tests/serialize/test_resource_warning.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/serialize/test_serialize_config.py` & `spacy-4.0.0.dev0/spacy/tests/serialize/test_serialize_config.py`

 * *Files 6% similar despite different names*

```diff
@@ -118,41 +118,19 @@
 
 [pretraining]
 """
 
 
 parser_config_string_upper = """
 [model]
-@architectures = "spacy.TransitionBasedParser.v2"
+@architectures = "spacy.TransitionBasedParser.v3"
 state_type = "parser"
 extra_state_tokens = false
 hidden_width = 66
 maxout_pieces = 2
-use_upper = true
-
-[model.tok2vec]
-@architectures = "spacy.HashEmbedCNN.v1"
-pretrained_vectors = null
-width = 333
-depth = 4
-embed_size = 5555
-window_size = 1
-maxout_pieces = 7
-subword_features = false
-"""
-
-
-parser_config_string_no_upper = """
-[model]
-@architectures = "spacy.TransitionBasedParser.v2"
-state_type = "parser"
-extra_state_tokens = false
-hidden_width = 66
-maxout_pieces = 2
-use_upper = false
 
 [model.tok2vec]
 @architectures = "spacy.HashEmbedCNN.v1"
 pretrained_vectors = null
 width = 333
 depth = 4
 embed_size = 5555
@@ -175,15 +153,14 @@
     )
     parser = build_tb_parser_model(
         tok2vec=tok2vec,
         state_type="parser",
         extra_state_tokens=True,
         hidden_width=65,
         maxout_pieces=5,
-        use_upper=True,
     )
     return parser
 
 
 @pytest.mark.issue(8190)
 def test_issue8190():
     """Test that config overrides are not lost after load is complete."""
@@ -281,40 +258,43 @@
     nlp.add_pipe("parser", config=parser_cfg)
     nlp.initialize()
 
     with make_tempdir() as d:
         nlp.to_disk(d)
         nlp2 = spacy.load(d)
         model = nlp2.get_pipe("parser").model
-        model.get_ref("tok2vec")
-        # check that we have the correct settings, not the default ones
-        assert model.get_ref("upper").get_dim("nI") == 65
-        assert model.get_ref("lower").get_dim("nI") == 65
+        assert model.get_ref("tok2vec") is not None
+        assert model.has_param("hidden_W")
+        assert model.has_param("hidden_b")
+        output = model.get_ref("output")
+        assert output is not None
+        assert output.has_param("W")
+        assert output.has_param("b")
 
 
-@pytest.mark.parametrize(
-    "parser_config_string", [parser_config_string_upper, parser_config_string_no_upper]
-)
+@pytest.mark.parametrize("parser_config_string", [parser_config_string_upper])
 def test_serialize_parser(parser_config_string):
     """Create a non-default parser config to check nlp serializes it correctly"""
     nlp = English()
     model_config = Config().from_str(parser_config_string)
     parser = nlp.add_pipe("parser", config=model_config)
     parser.add_label("nsubj")
     nlp.initialize()
 
     with make_tempdir() as d:
         nlp.to_disk(d)
         nlp2 = spacy.load(d)
         model = nlp2.get_pipe("parser").model
-        model.get_ref("tok2vec")
-        # check that we have the correct settings, not the default ones
-        if model.attrs["has_upper"]:
-            assert model.get_ref("upper").get_dim("nI") == 66
-        assert model.get_ref("lower").get_dim("nI") == 66
+        assert model.get_ref("tok2vec") is not None
+        assert model.has_param("hidden_W")
+        assert model.has_param("hidden_b")
+        output = model.get_ref("output")
+        assert output is not None
+        assert output.has_param("b")
+        assert output.has_param("W")
 
 
 def test_config_nlp_roundtrip():
     """Test that a config produced by the nlp object passes training config
     validation."""
     nlp = English()
     nlp.add_pipe("entity_ruler")
@@ -453,17 +433,15 @@
     config = Config({"nlp": {"lang": "en"}, "training": {"extra": "hello"}})
     nlp = load_model_from_config(config, auto_fill=True, validate=False)
     assert "extra" not in nlp.config["training"]
     # Make sure the config generated is valid
     load_model_from_config(nlp.config)
 
 
-@pytest.mark.parametrize(
-    "parser_config_string", [parser_config_string_upper, parser_config_string_no_upper]
-)
+@pytest.mark.parametrize("parser_config_string", [parser_config_string_upper])
 def test_config_validate_literal(parser_config_string):
     nlp = English()
     config = Config().from_str(parser_config_string)
     config["model"]["state_type"] = "nonsense"
     with pytest.raises(ConfigValidationError):
         nlp.add_pipe("parser", config=config)
     config["model"]["state_type"] = "ner"
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/serialize/test_serialize_doc.py` & `spacy-4.0.0.dev0/spacy/tests/serialize/test_serialize_doc.py`

 * *Files 8% similar despite different names*

```diff
@@ -209,17 +209,10 @@
     assert not new_doc.user_data
     new_doc = Doc(en_vocab).from_bytes(doc.to_bytes(exclude=["user_data"]))
     assert not new_doc.user_data
 
 
 def test_serialize_doc_span_groups(en_vocab):
     doc = Doc(en_vocab, words=["hello", "world", "!"])
-    span = doc[0:2]
-    span.label_ = "test_serialize_doc_span_groups_label"
-    span.id_ = "test_serialize_doc_span_groups_id"
-    span.kb_id_ = "test_serialize_doc_span_groups_kb_id"
-    doc.spans["content"] = [span]
+    doc.spans["content"] = [doc[0:2]]
     new_doc = Doc(en_vocab).from_bytes(doc.to_bytes())
     assert len(new_doc.spans["content"]) == 1
-    assert new_doc.spans["content"][0].label_ == "test_serialize_doc_span_groups_label"
-    assert new_doc.spans["content"][0].id_ == "test_serialize_doc_span_groups_id"
-    assert new_doc.spans["content"][0].kb_id_ == "test_serialize_doc_span_groups_kb_id"
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/serialize/test_serialize_docbin.py` & `spacy-4.0.0.dev0/spacy/tests/serialize/test_serialize_docbin.py`

 * *Files 13% similar despite different names*

```diff
@@ -45,35 +45,28 @@
         attrs=["LEMMA", "ENT_IOB", "ENT_TYPE", "NORM", "ENT_ID"], store_user_data=True
     )
     texts = ["Some text", "Lots of texts...", "..."]
     cats = {"A": 0.5}
     nlp = English()
     for doc in nlp.pipe(texts):
         doc.cats = cats
-        span = doc[0:2]
-        span.label_ = "UNUSUAL_SPAN_LABEL"
-        span.id_ = "UNUSUAL_SPAN_ID"
-        span.kb_id_ = "UNUSUAL_SPAN_KB_ID"
-        doc.spans["start"] = [span]
+        doc.spans["start"] = [doc[0:2]]
         doc[0].norm_ = "UNUSUAL_TOKEN_NORM"
         doc[0].ent_id_ = "UNUSUAL_TOKEN_ENT_ID"
         doc_bin.add(doc)
     bytes_data = doc_bin.to_bytes()
 
     # Deserialize later, e.g. in a new process
     nlp = spacy.blank("en")
     doc_bin = DocBin().from_bytes(bytes_data)
     reloaded_docs = list(doc_bin.get_docs(nlp.vocab))
     for i, doc in enumerate(reloaded_docs):
         assert doc.text == texts[i]
         assert doc.cats == cats
         assert len(doc.spans) == 1
-        assert doc.spans["start"][0].label_ == "UNUSUAL_SPAN_LABEL"
-        assert doc.spans["start"][0].id_ == "UNUSUAL_SPAN_ID"
-        assert doc.spans["start"][0].kb_id_ == "UNUSUAL_SPAN_KB_ID"
         assert doc[0].norm_ == "UNUSUAL_TOKEN_NORM"
         assert doc[0].ent_id_ == "UNUSUAL_TOKEN_ENT_ID"
 
 
 def test_serialize_doc_bin_unknown_spaces(en_vocab):
     doc1 = Doc(en_vocab, words=["that", "'s"])
     assert doc1.has_unknown_spaces
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/serialize/test_serialize_extension_attrs.py` & `spacy-4.0.0.dev0/spacy/tests/serialize/test_serialize_extension_attrs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/serialize/test_serialize_kb.py` & `spacy-4.0.0.dev0/spacy/tests/serialize/test_serialize_kb.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,14 +1,11 @@
-from pathlib import Path
-from typing import Callable, Iterable, Any, Dict
+from typing import Callable
 
-import srsly
-
-from spacy import util, Errors
-from spacy.util import ensure_path, registry, load_model_from_config, SimpleFrozenList
+from spacy import util
+from spacy.util import ensure_path, registry, load_model_from_config
 from spacy.kb.kb_in_memory import InMemoryLookupKB
 from spacy.vocab import Vocab
 from thinc.api import Config
 
 from ..util import make_tempdir
 from numpy import zeros
 
@@ -90,86 +87,36 @@
     lang = "en"
     pipeline = ["entity_linker"]
 
     [components]
 
     [components.entity_linker]
     factory = "entity_linker"
-    
-    [components.entity_linker.generate_empty_kb]
-    @misc = "kb_test.CustomEmptyKB.v1"
-    
+
     [initialize]
 
     [initialize.components]
 
     [initialize.components.entity_linker]
 
     [initialize.components.entity_linker.kb_loader]
-    @misc = "kb_test.CustomKB.v1"
+    @misc = "spacy.CustomKB.v1"
     entity_vector_length = 342
     custom_field = 666
     """
 
     class SubInMemoryLookupKB(InMemoryLookupKB):
         def __init__(self, vocab, entity_vector_length, custom_field):
             super().__init__(vocab, entity_vector_length)
             self.custom_field = custom_field
 
-        def to_disk(self, path, exclude: Iterable[str] = SimpleFrozenList()):
-            """We overwrite InMemoryLookupKB.to_disk() to ensure that self.custom_field is stored as well."""
-            path = ensure_path(path)
-            if not path.exists():
-                path.mkdir(parents=True)
-            if not path.is_dir():
-                raise ValueError(Errors.E928.format(loc=path))
-
-            def serialize_custom_fields(file_path: Path) -> None:
-                srsly.write_json(file_path, {"custom_field": self.custom_field})
-
-            serialize = {
-                "contents": lambda p: self.write_contents(p),
-                "strings.json": lambda p: self.vocab.strings.to_disk(p),
-                "custom_fields": lambda p: serialize_custom_fields(p),
-            }
-            util.to_disk(path, serialize, exclude)
-
-        def from_disk(self, path, exclude: Iterable[str] = SimpleFrozenList()):
-            """We overwrite InMemoryLookupKB.from_disk() to ensure that self.custom_field is loaded as well."""
-            path = ensure_path(path)
-            if not path.exists():
-                raise ValueError(Errors.E929.format(loc=path))
-            if not path.is_dir():
-                raise ValueError(Errors.E928.format(loc=path))
-
-            def deserialize_custom_fields(file_path: Path) -> None:
-                self.custom_field = srsly.read_json(file_path)["custom_field"]
-
-            deserialize: Dict[str, Callable[[Any], Any]] = {
-                "contents": lambda p: self.read_contents(p),
-                "strings.json": lambda p: self.vocab.strings.from_disk(p),
-                "custom_fields": lambda p: deserialize_custom_fields(p),
-            }
-            util.from_disk(path, deserialize, exclude)
-
-    @registry.misc("kb_test.CustomEmptyKB.v1")
-    def empty_custom_kb() -> Callable[[Vocab, int], SubInMemoryLookupKB]:
-        def empty_kb_factory(vocab: Vocab, entity_vector_length: int):
-            return SubInMemoryLookupKB(
-                vocab=vocab,
-                entity_vector_length=entity_vector_length,
-                custom_field=0,
-            )
-
-        return empty_kb_factory
-
-    @registry.misc("kb_test.CustomKB.v1")
+    @registry.misc("spacy.CustomKB.v1")
     def custom_kb(
         entity_vector_length: int, custom_field: int
-    ) -> Callable[[Vocab], SubInMemoryLookupKB]:
+    ) -> Callable[[Vocab], InMemoryLookupKB]:
         def custom_kb_factory(vocab):
             kb = SubInMemoryLookupKB(
                 vocab=vocab,
                 entity_vector_length=entity_vector_length,
                 custom_field=custom_field,
             )
             kb.add_entity("random_entity", 0.0, zeros(entity_vector_length))
@@ -188,10 +135,10 @@
 
     # Make sure the custom KB is serialized correctly
     with make_tempdir() as tmp_dir:
         nlp.to_disk(tmp_dir)
         nlp2 = util.load_model_from_path(tmp_dir)
         entity_linker2 = nlp2.get_pipe("entity_linker")
         # After IO, the KB is the standard one
-        assert type(entity_linker2.kb) == SubInMemoryLookupKB
+        assert type(entity_linker2.kb) == InMemoryLookupKB
         assert entity_linker2.kb.entity_vector_length == 342
-        assert entity_linker2.kb.custom_field == 666
+        assert not hasattr(entity_linker2.kb, "custom_field")
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/serialize/test_serialize_language.py` & `spacy-4.0.0.dev0/spacy/tests/serialize/test_serialize_language.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/serialize/test_serialize_pipeline.py` & `spacy-4.0.0.dev0/spacy/tests/serialize/test_serialize_pipeline.py`

 * *Files 5% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 import srsly
 from thinc.api import Linear
 
 import spacy
 from spacy import Vocab, load, registry
 from spacy.lang.en import English
 from spacy.language import Language
-from spacy.pipeline import DependencyParser, EntityRecognizer, EntityRuler
+from spacy.pipeline import DependencyParser, EntityRecognizer
 from spacy.pipeline import SentenceRecognizer, Tagger, TextCategorizer
 from spacy.pipeline import TrainablePipe
 from spacy.pipeline.dep_parser import DEFAULT_PARSER_MODEL
 from spacy.pipeline.senter import DEFAULT_SENTER_MODEL
 from spacy.pipeline.tagger import DEFAULT_TAGGER_MODEL
 from spacy.pipeline.textcat import DEFAULT_SINGLE_TEXTCAT_MODEL
 from spacy.util import ensure_path, load_model
@@ -81,89 +81,46 @@
         {"label": "HELLO", "pattern": "hello world"},
         {"label": "BYE", "pattern": [{"LOWER": "bye"}, {"LOWER": "bye"}]},
         {"label": "HELLO", "pattern": [{"ORTH": "HELLO"}]},
         {"label": "COMPLEX", "pattern": [{"ORTH": "foo", "OP": "*"}]},
         {"label": "TECH_ORG", "pattern": "Apple", "id": "a1"},
     ]
     nlp = Language(vocab=en_vocab)
-    ruler = EntityRuler(nlp, patterns=patterns, overwrite_ents=True)
+    ruler = nlp.add_pipe("entity_ruler", config={"overwrite_ents": True})
+    ruler.add_patterns(patterns)
     ruler_bytes = ruler.to_bytes()
     assert len(ruler) == len(patterns)
     assert len(ruler.labels) == 4
-    assert ruler.overwrite
-    new_ruler = EntityRuler(nlp)
+    new_ruler = nlp.add_pipe(
+        "entity_ruler", name="new_ruler", config={"overwrite_ents": True}
+    )
     new_ruler = new_ruler.from_bytes(ruler_bytes)
     assert len(new_ruler) == len(ruler)
     assert len(new_ruler.labels) == 4
-    assert new_ruler.overwrite == ruler.overwrite
-    assert new_ruler.ent_id_sep == ruler.ent_id_sep
-
-
-@pytest.mark.issue(3526)
-def test_issue_3526_2(en_vocab):
-    patterns = [
-        {"label": "HELLO", "pattern": "hello world"},
-        {"label": "BYE", "pattern": [{"LOWER": "bye"}, {"LOWER": "bye"}]},
-        {"label": "HELLO", "pattern": [{"ORTH": "HELLO"}]},
-        {"label": "COMPLEX", "pattern": [{"ORTH": "foo", "OP": "*"}]},
-        {"label": "TECH_ORG", "pattern": "Apple", "id": "a1"},
-    ]
-    nlp = Language(vocab=en_vocab)
-    ruler = EntityRuler(nlp, patterns=patterns, overwrite_ents=True)
-    bytes_old_style = srsly.msgpack_dumps(ruler.patterns)
-    new_ruler = EntityRuler(nlp)
-    new_ruler = new_ruler.from_bytes(bytes_old_style)
-    assert len(new_ruler) == len(ruler)
-    for pattern in ruler.patterns:
-        assert pattern in new_ruler.patterns
-    assert new_ruler.overwrite is not ruler.overwrite
-
-
-@pytest.mark.issue(3526)
-def test_issue_3526_3(en_vocab):
-    patterns = [
-        {"label": "HELLO", "pattern": "hello world"},
-        {"label": "BYE", "pattern": [{"LOWER": "bye"}, {"LOWER": "bye"}]},
-        {"label": "HELLO", "pattern": [{"ORTH": "HELLO"}]},
-        {"label": "COMPLEX", "pattern": [{"ORTH": "foo", "OP": "*"}]},
-        {"label": "TECH_ORG", "pattern": "Apple", "id": "a1"},
-    ]
-    nlp = Language(vocab=en_vocab)
-    ruler = EntityRuler(nlp, patterns=patterns, overwrite_ents=True)
-    with make_tempdir() as tmpdir:
-        out_file = tmpdir / "entity_ruler"
-        srsly.write_jsonl(out_file.with_suffix(".jsonl"), ruler.patterns)
-        new_ruler = EntityRuler(nlp).from_disk(out_file)
-        for pattern in ruler.patterns:
-            assert pattern in new_ruler.patterns
-        assert len(new_ruler) == len(ruler)
-        assert new_ruler.overwrite is not ruler.overwrite
 
 
 @pytest.mark.issue(3526)
 def test_issue_3526_4(en_vocab):
     nlp = Language(vocab=en_vocab)
     patterns = [{"label": "ORG", "pattern": "Apple"}]
     config = {"overwrite_ents": True}
     ruler = nlp.add_pipe("entity_ruler", config=config)
     ruler.add_patterns(patterns)
     with make_tempdir() as tmpdir:
         nlp.to_disk(tmpdir)
         ruler = nlp.get_pipe("entity_ruler")
         assert ruler.patterns == [{"label": "ORG", "pattern": "Apple"}]
-        assert ruler.overwrite is True
         nlp2 = load(tmpdir)
         new_ruler = nlp2.get_pipe("entity_ruler")
         assert new_ruler.patterns == [{"label": "ORG", "pattern": "Apple"}]
-        assert new_ruler.overwrite is True
 
 
 @pytest.mark.issue(4042)
 def test_issue4042():
-    """Test that serialization of an EntityRuler before NER works fine."""
+    """Test that serialization of an entity_ruler before NER works fine."""
     nlp = English()
     # add ner pipe
     ner = nlp.add_pipe("ner")
     ner.add_label("SOME_LABEL")
     nlp.initialize()
     # Add entity ruler
     patterns = [
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/serialize/test_serialize_span_groups.py` & `spacy-4.0.0.dev0/spacy/tests/serialize/test_serialize_span_groups.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 import pytest
 
 from spacy.tokens import Span, SpanGroup
-from spacy.tokens._dict_proxies import SpanGroups
+from spacy.tokens.span_groups import SpanGroups
 
 
 @pytest.mark.issue(10685)
 def test_issue10685(en_tokenizer):
     """Test `SpanGroups` de/serialization"""
     # Start with a Doc with no SpanGroups
     doc = en_tokenizer("Will it blend?")
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/serialize/test_serialize_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/serialize/test_serialize_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/serialize/test_serialize_vocab_strings.py` & `spacy-4.0.0.dev0/spacy/tests/serialize/test_serialize_vocab_strings.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/test_cli.py` & `spacy-4.0.0.dev0/spacy/tests/test_cli.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,22 +1,22 @@
 import os
 import math
 from collections import Counter
 from typing import Tuple, List, Dict, Any
+import pkg_resources
 import time
 from pathlib import Path
 
 import spacy
 import numpy
 import pytest
 import srsly
 from click import NoSuchOption
 from packaging.specifiers import SpecifierSet
 from thinc.api import Config, ConfigValidationError
-from spacy.tokens import DocBin
 
 from spacy import about
 from spacy.cli import info
 from spacy.cli._util import is_subpath_of, load_project_config, walk_directory
 from spacy.cli._util import parse_config_overrides, string_to_list
 from spacy.cli._util import substitute_project_variables
 from spacy.cli._util import validate_project_commands
@@ -24,17 +24,15 @@
 from spacy.cli.debug_data import _compile_gold, _get_labels_from_model
 from spacy.cli.debug_data import _get_labels_from_spancat
 from spacy.cli.debug_data import _get_distribution, _get_kl_divergence
 from spacy.cli.debug_data import _get_span_characteristics
 from spacy.cli.debug_data import _print_span_characteristics
 from spacy.cli.debug_data import _get_spans_length_freq_dist
 from spacy.cli.download import get_compatibility, get_version
-from spacy.cli.evaluate import render_parses
 from spacy.cli.init_config import RECOMMENDATIONS, init_config, fill_config
-from spacy.cli.init_pipeline import _init_labels
 from spacy.cli.package import get_third_party_dependencies
 from spacy.cli.package import _is_permitted_package_name
 from spacy.cli.project.remote_storage import RemoteStorage
 from spacy.cli.project.run import _check_requirements
 from spacy.cli.validate import get_model_pkgs
 from spacy.cli.apply import apply
 from spacy.cli.find_threshold import find_threshold
@@ -45,14 +43,15 @@
 from spacy.tokens import Doc, DocBin
 from spacy.tokens.span import Span
 from spacy.training import Example, docs_to_json, offsets_to_biluo_tags
 from spacy.training.converters import conll_ner_to_docs, conllu_to_docs
 from spacy.training.converters import iob_to_docs
 from spacy.util import ENV_VARS, get_minor_version, load_model_from_config, load_config
 
+from ..cli.init_pipeline import _init_labels
 from .util import make_tempdir
 
 
 @pytest.mark.issue(4665)
 def test_cli_converters_conllu_empty_heads_ner():
     """
     conllu_to_docs should not raise an exception if the HEAD column contains an
@@ -142,78 +141,14 @@
         cfg = load_project_config(d)
         # Check that the directories are interpolated and created correctly
         assert os.path.exists(d / "cfg")
         assert os.path.exists(d / f"{lang_var}_model")
     assert cfg["commands"][0]["script"][0] == f"hello {lang_var}"
 
 
-@pytest.mark.issue(12566)
-@pytest.mark.parametrize(
-    "factory,output_file",
-    [("deps", "parses.html"), ("ents", "entities.html"), ("spans", "spans.html")],
-)
-def test_issue12566(factory: str, output_file: str):
-    """
-    Test if all displaCy types (ents, dep, spans) produce an HTML file
-    """
-    with make_tempdir() as tmp_dir:
-        # Create sample spaCy file
-        doc_json = {
-            "ents": [
-                {"end": 54, "label": "nam_adj_country", "start": 44},
-                {"end": 83, "label": "nam_liv_person", "start": 69},
-                {"end": 100, "label": "nam_pro_title_book", "start": 86},
-            ],
-            "spans": {
-                "sc": [
-                    {"end": 54, "kb_id": "", "label": "nam_adj_country", "start": 44},
-                    {"end": 83, "kb_id": "", "label": "nam_liv_person", "start": 69},
-                    {
-                        "end": 100,
-                        "kb_id": "",
-                        "label": "nam_pro_title_book",
-                        "start": 86,
-                    },
-                ]
-            },
-            "text": "Niedawno czyta em now ksik znakomitego szkockiego medioznawcy , "
-            "Briana McNaira - Cultural Chaos .",
-            "tokens": [
-                # fmt: off
-                {"id": 0, "start": 0, "end": 8, "tag": "ADV", "pos": "ADV", "morph": "Degree=Pos", "lemma": "niedawno", "dep": "advmod", "head": 1, },
-                {"id": 1, "start": 9, "end": 15, "tag": "PRAET", "pos": "VERB", "morph": "Animacy=Hum|Aspect=Imp|Gender=Masc|Mood=Ind|Number=Sing|Tense=Past|VerbForm=Fin|Voice=Act", "lemma": "czyta", "dep": "ROOT", "head": 1, },
-                {"id": 2, "start": 16, "end": 18, "tag": "AGLT", "pos": "NOUN", "morph": "Animacy=Inan|Case=Ins|Gender=Masc|Number=Sing", "lemma": "em", "dep": "iobj", "head": 1, },
-                {"id": 3, "start": 19, "end": 23, "tag": "ADJ", "pos": "ADJ", "morph": "Case=Acc|Degree=Pos|Gender=Fem|Number=Sing", "lemma": "nowy", "dep": "amod", "head": 4, },
-                {"id": 4, "start": 24, "end": 31, "tag": "SUBST", "pos": "NOUN", "morph": "Case=Acc|Gender=Fem|Number=Sing", "lemma": "ksika", "dep": "obj", "head": 1, },
-                {"id": 5, "start": 32, "end": 43, "tag": "ADJ", "pos": "ADJ", "morph": "Animacy=Nhum|Case=Gen|Degree=Pos|Gender=Masc|Number=Sing", "lemma": "znakomit", "dep": "acl", "head": 4, },
-                {"id": 6, "start": 44, "end": 54, "tag": "ADJ", "pos": "ADJ", "morph": "Animacy=Hum|Case=Gen|Degree=Pos|Gender=Masc|Number=Sing", "lemma": "szkockiy", "dep": "amod", "head": 7, },
-                {"id": 7, "start": 55, "end": 66, "tag": "SUBST", "pos": "NOUN", "morph": "Animacy=Hum|Case=Gen|Gender=Masc|Number=Sing", "lemma": "medioznawca", "dep": "iobj", "head": 5, },
-                {"id": 8, "start": 67, "end": 68, "tag": "INTERP", "pos": "PUNCT", "morph": "PunctType=Comm", "lemma": ",", "dep": "punct", "head": 9, },
-                {"id": 9, "start": 69, "end": 75, "tag": "SUBST", "pos": "PROPN", "morph": "Animacy=Hum|Case=Gen|Gender=Masc|Number=Sing", "lemma": "Brian", "dep": "nmod", "head": 4, },
-                {"id": 10, "start": 76, "end": 83, "tag": "SUBST", "pos": "PROPN", "morph": "Animacy=Hum|Case=Gen|Gender=Masc|Number=Sing", "lemma": "McNair", "dep": "flat", "head": 9, },
-                {"id": 11, "start": 84, "end": 85, "tag": "INTERP", "pos": "PUNCT", "morph": "PunctType=Dash", "lemma": "-", "dep": "punct", "head": 12, },
-                {"id": 12, "start": 86, "end": 94, "tag": "SUBST", "pos": "PROPN", "morph": "Animacy=Inan|Case=Nom|Gender=Masc|Number=Sing", "lemma": "Cultural", "dep": "conj", "head": 4, },
-                {"id": 13, "start": 95, "end": 100, "tag": "SUBST", "pos": "NOUN", "morph": "Animacy=Inan|Case=Nom|Gender=Masc|Number=Sing", "lemma": "Chaos", "dep": "flat", "head": 12, },
-                {"id": 14, "start": 101, "end": 102, "tag": "INTERP", "pos": "PUNCT", "morph": "PunctType=Peri", "lemma": ".", "dep": "punct", "head": 1, },
-                # fmt: on
-            ],
-        }
-
-        # Create a .spacy file
-        nlp = spacy.blank("pl")
-        doc = Doc(nlp.vocab).from_json(doc_json)
-
-        # Run the evaluate command and check if the html files exist
-        render_parses(
-            docs=[doc], output_path=tmp_dir, model_name="", limit=1, **{factory: True}
-        )
-
-        assert (tmp_dir / output_file).is_file()
-
-
 def test_cli_info():
     nlp = Dutch()
     nlp.add_pipe("textcat")
     with make_tempdir() as tmp_dir:
         nlp.to_disk(tmp_dir)
         raw_data = info(tmp_dir, exclude=[""])
         assert raw_data["lang"] == "nl"
@@ -614,22 +549,15 @@
     with pytest.raises(SystemExit):
         parse_config_overrides([])
     del os.environ[ENV_VARS.CONFIG_OVERRIDES]
 
 
 @pytest.mark.parametrize("lang", ["en", "nl"])
 @pytest.mark.parametrize(
-    "pipeline",
-    [
-        ["tagger", "parser", "ner"],
-        [],
-        ["ner", "textcat", "sentencizer"],
-        ["morphologizer", "spancat", "entity_linker"],
-        ["spancat_singlelabel", "textcat_multilabel"],
-    ],
+    "pipeline", [["tagger", "parser", "ner"], [], ["ner", "textcat", "sentencizer"]]
 )
 @pytest.mark.parametrize("optimize", ["efficiency", "accuracy"])
 @pytest.mark.parametrize("pretraining", [True, False])
 def test_init_config(lang, pipeline, optimize, pretraining):
     # TODO: add more tests and also check for GPU with transformers
     config = init_config(
         lang=lang,
@@ -686,24 +614,26 @@
     ],
 )
 def test_string_to_list_intify(value):
     assert string_to_list(value, intify=False) == ["1", "2", "3"]
     assert string_to_list(value, intify=True) == [1, 2, 3]
 
 
+@pytest.mark.skip(reason="Temporarily skip for dev version")
 def test_download_compatibility():
     spec = SpecifierSet("==" + about.__version__)
     spec.prereleases = False
     if about.__version__ in spec:
         model_name = "en_core_web_sm"
         compatibility = get_compatibility()
         version = get_version(model_name, compatibility)
         assert get_minor_version(about.__version__) == get_minor_version(version)
 
 
+@pytest.mark.skip(reason="Temporarily skip for dev version")
 def test_validate_compatibility_table():
     spec = SpecifierSet("==" + about.__version__)
     spec.prereleases = False
     if about.__version__ in spec:
         model_pkgs, compat = get_model_pkgs()
         spacy_version = get_minor_version(about.__version__)
         current_compat = compat.get(spacy_version, {})
@@ -1085,14 +1015,16 @@
         filename = "a.txt"
         remote = RemoteStorage(d / "root", str(d / "remote"))
         assert remote.pull(filename, command_hash="aaaa") is None
         assert remote.pull(filename) is None
 
 
 def test_cli_find_threshold(capsys):
+    thresholds = numpy.linspace(0, 1, 10)
+
     def make_examples(nlp: Language) -> List[Example]:
         docs: List[Example] = []
 
         for t in [
             (
                 "I am angry and confused in the Bank of America.",
                 {
@@ -1140,39 +1072,43 @@
         # mostly as a smoke test.
         nlp, examples = init_nlp()
         DocBin(docs=[example.reference for example in examples]).to_disk(
             docs_dir / "docs.spacy"
         )
         with make_tempdir() as nlp_dir:
             nlp.to_disk(nlp_dir)
-            best_threshold, best_score, res = find_threshold(
+            res = find_threshold(
                 model=nlp_dir,
                 data_path=docs_dir / "docs.spacy",
                 pipe_name="tc_multi",
                 threshold_key="threshold",
                 scores_key="cats_macro_f",
                 silent=True,
             )
-            assert best_score == max(res.values())
-            assert res[1.0] == 0.0
+            assert res[0] != thresholds[0]
+            assert thresholds[0] < res[0] < thresholds[9]
+            assert res[1] == 1.0
+            assert res[2][1.0] == 0.0
 
         # Test with spancat.
         nlp, _ = init_nlp((("spancat", {}),))
         with make_tempdir() as nlp_dir:
             nlp.to_disk(nlp_dir)
-            best_threshold, best_score, res = find_threshold(
+            res = find_threshold(
                 model=nlp_dir,
                 data_path=docs_dir / "docs.spacy",
                 pipe_name="spancat",
                 threshold_key="threshold",
                 scores_key="spans_sc_f",
                 silent=True,
             )
-            assert best_score == max(res.values())
-            assert res[1.0] == 0.0
+            assert res[0] != thresholds[0]
+            assert thresholds[0] < res[0] < thresholds[8]
+            assert res[1] >= 0.6
+            assert res[2][1.0] == 0.0
 
         # Having multiple textcat_multilabel components should work, since the name has to be specified.
         nlp, _ = init_nlp((("textcat_multilabel", {}),))
         with make_tempdir() as nlp_dir:
             nlp.to_disk(nlp_dir)
             assert find_threshold(
                 model=nlp_dir,
@@ -1194,15 +1130,14 @@
                     pipe_name="_",
                     threshold_key="threshold",
                     scores_key="cats_macro_f",
                     silent=True,
                 )
 
 
-@pytest.mark.filterwarnings("ignore::DeprecationWarning")
 @pytest.mark.parametrize(
     "reqs,output",
     [
         [
             """
             spacy
 
@@ -1227,16 +1162,14 @@
             """# comment
              spacyunknowndoesnotexist12345""",
             (True, False),
         ],
     ],
 )
 def test_project_check_requirements(reqs, output):
-    import pkg_resources
-
     # excessive guard against unlikely package name
     try:
         pkg_resources.require("spacyunknowndoesnotexist12345")
     except pkg_resources.DistributionNotFound:
         assert output == _check_requirements([req.strip() for req in reqs.split("\n")])
 
 
@@ -1272,73 +1205,7 @@
 
         assert (len(walk_directory(d))) == 7
         assert (len(walk_directory(d, suffix=None))) == 7
         assert (len(walk_directory(d, suffix="json"))) == 1
         assert (len(walk_directory(d, suffix="iob"))) == 2
         assert (len(walk_directory(d, suffix="conll"))) == 3
         assert (len(walk_directory(d, suffix="pdf"))) == 0
-
-
-def test_debug_data_trainable_lemmatizer_basic():
-    examples = [
-        ("She likes green eggs", {"lemmas": ["she", "like", "green", "egg"]}),
-        ("Eat blue ham", {"lemmas": ["eat", "blue", "ham"]}),
-    ]
-    nlp = Language()
-    train_examples = []
-    for t in examples:
-        train_examples.append(Example.from_dict(nlp.make_doc(t[0]), t[1]))
-
-    data = _compile_gold(train_examples, ["trainable_lemmatizer"], nlp, True)
-    # ref test_edit_tree_lemmatizer::test_initialize_from_labels
-    # this results in 4 trees
-    assert len(data["lemmatizer_trees"]) == 4
-
-
-def test_debug_data_trainable_lemmatizer_partial():
-    partial_examples = [
-        # partial annotation
-        ("She likes green eggs", {"lemmas": ["", "like", "green", ""]}),
-        # misaligned partial annotation
-        (
-            "He hates green eggs",
-            {
-                "words": ["He", "hat", "es", "green", "eggs"],
-                "lemmas": ["", "hat", "e", "green", ""],
-            },
-        ),
-    ]
-    nlp = Language()
-    train_examples = []
-    for t in partial_examples:
-        train_examples.append(Example.from_dict(nlp.make_doc(t[0]), t[1]))
-
-    data = _compile_gold(train_examples, ["trainable_lemmatizer"], nlp, True)
-    assert data["partial_lemma_annotations"] == 2
-
-
-def test_debug_data_trainable_lemmatizer_low_cardinality():
-    low_cardinality_examples = [
-        ("She likes green eggs", {"lemmas": ["no", "no", "no", "no"]}),
-        ("Eat blue ham", {"lemmas": ["no", "no", "no"]}),
-    ]
-    nlp = Language()
-    train_examples = []
-    for t in low_cardinality_examples:
-        train_examples.append(Example.from_dict(nlp.make_doc(t[0]), t[1]))
-
-    data = _compile_gold(train_examples, ["trainable_lemmatizer"], nlp, True)
-    assert data["n_low_cardinality_lemmas"] == 2
-
-
-def test_debug_data_trainable_lemmatizer_not_annotated():
-    unannotated_examples = [
-        ("She likes green eggs", {}),
-        ("Eat blue ham", {}),
-    ]
-    nlp = Language()
-    train_examples = []
-    for t in unannotated_examples:
-        train_examples.append(Example.from_dict(nlp.make_doc(t[0]), t[1]))
-
-    data = _compile_gold(train_examples, ["trainable_lemmatizer"], nlp, True)
-    assert data["no_lemma_annotations"] == 2
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/test_displacy.py` & `spacy-4.0.0.dev0/spacy/tests/test_displacy.py`

 * *Files 4% similar despite different names*

```diff
@@ -271,28 +271,14 @@
         {"lemma": None, "text": words[3], "tag": pos[3]},
     ]
     assert deps["arcs"] == [
         {"start": 0, "end": 1, "label": "nsubj", "dir": "left"},
         {"start": 2, "end": 3, "label": "det", "dir": "left"},
         {"start": 1, "end": 3, "label": "attr", "dir": "right"},
     ]
-    # Test that displacy.parse_deps converts Span to Doc
-    deps = displacy.parse_deps(doc[:])
-    assert isinstance(deps, dict)
-    assert deps["words"] == [
-        {"lemma": None, "text": words[0], "tag": pos[0]},
-        {"lemma": None, "text": words[1], "tag": pos[1]},
-        {"lemma": None, "text": words[2], "tag": pos[2]},
-        {"lemma": None, "text": words[3], "tag": pos[3]},
-    ]
-    assert deps["arcs"] == [
-        {"start": 0, "end": 1, "label": "nsubj", "dir": "left"},
-        {"start": 2, "end": 3, "label": "det", "dir": "left"},
-        {"start": 1, "end": 3, "label": "attr", "dir": "right"},
-    ]
 
 
 def test_displacy_invalid_arcs():
     renderer = DependencyRenderer()
     words = [{"text": "This", "tag": "DET"}, {"text": "is", "tag": "VERB"}]
     arcs = [
         {"start": 0, "end": 1, "label": "nsubj", "dir": "left"},
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/test_language.py` & `spacy-4.0.0.dev0/spacy/tests/test_language.py`

 * *Files 0% similar despite different names*

```diff
@@ -42,15 +42,15 @@
     if not doc.has_annotation("SENT_START"):
         raise ValueError("no sents")
     return doc
 
 
 def warn_error(proc_name, proc, docs, e):
     logger = logging.getLogger("spacy")
-    logger.warning("Trouble with component %s.", proc_name)
+    logger.warning(f"Trouble with component {proc_name}.")
 
 
 @pytest.fixture
 def nlp():
     nlp = Language(Vocab())
     textcat = nlp.add_pipe("textcat")
     for label in ("POSITIVE", "NEGATIVE"):
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/test_misc.py` & `spacy-4.0.0.dev0/spacy/tests/test_misc.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,18 +1,16 @@
 import pytest
 import os
 import ctypes
 from pathlib import Path
 from spacy.about import __version__ as spacy_version
 from spacy import util
 from spacy import prefer_gpu, require_gpu, require_cpu
-from spacy.ml._precomputable_affine import PrecomputableAffine
-from spacy.ml._precomputable_affine import _backprop_precomputable_affine_padding
-from spacy.util import dot_to_object, SimpleFrozenList, import_file
-from spacy.util import to_ternary_int, find_available_port
+from spacy.util import dot_to_object, SimpleFrozenList, import_file, to_ternary_int
+from spacy.util import find_available_port
 from thinc.api import Config, Optimizer, ConfigValidationError
 from thinc.api import get_current_ops, set_current_ops, NumpyOps, CupyOps, MPSOps
 from thinc.compat import has_cupy_gpu, has_torch_mps_gpu
 from spacy.training.batchers import minibatch_by_words
 from spacy.lang.en import English
 from spacy.lang.nl import Dutch
 from spacy.language import DEFAULT_CONFIG_PATH
@@ -77,42 +75,14 @@
 @pytest.mark.parametrize("package", ["thinc"])
 def test_util_get_package_path(package):
     """Test that a Path object is returned for a package name."""
     path = util.get_package_path(package)
     assert isinstance(path, Path)
 
 
-def test_PrecomputableAffine(nO=4, nI=5, nF=3, nP=2):
-    model = PrecomputableAffine(nO=nO, nI=nI, nF=nF, nP=nP).initialize()
-    assert model.get_param("W").shape == (nF, nO, nP, nI)
-    tensor = model.ops.alloc((10, nI))
-    Y, get_dX = model.begin_update(tensor)
-    assert Y.shape == (tensor.shape[0] + 1, nF, nO, nP)
-    dY = model.ops.alloc((15, nO, nP))
-    ids = model.ops.alloc((15, nF))
-    ids[1, 2] = -1
-    dY[1] = 1
-    assert not model.has_grad("pad")
-    d_pad = _backprop_precomputable_affine_padding(model, dY, ids)
-    assert d_pad[0, 2, 0, 0] == 1.0
-    ids.fill(0.0)
-    dY.fill(0.0)
-    dY[0] = 0
-    ids[1, 2] = 0
-    ids[1, 1] = -1
-    ids[1, 0] = -1
-    dY[1] = 1
-    ids[2, 0] = -1
-    dY[2] = 5
-    d_pad = _backprop_precomputable_affine_padding(model, dY, ids)
-    assert d_pad[0, 0, 0, 0] == 6
-    assert d_pad[0, 1, 0, 0] == 1
-    assert d_pad[0, 2, 0, 0] == 0
-
-
 def test_prefer_gpu():
     current_ops = get_current_ops()
     if has_cupy_gpu:
         assert prefer_gpu()
         assert isinstance(get_current_ops(), CupyOps)
     elif has_torch_mps_gpu:
         assert prefer_gpu()
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/test_models.py` & `spacy-4.0.0.dev0/spacy/tests/test_models.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/test_pickles.py` & `spacy-4.0.0.dev0/spacy/tests/test_pickles.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/test_scorer.py` & `spacy-4.0.0.dev0/spacy/tests/test_scorer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/test_ty.py` & `spacy-4.0.0.dev0/spacy/tests/test_ty.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/tokenizer/sun.txt` & `spacy-4.0.0.dev0/spacy/tests/tokenizer/sun.txt`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/tokenizer/test_exceptions.py` & `spacy-4.0.0.dev0/spacy/tests/tokenizer/test_exceptions.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/tokenizer/test_explain.py` & `spacy-4.0.0.dev0/spacy/tests/tokenizer/test_explain.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/tokenizer/test_naughty_strings.py` & `spacy-4.0.0.dev0/spacy/tests/tokenizer/test_naughty_strings.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/tokenizer/test_tokenizer.py` & `spacy-4.0.0.dev0/spacy/tests/tokenizer/test_tokenizer.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/tokenizer/test_urls.py` & `spacy-4.0.0.dev0/spacy/tests/tokenizer/test_urls.py`

 * *Files 2% similar despite different names*

```diff
@@ -29,14 +29,17 @@
     "http://userid@example.com/",
     "http://userid@example.com:8080",
     "http://userid@example.com:8080/",
     "http://userid:password@example.com",
     "http://userid:password@example.com/",
     "http://142.42.1.1/",
     "http://142.42.1.1:8080/",
+    "http://10.140.12.13/foo",
+    "http://10.140.12.13/foo/bar?arg1=baz&arg2=taz",
+    "http://10.1.1.1",
     "http://foo.com/blah_(wikipedia)#cite-1",
     "http://foo.com/blah_(wikipedia)_blah#cite-1",
     "http://foo.com/unicode_()_in_parens",
     "http://foo.com/(something)?after=parens",
     "http://code.google.com/events/#&product=browser",
     "http://j.mp",
     "ftp://foo.bar/baz",
@@ -90,23 +93,23 @@
     "rdar://1234",
     "h://test",
     "http:// shouldfail.com",
     ":// should fail",
     "http://foo.bar/foo(bar)baz quux",
     "http://-error-.invalid/",
     "http://a.b-.co",
+    # Loopback and broadcast addresses should be excluded
     "http://0.0.0.0",
     "http://10.1.1.0",
     "http://10.1.1.255",
     "http://224.1.1.1",
     "http://123.123.123",
     "http://3628126748",
     "http://.www.foo.bar/",
     "http://.www.foo.bar./",
-    "http://10.1.1.1",
     "NASDAQ:GOOG",
     "http://-a.b.co",
     pytest.param("foo.com", marks=pytest.mark.xfail()),
     "http://1.1.1.1.1",
     "http://www.foo.bar./",
 ]
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/tokenizer/test_whitespace.py` & `spacy-4.0.0.dev0/spacy/tests/tokenizer/test_whitespace.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/training/test_augmenters.py` & `spacy-4.0.0.dev0/spacy/tests/training/test_augmenters.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/training/test_logger.py` & `spacy-4.0.0.dev0/spacy/tests/training/test_logger.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/training/test_new_example.py` & `spacy-4.0.0.dev0/spacy/tests/training/test_new_example.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/training/test_pretraining.py` & `spacy-4.0.0.dev0/spacy/tests/training/test_pretraining.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,24 +1,22 @@
 from pathlib import Path
 import numpy as np
 import pytest
 import srsly
-from thinc.api import Config, get_current_ops
-
-from spacy import util
-from spacy.lang.en import English
-from spacy.training.initialize import init_nlp
-from spacy.training.loop import train
-from spacy.training.pretrain import pretrain
-from spacy.tokens import Doc, DocBin
-from spacy.language import DEFAULT_CONFIG_PRETRAIN_PATH, DEFAULT_CONFIG_PATH
-from spacy.ml.models.multi_task import create_pretrain_vectors
-from spacy.vectors import Vectors
 from spacy.vocab import Vocab
+from thinc.api import Config
+
 from ..util import make_tempdir
+from ... import util
+from ...lang.en import English
+from ...training.initialize import init_nlp
+from ...training.loop import train
+from ...training.pretrain import pretrain
+from ...tokens import Doc, DocBin
+from ...language import DEFAULT_CONFIG_PRETRAIN_PATH, DEFAULT_CONFIG_PATH
 
 pretrain_string_listener = """
 [nlp]
 lang = "en"
 pipeline = ["tok2vec", "tagger"]
 
 [components]
@@ -161,36 +159,31 @@
     filled = nlp.config
     pretrain_config = util.load_config(DEFAULT_CONFIG_PRETRAIN_PATH)
     filled = pretrain_config.merge(filled)
     assert "PretrainCharacters" in filled["pretraining"]["objective"]["@architectures"]
 
 
 @pytest.mark.parametrize("objective", CHAR_OBJECTIVES)
-@pytest.mark.parametrize("skip_last", (True, False))
-def test_pretraining_tok2vec_characters(objective, skip_last):
+def test_pretraining_tok2vec_characters(objective):
     """Test that pretraining works with the character objective"""
     config = Config().from_str(pretrain_string_listener)
     config["pretraining"]["objective"] = objective
     nlp = util.load_model_from_config(config, auto_fill=True, validate=False)
     filled = nlp.config
     pretrain_config = util.load_config(DEFAULT_CONFIG_PRETRAIN_PATH)
     filled = pretrain_config.merge(filled)
     with make_tempdir() as tmp_dir:
         file_path = write_sample_jsonl(tmp_dir)
         filled["paths"]["raw_text"] = file_path
         filled = filled.interpolate()
         assert filled["pretraining"]["component"] == "tok2vec"
-        pretrain(filled, tmp_dir, skip_last=skip_last)
+        pretrain(filled, tmp_dir)
         assert Path(tmp_dir / "model0.bin").exists()
         assert Path(tmp_dir / "model4.bin").exists()
         assert not Path(tmp_dir / "model5.bin").exists()
-        if skip_last:
-            assert not Path(tmp_dir / "model-last.bin").exists()
-        else:
-            assert Path(tmp_dir / "model-last.bin").exists()
 
 
 @pytest.mark.parametrize("objective", VECTOR_OBJECTIVES)
 def test_pretraining_tok2vec_vectors_fail(objective):
     """Test that pretraining doesn't works with the vectors objective if there are no static vectors"""
     config = Config().from_str(pretrain_string_listener)
     config["pretraining"]["objective"] = objective
@@ -238,15 +231,14 @@
         filled["paths"]["raw_text"] = file_path
         filled["pretraining"]["component"] = "tagger"
         filled["pretraining"]["layer"] = "tok2vec"
         filled = filled.interpolate()
         pretrain(filled, tmp_dir)
         assert Path(tmp_dir / "model0.bin").exists()
         assert Path(tmp_dir / "model4.bin").exists()
-        assert Path(tmp_dir / "model-last.bin").exists()
         assert not Path(tmp_dir / "model5.bin").exists()
 
 
 def test_pretraining_tagger():
     """Test pretraining of the tagger itself will throw an error (not an appropriate tok2vec layer)"""
     config = Config().from_str(pretrain_string_internal)
     nlp = util.load_model_from_config(config, auto_fill=True, validate=False)
@@ -350,30 +342,7 @@
     }
     for word, vector in vector_data.items():
         vocab.set_vector(word, vector)
     nlp_path = tmp_dir / "vectors_model"
     nlp = English(vocab)
     nlp.to_disk(nlp_path)
     return str(nlp_path)
-
-
-def test_pretrain_default_vectors():
-    nlp = English()
-    nlp.add_pipe("tok2vec")
-    nlp.initialize()
-
-    # default vectors are supported
-    nlp.vocab.vectors = Vectors(shape=(10, 10))
-    create_pretrain_vectors(1, 1, "cosine")(nlp.vocab, nlp.get_pipe("tok2vec").model)
-
-    # floret vectors are supported
-    nlp.vocab.vectors = Vectors(
-        data=get_current_ops().xp.zeros((10, 10)), mode="floret", hash_count=1
-    )
-    create_pretrain_vectors(1, 1, "cosine")(nlp.vocab, nlp.get_pipe("tok2vec").model)
-
-    # error for no vectors
-    with pytest.raises(ValueError, match="E875"):
-        nlp.vocab.vectors = Vectors()
-        create_pretrain_vectors(1, 1, "cosine")(
-            nlp.vocab, nlp.get_pipe("tok2vec").model
-        )
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/training/test_readers.py` & `spacy-4.0.0.dev0/spacy/tests/training/test_readers.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/training/test_rehearse.py` & `spacy-4.0.0.dev0/spacy/tests/training/test_rehearse.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/training/test_training.py` & `spacy-4.0.0.dev0/spacy/tests/training/test_training.py`

 * *Files 1% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 import pytest
 import spacy
 import srsly
 from spacy.lang.en import English
 from spacy.tokens import Doc, DocBin
 from spacy.training import Alignment, Corpus, Example, biluo_tags_to_offsets
 from spacy.training import biluo_tags_to_spans, docs_to_json, iob_to_biluo
-from spacy.training import offsets_to_biluo_tags
+from spacy.training import offsets_to_biluo_tags, validate_distillation_examples
 from spacy.training.alignment_array import AlignmentArray
 from spacy.training.align import get_alignments
 from spacy.training.converters import json_to_docs
 from spacy.training.loop import train_while_improving
 from spacy.util import get_words_and_spaces, load_model_from_path, minibatch
 from spacy.util import load_config_from_str
 from thinc.api import compounding, Adam
@@ -361,14 +361,27 @@
     example = Example.from_dict(
         predicted, {"words": words, "entities": ["U-LOC", None, None, None]}
     )
     ner_tags = example.get_aligned_ner()
     assert ner_tags == ["U-LOC", None, None, None]
 
 
+def test_validate_distillation_examples(en_vocab):
+    words = ["a", "b", "c", "d"]
+    spaces = [True, True, False, True]
+    predicted = Doc(en_vocab, words=words, spaces=spaces)
+
+    example = Example.from_dict(predicted, {})
+    validate_distillation_examples([example], "test_validate_distillation_examples")
+
+    example = Example.from_dict(predicted, {"words": words + ["e"]})
+    with pytest.raises(ValueError, match=r"distillation"):
+        validate_distillation_examples([example], "test_validate_distillation_examples")
+
+
 @pytest.mark.filterwarnings("ignore::UserWarning")
 def test_json_to_docs_no_ner(en_vocab):
     data = [
         {
             "id": 1,
             "paragraphs": [
                 {
@@ -901,15 +914,17 @@
     ner.add_label("LOC")
     train_examples = []
     for t in train_data:
         train_examples.append(Example.from_dict(nlp.make_doc(t[0]), t[1]))
     optimizer = nlp.initialize()
     for i in range(5):
         losses = {}
-        batches = minibatch(train_examples, size=compounding(4.0, 32.0, 1.001))
+        batches = minibatch(
+            train_examples, size=compounding(4.0, 32.0, 1.001).to_generator()
+        )
         for batch in batches:
             nlp.update(batch, sgd=optimizer, losses=losses)
 
 
 def test_split_sents(merged_dict):
     nlp = English()
     example = Example.from_dict(
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/util.py` & `spacy-4.0.0.dev0/spacy/tests/util.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,11 +1,10 @@
 import numpy
 import tempfile
 import contextlib
-import re
 import srsly
 from spacy.tokens import Doc
 from spacy.vocab import Vocab
 from spacy.util import make_tempdir  # noqa: F401
 from spacy.training import split_bilu_label
 from thinc.api import get_current_ops
 
@@ -92,11 +91,7 @@
     """Assert that two packed msgpack messages are equal."""
     msg1 = srsly.msgpack_loads(b1)
     msg2 = srsly.msgpack_loads(b2)
     assert sorted(msg1.keys()) == sorted(msg2.keys())
     for (k1, v1), (k2, v2) in zip(sorted(msg1.items()), sorted(msg2.items())):
         assert k1 == k2
         assert v1 == v2
-
-
-def normalize_whitespace(s):
-    return re.sub(r"\s+", " ", s)
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/vocab_vectors/test_lexeme.py` & `spacy-4.0.0.dev0/spacy/tests/vocab_vectors/test_lexeme.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/vocab_vectors/test_lookups.py` & `spacy-4.0.0.dev0/spacy/tests/vocab_vectors/test_lookups.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/vocab_vectors/test_similarity.py` & `spacy-4.0.0.dev0/spacy/tests/vocab_vectors/test_similarity.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/vocab_vectors/test_stringstore.py` & `spacy-4.0.0.dev0/spacy/tests/vocab_vectors/test_stringstore.py`

 * *Files 12% similar despite different names*

```diff
@@ -20,50 +20,71 @@
     assert apple_hash == 8566208034543834098
     assert stringstore[apple_hash] == "apple"
     assert "apple" in stringstore
     assert "cherry" not in stringstore
     stringstore.add("orange")
     all_strings = [s for s in stringstore]
     assert all_strings == ["apple", "orange"]
+    assert all_strings == list(stringstore.keys())
+    all_strings_and_hashes = list(stringstore.items())
+    assert all_strings_and_hashes == [
+        ("apple", 8566208034543834098),
+        ("orange", 2208928596161743350),
+    ]
+    all_hashes = list(stringstore.values())
+    assert all_hashes == [8566208034543834098, 2208928596161743350]
     banana_hash = stringstore.add("banana")
     assert len(stringstore) == 3
     assert banana_hash == 2525716904149915114
     assert stringstore[banana_hash] == "banana"
     assert stringstore["banana"] == banana_hash
 
 
-@pytest.mark.parametrize("text1,text2,text3", [(b"Hello", b"goodbye", b"hello")])
-def test_stringstore_save_bytes(stringstore, text1, text2, text3):
-    key = stringstore.add(text1)
-    assert stringstore[text1] == key
-    assert stringstore[text2] != key
-    assert stringstore[text3] != key
+@pytest.mark.parametrize(
+    "val_bytes,val_float,val_list,val_text,val_hash",
+    [(b"Hello", 1.1, ["abc"], "apple", 8566208034543834098)],
+)
+def test_stringstore_type_checking(
+    stringstore, val_bytes, val_float, val_list, val_text, val_hash
+):
+    with pytest.raises(TypeError):
+        assert stringstore[val_bytes]
+
+    with pytest.raises(TypeError):
+        stringstore.add(val_float)
+
+    with pytest.raises(TypeError):
+        assert val_list not in stringstore
+
+    key = stringstore.add(val_text)
+    assert val_hash == key
+    assert stringstore[val_hash] == val_text
 
 
 @pytest.mark.parametrize("text1,text2,text3", [("Hello", "goodbye", "hello")])
 def test_stringstore_save_unicode(stringstore, text1, text2, text3):
     key = stringstore.add(text1)
     assert stringstore[text1] == key
     assert stringstore[text2] != key
     assert stringstore[text3] != key
 
 
-@pytest.mark.parametrize("text", [b"A"])
+@pytest.mark.parametrize("text", ["A"])
 def test_stringstore_retrieve_id(stringstore, text):
     key = stringstore.add(text)
     assert len(stringstore) == 1
-    assert stringstore[key] == text.decode("utf8")
+    assert stringstore[key] == text
     with pytest.raises(KeyError):
         stringstore[20000]
 
 
-@pytest.mark.parametrize("text1,text2", [(b"0123456789", b"A")])
+@pytest.mark.parametrize("text1,text2", [("0123456789", "A")])
 def test_stringstore_med_string(stringstore, text1, text2):
     store = stringstore.add(text1)
-    assert stringstore[store] == text1.decode("utf8")
+    assert stringstore[store] == text1
     stringstore.add(text2)
     assert stringstore[text1] == store
 
 
 def test_stringstore_long_string(stringstore):
     text = "INFORMATIVE](http://www.google.com/search?as_q=RedditMonkey&amp;hl=en&amp;num=50&amp;btnG=Google+Search&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;lr=&amp;as_ft=i&amp;as_filetype=&amp;as_qdr=all&amp;as_nlo=&amp;as_nhi=&amp;as_occt=any&amp;as_dt=i&amp;as_sitesearch=&amp;as_rights=&amp;safe=off"
     store = stringstore.add(text)
```

### Comparing `spacy-3.6.0.dev0/spacy/tests/vocab_vectors/test_vectors.py` & `spacy-4.0.0.dev0/spacy/tests/vocab_vectors/test_vectors.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tests/vocab_vectors/test_vocab_api.py` & `spacy-4.0.0.dev0/spacy/tests/vocab_vectors/test_vocab_api.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tokenizer.pxd` & `spacy-4.0.0.dev0/spacy/tokenizer.pxd`

 * *Files 16% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 from libcpp.vector cimport vector
 from preshed.maps cimport PreshMap
 from cymem.cymem cimport Pool
 
 from .typedefs cimport hash_t
 from .structs cimport LexemeC, SpanC, TokenC
-from .strings cimport StringStore
 from .tokens.doc cimport Doc
 from .vocab cimport Vocab, LexemesOrTokens, _Cached
 from .matcher.phrasematcher cimport PhraseMatcher
 
 
 cdef class Tokenizer:
     cdef Pool mem
@@ -19,19 +18,15 @@
     cdef object _token_match
     cdef object _url_match
     cdef object _prefix_search
     cdef object _suffix_search
     cdef object _infix_finditer
     cdef object _rules
     cdef PhraseMatcher _special_matcher
-    # TODO convert to bool in v4
-    cdef int _faster_heuristics
-    # TODO next one is unused and should be removed in v4
-    # https://github.com/explosion/spaCy/pull/9150
-    cdef int _unused_int2
+    cdef bint _faster_heuristics
 
     cdef Doc _tokenize_affixes(self, str string, bint with_special_cases)
     cdef int _apply_special_cases(self, Doc doc) except -1
     cdef void _filter_special_spans(self, vector[SpanC] &original,
                             vector[SpanC] &filtered, int doc_len) nogil
     cdef object _prepare_special_spans(self, Doc doc,
                                        vector[SpanC] &filtered)
```

### Comparing `spacy-3.6.0.dev0/spacy/tokenizer.pyx` & `spacy-4.0.0.dev0/spacy/tokenizer.pyx`

 * *Files 2% similar despite different names*

```diff
@@ -4,25 +4,24 @@
 from libc.string cimport memcpy, memset
 from libcpp.set cimport set as stdset
 from cymem.cymem cimport Pool
 from preshed.maps cimport PreshMap
 cimport cython
 
 import re
-import warnings
 
 from .tokens.doc cimport Doc
 from .strings cimport hash_string
 from .lexeme cimport EMPTY_LEXEME
 
 from .attrs import intify_attrs
 from .symbols import ORTH, NORM
-from .errors import Errors, Warnings
+from .errors import Errors
 from . import util
-from .util import registry, get_words_and_spaces
+from .util import get_words_and_spaces
 from .attrs import intify_attrs
 from .symbols import ORTH
 from .scorer import Scorer
 from .training import validate_examples
 from .tokens import Span
 
 
@@ -124,18 +123,18 @@
             self._flush_specials()
             self._cache = PreshMap()
             self._specials = PreshMap()
             self._load_special_cases(rules)
 
     property faster_heuristics:
         def __get__(self):
-            return bool(self._faster_heuristics)
+            return self._faster_heuristics
 
         def __set__(self, faster_heuristics):
-            self._faster_heuristics = bool(faster_heuristics)
+            self._faster_heuristics = faster_heuristics
             self._reload_special_cases()
 
     def __reduce__(self):
         args = (self.vocab,
                 self.rules,
                 self.prefix_search,
                 self.suffix_search,
@@ -578,15 +577,15 @@
         additional features beyond `ORTH` and `NORM` are not set by the
         exception.
 
         chunk (str): The string to specially tokenize.
         substrings (iterable): A sequence of dicts, where each dict describes
             a token and its attributes.
         """
-        attrs = [intify_attrs(spec, _do_deprecated=True) for spec in substrings]
+        attrs = [intify_attrs(spec) for spec in substrings]
         orth = "".join([spec[ORTH] for spec in attrs])
         if chunk != orth:
             raise ValueError(Errors.E997.format(chunk=chunk, orth=orth, token_attrs=substrings))
         for substring in attrs:
             for attr in substring:
                 if attr not in (ORTH, NORM):
                     raise ValueError(Errors.E1005.format(attr=self.vocab.strings[attr], chunk=chunk))
@@ -611,15 +610,15 @@
         stale_special = <_Cached*>self._specials.get(key)
         self._specials.set(key, cached)
         if stale_special is not NULL:
             self.mem.free(stale_special)
         self._rules[string] = substrings
         self._flush_cache()
         if not self.faster_heuristics or self.find_prefix(string) or self.find_infix(string) or self.find_suffix(string) or " " in string:
-            self._special_matcher.add(string, None, self._tokenize_affixes(string, False))
+            self._special_matcher.add(string, [self._tokenize_affixes(string, False)])
 
     def _reload_special_cases(self):
         self._flush_cache()
         self._flush_specials()
         self._load_special_cases(self._rules)
 
     def explain(self, text):
@@ -646,15 +645,15 @@
         if token_match is None:
             token_match = re.compile("a^").match
         url_match = self.url_match
         if url_match is None:
             url_match = re.compile("a^").match
         special_cases = {}
         for orth, special_tokens in self.rules.items():
-            special_cases[orth] = [intify_attrs(special_token, strings_map=self.vocab.strings, _do_deprecated=True) for special_token in special_tokens]
+            special_cases[orth] = [intify_attrs(special_token, strings_map=self.vocab.strings) for special_token in special_tokens]
         tokens = []
         for substring in text.split():
             suffixes = []
             while substring:
                 if substring in special_cases:
                     tokens.extend(("SPECIAL-" + str(i + 1), self.vocab.strings[e[ORTH]]) for i, e in enumerate(special_cases[substring]))
                     substring = ''
@@ -830,20 +829,18 @@
             self.suffix_search = re.compile(data["suffix_search"]).search
         if "infix_finditer" in data and isinstance(data["infix_finditer"], str):
             self.infix_finditer = re.compile(data["infix_finditer"]).finditer
         if "token_match" in data and isinstance(data["token_match"], str):
             self.token_match = re.compile(data["token_match"]).match
         if "url_match" in data and isinstance(data["url_match"], str):
             self.url_match = re.compile(data["url_match"]).match
-        if "faster_heuristics" in data:
-            self.faster_heuristics = data["faster_heuristics"]
-        # always load rules last so that all other settings are set before the
-        # internal tokenization for the phrase matcher
         if "rules" in data and isinstance(data["rules"], dict):
             self.rules = data["rules"]
+        if "faster_heuristics" in data:
+            self.faster_heuristics = data["faster_heuristics"]
         return self
 
 
 def _get_regex_pattern(regex):
     """Get a pattern string for a regex, or None if the pattern is None."""
     return None if regex is None else regex.__self__.pattern
```

### Comparing `spacy-3.6.0.dev0/spacy/tokens/_dict_proxies.py` & `spacy-4.0.0.dev0/spacy/tokens/span_groups.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tokens/_retokenize.pyi` & `spacy-4.0.0.dev0/spacy/tokens/retokenizer.pyi`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tokens/_retokenize.pyx` & `spacy-4.0.0.dev0/spacy/tokens/retokenizer.pyx`

 * *Files 0% similar despite different names*

```diff
@@ -14,15 +14,15 @@
 from ..attrs cimport MORPH, NORM
 from ..vocab cimport Vocab
 
 from .underscore import is_writable_attr
 from ..attrs import intify_attrs
 from ..util import SimpleFrozenDict
 from ..errors import Errors
-from ..strings import get_string_id
+from ..strings cimport get_string_id
 
 
 cdef class Retokenizer:
     """Helper class for doc.retokenize() context manager.
 
     DOCS: https://spacy.io/api/doc#retokenize
     USAGE: https://spacy.io/usage/linguistic-features#retokenization
```

### Comparing `spacy-3.6.0.dev0/spacy/tokens/_serialize.py` & `spacy-4.0.0.dev0/spacy/tokens/doc_bin.py`

 * *Files 4% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 
 from .doc import Doc
 from ..vocab import Vocab
 from ..compat import copy_reg
 from ..attrs import SPACY, ORTH, intify_attr, IDS
 from ..errors import Errors
 from ..util import ensure_path, SimpleFrozenList
-from ._dict_proxies import SpanGroups
+from .span_groups import SpanGroups
 
 # fmt: off
 ALL_ATTRS = ("ORTH", "NORM", "TAG", "HEAD", "DEP", "ENT_IOB", "ENT_TYPE", "ENT_KB_ID", "ENT_ID", "LEMMA", "MORPH", "POS", "SENT_START")
 # fmt: on
 
 
 class DocBin:
@@ -120,18 +120,14 @@
         self.cats.append(doc.cats)
         if self.store_user_data:
             self.user_data.append(srsly.msgpack_dumps(doc.user_data))
         self.span_groups.append(doc.spans.to_bytes())
         for key, group in doc.spans.items():
             for span in group:
                 self.strings.add(span.label_)
-                if span.kb_id in span.doc.vocab.strings:
-                    self.strings.add(span.kb_id_)
-                if span.id in span.doc.vocab.strings:
-                    self.strings.add(span.id_)
 
     def get_docs(self, vocab: Vocab) -> Iterator[Doc]:
         """Recover Doc objects from the annotations, using the given vocab.
         Note that the user data of each doc will be read (if available) and returned,
         regardless of the setting of 'self.store_user_data'.
 
         vocab (Vocab): The shared vocab.
```

### Comparing `spacy-3.6.0.dev0/spacy/tokens/doc.pxd` & `spacy-4.0.0.dev0/spacy/tokens/doc.pxd`

 * *Files 1% similar despite different names*

```diff
@@ -44,15 +44,15 @@
     cdef public object tensor
     cdef public object cats
     cdef public object user_data
     cdef readonly object spans
 
     cdef TokenC* c
 
-    cdef public float sentiment
+    cdef public dict activations
 
     cdef public dict user_hooks
     cdef public dict user_token_hooks
     cdef public dict user_span_hooks
 
     cdef public bint has_unknown_spaces
```

### Comparing `spacy-3.6.0.dev0/spacy/tokens/doc.pyi` & `spacy-4.0.0.dev0/spacy/tokens/doc.pyi`

 * *Files 7% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 from typing import Callable, Protocol, Iterable, Iterator, Optional
 from typing import Union, Tuple, List, Dict, Any, overload
 from cymem.cymem import Pool
-from thinc.types import Floats1d, Floats2d, Ints2d
+from thinc.types import ArrayXd, Floats1d, Floats2d, Ints2d, Ragged
 from .span import Span
 from .token import Token
-from ._dict_proxies import SpanGroups
-from ._retokenize import Retokenizer
+from .span_groups import SpanGroups
+from .retokenizer import Retokenizer
 from ..lexeme import Lexeme
 from ..vocab import Vocab
 from .underscore import Underscore
 from pathlib import Path
 import numpy as np
 
 class DocMethod(Protocol):
@@ -17,15 +17,15 @@
 
 class Doc:
     vocab: Vocab
     mem: Pool
     spans: SpanGroups
     max_length: int
     length: int
-    sentiment: float
+    activations: Dict[str, Dict[str, Union[ArrayXd, Ragged]]]
     cats: Dict[str, float]
     user_hooks: Dict[str, Callable[..., Any]]
     user_token_hooks: Dict[str, Callable[..., Any]]
     user_span_hooks: Dict[str, Callable[..., Any]]
     tensor: np.ndarray[Any, np.dtype[np.float_]]
     user_data: Dict[str, Any]
     has_unknown_spaces: bool
@@ -104,15 +104,14 @@
         self,
         start_idx: int,
         end_idx: int,
         label: Union[int, str] = ...,
         kb_id: Union[int, str] = ...,
         vector: Optional[Floats1d] = ...,
         alignment_mode: str = ...,
-        span_id: Union[int, str] = ...,
     ) -> Span: ...
     def similarity(self, other: Union[Doc, Span, Token, Lexeme]) -> float: ...
     @property
     def has_vector(self) -> bool: ...
     vector: Floats1d
     vector_norm: float
     @property
```

### Comparing `spacy-3.6.0.dev0/spacy/tokens/doc.pyx` & `spacy-4.0.0.dev0/spacy/tokens/doc.pyx`

 * *Files 1% similar despite different names*

```diff
@@ -15,15 +15,15 @@
 import srsly
 from thinc.api import get_array_module, get_current_ops
 from thinc.util import copy_array
 import warnings
 
 from .span cimport Span
 from .token cimport MISSING_DEP
-from ._dict_proxies import SpanGroups
+from .span_groups import SpanGroups
 from .token cimport Token
 from ..lexeme cimport Lexeme, EMPTY_LEXEME
 from ..typedefs cimport attr_t, flags_t
 from ..attrs cimport attr_id_t
 from ..attrs cimport LENGTH, POS, LEMMA, TAG, MORPH, DEP, HEAD, SPACY, ENT_IOB
 from ..attrs cimport ENT_TYPE, ENT_ID, ENT_KB_ID, SENT_START, IDX, NORM
 
@@ -31,16 +31,16 @@
 from ..compat import copy_reg, pickle
 from ..errors import Errors, Warnings
 from ..morphology import Morphology
 from .. import util
 from .. import parts_of_speech
 from .. import schemas
 from .underscore import Underscore, get_ext_args
-from ._retokenize import Retokenizer
-from ._serialize import ALL_ATTRS as DOCBIN_ALL_ATTRS
+from .retokenizer import Retokenizer
+from .doc_bin import ALL_ATTRS as DOCBIN_ALL_ATTRS
 from ..util import get_words_and_spaces
 
 DEF PADDING = 5
 
 
 cdef int bounds_check(int i, int length, int padding) except -1:
     if (i + padding) < 0:
@@ -239,16 +239,16 @@
         for i in range(size + (PADDING*2)):
             data_start[i].lex = &EMPTY_LEXEME
             data_start[i].l_edge = i
             data_start[i].r_edge = i
         self.c = data_start + PADDING
         self.max_length = size
         self.length = 0
-        self.sentiment = 0.0
         self.cats = {}
+        self.activations = {}
         self.user_hooks = {}
         self.user_token_hooks = {}
         self.user_span_hooks = {}
         self.tensor = numpy.zeros((0,), dtype="float32")
         self.user_data = {} if user_data is None else user_data
         self._vector = None
         self.noun_chunks_iterator = self.vocab.get_noun_chunks
@@ -524,30 +524,33 @@
         """Create a `Span` object from the slice
         `doc.text[start_idx : end_idx]`. Returns None if no valid `Span` can be
         created.
 
         doc (Doc): The parent document.
         start_idx (int): The index of the first character of the span.
         end_idx (int): The index of the first character after the span.
-        label (Union[int, str]): A label to attach to the Span, e.g. for
+        label (uint64 or string): A label to attach to the Span, e.g. for
             named entities.
-        kb_id (Union[int, str]):  An ID from a KB to capture the meaning of a
+        kb_id (uint64 or string):  An ID from a KB to capture the meaning of a
             named entity.
         vector (ndarray[ndim=1, dtype='float32']): A meaning representation of
             the span.
         alignment_mode (str): How character indices are aligned to token
             boundaries. Options: "strict" (character indices must be aligned
             with token boundaries), "contract" (span of all tokens completely
             within the character span), "expand" (span of all tokens at least
             partially covered by the character span). Defaults to "strict".
-        span_id (Union[int, str]): An identifier to associate with the span.
         RETURNS (Span): The newly constructed object.
 
         DOCS: https://spacy.io/api/doc#char_span
         """
+        if not isinstance(label, int):
+            label = self.vocab.strings.add(label)
+        if not isinstance(kb_id, int):
+            kb_id = self.vocab.strings.add(kb_id)
         alignment_modes = ("strict", "contract", "expand")
         if alignment_mode not in alignment_modes:
             raise ValueError(
                 Errors.E202.format(
                     name="alignment",
                     mode=alignment_mode,
                     modes=", ".join(alignment_modes),
@@ -802,35 +805,41 @@
             for i in range(span.start, span.end):
                 if i == span.start:
                     self.c[i].ent_iob = 3
                 else:
                     self.c[i].ent_iob = 1
                 self.c[i].ent_type = span.label
                 self.c[i].ent_kb_id = span.kb_id
-                # for backwards compatibility in v3, only set ent_id from
-                # span.id if it's set, otherwise don't override
-                self.c[i].ent_id = span.id if span.id else self.c[i].ent_id
+                self.c[i].ent_id = span.id
         for span in blocked:
             for i in range(span.start, span.end):
                 self.c[i].ent_iob = 3
                 self.c[i].ent_type = 0
+                self.c[i].ent_kb_id = 0
+                self.c[i].ent_id = 0
         for span in missing:
             for i in range(span.start, span.end):
                 self.c[i].ent_iob = 0
                 self.c[i].ent_type = 0
+                self.c[i].ent_kb_id = 0
+                self.c[i].ent_id = 0
         for span in outside:
             for i in range(span.start, span.end):
                 self.c[i].ent_iob = 2
                 self.c[i].ent_type = 0
+                self.c[i].ent_kb_id = 0
+                self.c[i].ent_id = 0
 
         # Set tokens outside of all provided spans
         if default != SetEntsDefault.unmodified:
             for i in range(self.length):
                 if i not in seen_tokens:
                     self.c[i].ent_type = 0
+                    self.c[i].ent_kb_id = 0
+                    self.c[i].ent_id = 0
                     if default == SetEntsDefault.outside:
                         self.c[i].ent_iob = 2
                     elif default == SetEntsDefault.missing:
                         self.c[i].ent_iob = 0
                     elif default == SetEntsDefault.blocked:
                         self.c[i].ent_iob = 3
 
@@ -962,30 +971,34 @@
             # Handle inputs like doc.to_array(ORTH)
             py_attr_ids = [py_attr_ids]
         # Allow strings, e.g. 'lemma' or 'LEMMA'
         try:
             py_attr_ids = [(IDS[id_.upper()] if hasattr(id_, "upper") else id_)
                        for id_ in py_attr_ids]
         except KeyError as msg:
-            keys = [k for k in IDS.keys() if not k.startswith("FLAG")]
+            keys = list(IDS.keys())
             raise KeyError(Errors.E983.format(dict="IDS", key=msg, keys=keys)) from None
         # Make an array from the attributes --- otherwise our inner loop is
         # Python dict iteration.
-        cdef np.ndarray attr_ids = numpy.asarray(py_attr_ids, dtype="i")
-        output = numpy.ndarray(shape=(self.length, len(attr_ids)), dtype=numpy.uint64)
+        cdef Pool mem = Pool()
+        cdef int n_attrs = len(py_attr_ids)
+        cdef attr_id_t* c_attr_ids
+        if n_attrs > 0:
+            c_attr_ids = <attr_id_t*>mem.alloc(n_attrs, sizeof(attr_id_t))
+            for i, attr_id in enumerate(py_attr_ids):
+                c_attr_ids[i] = attr_id
+        output = numpy.ndarray(shape=(self.length, n_attrs), dtype=numpy.uint64)
         c_output = <attr_t*>output.data
-        c_attr_ids = <attr_id_t*>attr_ids.data
         cdef TokenC* token
-        cdef int nr_attr = attr_ids.shape[0]
         for i in range(self.length):
             token = &self.c[i]
-            for j in range(nr_attr):
-                c_output[i*nr_attr + j] = get_token_attr(token, c_attr_ids[j])
+            for j in range(n_attrs):
+                c_output[i*n_attrs + j] = get_token_attr(token, c_attr_ids[j])
         # Handle 1d case
-        return output if len(attr_ids) >= 2 else output.reshape((self.length,))
+        return output if n_attrs >= 2 else output.reshape((self.length,))
 
     def count_by(self, attr_id_t attr_id, exclude=None, object counts=None):
         """Count the frequencies of a given attribute. Produces a dict of
         `{attribute (int): count (ints)}` frequencies, keyed by the values of
         the given attribute ID.
 
         attr_id (int): The attribute ID to key the counts.
@@ -1161,21 +1174,30 @@
         char_offset = 0
         for doc in docs:
             concat_words.extend(t.text for t in doc)
             concat_spaces.extend(bool(t.whitespace_) for t in doc)
 
             if "user_data" not in exclude:
                 for key, value in doc.user_data.items():
-                    if isinstance(key, tuple) and len(key) == 4 and key[0] == "._.":
-                        data_type, name, start, end = key
+                    if isinstance(key, tuple) and len(key) >= 4 and key[0] == "._.":
+                        data_type = key[0]
+                        name = key[1]
+                        start = key[2]
+                        end = key[3]
                         if start is not None or end is not None:
                             start += char_offset
                             if end is not None:
                                 end += char_offset
-                            concat_user_data[(data_type, name, start, end)] = copy.copy(value)
+                                _label = key[4]
+                                _kb_id = key[5]
+                                _span_id = key[6]
+                                concat_user_data[(data_type, name, start, end, _label, _kb_id, _span_id)] = copy.copy(value)
+                            else:
+                                concat_user_data[(data_type, name, start, end)] = copy.copy(value)
+
                         else:
                             warnings.warn(Warnings.W101.format(name=name))
                     else:
                         warnings.warn(Warnings.W102.format(key=key, value=value))
             if "spans" not in exclude:
                 for key in doc.spans:
                     # if a spans key is in any doc, include it in the merged doc
@@ -1253,15 +1275,14 @@
     def copy(self):
         cdef Doc other = Doc(self.vocab)
         other._vector = copy.deepcopy(self._vector)
         other._vector_norm = copy.deepcopy(self._vector_norm)
         other.tensor = copy.deepcopy(self.tensor)
         other.cats = copy.deepcopy(self.cats)
         other.user_data = copy.deepcopy(self.user_data)
-        other.sentiment = self.sentiment
         other.has_unknown_spaces = self.has_unknown_spaces
         other.user_hooks = dict(self.user_hooks)
         other.user_token_hooks = dict(self.user_token_hooks)
         other.user_span_hooks = dict(self.user_span_hooks)
         other.length = self.length
         other.max_length = self.max_length
         other.spans = self.spans.copy(doc=other)
@@ -1342,27 +1363,22 @@
             strings.add(token.ent_type_)
             strings.add(token.ent_kb_id_)
             strings.add(token.ent_id_)
             strings.add(token.norm_)
         for group in self.spans.values():
             for span in group:
                 strings.add(span.label_)
-                if span.kb_id in span.doc.vocab.strings:
-                    strings.add(span.kb_id_)
-                if span.id in span.doc.vocab.strings:
-                    strings.add(span.id_)
         # Msgpack doesn't distinguish between lists and tuples, which is
         # vexing for user data. As a best guess, we *know* that within
         # keys, we must have tuples. In values we just have to hope
         # users don't mind getting a list instead of a tuple.
         serializers = {
             "text": lambda: self.text,
             "array_head": lambda: array_head,
             "array_body": lambda: self.to_array(array_head),
-            "sentiment": lambda: self.sentiment,
             "tensor": lambda: self.tensor,
             "cats": lambda: self.cats,
             "spans": lambda: self.spans.to_bytes(),
             "strings": lambda: list(strings),
             "has_unknown_spaces": lambda: self.has_unknown_spaces
         }
         if "user_data" not in exclude and self.user_data:
@@ -1392,16 +1408,14 @@
         # users don't mind getting a list instead of a tuple.
         if "user_data" not in exclude and "user_data_keys" in msg:
             user_data_keys = srsly.msgpack_loads(msg["user_data_keys"], use_list=False)
             user_data_values = srsly.msgpack_loads(msg["user_data_values"])
             for key, value in zip(user_data_keys, user_data_values):
                 self.user_data[key] = value
         cdef int i, start, end, has_space
-        if "sentiment" not in exclude and "sentiment" in msg:
-            self.sentiment = msg["sentiment"]
         if "tensor" not in exclude and "tensor" in msg:
             self.tensor = msg["tensor"]
         if "cats" not in exclude and "cats" in msg:
             self.cats = msg["cats"]
         if "strings" not in exclude and "strings" in msg:
             for s in msg["strings"]:
                 self.vocab.strings.add(s)
@@ -1620,15 +1634,19 @@
                 self[start]._.set(token_attr, value)
                 
         for span_attr in doc_json.get("underscore_span", {}):
             if not Span.has_extension(span_attr):
                 Span.set_extension(span_attr)
             for span_data in doc_json["underscore_span"][span_attr]:
                 value = span_data["value"]
-                self.char_span(span_data["start"], span_data["end"])._.set(span_attr, value)
+                span = self.char_span(span_data["start"], span_data["end"])
+                span.label = span_data["label"]
+                span.kb_id = span_data["kb_id"]
+                span.id = span_data["id"]
+                span._.set(span_attr, value)
         return self
 
     def to_json(self, underscore=None):
         """Convert a Doc to JSON.
 
         underscore (list): Optional list of string names of custom doc._.
         attributes. Attribute values need to be JSON-serializable. Values will
@@ -1698,21 +1716,24 @@
                             # Token attribute
                             if start is not None and end is None:
                                 if "underscore_token" not in data:
                                     data["underscore_token"] = {}
                                 if attr not in data["underscore_token"]:
                                     data["underscore_token"][attr] = []
                                 data["underscore_token"][attr].append({"start": start, "value": value})
-                            # Span attribute
-                            elif start is not None and end is not None:
+                            # Else span attribute
+                            elif end is not None:
+                                _label = data_key[4]
+                                _kb_id = data_key[5]
+                                _span_id = data_key[6]
                                 if "underscore_span" not in data:
                                     data["underscore_span"] = {}
                                 if attr not in data["underscore_span"]:
                                     data["underscore_span"][attr] = []
-                                data["underscore_span"][attr].append({"start": start, "end": end, "value": value})
+                                data["underscore_span"][attr].append({"start": start, "end": end, "value": value, "label": _label, "kb_id": _kb_id, "id":_span_id})
 
             for attr in underscore:
                 if attr not in user_keys:
                     raise ValueError(Errors.E106.format(attr=attr, opts=underscore))
         return data
 
     def to_utf8_array(self, int nr_char=-1):
```

### Comparing `spacy-3.6.0.dev0/spacy/tokens/graph.pyx` & `spacy-4.0.0.dev0/spacy/tokens/graph.pyx`

 * *Files 0% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 cimport cython
 import weakref
 from preshed.maps cimport map_get_unless_missing
 from murmurhash.mrmr cimport hash64
 
 from .. import Errors
 from ..typedefs cimport hash_t
-from ..strings import get_string_id
+from ..strings cimport get_string_id
 from ..structs cimport EdgeC, GraphC
 from .token import Token
 
 
 @cython.freelist(8)
 cdef class Edge:
     cdef readonly Graph graph
```

### Comparing `spacy-3.6.0.dev0/spacy/tokens/morphanalysis.pyi` & `spacy-4.0.0.dev0/spacy/tokens/morphanalysis.pyi`

 * *Files 7% similar despite different names*

```diff
@@ -1,20 +1,20 @@
-from typing import Any, Dict, Iterator, List, Optional, Union
+from typing import Any, Dict, Iterator, List, Union
 from ..vocab import Vocab
 
 class MorphAnalysis:
     def __init__(
         self, vocab: Vocab, features: Union[Dict[str, str], str] = ...
     ) -> None: ...
     @classmethod
     def from_id(cls, vocab: Vocab, key: Any) -> MorphAnalysis: ...
     def __contains__(self, feature: str) -> bool: ...
     def __iter__(self) -> Iterator[str]: ...
     def __len__(self) -> int: ...
     def __hash__(self) -> int: ...
     def __eq__(self, other: MorphAnalysis) -> bool: ...  # type: ignore[override]
     def __ne__(self, other: MorphAnalysis) -> bool: ...  # type: ignore[override]
-    def get(self, field: Any, default: Optional[List[str]]) -> List[str]: ...
+    def get(self, field: Any) -> List[str]: ...
     def to_json(self) -> str: ...
     def to_dict(self) -> Dict[str, str]: ...
     def __str__(self) -> str: ...
     def __repr__(self) -> str: ...
```

### Comparing `spacy-3.6.0.dev0/spacy/tokens/morphanalysis.pyx` & `spacy-4.0.0.dev0/spacy/tokens/morphanalysis.pyx`

 * *Files 18% similar despite different names*

```diff
@@ -1,83 +1,83 @@
 from libc.string cimport memset
 cimport numpy as np
 
 from ..errors import Errors
 from ..morphology import Morphology
 from ..vocab cimport Vocab
 from ..typedefs cimport hash_t, attr_t
-from ..morphology cimport list_features, check_feature, get_by_field
+from ..morphology cimport list_features, check_feature, get_by_field, MorphAnalysisC
+from libcpp.memory cimport shared_ptr
+from cython.operator cimport dereference as deref
+
+
+cdef shared_ptr[MorphAnalysisC] EMPTY_MORPH_TAG = shared_ptr[MorphAnalysisC](new MorphAnalysisC())
 
 
 cdef class MorphAnalysis:
     """Control access to morphological features for a token."""
     def __init__(self, Vocab vocab, features=dict()):
         self.vocab = vocab
         self.key = self.vocab.morphology.add(features)
-        analysis = <const MorphAnalysisC*>self.vocab.morphology.tags.get(self.key)
-        if analysis is not NULL:
-            self.c = analysis[0]
+        self._init_c(self.key)
+
+    cdef void _init_c(self, hash_t key):
+        cdef shared_ptr[MorphAnalysisC] analysis = self.vocab.morphology.get_morph_c(key)
+        if analysis:
+            self.c = analysis
         else:
-            memset(&self.c, 0, sizeof(self.c))
+            self.c = EMPTY_MORPH_TAG
 
     @classmethod
     def from_id(cls, Vocab vocab, hash_t key):
         """Create a morphological analysis from a given ID."""
-        cdef MorphAnalysis morph = MorphAnalysis.__new__(MorphAnalysis, vocab)
+        cdef MorphAnalysis morph = MorphAnalysis(vocab)
         morph.vocab = vocab
         morph.key = key
-        analysis = <const MorphAnalysisC*>vocab.morphology.tags.get(key)
-        if analysis is not NULL:
-            morph.c = analysis[0]
-        else:
-            memset(&morph.c, 0, sizeof(morph.c))
+        morph._init_c(key)
         return morph
 
     def __contains__(self, feature):
         """Test whether the morphological analysis contains some feature."""
         cdef attr_t feat_id = self.vocab.strings.as_int(feature)
-        return check_feature(&self.c, feat_id)
+        return check_feature(self.c, feat_id)
 
     def __iter__(self):
         """Iterate over the features in the analysis."""
         cdef attr_t feature
-        for feature in list_features(&self.c):
+        for feature in list_features(self.c):
             yield self.vocab.strings[feature]
 
     def __len__(self):
         """The number of features in the analysis."""
-        return self.c.length
+        return deref(self.c).features.size()
 
     def __hash__(self):
         return self.key
 
     def __eq__(self, other):
         if isinstance(other, str):
             raise ValueError(Errors.E977)
         return self.key == other.key
 
     def __ne__(self, other):
         return self.key != other.key
 
-    def get(self, field, default=None):
+    def get(self, field):
         """Retrieve feature values by field."""
         cdef attr_t field_id = self.vocab.strings.as_int(field)
-        cdef np.ndarray results = get_by_field(&self.c, field_id)
-        if len(results) == 0:
-            if default is None:
-                default = []
-            return default
+        cdef np.ndarray results = get_by_field(self.c, field_id)
         features = [self.vocab.strings[result] for result in results]
         return [f.split(Morphology.FIELD_SEP)[1] for f in features]
 
     def to_json(self):
         """Produce a json serializable representation as a UD FEATS-style
         string.
         """
-        morph_string = self.vocab.strings[self.c.key]
+        morph_string = self.vocab.strings[deref(self.c).key]
         if morph_string == self.vocab.morphology.EMPTY_MORPH:
             return ""
         return morph_string
 
     def to_dict(self):
         """Produce a dict representation.
         """
```

### Comparing `spacy-3.6.0.dev0/spacy/tokens/span.pxd` & `spacy-4.0.0.dev0/spacy/tokens/span.pxd`

 * *Files 27% similar despite different names*

```diff
@@ -1,25 +1,28 @@
+from libcpp.memory cimport shared_ptr
 cimport numpy as np
 
 from .doc cimport Doc
 from ..typedefs cimport attr_t
 from ..structs cimport SpanC
 
 
 cdef class Span:
     cdef readonly Doc doc
-    cdef SpanC c
+    cdef shared_ptr[SpanC] c
     cdef public _vector
     cdef public _vector_norm
 
     @staticmethod
-    cdef inline Span cinit(Doc doc, SpanC span):
+    cdef inline Span cinit(Doc doc, const shared_ptr[SpanC] &span):
         cdef Span self = Span.__new__(
             Span,
             doc,
-            start=span.start,
-            end=span.end
+            start=span.get().start,
+            end=span.get().end
         )
         self.c = span
         return self
 
     cpdef np.ndarray to_array(self, object features)
+
+    cdef SpanC* span_c(self)
```

### Comparing `spacy-3.6.0.dev0/spacy/tokens/span.pyi` & `spacy-4.0.0.dev0/spacy/tokens/span.pyi`

 * *Files 15% similar despite different names*

```diff
@@ -78,33 +78,28 @@
     @property
     def vector(self) -> Floats1d: ...
     @property
     def vector_norm(self) -> float: ...
     @property
     def tensor(self) -> FloatsXd: ...
     @property
-    def sentiment(self) -> float: ...
-    @property
     def text(self) -> str: ...
     @property
     def text_with_ws(self) -> str: ...
     @property
     def noun_chunks(self) -> Iterator[Span]: ...
     @property
     def root(self) -> Token: ...
     def char_span(
         self,
         start_idx: int,
         end_idx: int,
         label: Union[int, str] = ...,
         kb_id: Union[int, str] = ...,
         vector: Optional[Floats1d] = ...,
-        id: Union[int, str] = ...,
-        alignment_mode: str = ...,
-        span_id: Union[int, str] = ...,
     ) -> Span: ...
     @property
     def conjuncts(self) -> Tuple[Token]: ...
     @property
     def lefts(self) -> Iterator[Token]: ...
     @property
     def rights(self) -> Iterator[Token]: ...
@@ -114,19 +109,27 @@
     def n_rights(self) -> int: ...
     @property
     def subtree(self) -> Iterator[Token]: ...
     start: int
     end: int
     start_char: int
     end_char: int
-    label: int
-    kb_id: int
-    id: int
-    ent_id: int
-    ent_id_: str
+    @property
+    def label(self) -> int: ...
+    @property
+    def kb_id(self) -> int: ...
+    @property
+    def id(self) -> int: ...
+    @property
+    def ent_id(self) -> int: ...
     @property
     def orth_(self) -> str: ...
     @property
     def lemma_(self) -> str: ...
-    label_: str
-    kb_id_: str
-    id_: str
+    @property
+    def label_(self) -> str: ...
+    @property
+    def kb_id_(self) -> str: ...
+    @property
+    def id_(self) -> str: ...
+    @property
+    def ent_id_(self) -> str: ...
```

### Comparing `spacy-3.6.0.dev0/spacy/tokens/span.pyx` & `spacy-4.0.0.dev0/spacy/tokens/span.pyx`

 * *Files 11% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 cimport numpy as np
 from libc.math cimport sqrt
+from libcpp.memory cimport make_shared
 
 import numpy
 from thinc.api import get_array_module
 import warnings
 import copy
 
 from .doc cimport token_by_start, token_by_end, get_token_attr, _get_lca_matrix
@@ -110,34 +111,37 @@
             raise ValueError(Errors.E084.format(label=label))
 
         start_char = doc[start].idx if start < doc.length else len(doc.text)
         if start == end:
             end_char = start_char
         else:
             end_char = doc[end - 1].idx + len(doc[end - 1])
-        self.c = SpanC(
+        self.c = make_shared[SpanC](SpanC(
             label=label,
             kb_id=kb_id,
             id=span_id,
             start=start,
             end=end,
             start_char=start_char,
             end_char=end_char,
-        )
+        ))
         self._vector = vector
         self._vector_norm = vector_norm
 
     def __richcmp__(self, Span other, int op):
         if other is None:
             if op == 0 or op == 1 or op == 2:
                 return False
             else:
                 return True
-        self_tuple = (self.c.start_char, self.c.end_char, self.c.label, self.c.kb_id, self.id, self.doc)
-        other_tuple = (other.c.start_char, other.c.end_char, other.c.label, other.c.kb_id, other.id, other.doc)
+
+        cdef SpanC* span_c = self.span_c()
+        cdef SpanC* other_span_c = other.span_c()
+        self_tuple = (span_c.start_char, span_c.end_char, span_c.label, span_c.kb_id, self.id, self.doc)
+        other_tuple = (other_span_c.start_char, other_span_c.end_char, other_span_c.label, other_span_c.kb_id, other.id, other.doc)
         # <
         if op == 0:
             return self_tuple < other_tuple
         # <=
         elif op == 1:
             return self_tuple <= other_tuple
         # ==
@@ -150,74 +154,78 @@
         elif op == 4:
             return self_tuple > other_tuple
         # >=
         elif op == 5:
             return self_tuple >= other_tuple
 
     def __hash__(self):
-        return hash((self.doc, self.c.start_char, self.c.end_char, self.c.label, self.c.kb_id, self.c.id))
+        cdef SpanC* span_c = self.span_c()
+        return hash((self.doc, span_c.start_char, span_c.end_char, span_c.label, span_c.kb_id, span_c.id))
 
     def __len__(self):
         """Get the number of tokens in the span.
 
         RETURNS (int): The number of tokens in the span.
 
         DOCS: https://spacy.io/api/span#len
         """
-        if self.c.end < self.c.start:
+        cdef SpanC* span_c = self.span_c()
+        if span_c.end < span_c.start:
             return 0
-        return self.c.end - self.c.start
+        return span_c.end - span_c.start
 
     def __repr__(self):
         return self.text
 
     def __getitem__(self, object i):
         """Get a `Token` or a `Span` object
 
         i (int or tuple): The index of the token within the span, or slice of
             the span to get.
         RETURNS (Token or Span): The token at `span[i]`.
 
         DOCS: https://spacy.io/api/span#getitem
         """
+        cdef SpanC* span_c = self.span_c()
         if isinstance(i, slice):
             start, end = normalize_slice(len(self), i.start, i.stop, i.step)
             return Span(self.doc, start + self.start, end + self.start)
         else:
             if i < 0:
-                token_i = self.c.end + i
+                token_i = span_c.end + i
             else:
-                token_i = self.c.start + i
-            if self.c.start <= token_i < self.c.end:
+                token_i = span_c.start + i
+            if span_c.start <= token_i < span_c.end:
                 return self.doc[token_i]
             else:
                 raise IndexError(Errors.E1002)
 
     def __iter__(self):
         """Iterate over `Token` objects.
 
         YIELDS (Token): A `Token` object.
 
         DOCS: https://spacy.io/api/span#iter
         """
-        for i in range(self.c.start, self.c.end):
+        cdef SpanC* span_c = self.span_c()
+        for i in range(span_c.start, span_c.end):
             yield self.doc[i]
 
     def __reduce__(self):
         raise NotImplementedError(Errors.E112)
 
     @property
     def _(self):
+        cdef SpanC* span_c = self.span_c()
         """Custom extension attributes registered via `set_extension`."""
         return Underscore(Underscore.span_extensions, self,
-                          start=self.c.start_char, end=self.c.end_char)
+                          start=span_c.start_char, end=span_c.end_char, label=self.label, kb_id=self.kb_id, span_id=self.id)
 
     def as_doc(self, *, bint copy_user_data=False, array_head=None, array=None):
         """Create a `Doc` object with a copy of the `Span`'s data.
-
         copy_user_data (bool): Whether or not to copy the original doc's user data.
         array_head (tuple): `Doc` array attrs, can be passed in to speed up computation.
         array (ndarray): `Doc` as array, can be passed in to speed up computation.
         RETURNS (Doc): The `Doc` copy of the span.
 
         DOCS: https://spacy.io/api/span#as_doc
         """
@@ -262,46 +270,57 @@
                 if cat_start == self.start_char and cat_end == self.end_char:
                     doc.cats[cat_label] = value
         if copy_user_data:
             user_data = {}
             char_offset = self.start_char
             for key, value in self.doc.user_data.items():
                 if isinstance(key, tuple) and len(key) == 4 and key[0] == "._.":
-                    data_type, name, start, end = key
+                    data_type = key[0]
+                    name = key[1]
+                    start = key[2]
+                    end = key[3]
                     if start is not None or end is not None:
                         start -= char_offset
+                        # Check if Span object
                         if end is not None:
                             end -= char_offset
-                        user_data[(data_type, name, start, end)] = copy.copy(value)
+                            _label = key[4]
+                            _kb_id = key[5]
+                            _span_id = key[6]
+                            user_data[(data_type, name, start, end, _label, _kb_id, _span_id)] = copy.copy(value)
+                        # Else Token object
+                        else:
+                            user_data[(data_type, name, start, end)] = copy.copy(value)
                 else:
                     user_data[key] = copy.copy(value)
             doc.user_data = user_data
         return doc
 
     def _fix_dep_copy(self, attrs, array):
         """ Rewire dependency links to make sure their heads fall into the span
         while still keeping the correct number of sentences. """
         cdef int length = len(array)
         cdef attr_t value
         cdef int i, head_col, ancestor_i
+        cdef SpanC* span_c = self.span_c()
         old_to_new_root = dict()
         if HEAD in attrs:
             head_col = attrs.index(HEAD)
             for i in range(length):
                 # if the HEAD refers to a token outside this span, find a more appropriate ancestor
                 token = self[i]
-                ancestor_i = token.head.i - self.c.start   # span offset
+                ancestor_i = token.head.i - span_c.start   # span offset
                 if ancestor_i not in range(length):
                     if DEP in attrs:
                         array[i, attrs.index(DEP)] = dep
 
                     # try finding an ancestor within this span
                     ancestors = token.ancestors
                     for ancestor in ancestors:
-                        ancestor_i = ancestor.i - self.c.start
+                        ancestor_i = ancestor.i - span_c.start
                         if ancestor_i in range(length):
                             array[i, head_col] = numpy.int32(ancestor_i - i).astype(numpy.uint64)
 
                 # if there is no appropriate ancestor, define a new artificial root
                 value = array[i, head_col]
                 if (i+value) not in range(length):
                     new_root = old_to_new_root.get(ancestor_i, None)
@@ -322,15 +341,16 @@
         the span, LCA[i, j] will be -1.
 
         RETURNS (np.array[ndim=2, dtype=numpy.int32]): LCA matrix with shape
             (n, n), where n = len(self).
 
         DOCS: https://spacy.io/api/span#get_lca_matrix
         """
-        return numpy.asarray(_get_lca_matrix(self.doc, self.c.start, self.c.end))
+        cdef SpanC* span_c = self.span_c()
+        return numpy.asarray(_get_lca_matrix(self.doc, span_c.start, span_c.end))
 
     def similarity(self, other):
         """Make a semantic similarity estimate. The default estimate is cosine
         similarity using an average of word vectors.
 
         other (object): The object to compare with. By default, accepts `Doc`,
             `Span`, `Token` and `Lexeme` objects.
@@ -358,15 +378,15 @@
                 warnings.warn(Warnings.W008.format(obj="Span"))
             return 0.0
         vector = self.vector
         xp = get_array_module(vector)
         result = xp.dot(vector, other.vector) / (self.vector_norm * other.vector_norm)
         # ensure we get a scalar back (numpy does this automatically but cupy doesn't)
         return result.item()
-
+    
     cpdef np.ndarray to_array(self, object py_attr_ids):
         """Given a list of M attribute IDs, export the tokens to a numpy
         `ndarray` of shape `(N, M)`, where `N` is the length of the document.
         The values will be 32-bit integers.
 
         attr_ids (list[int]): A list of attribute ID ints.
         RETURNS (numpy.ndarray[long, ndim=2]): A feature matrix, with one row
@@ -419,14 +439,17 @@
                 n += 1
                 if n >= self.doc.length:
                     break
             return self.doc[start:end]
         else:
             raise ValueError(Errors.E030)
 
+    cdef SpanC* span_c(self):
+        return self.c.get()
+
     @property
     def sents(self):
         """Obtain the sentences that contain this span. If the given span
         crosses sentence boundaries, return all sentences it is a part of.
 
         RETURNS (Iterable[Span]): All sentences that the span is a part of.
 
@@ -456,35 +479,35 @@
             # Now, find all the sentences in the span
             for i in range(start + 1, self.doc.length):
                 if self.doc.c[i].sent_start == 1:
                     yield Span(self.doc, start, i)
                     start = i
                     if start >= self.end:
                         break
-                elif i == self.doc.length - 1:
-                    yield Span(self.doc, start, self.doc.length)
+            if start < self.end:
+                yield Span(self.doc, start, self.end)
 
-            # Ensure that trailing parts of the Span instance are included in last element of .sents.
-            if start == self.doc.length - 1:
-                yield Span(self.doc, start, self.doc.length)
 
     @property
     def ents(self):
         """The named entities that fall completely within the span. Returns
         a tuple of `Span` objects.
 
         RETURNS (tuple): Entities in the span, one `Span` per entity.
 
         DOCS: https://spacy.io/api/span#ents
         """
         cdef Span ent
+        cdef SpanC* span_c = self.span_c()
+        cdef SpanC* ent_span_c
         ents = []
         for ent in self.doc.ents:
-            if ent.c.start >= self.c.start:
-                if ent.c.end <= self.c.end:
+            ent_span_c = ent.span_c()
+            if ent_span_c.start >= span_c.start:
+                if ent_span_c.end <= span_c.end:
                     ents.append(ent)
                 else:
                     break
         return ents
 
     @property
     def has_vector(self):
@@ -549,24 +572,14 @@
             representing the span's semantics.
         """
         if self.doc.tensor is None:
             return None
         return self.doc.tensor[self.start : self.end]
 
     @property
-    def sentiment(self):
-        """RETURNS (float): A scalar value indicating the positivity or
-            negativity of the span.
-        """
-        if "sentiment" in self.doc.user_span_hooks:
-            return self.doc.user_span_hooks["sentiment"](self)
-        else:
-            return sum([token.sentiment for token in self]) / len(self)
-
-    @property
     def text(self):
         """RETURNS (str): The original verbatim text of the span."""
         text = self.text_with_ws
         if len(self) > 0 and self[-1].whitespace_:
             text = text[:-1]
         return text
 
@@ -611,63 +624,58 @@
         DOCS: https://spacy.io/api/span#root
         """
         if "root" in self.doc.user_span_hooks:
             return self.doc.user_span_hooks["root"](self)
         # This should probably be called 'head', and the other one called
         # 'gov'. But we went with 'head' elsewhere, and now we're stuck =/
         cdef int i
+        cdef SpanC* span_c = self.span_c()
         # First, we scan through the Span, and check whether there's a word
         # with head==0, i.e. a sentence root. If so, we can return it. The
         # longer the span, the more likely it contains a sentence root, and
         # in this case we return in linear time.
-        for i in range(self.c.start, self.c.end):
+        for i in range(span_c.start, span_c.end):
             if self.doc.c[i].head == 0:
                 return self.doc[i]
         # If we don't have a sentence root, we do something that's not so
         # algorithmically clever, but I think should be quite fast,
         # especially for short spans.
         # For each word, we count the path length, and arg min this measure.
         # We could use better tree logic to save steps here...But I
         # think this should be okay.
         cdef int current_best = self.doc.length
         cdef int root = -1
-        for i in range(self.c.start, self.c.end):
-            if self.c.start <= (i+self.doc.c[i].head) < self.c.end:
+        for i in range(span_c.start, span_c.end):
+            if span_c.start <= (i+self.doc.c[i].head) < span_c.end:
                 continue
             words_to_root = _count_words_to_root(&self.doc.c[i], self.doc.length)
             if words_to_root < current_best:
                 current_best = words_to_root
                 root = i
         if root == -1:
-            return self.doc[self.c.start]
+            return self.doc[span_c.start]
         else:
             return self.doc[root]
 
-    def char_span(self, int start_idx, int end_idx, label=0, kb_id=0, vector=None, id=0, alignment_mode="strict", span_id=0):
+    def char_span(self, int start_idx, int end_idx, label=0, kb_id=0, vector=None, id=0):
         """Create a `Span` object from the slice `span.text[start : end]`.
 
         start (int): The index of the first character of the span.
         end (int): The index of the first character after the span.
-        label (Union[int, str]): A label to attach to the Span, e.g. for
+        label (uint64 or string): A label to attach to the Span, e.g. for
             named entities.
-        kb_id (Union[int, str]):  An ID from a KB to capture the meaning of a named entity.
+        kb_id (uint64 or string):  An ID from a KB to capture the meaning of a named entity.
         vector (ndarray[ndim=1, dtype='float32']): A meaning representation of
             the span.
-        id (Union[int, str]): Unused.
-        alignment_mode (str): How character indices are aligned to token
-            boundaries. Options: "strict" (character indices must be aligned
-            with token boundaries), "contract" (span of all tokens completely
-            within the character span), "expand" (span of all tokens at least
-            partially covered by the character span). Defaults to "strict".
-        span_id (Union[int, str]): An identifier to associate with the span.
         RETURNS (Span): The newly constructed object.
         """
-        start_idx += self.c.start_char
-        end_idx += self.c.start_char
-        return self.doc.char_span(start_idx, end_idx, label=label, kb_id=kb_id, vector=vector, alignment_mode=alignment_mode, span_id=span_id)
+        cdef SpanC* span_c = self.span_c()
+        start_idx += span_c.start_char
+        end_idx += span_c.start_char
+        return self.doc.char_span(start_idx, end_idx, label=label, kb_id=kb_id, vector=vector)
 
     @property
     def conjuncts(self):
         """Tokens that are conjoined to the span's root.
 
         RETURNS (tuple): A tuple of Token objects.
 
@@ -739,84 +747,91 @@
             yield from word.subtree
         yield from self
         for word in self.rights:
             yield from word.subtree
 
     property start:
         def __get__(self):
-            return self.c.start
+            return self.span_c().start
 
         def __set__(self, int start):
             if start < 0:
                 raise IndexError(Errors.E1032.format(var="start", forbidden="< 0", value=start))
-            self.c.start = start
+            self.span_c().start = start
 
     property end:
         def __get__(self):
-            return self.c.end
+            return self.span_c().end
 
         def __set__(self, int end):
             if end < 0:
                 raise IndexError(Errors.E1032.format(var="end", forbidden="< 0", value=end))
-            self.c.end = end
+            self.span_c().end = end
 
     property start_char:
         def __get__(self):
-            return self.c.start_char
+            return self.span_c().start_char
 
         def __set__(self, int start_char):
             if start_char < 0:
                 raise IndexError(Errors.E1032.format(var="start_char", forbidden="< 0", value=start_char))
-            self.c.start_char = start_char
+            self.span_c().start_char = start_char
 
     property end_char:
         def __get__(self):
-            return self.c.end_char
+            return self.span_c().end_char
 
         def __set__(self, int end_char):
             if end_char < 0:
                 raise IndexError(Errors.E1032.format(var="end_char", forbidden="< 0", value=end_char))
-            self.c.end_char = end_char
+            self.span_c().end_char = end_char
 
     property label:
         def __get__(self):
-            return self.c.label
+            return self.span_c().label
 
         def __set__(self, attr_t label):
-            self.c.label = label
+            if label != self.span_c().label :
+                old_label = self.span_c().label
+                self.span_c().label = label
+                new = Underscore(Underscore.span_extensions, self, start=self.span_c().start_char, end=self.span_c().end_char, label=self.label, kb_id=self.kb_id, span_id=self.id)
+                old = Underscore(Underscore.span_extensions, self, start=self.span_c().start_char, end=self.span_c().end_char, label=old_label, kb_id=self.kb_id, span_id=self.id)
+                Underscore._replace_keys(old, new)
 
     property kb_id:
         def __get__(self):
-            return self.c.kb_id
+            return self.span_c().kb_id
 
         def __set__(self, attr_t kb_id):
-            self.c.kb_id = kb_id
+            if kb_id != self.span_c().kb_id :
+                old_kb_id = self.span_c().kb_id
+                self.span_c().kb_id = kb_id
+                new = Underscore(Underscore.span_extensions, self, start=self.span_c().start_char, end=self.span_c().end_char, label=self.label, kb_id=self.kb_id, span_id=self.id)
+                old = Underscore(Underscore.span_extensions, self, start=self.span_c().start_char, end=self.span_c().end_char, label=self.label, kb_id=old_kb_id, span_id=self.id)
+                Underscore._replace_keys(old, new)
 
     property id:
         def __get__(self):
-            return self.c.id
+            return self.span_c().id
 
         def __set__(self, attr_t id):
-            self.c.id = id
+            if id != self.span_c().id :
+                old_id = self.span_c().id
+                self.span_c().id = id
+                new = Underscore(Underscore.span_extensions, self, start=self.span_c().start_char, end=self.span_c().end_char, label=self.label, kb_id=self.kb_id, span_id=self.id)
+                old = Underscore(Underscore.span_extensions, self, start=self.span_c().start_char, end=self.span_c().end_char, label=self.label, kb_id=self.kb_id, span_id=old_id)
+                Underscore._replace_keys(old, new)
 
     property ent_id:
-        """RETURNS (uint64): The entity ID."""
+        """Alias for the span's ID."""
         def __get__(self):
-            return self.root.ent_id
+            return self.id
 
-        def __set__(self, hash_t key):
-            raise NotImplementedError(Errors.E200.format(attr="ent_id"))
-
-    property ent_id_:
-        """RETURNS (str): The (string) entity ID."""
-        def __get__(self):
-            return self.root.ent_id_
-
-        def __set__(self, str key):
-            raise NotImplementedError(Errors.E200.format(attr="ent_id_"))
+        def __set__(self, attr_t ent_id):
+            self.id = ent_id
 
     @property
     def orth_(self):
         """Verbatim text content (identical to `Span.text`). Exists mostly for
         consistency with other attributes.
 
         RETURNS (str): The span's text."""
@@ -824,37 +839,46 @@
 
     @property
     def lemma_(self):
         """RETURNS (str): The span's lemma."""
         return "".join([t.lemma_ + t.whitespace_ for t in self]).strip()
 
     property label_:
-        """RETURNS (str): The span's label."""
+        """The span's label."""
         def __get__(self):
             return self.doc.vocab.strings[self.label]
 
         def __set__(self, str label_):
             self.label = self.doc.vocab.strings.add(label_)
 
     property kb_id_:
-        """RETURNS (str): The span's KB ID."""
+        """The span's KB ID."""
         def __get__(self):
             return self.doc.vocab.strings[self.kb_id]
 
         def __set__(self, str kb_id_):
             self.kb_id = self.doc.vocab.strings.add(kb_id_)
 
     property id_:
-        """RETURNS (str): The span's ID."""
+        """The span's ID."""
         def __get__(self):
             return self.doc.vocab.strings[self.id]
 
         def __set__(self, str id_):
             self.id = self.doc.vocab.strings.add(id_)
 
+    property ent_id_:
+        """Alias for the span's ID."""
+        def __get__(self):
+            return self.id_
+
+        def __set__(self, str ent_id_):
+            self.id_ = ent_id_
+
+
 
 cdef int _count_words_to_root(const TokenC* token, int sent_length) except -1:
     # Don't allow spaces to be the root, if there are
     # better candidates
     if Lexeme.c_check_flag(token.lex, IS_SPACE) and token.l_kids == 0 and token.r_kids == 0:
         return sent_length-1
     if Lexeme.c_check_flag(token.lex, IS_PUNCT) and token.l_kids == 0 and token.r_kids == 0:
```

### Comparing `spacy-3.6.0.dev0/spacy/tokens/span_group.pyi` & `spacy-4.0.0.dev0/spacy/tokens/span_group.pyi`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tokens/span_group.pyx` & `spacy-4.0.0.dev0/spacy/tokens/span_group.pyx`

 * *Files 3% similar despite different names*

```diff
@@ -2,14 +2,15 @@
 import weakref
 import struct
 from copy import deepcopy
 import srsly
 
 from spacy.errors import Errors
 from .span cimport Span
+from libcpp.memory cimport make_shared
 
 
 cdef class SpanGroup:
     """A group of spans that all belong to the same Doc object. The group
     can be named, and you can attach additional attributes to it. Span groups
     are generally accessed via the `doc.spans` attribute. The `doc.spans`
     attribute will convert lists of spans into a `SpanGroup` object for you
@@ -193,36 +194,38 @@
     def to_bytes(self):
         """Serialize the SpanGroup's contents to a byte string.
 
         RETURNS (bytes): The serialized span group.
 
         DOCS: https://spacy.io/api/spangroup#to_bytes
         """
+        cdef SpanC* span_c
         output = {"name": self.name, "attrs": self.attrs, "spans": []}
         cdef int i
         for i in range(self.c.size()):
             span = self.c[i]
+            span_c = span.get()
             # The struct.pack here is probably overkill, but it might help if
             # you're saving tonnes of spans, and it doesn't really add any
             # complexity. We do take care to specify little-endian byte order
             # though, to ensure the message can be loaded back on a different
             # arch.
             # Q: uint64_t
             # q: int64_t
             # L: uint32_t
             # l: int32_t
             output["spans"].append(struct.pack(
                 ">QQQllll",
-                span.id,
-                span.kb_id,
-                span.label,
-                span.start,
-                span.end,
-                span.start_char,
-                span.end_char
+                span_c.id,
+                span_c.kb_id,
+                span_c.label,
+                span_c.start,
+                span_c.end,
+                span_c.start_char,
+                span_c.end_char
             ))
         return srsly.msgpack_dumps(output)
 
     def from_bytes(self, bytes_data):
         """Deserialize the SpanGroup's contents from a byte string.
 
         bytes_data (bytes): The span group to load.
@@ -241,18 +244,18 @@
             span.id = items[0]
             span.kb_id = items[1]
             span.label = items[2]
             span.start = items[3]
             span.end = items[4]
             span.start_char = items[5]
             span.end_char = items[6]
-            self.c.push_back(span)
+            self.c.push_back(make_shared[SpanC](span))
         return self
 
-    cdef void push_back(self, SpanC span) nogil:
+    cdef void push_back(self, const shared_ptr[SpanC] &span):
         self.c.push_back(span)
 
     def copy(self, doc: Optional["Doc"] = None) -> SpanGroup:
         """Clones the span group.
 
         doc (Doc): New reference document to which the copy is bound.
         RETURNS (SpanGroup): A copy of the span group.
```

### Comparing `spacy-3.6.0.dev0/spacy/tokens/token.pxd` & `spacy-4.0.0.dev0/spacy/tokens/token.pxd`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/tokens/token.pyi` & `spacy-4.0.0.dev0/spacy/tokens/token.pyi`

 * *Files 1% similar despite different names*

```diff
@@ -75,16 +75,14 @@
     @property
     def text(self) -> str: ...
     @property
     def text_with_ws(self) -> str: ...
     @property
     def prob(self) -> float: ...
     @property
-    def sentiment(self) -> float: ...
-    @property
     def lang(self) -> int: ...
     @property
     def idx(self) -> int: ...
     @property
     def cluster(self) -> int: ...
     @property
     def orth(self) -> int: ...
```

### Comparing `spacy-3.6.0.dev0/spacy/tokens/token.pyx` & `spacy-4.0.0.dev0/spacy/tokens/token.pyx`

 * *Files 2% similar despite different names*

```diff
@@ -18,14 +18,15 @@
 from .morphanalysis cimport MorphAnalysis
 from .doc cimport set_children_from_heads
 
 from .. import parts_of_speech
 from ..errors import Errors, Warnings
 from ..attrs import IOB_STRINGS
 from .underscore import Underscore, get_ext_args
+from cython.operator cimport dereference as deref
 
 
 cdef class Token:
     """An individual token  i.e. a word, punctuation symbol, whitespace,
     etc.
 
     DOCS: https://spacy.io/api/token
@@ -227,15 +228,15 @@
         def __get__(self):
             return MorphAnalysis.from_id(self.vocab, self.c.morph)
 
         def __set__(self, MorphAnalysis morph):
             # Check that the morph has the same vocab
             if self.vocab != morph.vocab:
                 raise ValueError(Errors.E1013)
-            self.c.morph = morph.c.key
+            self.c.morph = deref(morph.c).key
 
     def set_morph(self, features):
         cdef hash_t key
         if features is None:
             self.c.morph = 0
         elif isinstance(features, MorphAnalysis):
             self.morph = features
@@ -279,22 +280,14 @@
 
     @property
     def prob(self):
         """RETURNS (float): Smoothed log probability estimate of token type."""
         return self.vocab[self.c.lex.orth].prob
 
     @property
-    def sentiment(self):
-        """RETURNS (float): A scalar value indicating the positivity or
-            negativity of the token."""
-        if "sentiment" in self.doc.user_token_hooks:
-            return self.doc.user_token_hooks["sentiment"](self)
-        return self.vocab[self.c.lex.orth].sentiment
-
-    @property
     def lang(self):
         """RETURNS (uint64): ID of the language of the parent document's
             vocabulary.
         """
         return self.c.lex.lang
 
     @property
```

### Comparing `spacy-3.6.0.dev0/spacy/tokens/underscore.py` & `spacy-4.0.0.dev0/spacy/tokens/underscore.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 from typing import Dict, Any, List, Optional, Tuple, Union, TYPE_CHECKING
 import functools
 import copy
 from ..errors import Errors
+from .span import Span
 
 if TYPE_CHECKING:
     from .doc import Doc
-    from .span import Span
     from .token import Token
 
 
 class Underscore:
     mutable_types = (dict, list, set)
     doc_extensions: Dict[Any, Any] = {}
     span_extensions: Dict[Any, Any] = {}
@@ -21,25 +21,32 @@
 
     def __init__(
         self,
         extensions: Dict[str, Any],
         obj: Union["Doc", "Span", "Token"],
         start: Optional[int] = None,
         end: Optional[int] = None,
+        label: int = 0,
+        kb_id: int = 0,
+        span_id: int = 0,
     ):
         object.__setattr__(self, "_extensions", extensions)
         object.__setattr__(self, "_obj", obj)
         # Assumption is that for doc values, _start and _end will both be None
         # Span will set non-None values for _start and _end
         # Token will have _start be non-None, _end be None
         # This lets us key everything into the doc.user_data dictionary,
         # (see _get_key), and lets us use a single Underscore class.
         object.__setattr__(self, "_doc", obj.doc)
         object.__setattr__(self, "_start", start)
         object.__setattr__(self, "_end", end)
+        if type(obj) == Span:
+            object.__setattr__(self, "_label", label)
+            object.__setattr__(self, "_kb_id", kb_id)
+            object.__setattr__(self, "_span_id", span_id)
 
     def __dir__(self) -> List[str]:
         # Hack to enable autocomplete on custom extensions
         extensions = list(self._extensions.keys())
         return ["set", "get", "has"] + extensions
 
     def __getattr__(self, name: str) -> Any:
@@ -84,16 +91,47 @@
 
     def get(self, name: str) -> Any:
         return self.__getattr__(name)
 
     def has(self, name: str) -> bool:
         return name in self._extensions
 
-    def _get_key(self, name: str) -> Tuple[str, str, Optional[int], Optional[int]]:
-        return ("._.", name, self._start, self._end)
+    def _get_key(
+        self, name: str
+    ) -> Union[
+        Tuple[str, str, Optional[int], Optional[int]],
+        Tuple[str, str, Optional[int], Optional[int], int, int, int],
+    ]:
+        if hasattr(self, "_label"):
+            return (
+                "._.",
+                name,
+                self._start,
+                self._end,
+                self._label,
+                self._kb_id,
+                self._span_id,
+            )
+        else:
+            return "._.", name, self._start, self._end
+
+    @staticmethod
+    def _replace_keys(old_underscore: "Underscore", new_underscore: "Underscore"):
+        """
+        This function is called by Span when its kb_id or label are re-assigned.
+        It checks if any user_data is stored for this span and replaces the keys
+        """
+        for name in old_underscore._extensions:
+            old_key = old_underscore._get_key(name)
+            old_doc = old_underscore._doc
+            new_key = new_underscore._get_key(name)
+            if old_key != new_key and old_key in old_doc.user_data:
+                old_underscore._doc.user_data[
+                    new_key
+                ] = old_underscore._doc.user_data.pop(old_key)
 
     @classmethod
     def get_state(cls) -> Tuple[Dict[Any, Any], Dict[Any, Any], Dict[Any, Any]]:
         return cls.token_extensions, cls.span_extensions, cls.doc_extensions
 
     @classmethod
     def load_state(
```

### Comparing `spacy-3.6.0.dev0/spacy/training/align.pyx` & `spacy-4.0.0.dev0/spacy/training/align.pyx`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/training/alignment.py` & `spacy-4.0.0.dev0/spacy/training/alignment.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/training/alignment_array.pyx` & `spacy-4.0.0.dev0/spacy/training/alignment_array.pyx`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/training/augment.py` & `spacy-4.0.0.dev0/spacy/training/augment.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/training/batchers.py` & `spacy-4.0.0.dev0/spacy/training/batchers.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,94 +1,96 @@
 from typing import Union, Iterable, Sequence, TypeVar, List, Callable, Iterator
 from typing import Optional, Any
 from functools import partial
 import itertools
+from thinc.schedules import Schedule
 
 from ..util import registry, minibatch
 
 
-Sizing = Union[Sequence[int], int]
+SizingSchedule = Union[Iterable[int], int, Schedule]
+Sizing = Union[Iterable[int], int]
 ItemT = TypeVar("ItemT")
 BatcherT = Callable[[Iterable[ItemT]], Iterable[List[ItemT]]]
 
 
 @registry.batchers("spacy.batch_by_padded.v1")
 def configure_minibatch_by_padded_size(
     *,
-    size: Sizing,
+    size: SizingSchedule,
     buffer: int,
     discard_oversize: bool,
     get_length: Optional[Callable[[ItemT], int]] = None
 ) -> BatcherT:
     """Create a batcher that uses the `batch_by_padded_size` strategy.
 
     The padded size is defined as the maximum length of sequences within the
     batch multiplied by the number of sequences in the batch.
 
-    size (int or Sequence[int]): The largest padded size to batch sequences into.
-        Can be a single integer, or a sequence, allowing for variable batch sizes.
+    size (int, Iterable[int] or Schedule): The largest padded size to batch sequences
+        into. Can be a single integer, or a sequence, allowing for variable batch sizes.
     buffer (int): The number of sequences to accumulate before sorting by length.
         A larger buffer will result in more even sizing, but if the buffer is
         very large, the iteration order will be less random, which can result
         in suboptimal training.
     discard_oversize (bool): Whether to discard sequences that are by themselves
         longer than the largest padded batch size.
     get_length (Callable or None): Function to get the length of a sequence item.
         The `len` function is used by default.
     """
     # Avoid displacing optional values from the underlying function.
     optionals = {"get_length": get_length} if get_length is not None else {}
     return partial(
         minibatch_by_padded_size,
-        size=size,
+        size=_schedule_to_sizing(size),
         buffer=buffer,
         discard_oversize=discard_oversize,
         **optionals
     )
 
 
 @registry.batchers("spacy.batch_by_words.v1")
 def configure_minibatch_by_words(
     *,
-    size: Sizing,
+    size: SizingSchedule,
     tolerance: float,
     discard_oversize: bool,
     get_length: Optional[Callable[[ItemT], int]] = None
 ) -> BatcherT:
     """Create a batcher that uses the "minibatch by words" strategy.
 
-    size (int or Sequence[int]): The target number of words per batch.
+    size (int, Iterable[int] or Schedule): The target number of words per batch.
         Can be a single integer, or a sequence, allowing for variable batch sizes.
     tolerance (float): What percentage of the size to allow batches to exceed.
     discard_oversize (bool): Whether to discard sequences that by themselves
         exceed the tolerated size.
     get_length (Callable or None): Function to get the length of a sequence
         item. The `len` function is used by default.
     """
     optionals = {"get_length": get_length} if get_length is not None else {}
     return partial(
         minibatch_by_words,
-        size=size,
+        size=_schedule_to_sizing(size),
         tolerance=tolerance,
         discard_oversize=discard_oversize,
         **optionals
     )
 
 
 @registry.batchers("spacy.batch_by_sequence.v1")
 def configure_minibatch(
-    size: Sizing, get_length: Optional[Callable[[ItemT], int]] = None
+    size: SizingSchedule, get_length: Optional[Callable[[ItemT], int]] = None
 ) -> BatcherT:
     """Create a batcher that creates batches of the specified size.
 
-    size (int or Sequence[int]): The target number of items per batch.
+    size (int, Iterable[int] or Schedule): The target number of items per batch.
         Can be a single integer, or a sequence, allowing for variable batch sizes.
     """
     optionals = {"get_length": get_length} if get_length is not None else {}
-    return partial(minibatch, size=size, **optionals)
+    return partial(minibatch, size=_schedule_to_sizing(size), **optionals)
 
 
 def minibatch_by_padded_size(
     seqs: Iterable[ItemT],
     size: Sizing,
     buffer: int = 256,
     discard_oversize: bool = False,
@@ -96,26 +98,26 @@
 ) -> Iterable[List[ItemT]]:
     """Minibatch a sequence by the size of padded batches that would result,
     with sequences binned by length within a window.
 
     The padded size is defined as the maximum length of sequences within the
     batch multiplied by the number of sequences in the batch.
 
-    size (int or Sequence[int]): The largest padded size to batch sequences into.
+    size (int or Iterable[int]): The largest padded size to batch sequences into.
     buffer (int): The number of sequences to accumulate before sorting by length.
         A larger buffer will result in more even sizing, but if the buffer is
         very large, the iteration order will be less random, which can result
         in suboptimal training.
     discard_oversize (bool): Whether to discard sequences that are by themselves
         longer than the largest padded batch size.
     get_length (Callable or None): Function to get the length of a sequence item.
         The `len` function is used by default.
     """
     if isinstance(size, int):
-        size_ = itertools.repeat(size)  # type: Iterator[int]
+        size_: Iterator[int] = itertools.repeat(size)
     else:
         size_ = iter(size)
     for outer_batch in minibatch(seqs, size=buffer):
         outer_batch = list(outer_batch)
         target_size = next(size_)
         for indices in _batch_by_length(outer_batch, target_size, get_length):
             subbatch = [outer_batch[i] for i in indices]
@@ -134,24 +136,24 @@
     get_length=len,
 ) -> Iterable[List[ItemT]]:
     """Create minibatches of roughly a given number of words. If any examples
     are longer than the specified batch length, they will appear in a batch by
     themselves, or be discarded if discard_oversize=True.
 
     seqs (Iterable[Sequence]): The sequences to minibatch.
-    size (int or Sequence[int]): The target number of words per batch.
+    size (int or Iterable[int]): The target number of words per batch.
         Can be a single integer, or a sequence, allowing for variable batch sizes.
     tolerance (float): What percentage of the size to allow batches to exceed.
     discard_oversize (bool): Whether to discard sequences that by themselves
         exceed the tolerated size.
     get_length (Callable or None): Function to get the length of a sequence
         item. The `len` function is used by default.
     """
     if isinstance(size, int):
-        size_ = itertools.repeat(size)  # type: Iterator[int]
+        size_: Iterator[int] = itertools.repeat(size)
     else:
         size_ = iter(size)
     target_size = next(size_)
     tol_size = target_size * tolerance
     batch = []
     overflow = []
     batch_size = 0
@@ -226,7 +228,13 @@
     if batch:
         batches.append(batch)
     # Check lengths match
     assert sum(len(b) for b in batches) == len(seqs)
     batches = [list(sorted(batch)) for batch in batches]
     batches.reverse()
     return batches
+
+
+def _schedule_to_sizing(size: SizingSchedule) -> Sizing:
+    if isinstance(size, Schedule):
+        return size.to_generator()
+    return size
```

### Comparing `spacy-3.6.0.dev0/spacy/training/callbacks.py` & `spacy-4.0.0.dev0/spacy/training/callbacks.py`

 * *Files 13% similar despite different names*

```diff
@@ -7,26 +7,26 @@
 @registry.callbacks("spacy.copy_from_base_model.v1")
 def create_copy_from_base_model(
     tokenizer: Optional[str] = None,
     vocab: Optional[str] = None,
 ) -> Callable[[Language], Language]:
     def copy_from_base_model(nlp):
         if tokenizer:
-            logger.info("Copying tokenizer from: %s", tokenizer)
+            logger.info(f"Copying tokenizer from: {tokenizer}")
             base_nlp = load_model(tokenizer)
             if nlp.config["nlp"]["tokenizer"] == base_nlp.config["nlp"]["tokenizer"]:
                 nlp.tokenizer.from_bytes(base_nlp.tokenizer.to_bytes(exclude=["vocab"]))
             else:
                 raise ValueError(
                     Errors.E872.format(
                         curr_config=nlp.config["nlp"]["tokenizer"],
                         base_config=base_nlp.config["nlp"]["tokenizer"],
                     )
                 )
         if vocab:
-            logger.info("Copying vocab from: %s", vocab)
+            logger.info(f"Copying vocab from: {vocab}")
             # only reload if the vocab is from a different model
             if tokenizer != vocab:
                 base_nlp = load_model(vocab)
             nlp.vocab.from_bytes(base_nlp.vocab.to_bytes())
 
     return copy_from_base_model
```

### Comparing `spacy-3.6.0.dev0/spacy/training/converters/conll_ner_to_docs.py` & `spacy-4.0.0.dev0/spacy/training/converters/conll_ner_to_docs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/training/converters/conllu_to_docs.py` & `spacy-4.0.0.dev0/spacy/training/converters/conllu_to_docs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/training/converters/iob_to_docs.py` & `spacy-4.0.0.dev0/spacy/training/converters/iob_to_docs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/training/converters/json_to_docs.py` & `spacy-4.0.0.dev0/spacy/training/converters/json_to_docs.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/training/corpus.py` & `spacy-4.0.0.dev0/spacy/training/corpus.py`

 * *Files 11% similar despite different names*

```diff
@@ -25,15 +25,15 @@
     gold_preproc: bool,
     max_length: int = 0,
     limit: int = 0,
     augmenter: Optional[Callable] = None,
 ) -> Callable[["Language"], Iterable[Example]]:
     if path is None:
         raise ValueError(Errors.E913)
-    util.logger.debug("Loading corpus from path: %s", path)
+    util.logger.debug(f"Loading corpus from path: {path}")
     return Corpus(
         path,
         gold_preproc=gold_preproc,
         max_length=max_length,
         limit=limit,
         augmenter=augmenter,
     )
@@ -54,36 +54,14 @@
     # I decided not to give this a generic name, because I don't want people to
     # use it for arbitrary stuff, as I want this require arg with default False.
     if not require and not path.exists():
         return None
     return srsly.read_json(path)
 
 
-@util.registry.readers("spacy.PlainTextCorpus.v1")
-def create_plain_text_reader(
-    path: Optional[Path],
-    min_length: int = 0,
-    max_length: int = 0,
-) -> Callable[["Language"], Iterable[Doc]]:
-    """Iterate Example objects from a file or directory of plain text
-    UTF-8 files with one line per doc.
-
-    path (Path): The directory or filename to read from.
-    min_length (int): Minimum document length (in tokens). Shorter documents
-        will be skipped. Defaults to 0, which indicates no limit.
-    max_length (int): Maximum document length (in tokens). Longer documents will
-        be skipped. Defaults to 0, which indicates no limit.
-
-    DOCS: https://spacy.io/api/corpus#plaintextcorpus
-    """
-    if path is None:
-        raise ValueError(Errors.E913)
-    return PlainTextCorpus(path, min_length=min_length, max_length=max_length)
-
-
 def walk_corpus(path: Union[str, Path], file_type) -> List[Path]:
     path = util.ensure_path(path)
     if not path.is_dir() and path.parts[-1].endswith(file_type):
         return [path]
     orig_path = path
     paths = [path]
     locs = []
@@ -275,56 +253,7 @@
                     continue
                 else:
                     words = [w.text for w in doc]
                     spaces = [bool(w.whitespace_) for w in doc]
                     # We don't *need* an example here, but it seems nice to
                     # make it match the Corpus signature.
                     yield Example(doc, Doc(nlp.vocab, words=words, spaces=spaces))
-
-
-class PlainTextCorpus:
-    """Iterate Example objects from a file or directory of plain text
-    UTF-8 files with one line per doc.
-
-    path (Path): The directory or filename to read from.
-    min_length (int): Minimum document length (in tokens). Shorter documents
-        will be skipped. Defaults to 0, which indicates no limit.
-    max_length (int): Maximum document length (in tokens). Longer documents will
-        be skipped. Defaults to 0, which indicates no limit.
-
-    DOCS: https://spacy.io/api/corpus#plaintextcorpus
-    """
-
-    file_type = "txt"
-
-    def __init__(
-        self,
-        path: Optional[Union[str, Path]],
-        *,
-        min_length: int = 0,
-        max_length: int = 0,
-    ) -> None:
-        self.path = util.ensure_path(path)
-        self.min_length = min_length
-        self.max_length = max_length
-
-    def __call__(self, nlp: "Language") -> Iterator[Example]:
-        """Yield examples from the data.
-
-        nlp (Language): The current nlp object.
-        YIELDS (Example): The example objects.
-
-        DOCS: https://spacy.io/api/corpus#plaintextcorpus-call
-        """
-        for loc in walk_corpus(self.path, ".txt"):
-            with open(loc, encoding="utf-8") as f:
-                for text in f:
-                    text = text.rstrip("\r\n")
-                    if len(text):
-                        doc = nlp.make_doc(text)
-                        if self.min_length >= 1 and len(doc) < self.min_length:
-                            continue
-                        elif self.max_length >= 1 and len(doc) > self.max_length:
-                            continue
-                        # We don't *need* an example here, but it seems nice to
-                        # make it match the Corpus signature.
-                        yield Example(doc, doc.copy())
```

### Comparing `spacy-3.6.0.dev0/spacy/training/example.pyx` & `spacy-4.0.0.dev0/spacy/training/example.pyx`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,8 @@
 from collections.abc import Iterable as IterableInstance
-import warnings
 import numpy
 from murmurhash.mrmr cimport hash64
 
 from ..tokens.doc cimport Doc
 from ..tokens.span cimport Span
 from ..tokens.span import Span
 from ..attrs import IDS
@@ -43,14 +42,21 @@
         raise TypeError(err)
     wrong = set([type(eg) for eg in examples if not isinstance(eg, Example)])
     if wrong:
         err = Errors.E978.format(name=method, types=wrong)
         raise TypeError(err)
 
 
+def validate_distillation_examples(examples, method):
+    validate_examples(examples, method)
+    for eg in examples:
+        if [token.text for token in eg.reference] != [token.text for token in eg.predicted]:
+            raise ValueError(Errors.E4003)
+
+
 def validate_get_examples(get_examples, method):
     """Check that a generator of a batch of examples received during processing is valid:
     the callable produces a non-empty list of Example objects.
     This function lives here to prevent circular imports.
 
     get_examples (Callable[[], Iterable[Example]]): A function that produces a batch of examples.
     method (str): The method name to show in error messages.
```

### Comparing `spacy-3.6.0.dev0/spacy/training/gold_io.pyx` & `spacy-4.0.0.dev0/spacy/training/gold_io.pyx`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/training/initialize.py` & `spacy-4.0.0.dev0/spacy/training/initialize.py`

 * *Files 4% similar despite different names*

```diff
@@ -58,36 +58,35 @@
         )
     train_corpus, dev_corpus = resolve_dot_names(config, dot_names)
     optimizer = T["optimizer"]
     # Components that shouldn't be updated during training
     frozen_components = T["frozen_components"]
     # Sourced components that require resume_training
     resume_components = [p for p in sourced if p not in frozen_components]
-    logger.info("Pipeline: %s", nlp.pipe_names)
+    logger.info(f"Pipeline: {nlp.pipe_names}")
     if resume_components:
         with nlp.select_pipes(enable=resume_components):
-            logger.info("Resuming training for: %s", resume_components)
+            logger.info(f"Resuming training for: {resume_components}")
             nlp.resume_training(sgd=optimizer)
     # Make sure that listeners are defined before initializing further
     nlp._link_components()
     with nlp.select_pipes(disable=[*frozen_components, *resume_components]):
         if T["max_epochs"] == -1:
             sample_size = 100
             logger.debug(
-                "Due to streamed train corpus, using only first %s examples for initialization. "
-                "If necessary, provide all labels in [initialize]. "
-                "More info: https://spacy.io/api/cli#init_labels",
-                sample_size,
+                f"Due to streamed train corpus, using only first {sample_size} "
+                f"examples for initialization. If necessary, provide all labels "
+                f"in [initialize]. More info: https://spacy.io/api/cli#init_labels"
             )
             nlp.initialize(
                 lambda: islice(train_corpus(nlp), sample_size), sgd=optimizer
             )
         else:
             nlp.initialize(lambda: train_corpus(nlp), sgd=optimizer)
-        logger.info("Initialized pipeline components: %s", nlp.pipe_names)
+        logger.info(f"Initialized pipeline components: {nlp.pipe_names}")
     # Detect components with listeners that are not frozen consistently
     for name, proc in nlp.pipeline:
         for listener in getattr(
             proc, "listening_components", []
         ):  # e.g. tok2vec/transformer
             # Don't warn about components not in the pipeline
             if listener not in nlp.pipe_names:
@@ -106,15 +105,15 @@
     *,
     data: Optional[Path] = None,
     lookups: Optional[Lookups] = None,
     vectors: Optional[str] = None,
 ) -> None:
     if lookups:
         nlp.vocab.lookups = lookups
-        logger.info("Added vocab lookups: %s", ", ".join(lookups.tables))
+        logger.info(f"Added vocab lookups: {', '.join(lookups.tables)}")
     data_path = ensure_path(data)
     if data_path is not None:
         lex_attrs = srsly.read_jsonl(data_path)
         for lexeme in nlp.vocab:
             lexeme.rank = OOV_RANK
         for attrs in lex_attrs:
             if "settings" in attrs:
@@ -122,19 +121,19 @@
             lexeme = nlp.vocab[attrs["orth"]]
             lexeme.set_attrs(**attrs)
         if len(nlp.vocab):
             oov_prob = min(lex.prob for lex in nlp.vocab) - 1
         else:
             oov_prob = DEFAULT_OOV_PROB
         nlp.vocab.cfg.update({"oov_prob": oov_prob})
-        logger.info("Added %d lexical entries to the vocab", len(nlp.vocab))
+        logger.info(f"Added {len(nlp.vocab)} lexical entries to the vocab")
     logger.info("Created vocabulary")
     if vectors is not None:
         load_vectors_into_model(nlp, vectors)
-        logger.info("Added vectors: %s", vectors)
+        logger.info(f"Added vectors: {vectors}")
     # warn if source model vectors are not identical
     sourced_vectors_hashes = nlp.meta.pop("_sourced_vectors_hashes", {})
     vectors_hash = hash(nlp.vocab.vectors.to_bytes(exclude=["strings"]))
     for sourced_component, sourced_vectors_hash in sourced_vectors_hashes.items():
         if vectors_hash != sourced_vectors_hash:
             warnings.warn(Warnings.W113.format(name=sourced_component))
     logger.info("Finished initializing nlp object")
@@ -188,15 +187,15 @@
             errors = [{"loc": ["initialize", "init_tok2vec"], "msg": err}]
             raise ConfigValidationError(config=nlp.config, errors=errors)
         with init_tok2vec.open("rb") as file_:
             weights_data = file_.read()
     if weights_data is not None:
         layer = get_tok2vec_ref(nlp, P)
         layer.from_bytes(weights_data)
-        logger.info("Loaded pretrained weights from %s", init_tok2vec)
+        logger.info(f"Loaded pretrained weights from {init_tok2vec}")
         return True
     return False
 
 
 def convert_vectors(
     nlp: "Language",
     vectors_loc: Optional[Path],
@@ -213,21 +212,21 @@
         )
         for lex in nlp.vocab:
             if lex.rank and lex.rank != OOV_RANK:
                 nlp.vocab.vectors.add(lex.orth, row=lex.rank)  # type: ignore[attr-defined]
         nlp.vocab.deduplicate_vectors()
     else:
         if vectors_loc:
-            logger.info("Reading vectors from %s", vectors_loc)
+            logger.info(f"Reading vectors from {vectors_loc}")
             vectors_data, vector_keys, floret_settings = read_vectors(
                 vectors_loc,
                 truncate,
                 mode=mode,
             )
-            logger.info("Loaded vectors from %s", vectors_loc)
+            logger.info(f"Loaded vectors from {vectors_loc}")
         else:
             vectors_data, vector_keys = (None, None)
         if vector_keys is not None and mode != VectorsMode.floret:
             for word in vector_keys:
                 if word not in nlp.vocab:
                     nlp.vocab[word]
         if vectors_data is not None:
```

### Comparing `spacy-3.6.0.dev0/spacy/training/iob_utils.py` & `spacy-4.0.0.dev0/spacy/training/iob_utils.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/training/loggers.py` & `spacy-4.0.0.dev0/spacy/training/loggers.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/training/loop.py` & `spacy-4.0.0.dev0/spacy/training/loop.py`

 * *Files 1% similar despite different names*

```diff
@@ -96,15 +96,15 @@
     stdout.write(msg.info(f"Pipeline: {nlp.pipe_names}") + "\n")
     if frozen_components:
         stdout.write(msg.info(f"Frozen components: {frozen_components}") + "\n")
     if annotating_components:
         stdout.write(
             msg.info(f"Set annotations on update for: {annotating_components}") + "\n"
         )
-    stdout.write(msg.info(f"Initial learn rate: {optimizer.learn_rate}") + "\n")
+    stdout.write(msg.info(f"Initial learn rate: {optimizer.learn_rate(step=0)}") + "\n")
     with nlp.select_pipes(disable=frozen_components):
         log_step, finalize_logger = train_logger(nlp, stdout, stderr)
     try:
         for batch, info, is_best_checkpoint in training_step_iterator:
             if is_best_checkpoint is not None:
                 with nlp.select_pipes(disable=frozen_components):
                     update_meta(T, nlp, info)
@@ -200,15 +200,15 @@
     losses: Dict[str, float] = {}
     words_seen = 0
     start_time = timer()
     for step, (epoch, batch) in enumerate(train_data):
         if before_update:
             before_update_args = {"step": step, "epoch": epoch}
             before_update(nlp, before_update_args)
-        dropout = next(dropouts)  # type: ignore
+        dropout = dropouts(optimizer.step)  # type: ignore
         for subbatch in subdivide_batch(batch, accumulate_gradient):
             nlp.update(
                 subbatch,
                 drop=dropout,
                 losses=losses,
                 sgd=False,  # type: ignore[arg-type]
                 exclude=exclude,
@@ -226,14 +226,15 @@
         optimizer.step_schedules()
         if not (step % eval_frequency):
             if optimizer.averages:
                 with nlp.use_params(optimizer.averages):
                     score, other_scores = evaluate()
             else:
                 score, other_scores = evaluate()
+            optimizer.last_score = score
             results.append((score, step))
             is_best_checkpoint = score == max(results)[0]
         else:
             score, other_scores = (None, None)
             is_best_checkpoint = None
         words_seen += sum(len(eg) for eg in batch)
         info = {
@@ -366,10 +367,10 @@
     components that don't exist anymore).
     """
     if path is not None and path.exists():
         for subdir in [path / DIR_MODEL_BEST, path / DIR_MODEL_LAST]:
             if subdir.exists():
                 try:
                     shutil.rmtree(str(subdir))
-                    logger.debug("Removed existing output directory: %s", subdir)
+                    logger.debug(f"Removed existing output directory: {subdir}")
                 except Exception as e:
                     raise IOError(Errors.E901.format(path=path)) from e
```

### Comparing `spacy-3.6.0.dev0/spacy/training/pretrain.py` & `spacy-4.0.0.dev0/spacy/training/pretrain.py`

 * *Files 2% similar despite different names*

```diff
@@ -20,15 +20,14 @@
 def pretrain(
     config: Config,
     output_dir: Path,
     resume_path: Optional[Path] = None,
     epoch_resume: Optional[int] = None,
     use_gpu: int = -1,
     silent: bool = True,
-    skip_last: bool = False,
 ):
     msg = Printer(no_print=silent)
     if config["training"]["seed"] is not None:
         fix_random_seed(config["training"]["seed"])
     allocator = config["training"]["gpu_allocator"]
     if use_gpu >= 0 and allocator:
         set_gpu_allocator(allocator)
@@ -57,54 +56,46 @@
             f"Pre-training tok2vec layer - starting at epoch {epoch_resume} - saving every {P['n_save_epoch']} epoch"
         )
     else:
         msg.divider(f"Pre-training tok2vec layer - starting at epoch {epoch_resume}")
     row_settings = {"widths": (3, 10, 10, 6, 4), "aligns": ("r", "r", "r", "r", "r")}
     msg.row(("#", "# Words", "Total Loss", "Loss", "w/s"), **row_settings)
 
-    def _save_model(epoch, is_temp=False, is_last=False):
+    def _save_model(epoch, is_temp=False):
         is_temp_str = ".temp" if is_temp else ""
         with model.use_params(optimizer.averages):
-            if is_last:
-                save_path = output_dir / f"model-last.bin"
-            else:
-                save_path = output_dir / f"model{epoch}{is_temp_str}.bin"
-            with (save_path).open("wb") as file_:
+            with (output_dir / f"model{epoch}{is_temp_str}.bin").open("wb") as file_:
                 file_.write(model.get_ref("tok2vec").to_bytes())
             log = {
                 "nr_word": tracker.nr_word,
                 "loss": tracker.loss,
                 "epoch_loss": tracker.epoch_loss,
                 "epoch": epoch,
             }
             with (output_dir / "log.jsonl").open("a") as file_:
                 file_.write(srsly.json_dumps(log) + "\n")
 
     # TODO: I think we probably want this to look more like the
     # 'create_train_batches' function?
-    try:
-        for epoch in range(epoch_resume, P["max_epochs"]):
-            for batch_id, batch in enumerate(batcher(corpus(nlp))):
-                docs = ensure_docs(batch)
-                loss = make_update(model, docs, optimizer, objective)
-                progress = tracker.update(epoch, loss, docs)
-                if progress:
-                    msg.row(progress, **row_settings)
-                if P["n_save_every"] and (batch_id % P["n_save_every"] == 0):
-                    _save_model(epoch, is_temp=True)
-
-            if P["n_save_epoch"]:
-                if epoch % P["n_save_epoch"] == 0 or epoch == P["max_epochs"] - 1:
-                    _save_model(epoch)
-            else:
+    for epoch in range(epoch_resume, P["max_epochs"]):
+        for batch_id, batch in enumerate(batcher(corpus(nlp))):
+            docs = ensure_docs(batch)
+            loss = make_update(model, docs, optimizer, objective)
+            progress = tracker.update(epoch, loss, docs)
+            if progress:
+                msg.row(progress, **row_settings)
+            if P["n_save_every"] and (batch_id % P["n_save_every"] == 0):
+                _save_model(epoch, is_temp=True)
+
+        if P["n_save_epoch"]:
+            if epoch % P["n_save_epoch"] == 0 or epoch == P["max_epochs"] - 1:
                 _save_model(epoch)
-            tracker.epoch_loss = 0.0
-    finally:
-        if not skip_last:
-            _save_model(P["max_epochs"], is_last=True)
+        else:
+            _save_model(epoch)
+        tracker.epoch_loss = 0.0
 
 
 def ensure_docs(examples_or_docs: Iterable[Union[Doc, Example]]) -> List[Doc]:
     docs = []
     for eg_or_doc in examples_or_docs:
         if isinstance(eg_or_doc, Doc):
             docs.append(eg_or_doc)
```

### Comparing `spacy-3.6.0.dev0/spacy/ty.py` & `spacy-4.0.0.dev0/spacy/ty.py`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/util.py` & `spacy-4.0.0.dev0/spacy/util.py`

 * *Files 0% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 import os
 import importlib
 import importlib.util
 import re
 from pathlib import Path
 import thinc
 from thinc.api import NumpyOps, get_current_ops, Adam, Config, Optimizer
-from thinc.api import ConfigValidationError, Model
+from thinc.api import ConfigValidationError, Model, constant as constant_schedule
 import functools
 import itertools
 import numpy
 import srsly
 import catalogue
 from catalogue import RegistryError, Registry
 import langcodes
@@ -28,44 +28,37 @@
 import tempfile
 import shutil
 import shlex
 import inspect
 import pkgutil
 import logging
 import socket
-import stat
 
 try:
     import cupy.random
 except ImportError:
     cupy = None
 
-# These are functions that were previously (v2.x) available from spacy.util
-# and have since moved to Thinc. We're importing them here so people's code
-# doesn't break, but they should always be imported from Thinc from now on,
-# not from spacy.util.
-from thinc.api import fix_random_seed, compounding, decaying  # noqa: F401
-
 
 from .symbols import ORTH
 from .compat import cupy, CudaStream, is_windows, importlib_metadata
-from .errors import Errors, Warnings, OLD_MODEL_SHORTCUTS
+from .errors import Errors, Warnings
 from . import about
 
 if TYPE_CHECKING:
     # This lets us add type hints for mypy etc. without causing circular imports
     from .language import Language, PipeCallable  # noqa: F401
     from .tokens import Doc, Span  # noqa: F401
     from .vocab import Vocab  # noqa: F401
 
 
 # fmt: off
 OOV_RANK = numpy.iinfo(numpy.uint64).max
 DEFAULT_OOV_PROB = -20
-LEXEME_NORM_LANGS = ["cs", "da", "de", "el", "en", "grc", "id", "lb", "mk", "pt", "ru", "sr", "ta", "th"]
+LEXEME_NORM_LANGS = ["cs", "da", "de", "el", "en", "id", "lb", "mk", "pt", "ru", "sr", "ta", "th"]
 
 # Default order of sections in the config file. Not all sections needs to exist,
 # and additional sections are added at the end, in alphabetical order.
 CONFIG_SECTION_ORDER = ["paths", "variables", "system", "nlp", "components", "corpora", "training", "pretraining", "initialize"]
 # fmt: on
 
 logger = logging.getLogger("spacy")
@@ -141,25 +134,16 @@
                 Errors.E893.format(
                     name=func_name, reg_name=registry_name, available=available
                 )
             ) from None
         return func
 
     @classmethod
-    def find(
-        cls, registry_name: str, func_name: str
-    ) -> Dict[str, Optional[Union[str, int]]]:
-        """Find information about a registered function, including the
-        module and path to the file it's defined in, the line number and the
-        docstring, if available.
-
-        registry_name (str): Name of the catalogue registry.
-        func_name (str): Name of the registered function.
-        RETURNS (Dict[str, Optional[Union[str, int]]]): The function info.
-        """
+    def find(cls, registry_name: str, func_name: str) -> Callable:
+        """Get info about a registered function from the registry."""
         # We're overwriting this classmethod so we're able to provide more
         # specific error messages and implement a fallback to spacy-legacy.
         if not hasattr(cls, registry_name):
             names = ", ".join(cls.get_registry_names()) or "none"
             raise RegistryError(Errors.E892.format(name=registry_name, available=names))
         reg = getattr(cls, registry_name)
         try:
@@ -440,16 +424,14 @@
             return get_lang_class(name.replace("blank:", ""))()
         if is_package(name):  # installed as package
             return load_model_from_package(name, **kwargs)  # type: ignore[arg-type]
         if Path(name).exists():  # path to model data directory
             return load_model_from_path(Path(name), **kwargs)  # type: ignore[arg-type]
     elif hasattr(name, "exists"):  # Path or Path-like to model data
         return load_model_from_path(name, **kwargs)  # type: ignore[arg-type]
-    if name in OLD_MODEL_SHORTCUTS:
-        raise IOError(Errors.E941.format(name=name, full=OLD_MODEL_SHORTCUTS[name]))  # type: ignore[index]
     raise IOError(Errors.E050.format(name=name))
 
 
 def load_model_from_package(
     name: str,
     *,
     vocab: Union["Vocab", bool] = True,
@@ -1047,23 +1029,16 @@
     """Execute a block in a temporary directory and remove the directory and
     its contents at the end of the with block.
 
     YIELDS (Path): The path of the temp directory.
     """
     d = Path(tempfile.mkdtemp())
     yield d
-
-    # On Windows, git clones use read-only files, which cause permission errors
-    # when being deleted. This forcibly fixes permissions.
-    def force_remove(rmfunc, path, ex):
-        os.chmod(path, stat.S_IWRITE)
-        rmfunc(path)
-
     try:
-        shutil.rmtree(str(d), onerror=force_remove)
+        shutil.rmtree(str(d))
     except PermissionError as e:
         warnings.warn(Warnings.W091.format(dir=d, msg=e))
 
 
 def is_cwd(path: Union[Path, str]) -> bool:
     """Check whether a path is the current working directory.
 
@@ -1606,15 +1581,15 @@
 def minibatch(items, size):
     """Iterate over batches of items. `size` may be an iterator,
     so that batch-size can vary on each step.
     """
     if isinstance(size, int):
         size_ = itertools.repeat(size)
     else:
-        size_ = size
+        size_ = iter(size)
     items = iter(items)
     while True:
         batch_size = next(size_)
         batch = list(itertools.islice(items, int(batch_size)))
         if len(batch) == 0:
             break
         yield list(batch)
```

### Comparing `spacy-3.6.0.dev0/spacy/vectors.pyx` & `spacy-4.0.0.dev0/spacy/vectors.pyx`

 * *Files identical despite different names*

### Comparing `spacy-3.6.0.dev0/spacy/vocab.pxd` & `spacy-4.0.0.dev0/spacy/vocab.pxd`

 * *Files 4% similar despite different names*

```diff
@@ -28,15 +28,14 @@
     cdef readonly StringStore strings
     cdef public Morphology morphology
     cdef public object _vectors
     cdef public object _lookups
     cdef public object writing_system
     cdef public object get_noun_chunks
     cdef readonly int length
-    cdef public object _unused_object # TODO remove in v4, see #9150
     cdef public object lex_attr_getters
     cdef public object cfg
 
     cdef const LexemeC* get(self, Pool mem, str string) except NULL
     cdef const LexemeC* get_by_orth(self, Pool mem, attr_t orth) except NULL
     cdef const TokenC* make_fused_token(self, substrings) except NULL
```

### Comparing `spacy-3.6.0.dev0/spacy/vocab.pyi` & `spacy-4.0.0.dev0/spacy/vocab.pyi`

 * *Files 2% similar despite different names*

```diff
@@ -68,12 +68,11 @@
     ) -> Vocab: ...
 
 def pickle_vocab(vocab: Vocab) -> Any: ...
 def unpickle_vocab(
     sstore: StringStore,
     vectors: Any,
     morphology: Any,
-    _unused_object: Any,
     lex_attr_getters: Any,
     lookups: Any,
     get_noun_chunks: Any,
 ) -> Vocab: ...
```

### Comparing `spacy-3.6.0.dev0/spacy/vocab.pyx` & `spacy-4.0.0.dev0/spacy/vocab.pyx`

 * *Files 2% similar despite different names*

```diff
@@ -264,16 +264,15 @@
             orth = id_or_string
         return Lexeme(self, orth)
 
     cdef const TokenC* make_fused_token(self, substrings) except NULL:
         cdef int i
         tokens = <TokenC*>self.mem.alloc(len(substrings) + 1, sizeof(TokenC))
         for i, props in enumerate(substrings):
-            props = intify_attrs(props, strings_map=self.strings,
-                                 _do_deprecated=True)
+            props = intify_attrs(props, strings_map=self.strings)
             token = &tokens[i]
             # Set the special tokens up to have arbitrary attributes
             lex = <LexemeC*>self.get_by_orth(self.mem, props[ORTH])
             token.lex = lex
             for attr_id, value in props.items():
                 Token.set_struct_attr(token, attr_id, value)
                 # NORM is the only one that overlaps between the two
@@ -555,29 +554,26 @@
         raise NotImplementedError
 
 
 def pickle_vocab(vocab):
     sstore = vocab.strings
     vectors = vocab.vectors
     morph = vocab.morphology
-    _unused_object = vocab._unused_object
     lex_attr_getters = srsly.pickle_dumps(vocab.lex_attr_getters)
     lookups = vocab.lookups
     get_noun_chunks = vocab.get_noun_chunks
     return (unpickle_vocab,
-            (sstore, vectors, morph, _unused_object, lex_attr_getters, lookups, get_noun_chunks))
+            (sstore, vectors, morph, lex_attr_getters, lookups, get_noun_chunks))
 
 
-def unpickle_vocab(sstore, vectors, morphology, _unused_object,
-                   lex_attr_getters, lookups, get_noun_chunks):
+def unpickle_vocab(sstore, vectors, morphology, lex_attr_getters, lookups, get_noun_chunks):
     cdef Vocab vocab = Vocab()
     vocab.vectors = vectors
     vocab.strings = sstore
     vocab.morphology = morphology
-    vocab._unused_object = _unused_object
     vocab.lex_attr_getters = srsly.pickle_loads(lex_attr_getters)
     vocab.lookups = lookups
     vocab.get_noun_chunks = get_noun_chunks
     return vocab
 
 
 copy_reg.pickle(Vocab, pickle_vocab, unpickle_vocab)
```

### Comparing `spacy-3.6.0.dev0/spacy.egg-info/PKG-INFO` & `spacy-4.0.0.dev0/spacy.egg-info/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: spacy
-Version: 3.6.0.dev0
+Version: 4.0.0.dev0
 Summary: Industrial-strength Natural Language Processing (NLP) in Python
 Home-page: https://spacy.io
 Author: Explosion
 Author-email: contact@explosion.ai
 License: MIT
 Project-URL: Release notes, https://github.com/explosion/spaCy/releases
 Project-URL: Source, https://github.com/explosion/spaCy
@@ -68,18 +68,15 @@
 state-of-the-art speed and **neural network models** for tagging,
 parsing, **named entity recognition**, **text classification** and more,
 multi-task learning with pretrained **transformers** like BERT, as well as a
 production-ready [**training system**](https://spacy.io/usage/training) and easy
 model packaging, deployment and workflow management. spaCy is commercial
 open-source software, released under the [MIT license](https://github.com/explosion/spaCy/blob/master/LICENSE).
 
- **We'd love to hear more about your experience with spaCy!**
-[Fill out our survey here.](https://form.typeform.com/to/aMel9q9f)
-
- **Version 3.5 out now!**
+ **Version 3.4 out now!**
 [Check out the release notes here.](https://github.com/explosion/spaCy/releases)
 
 [![Azure Pipelines](https://img.shields.io/azure-devops/build/explosion-ai/public/8/master.svg?logo=azure-pipelines&style=flat-square&label=build)](https://dev.azure.com/explosion-ai/public/_build?definitionId=8)
 [![Current Release Version](https://img.shields.io/github/release/explosion/spacy.svg?style=flat-square&logo=github)](https://github.com/explosion/spaCy/releases)
 [![pypi Version](https://img.shields.io/pypi/v/spacy.svg?style=flat-square&logo=pypi&logoColor=white)](https://pypi.org/project/spacy/)
 [![conda Version](https://img.shields.io/conda/vn/conda-forge/spacy.svg?style=flat-square&logo=conda-forge&logoColor=white)](https://anaconda.org/conda-forge/spacy)
 [![Python wheels](https://img.shields.io/badge/wheels-%E2%9C%93-4c1.svg?longCache=true&style=flat-square&logo=python&logoColor=white)](https://github.com/explosion/wheelwright/releases)
```

#### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: spacy Version: 3.6.0.dev0 Summary: Industrial-
+Metadata-Version: 2.1 Name: spacy Version: 4.0.0.dev0 Summary: Industrial-
 strength Natural Language Processing (NLP) in Python Home-page: https://
 spacy.io Author: Explosion Author-email: contact@explosion.ai License: MIT
 Project-URL: Release notes, https://github.com/explosion/spaCy/releases
 Project-URL: Source, https://github.com/explosion/spaCy Classifier: Development
 Status :: 5 - Production/Stable Classifier: Environment :: Console Classifier:
 Intended Audience :: Developers Classifier: Intended Audience :: Science/
 Research Classifier: License :: OSI Approved :: MIT License Classifier:
@@ -30,22 +30,20 @@
 tokenization and training for **70+ languages**. It features state-of-the-art
 speed and **neural network models** for tagging, parsing, **named entity
 recognition**, **text classification** and more, multi-task learning with
 pretrained **transformers** like BERT, as well as a production-ready
 [**training system**](https://spacy.io/usage/training) and easy model
 packaging, deployment and workflow management. spaCy is commercial open-source
 software, released under the [MIT license](https://github.com/explosion/spaCy/
-blob/master/LICENSE).  **We'd love to hear more about your experience with
-spaCy!** [Fill out our survey here.](https://form.typeform.com/to/aMel9q9f)
- **Version 3.5 out now!** [Check out the release notes here.](https://
-github.com/explosion/spaCy/releases) [![Azure Pipelines](https://
-img.shields.io/azure-devops/build/explosion-ai/public/8/master.svg?logo=azure-
-pipelines&style=flat-square&label=build)](https://dev.azure.com/explosion-ai/
-public/_build?definitionId=8) [![Current Release Version](https://
-img.shields.io/github/release/explosion/spacy.svg?style=flat-
+blob/master/LICENSE).  **Version 3.4 out now!** [Check out the release
+notes here.](https://github.com/explosion/spaCy/releases) [![Azure Pipelines]
+(https://img.shields.io/azure-devops/build/explosion-ai/public/8/
+master.svg?logo=azure-pipelines&style=flat-square&label=build)](https://
+dev.azure.com/explosion-ai/public/_build?definitionId=8) [![Current Release
+Version](https://img.shields.io/github/release/explosion/spacy.svg?style=flat-
 square&logo=github)](https://github.com/explosion/spaCy/releases) [![pypi
 Version](https://img.shields.io/pypi/v/spacy.svg?style=flat-
 square&logo=pypi&logoColor=white)](https://pypi.org/project/spacy/) [![conda
 Version](https://img.shields.io/conda/vn/conda-forge/spacy.svg?style=flat-
 square&logo=conda-forge&logoColor=white)](https://anaconda.org/conda-forge/
 spacy) [![Python wheels](https://img.shields.io/badge/wheels-%E2%9C%93-
 4c1.svg?longCache=true&style=flat-square&logo=python&logoColor=white)](https://
```

### Comparing `spacy-3.6.0.dev0/spacy.egg-info/SOURCES.txt` & `spacy-4.0.0.dev0/spacy.egg-info/SOURCES.txt`

 * *Files 2% similar despite different names*

```diff
@@ -53,15 +53,14 @@
 spacy.egg-info/not-zip-safe
 spacy.egg-info/requires.txt
 spacy.egg-info/top_level.txt
 spacy/cli/__init__.py
 spacy/cli/_util.py
 spacy/cli/apply.py
 spacy/cli/assemble.py
-spacy/cli/benchmark_speed.py
 spacy/cli/convert.py
 spacy/cli/debug_config.py
 spacy/cli/debug_data.py
 spacy/cli/debug_diff.py
 spacy/cli/debug_model.py
 spacy/cli/download.py
 spacy/cli/evaluate.py
@@ -289,18 +288,16 @@
 spacy/lang/ky/__init__.py
 spacy/lang/ky/examples.py
 spacy/lang/ky/lex_attrs.py
 spacy/lang/ky/punctuation.py
 spacy/lang/ky/stop_words.py
 spacy/lang/ky/tokenizer_exceptions.py
 spacy/lang/la/__init__.py
-spacy/lang/la/examples.py
 spacy/lang/la/lex_attrs.py
 spacy/lang/la/stop_words.py
-spacy/lang/la/syntax_iterators.py
 spacy/lang/la/tokenizer_exceptions.py
 spacy/lang/lb/__init__.py
 spacy/lang/lb/examples.py
 spacy/lang/lb/lex_attrs.py
 spacy/lang/lb/punctuation.py
 spacy/lang/lb/stop_words.py
 spacy/lang/lb/tokenizer_exceptions.py
@@ -397,21 +394,19 @@
 spacy/lang/sq/__init__.py
 spacy/lang/sq/examples.py
 spacy/lang/sq/stop_words.py
 spacy/lang/sr/__init__.py
 spacy/lang/sr/examples.py
 spacy/lang/sr/lemma_lookup_licence.txt
 spacy/lang/sr/lex_attrs.py
-spacy/lang/sr/punctuation.py
 spacy/lang/sr/stop_words.py
 spacy/lang/sr/tokenizer_exceptions.py
 spacy/lang/sv/__init__.py
 spacy/lang/sv/examples.py
 spacy/lang/sv/lex_attrs.py
-spacy/lang/sv/punctuation.py
 spacy/lang/sv/stop_words.py
 spacy/lang/sv/syntax_iterators.py
 spacy/lang/sv/tokenizer_exceptions.py
 spacy/lang/ta/__init__.py
 spacy/lang/ta/examples.py
 spacy/lang/ta/lex_attrs.py
 spacy/lang/ta/stop_words.py
@@ -484,75 +479,77 @@
 spacy/matcher/matcher.pyi
 spacy/matcher/matcher.pyx
 spacy/matcher/phrasematcher.pxd
 spacy/matcher/phrasematcher.pyi
 spacy/matcher/phrasematcher.pyx
 spacy/matcher/polyleven.c
 spacy/ml/__init__.py
-spacy/ml/_character_embed.py
-spacy/ml/_precomputable_affine.py
 spacy/ml/callbacks.py
+spacy/ml/character_embed.py
 spacy/ml/extract_ngrams.py
 spacy/ml/extract_spans.py
 spacy/ml/featureextractor.py
-spacy/ml/parser_model.pxd
-spacy/ml/parser_model.pyx
 spacy/ml/staticvectors.py
-spacy/ml/tb_framework.py
+spacy/ml/tb_framework.pxd
+spacy/ml/tb_framework.pyx
 spacy/ml/models/__init__.py
 spacy/ml/models/entity_linker.py
 spacy/ml/models/multi_task.py
 spacy/ml/models/parser.py
 spacy/ml/models/spancat.py
 spacy/ml/models/tagger.py
 spacy/ml/models/textcat.py
 spacy/ml/models/tok2vec.py
 spacy/pipeline/__init__.py
-spacy/pipeline/attributeruler.py
-spacy/pipeline/dep_parser.pyx
+spacy/pipeline/attribute_ruler.py
+spacy/pipeline/dep_parser.py
 spacy/pipeline/edit_tree_lemmatizer.py
 spacy/pipeline/entity_linker.py
 spacy/pipeline/entityruler.py
 spacy/pipeline/functions.py
 spacy/pipeline/lemmatizer.py
 spacy/pipeline/morphologizer.pyx
-spacy/pipeline/multitask.pyx
-spacy/pipeline/ner.pyx
+spacy/pipeline/ner.py
 spacy/pipeline/pipe.pxd
 spacy/pipeline/pipe.pyi
 spacy/pipeline/pipe.pyx
 spacy/pipeline/sentencizer.pyx
 spacy/pipeline/senter.pyx
 spacy/pipeline/span_ruler.py
 spacy/pipeline/spancat.py
 spacy/pipeline/tagger.pyx
 spacy/pipeline/textcat.py
 spacy/pipeline/textcat_multilabel.py
 spacy/pipeline/tok2vec.py
 spacy/pipeline/trainable_pipe.pxd
 spacy/pipeline/trainable_pipe.pyx
-spacy/pipeline/transition_parser.pxd
 spacy/pipeline/transition_parser.pyx
 spacy/pipeline/_edit_tree_internals/__init__.py
 spacy/pipeline/_edit_tree_internals/edit_trees.pxd
 spacy/pipeline/_edit_tree_internals/edit_trees.pyx
 spacy/pipeline/_edit_tree_internals/schemas.py
 spacy/pipeline/_parser_internals/__init__.pxd
 spacy/pipeline/_parser_internals/__init__.py
 spacy/pipeline/_parser_internals/_beam_utils.pxd
 spacy/pipeline/_parser_internals/_beam_utils.pyx
+spacy/pipeline/_parser_internals/_parser_utils.pxd
+spacy/pipeline/_parser_internals/_parser_utils.pyx
 spacy/pipeline/_parser_internals/_state.pxd
 spacy/pipeline/_parser_internals/_state.pyx
 spacy/pipeline/_parser_internals/arc_eager.pxd
 spacy/pipeline/_parser_internals/arc_eager.pyx
+spacy/pipeline/_parser_internals/batch.pxd
+spacy/pipeline/_parser_internals/batch.pyx
 spacy/pipeline/_parser_internals/ner.pxd
 spacy/pipeline/_parser_internals/ner.pyx
 spacy/pipeline/_parser_internals/nonproj.hh
 spacy/pipeline/_parser_internals/nonproj.pxd
 spacy/pipeline/_parser_internals/nonproj.pyx
+spacy/pipeline/_parser_internals/search.pxd
+spacy/pipeline/_parser_internals/search.pyx
 spacy/pipeline/_parser_internals/stateclass.pxd
 spacy/pipeline/_parser_internals/stateclass.pyx
 spacy/pipeline/_parser_internals/transition_system.pxd
 spacy/pipeline/_parser_internals/transition_system.pyx
 spacy/pipeline/legacy/__init__.py
 spacy/pipeline/legacy/entity_linker.py
 spacy/tests/__init__.py
@@ -564,14 +561,15 @@
 spacy/tests/test_displacy.py
 spacy/tests/test_errors.py
 spacy/tests/test_language.py
 spacy/tests/test_misc.py
 spacy/tests/test_models.py
 spacy/tests/test_pickles.py
 spacy/tests/test_scorer.py
+spacy/tests/test_symbols.py
 spacy/tests/test_ty.py
 spacy/tests/util.py
 spacy/tests/doc/__init__.py
 spacy/tests/doc/test_add_entities.py
 spacy/tests/doc/test_array.py
 spacy/tests/doc/test_creation.py
 spacy/tests/doc/test_doc_api.py
@@ -699,15 +697,14 @@
 spacy/tests/lang/ko/test_lemmatization.py
 spacy/tests/lang/ko/test_serialize.py
 spacy/tests/lang/ko/test_tokenizer.py
 spacy/tests/lang/ky/__init__.py
 spacy/tests/lang/ky/test_tokenizer.py
 spacy/tests/lang/la/__init__.py
 spacy/tests/lang/la/test_exception.py
-spacy/tests/lang/la/test_noun_chunks.py
 spacy/tests/lang/la/test_text.py
 spacy/tests/lang/lb/__init__.py
 spacy/tests/lang/lb/test_exceptions.py
 spacy/tests/lang/lb/test_prefix_suffix_infix.py
 spacy/tests/lang/lb/test_text.py
 spacy/tests/lang/lg/__init__.py
 spacy/tests/lang/lg/test_tokenizer.py
@@ -750,15 +747,14 @@
 spacy/tests/lang/sl/test_text.py
 spacy/tests/lang/sl/test_tokenizer.py
 spacy/tests/lang/sq/__init__.py
 spacy/tests/lang/sq/test_text.py
 spacy/tests/lang/sq/test_tokenizer.py
 spacy/tests/lang/sr/__init__.py
 spacy/tests/lang/sr/test_exceptions.py
-spacy/tests/lang/sr/test_lex_attrs.py
 spacy/tests/lang/sr/test_tokenizer.py
 spacy/tests/lang/sv/__init__.py
 spacy/tests/lang/sv/test_exceptions.py
 spacy/tests/lang/sv/test_lex_attrs.py
 spacy/tests/lang/sv/test_noun_chunks.py
 spacy/tests/lang/sv/test_prefix_suffix_infix.py
 spacy/tests/lang/sv/test_text.py
@@ -815,23 +811,25 @@
 spacy/tests/morphology/test_morph_pickle.py
 spacy/tests/package/__init__.py
 spacy/tests/package/pyproject.toml
 spacy/tests/package/requirements.txt
 spacy/tests/package/setup.cfg
 spacy/tests/package/test_requirements.py
 spacy/tests/parser/__init__.py
+spacy/tests/parser/_search.pyx
 spacy/tests/parser/test_add_label.py
 spacy/tests/parser/test_arc_eager_oracle.py
 spacy/tests/parser/test_ner.py
 spacy/tests/parser/test_neural_parser.py
 spacy/tests/parser/test_nn_beam.py
 spacy/tests/parser/test_nonproj.py
 spacy/tests/parser/test_parse.py
 spacy/tests/parser/test_parse_navigate.py
 spacy/tests/parser/test_preset_sbd.py
+spacy/tests/parser/test_search.py
 spacy/tests/parser/test_space_attachment.py
 spacy/tests/parser/test_state.py
 spacy/tests/pipeline/__init__.py
 spacy/tests/pipeline/test_analysis.py
 spacy/tests/pipeline/test_annotates_on_update.py
 spacy/tests/pipeline/test_attributeruler.py
 spacy/tests/pipeline/test_edit_tree_lemmatizer.py
@@ -869,15 +867,14 @@
 spacy/tests/tokenizer/test_explain.py
 spacy/tests/tokenizer/test_naughty_strings.py
 spacy/tests/tokenizer/test_tokenizer.py
 spacy/tests/tokenizer/test_urls.py
 spacy/tests/tokenizer/test_whitespace.py
 spacy/tests/training/__init__.py
 spacy/tests/training/test_augmenters.py
-spacy/tests/training/test_corpus.py
 spacy/tests/training/test_logger.py
 spacy/tests/training/test_new_example.py
 spacy/tests/training/test_pretraining.py
 spacy/tests/training/test_readers.py
 spacy/tests/training/test_rehearse.py
 spacy/tests/training/test_training.py
 spacy/tests/vocab_vectors/__init__.py
@@ -885,32 +882,32 @@
 spacy/tests/vocab_vectors/test_lookups.py
 spacy/tests/vocab_vectors/test_similarity.py
 spacy/tests/vocab_vectors/test_stringstore.py
 spacy/tests/vocab_vectors/test_vectors.py
 spacy/tests/vocab_vectors/test_vocab_api.py
 spacy/tokens/__init__.pxd
 spacy/tokens/__init__.py
-spacy/tokens/_dict_proxies.py
-spacy/tokens/_retokenize.pyi
-spacy/tokens/_retokenize.pyx
-spacy/tokens/_serialize.py
 spacy/tokens/doc.pxd
 spacy/tokens/doc.pyi
 spacy/tokens/doc.pyx
+spacy/tokens/doc_bin.py
 spacy/tokens/graph.pxd
 spacy/tokens/graph.pyx
 spacy/tokens/morphanalysis.pxd
 spacy/tokens/morphanalysis.pyi
 spacy/tokens/morphanalysis.pyx
+spacy/tokens/retokenizer.pyi
+spacy/tokens/retokenizer.pyx
 spacy/tokens/span.pxd
 spacy/tokens/span.pyi
 spacy/tokens/span.pyx
 spacy/tokens/span_group.pxd
 spacy/tokens/span_group.pyi
 spacy/tokens/span_group.pyx
+spacy/tokens/span_groups.py
 spacy/tokens/token.pxd
 spacy/tokens/token.pyi
 spacy/tokens/token.pyx
 spacy/tokens/underscore.py
 spacy/training/__init__.pxd
 spacy/training/__init__.py
 spacy/training/align.pyx
```

### Comparing `spacy-3.6.0.dev0/spacy.egg-info/requires.txt` & `spacy-4.0.0.dev0/spacy.egg-info/requires.txt`

 * *Files 15% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 spacy-legacy<3.1.0,>=3.0.11
 spacy-loggers<2.0.0,>=1.0.0
 murmurhash<1.1.0,>=0.28.0
 cymem<2.1.0,>=2.0.2
 preshed<3.1.0,>=3.0.2
-thinc<8.2.0,>=8.1.8
+thinc<9.1.0,>=9.0.0.dev2
 wasabi<1.2.0,>=0.9.1
 srsly<3.0.0,>=2.4.3
 catalogue<2.1.0,>=2.0.6
 typer<0.8.0,>=0.3.0
 pathy>=0.10.0
 smart-open<7.0.0,>=5.2.1
 tqdm<5.0.0,>=4.38.0
@@ -16,79 +16,79 @@
 pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4
 jinja2
 setuptools
 packaging>=20.0
 langcodes<4.0.0,>=3.2.0
 
 [:python_version < "3.8"]
-typing_extensions<4.5.0,>=3.7.4.1
+typing_extensions<4.2.0,>=3.7.4
 
 [apple]
 thinc-apple-ops<1.0.0,>=0.1.0.dev0
 
 [cuda]
-cupy<13.0.0,>=5.0.0b4
+cupy<12.0.0,>=5.0.0b4
 
 [cuda-autodetect]
-cupy-wheel<13.0.0,>=11.0.0
+cupy-wheel<12.0.0,>=11.0.0
 
 [cuda100]
-cupy-cuda100<13.0.0,>=5.0.0b4
+cupy-cuda100<12.0.0,>=5.0.0b4
 
 [cuda101]
-cupy-cuda101<13.0.0,>=5.0.0b4
+cupy-cuda101<12.0.0,>=5.0.0b4
 
 [cuda102]
-cupy-cuda102<13.0.0,>=5.0.0b4
+cupy-cuda102<12.0.0,>=5.0.0b4
 
 [cuda110]
-cupy-cuda110<13.0.0,>=5.0.0b4
+cupy-cuda110<12.0.0,>=5.0.0b4
 
 [cuda111]
-cupy-cuda111<13.0.0,>=5.0.0b4
+cupy-cuda111<12.0.0,>=5.0.0b4
 
 [cuda112]
-cupy-cuda112<13.0.0,>=5.0.0b4
+cupy-cuda112<12.0.0,>=5.0.0b4
 
 [cuda113]
-cupy-cuda113<13.0.0,>=5.0.0b4
+cupy-cuda113<12.0.0,>=5.0.0b4
 
 [cuda114]
-cupy-cuda114<13.0.0,>=5.0.0b4
+cupy-cuda114<12.0.0,>=5.0.0b4
 
 [cuda115]
-cupy-cuda115<13.0.0,>=5.0.0b4
+cupy-cuda115<12.0.0,>=5.0.0b4
 
 [cuda116]
-cupy-cuda116<13.0.0,>=5.0.0b4
+cupy-cuda116<12.0.0,>=5.0.0b4
 
 [cuda117]
-cupy-cuda117<13.0.0,>=5.0.0b4
+cupy-cuda117<12.0.0,>=5.0.0b4
 
 [cuda11x]
-cupy-cuda11x<13.0.0,>=11.0.0
+cupy-cuda11x<12.0.0,>=11.0.0
 
 [cuda80]
-cupy-cuda80<13.0.0,>=5.0.0b4
+cupy-cuda80<12.0.0,>=5.0.0b4
 
 [cuda90]
-cupy-cuda90<13.0.0,>=5.0.0b4
+cupy-cuda90<12.0.0,>=5.0.0b4
 
 [cuda91]
-cupy-cuda91<13.0.0,>=5.0.0b4
+cupy-cuda91<12.0.0,>=5.0.0b4
 
 [cuda92]
-cupy-cuda92<13.0.0,>=5.0.0b4
+cupy-cuda92<12.0.0,>=5.0.0b4
 
 [ja]
 sudachipy!=0.6.1,>=0.5.2
 sudachidict_core>=20211220
 
 [ko]
-natto-py>=0.9.0
+mecab-ko>=1.0.0
 
 [lookups]
 spacy_lookups_data<1.1.0,>=1.0.3
 
 [ray]
 spacy_ray<1.0.0,>=0.1.0
```

