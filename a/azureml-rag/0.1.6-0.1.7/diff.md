# Comparing `tmp/azureml_rag-0.1.6-py3-none-any.whl.zip` & `tmp/azureml_rag-0.1.7-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,60 +1,60 @@
-Zip file size: 170102 bytes, number of entries: 58
--rw-rw-rw-  2.0 fat      246 b- defN 23-Jun-01 06:13 azureml/rag/__init__.py
--rw-rw-rw-  2.0 fat    39881 b- defN 23-Jun-01 06:13 azureml/rag/documents.py
--rw-rw-rw-  2.0 fat    29244 b- defN 23-Jun-01 06:13 azureml/rag/embeddings.py
--rw-rw-rw-  2.0 fat     5058 b- defN 23-Jun-01 06:13 azureml/rag/mlindex.py
--rw-rw-rw-  2.0 fat     4149 b- defN 23-Jun-01 06:13 azureml/rag/models.py
--rw-rw-rw-  2.0 fat     4646 b- defN 23-Jun-01 06:14 azureml/rag/_asset_client/client.py
--rw-rw-rw-  2.0 fat      893 b- defN 23-Jun-01 06:15 azureml/rag/_asset_client/_restclient/__init__.py
--rw-rw-rw-  2.0 fat     4381 b- defN 23-Jun-01 06:15 azureml/rag/_asset_client/_restclient/_azure_machine_learning_workspaces.py
--rw-rw-rw-  2.0 fat     3538 b- defN 23-Jun-01 06:15 azureml/rag/_asset_client/_restclient/_configuration.py
--rw-rw-rw-  2.0 fat     1561 b- defN 23-Jun-01 06:15 azureml/rag/_asset_client/_restclient/_patch.py
--rw-rw-rw-  2.0 fat      495 b- defN 23-Jun-01 06:15 azureml/rag/_asset_client/_restclient/_version.py
--rw-rw-rw-  2.0 fat      399 b- defN 23-Jun-01 06:15 azureml/rag/_asset_client/_restclient/models.py
--rw-rw-rw-  2.0 fat      957 b- defN 23-Jun-01 06:16 azureml/rag/_asset_client/_restclient/dataset/__init__.py
--rw-rw-rw-  2.0 fat     3802 b- defN 23-Jun-01 06:16 azureml/rag/_asset_client/_restclient/dataset/_azure_machine_learning_workspaces.py
--rw-rw-rw-  2.0 fat     3144 b- defN 23-Jun-01 06:16 azureml/rag/_asset_client/_restclient/dataset/_configuration.py
--rw-rw-rw-  2.0 fat      694 b- defN 23-Jun-01 06:16 azureml/rag/_asset_client/_restclient/dataset/_patch.py
--rw-rw-rw-  2.0 fat    81019 b- defN 23-Jun-01 06:16 azureml/rag/_asset_client/_restclient/dataset/_serialization.py
--rw-rw-rw-  2.0 fat     1833 b- defN 23-Jun-01 06:16 azureml/rag/_asset_client/_restclient/dataset/_vendor.py
--rw-rw-rw-  2.0 fat      495 b- defN 23-Jun-01 06:16 azureml/rag/_asset_client/_restclient/dataset/_version.py
--rw-rw-rw-  2.0 fat     6787 b- defN 23-Jun-01 06:16 azureml/rag/_asset_client/_restclient/dataset/models/__init__.py
--rw-rw-rw-  2.0 fat     4635 b- defN 23-Jun-01 06:16 azureml/rag/_asset_client/_restclient/dataset/models/_azure_machine_learning_workspaces_enums.py
--rw-rw-rw-  2.0 fat   131448 b- defN 23-Jun-01 06:16 azureml/rag/_asset_client/_restclient/dataset/models/_models_py3.py
--rw-rw-rw-  2.0 fat      694 b- defN 23-Jun-01 06:16 azureml/rag/_asset_client/_restclient/dataset/models/_patch.py
--rw-rw-rw-  2.0 fat      585 b- defN 23-Jun-01 06:16 azureml/rag/_asset_client/_restclient/dataset/operations/__init__.py
--rw-rw-rw-  2.0 fat   104636 b- defN 23-Jun-01 06:16 azureml/rag/_asset_client/_restclient/dataset/operations/_data_version_operations.py
--rw-rw-rw-  2.0 fat    80785 b- defN 23-Jun-01 06:16 azureml/rag/_asset_client/_restclient/dataset/operations/_mlindex_operations.py
--rw-rw-rw-  2.0 fat      893 b- defN 23-Jun-01 06:16 azureml/rag/_asset_client/_restclient/runhistory/__init__.py
--rw-rw-rw-  2.0 fat     3902 b- defN 23-Jun-01 06:16 azureml/rag/_asset_client/_restclient/runhistory/_azure_machine_learning_workspaces.py
--rw-rw-rw-  2.0 fat     3226 b- defN 23-Jun-01 06:16 azureml/rag/_asset_client/_restclient/runhistory/_configuration.py
--rw-rw-rw-  2.0 fat     1561 b- defN 23-Jun-01 06:16 azureml/rag/_asset_client/_restclient/runhistory/_patch.py
--rw-rw-rw-  2.0 fat     1255 b- defN 23-Jun-01 06:16 azureml/rag/_asset_client/_restclient/runhistory/_vendor.py
--rw-rw-rw-  2.0 fat      495 b- defN 23-Jun-01 06:16 azureml/rag/_asset_client/_restclient/runhistory/_version.py
--rw-rw-rw-  2.0 fat    11705 b- defN 23-Jun-01 06:16 azureml/rag/_asset_client/_restclient/runhistory/models/__init__.py
--rw-rw-rw-  2.0 fat     2566 b- defN 23-Jun-01 06:16 azureml/rag/_asset_client/_restclient/runhistory/models/_azure_machine_learning_workspaces_enums.py
--rw-rw-rw-  2.0 fat   175170 b- defN 23-Jun-01 06:16 azureml/rag/_asset_client/_restclient/runhistory/models/_models.py
--rw-rw-rw-  2.0 fat   188776 b- defN 23-Jun-01 06:16 azureml/rag/_asset_client/_restclient/runhistory/models/_models_py3.py
--rw-rw-rw-  2.0 fat      563 b- defN 23-Jun-01 06:16 azureml/rag/_asset_client/_restclient/runhistory/operations/__init__.py
--rw-rw-rw-  2.0 fat   162796 b- defN 23-Jun-01 06:16 azureml/rag/_asset_client/_restclient/runhistory/operations/_runs_operations.py
--rw-rw-rw-  2.0 fat    10785 b- defN 23-Jun-01 06:14 azureml/rag/langchain/acs.py
--rw-rw-rw-  2.0 fat      183 b- defN 23-Jun-01 06:14 azureml/rag/tasks/__init__.py
--rw-rw-rw-  2.0 fat     2318 b- defN 23-Jun-01 06:14 azureml/rag/tasks/build_faiss.py
--rw-rw-rw-  2.0 fat     7945 b- defN 23-Jun-01 06:14 azureml/rag/tasks/crack_and_chunk.py
--rw-rw-rw-  2.0 fat     7502 b- defN 23-Jun-01 06:14 azureml/rag/tasks/embed.py
--rw-rw-rw-  2.0 fat     6328 b- defN 23-Jun-01 06:14 azureml/rag/tasks/embed_prs.py
--rw-rw-rw-  2.0 fat     2340 b- defN 23-Jun-01 06:14 azureml/rag/tasks/git_clone.py
--rw-rw-rw-  2.0 fat     3002 b- defN 23-Jun-01 06:14 azureml/rag/tasks/register_mlindex.py
--rw-rw-rw-  2.0 fat    19067 b- defN 23-Jun-01 06:14 azureml/rag/tasks/update_acs.py
--rw-rw-rw-  2.0 fat      208 b- defN 23-Jun-01 06:14 azureml/rag/utils/__init__.py
--rw-rw-rw-  2.0 fat       98 b- defN 23-Jun-01 06:14 azureml/rag/utils/_telemetry.json
--rw-rw-rw-  2.0 fat     1588 b- defN 23-Jun-01 06:14 azureml/rag/utils/azureml.py
--rw-rw-rw-  2.0 fat     7435 b- defN 23-Jun-01 06:14 azureml/rag/utils/connections.py
--rw-rw-rw-  2.0 fat     2634 b- defN 23-Jun-01 06:14 azureml/rag/utils/git.py
--rw-rw-rw-  2.0 fat     8529 b- defN 23-Jun-01 06:14 azureml/rag/utils/logging.py
--rw-rw-rw-  2.0 fat     1021 b- defN 23-Jun-01 06:21 azureml_rag-0.1.6.dist-info/LICENSE.txt
--rw-rw-rw-  2.0 fat     5572 b- defN 23-Jun-01 06:21 azureml_rag-0.1.6.dist-info/METADATA
--rw-rw-rw-  2.0 fat       97 b- defN 23-Jun-01 06:21 azureml_rag-0.1.6.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        8 b- defN 23-Jun-01 06:21 azureml_rag-0.1.6.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     6151 b- defN 23-Jun-01 06:21 azureml_rag-0.1.6.dist-info/RECORD
-58 files, 1161724 bytes uncompressed, 159866 bytes compressed:  86.2%
+Zip file size: 172843 bytes, number of entries: 58
+-rw-rw-rw-  2.0 fat      246 b- defN 23-Jun-09 01:00 azureml/rag/__init__.py
+-rw-rw-rw-  2.0 fat    39881 b- defN 23-Jun-09 01:00 azureml/rag/documents.py
+-rw-rw-rw-  2.0 fat    33664 b- defN 23-Jun-09 01:00 azureml/rag/embeddings.py
+-rw-rw-rw-  2.0 fat     5058 b- defN 23-Jun-09 01:00 azureml/rag/mlindex.py
+-rw-rw-rw-  2.0 fat     4149 b- defN 23-Jun-09 01:00 azureml/rag/models.py
+-rw-rw-rw-  2.0 fat     4646 b- defN 23-Jun-09 01:01 azureml/rag/_asset_client/client.py
+-rw-rw-rw-  2.0 fat      893 b- defN 23-Jun-09 01:02 azureml/rag/_asset_client/_restclient/__init__.py
+-rw-rw-rw-  2.0 fat     4381 b- defN 23-Jun-09 01:02 azureml/rag/_asset_client/_restclient/_azure_machine_learning_workspaces.py
+-rw-rw-rw-  2.0 fat     3538 b- defN 23-Jun-09 01:02 azureml/rag/_asset_client/_restclient/_configuration.py
+-rw-rw-rw-  2.0 fat     1561 b- defN 23-Jun-09 01:02 azureml/rag/_asset_client/_restclient/_patch.py
+-rw-rw-rw-  2.0 fat      495 b- defN 23-Jun-09 01:02 azureml/rag/_asset_client/_restclient/_version.py
+-rw-rw-rw-  2.0 fat      399 b- defN 23-Jun-09 01:02 azureml/rag/_asset_client/_restclient/models.py
+-rw-rw-rw-  2.0 fat      957 b- defN 23-Jun-09 01:02 azureml/rag/_asset_client/_restclient/dataset/__init__.py
+-rw-rw-rw-  2.0 fat     3802 b- defN 23-Jun-09 01:02 azureml/rag/_asset_client/_restclient/dataset/_azure_machine_learning_workspaces.py
+-rw-rw-rw-  2.0 fat     3144 b- defN 23-Jun-09 01:02 azureml/rag/_asset_client/_restclient/dataset/_configuration.py
+-rw-rw-rw-  2.0 fat      694 b- defN 23-Jun-09 01:02 azureml/rag/_asset_client/_restclient/dataset/_patch.py
+-rw-rw-rw-  2.0 fat    81019 b- defN 23-Jun-09 01:02 azureml/rag/_asset_client/_restclient/dataset/_serialization.py
+-rw-rw-rw-  2.0 fat     1833 b- defN 23-Jun-09 01:02 azureml/rag/_asset_client/_restclient/dataset/_vendor.py
+-rw-rw-rw-  2.0 fat      495 b- defN 23-Jun-09 01:02 azureml/rag/_asset_client/_restclient/dataset/_version.py
+-rw-rw-rw-  2.0 fat     6787 b- defN 23-Jun-09 01:03 azureml/rag/_asset_client/_restclient/dataset/models/__init__.py
+-rw-rw-rw-  2.0 fat     4635 b- defN 23-Jun-09 01:03 azureml/rag/_asset_client/_restclient/dataset/models/_azure_machine_learning_workspaces_enums.py
+-rw-rw-rw-  2.0 fat   131448 b- defN 23-Jun-09 01:03 azureml/rag/_asset_client/_restclient/dataset/models/_models_py3.py
+-rw-rw-rw-  2.0 fat      694 b- defN 23-Jun-09 01:03 azureml/rag/_asset_client/_restclient/dataset/models/_patch.py
+-rw-rw-rw-  2.0 fat      585 b- defN 23-Jun-09 01:03 azureml/rag/_asset_client/_restclient/dataset/operations/__init__.py
+-rw-rw-rw-  2.0 fat   104636 b- defN 23-Jun-09 01:03 azureml/rag/_asset_client/_restclient/dataset/operations/_data_version_operations.py
+-rw-rw-rw-  2.0 fat    80785 b- defN 23-Jun-09 01:03 azureml/rag/_asset_client/_restclient/dataset/operations/_mlindex_operations.py
+-rw-rw-rw-  2.0 fat      893 b- defN 23-Jun-09 01:02 azureml/rag/_asset_client/_restclient/runhistory/__init__.py
+-rw-rw-rw-  2.0 fat     3902 b- defN 23-Jun-09 01:02 azureml/rag/_asset_client/_restclient/runhistory/_azure_machine_learning_workspaces.py
+-rw-rw-rw-  2.0 fat     3226 b- defN 23-Jun-09 01:02 azureml/rag/_asset_client/_restclient/runhistory/_configuration.py
+-rw-rw-rw-  2.0 fat     1561 b- defN 23-Jun-09 01:02 azureml/rag/_asset_client/_restclient/runhistory/_patch.py
+-rw-rw-rw-  2.0 fat     1255 b- defN 23-Jun-09 01:02 azureml/rag/_asset_client/_restclient/runhistory/_vendor.py
+-rw-rw-rw-  2.0 fat      495 b- defN 23-Jun-09 01:02 azureml/rag/_asset_client/_restclient/runhistory/_version.py
+-rw-rw-rw-  2.0 fat    11705 b- defN 23-Jun-09 01:03 azureml/rag/_asset_client/_restclient/runhistory/models/__init__.py
+-rw-rw-rw-  2.0 fat     2566 b- defN 23-Jun-09 01:03 azureml/rag/_asset_client/_restclient/runhistory/models/_azure_machine_learning_workspaces_enums.py
+-rw-rw-rw-  2.0 fat   175170 b- defN 23-Jun-09 01:03 azureml/rag/_asset_client/_restclient/runhistory/models/_models.py
+-rw-rw-rw-  2.0 fat   188776 b- defN 23-Jun-09 01:03 azureml/rag/_asset_client/_restclient/runhistory/models/_models_py3.py
+-rw-rw-rw-  2.0 fat      563 b- defN 23-Jun-09 01:03 azureml/rag/_asset_client/_restclient/runhistory/operations/__init__.py
+-rw-rw-rw-  2.0 fat   162796 b- defN 23-Jun-09 01:03 azureml/rag/_asset_client/_restclient/runhistory/operations/_runs_operations.py
+-rw-rw-rw-  2.0 fat    10785 b- defN 23-Jun-09 01:01 azureml/rag/langchain/acs.py
+-rw-rw-rw-  2.0 fat      183 b- defN 23-Jun-09 01:01 azureml/rag/tasks/__init__.py
+-rw-rw-rw-  2.0 fat     2289 b- defN 23-Jun-09 01:01 azureml/rag/tasks/build_faiss.py
+-rw-rw-rw-  2.0 fat     9822 b- defN 23-Jun-09 01:01 azureml/rag/tasks/crack_and_chunk.py
+-rw-rw-rw-  2.0 fat     7502 b- defN 23-Jun-09 01:01 azureml/rag/tasks/embed.py
+-rw-rw-rw-  2.0 fat     6121 b- defN 23-Jun-09 01:01 azureml/rag/tasks/embed_prs.py
+-rw-rw-rw-  2.0 fat     3239 b- defN 23-Jun-09 01:01 azureml/rag/tasks/git_clone.py
+-rw-rw-rw-  2.0 fat     3327 b- defN 23-Jun-09 01:01 azureml/rag/tasks/register_mlindex.py
+-rw-rw-rw-  2.0 fat    21619 b- defN 23-Jun-09 01:01 azureml/rag/tasks/update_acs.py
+-rw-rw-rw-  2.0 fat      208 b- defN 23-Jun-09 01:01 azureml/rag/utils/__init__.py
+-rw-rw-rw-  2.0 fat       98 b- defN 23-Jun-09 01:01 azureml/rag/utils/_telemetry.json
+-rw-rw-rw-  2.0 fat     1588 b- defN 23-Jun-09 01:01 azureml/rag/utils/azureml.py
+-rw-rw-rw-  2.0 fat     7435 b- defN 23-Jun-09 01:01 azureml/rag/utils/connections.py
+-rw-rw-rw-  2.0 fat     2634 b- defN 23-Jun-09 01:01 azureml/rag/utils/git.py
+-rw-rw-rw-  2.0 fat    10082 b- defN 23-Jun-09 01:01 azureml/rag/utils/logging.py
+-rw-rw-rw-  2.0 fat     1021 b- defN 23-Jun-09 01:07 azureml_rag-0.1.7.dist-info/LICENSE.txt
+-rw-rw-rw-  2.0 fat     5571 b- defN 23-Jun-09 01:07 azureml_rag-0.1.7.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       97 b- defN 23-Jun-09 01:07 azureml_rag-0.1.7.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        8 b- defN 23-Jun-09 01:07 azureml_rag-0.1.7.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     6152 b- defN 23-Jun-09 01:07 azureml_rag-0.1.7.dist-info/RECORD
+58 files, 1173114 bytes uncompressed, 162607 bytes compressed:  86.1%
```

## zipnote {}

```diff
@@ -153,23 +153,23 @@
 
 Filename: azureml/rag/utils/git.py
 Comment: 
 
 Filename: azureml/rag/utils/logging.py
 Comment: 
 
-Filename: azureml_rag-0.1.6.dist-info/LICENSE.txt
+Filename: azureml_rag-0.1.7.dist-info/LICENSE.txt
 Comment: 
 
-Filename: azureml_rag-0.1.6.dist-info/METADATA
+Filename: azureml_rag-0.1.7.dist-info/METADATA
 Comment: 
 
-Filename: azureml_rag-0.1.6.dist-info/WHEEL
+Filename: azureml_rag-0.1.7.dist-info/WHEEL
 Comment: 
 
-Filename: azureml_rag-0.1.6.dist-info/top_level.txt
+Filename: azureml_rag-0.1.7.dist-info/top_level.txt
 Comment: 
 
-Filename: azureml_rag-0.1.6.dist-info/RECORD
+Filename: azureml_rag-0.1.7.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## azureml/rag/embeddings.py

```diff
@@ -24,20 +24,95 @@
 import pyarrow.parquet as pq
 import time
 import yaml
 
 from azureml.rag.documents import LazyDocument
 from azureml.rag.models import parse_model_uri
 from azureml.rag.utils.connections import get_connection_credential, get_connection_by_id_v2, workspace_connection_to_credential
-from azureml.rag.utils.logging import get_logger, track_activity, track_error
+from azureml.rag.utils.logging import get_logger, track_activity
 
 
 logger = get_logger(__name__)
 
 
+def patch_openai_embedding_retries(logger, activity_logger, max_seconds_retrying=540):
+    """Patch the openai embedding to retry on failure."""""
+    from datetime import datetime
+    from langchain.embeddings import openai as langchain_openai
+    from tenacity import (
+        retry,
+        retry_if_exception_type,
+        stop_after_attempt,
+        stop_after_delay,
+        wait_exponential,
+    )
+    from tenacity.stop import stop_base
+
+    def _log_it(retry_state) -> None:
+        if retry_state.outcome.failed:
+            ex = retry_state.outcome.exception()
+            verb, value = "raised", f"{ex.__class__.__name__}: {ex}"
+        else:
+            verb, value = "returned", retry_state.outcome.result()
+        logger.warning(
+            f"Retrying _embed_with_retry " f"in {retry_state.next_action.sleep} seconds as it {verb} {value}.",
+        )
+
+        if 'num_retries' not in activity_logger.activity_info:
+            activity_logger.activity_info['num_retries'] = 0
+            activity_logger.activity_info['time_spent_sleeping'] = 0
+        activity_logger.activity_info['num_retries'] += 1
+        activity_logger.activity_info['time_spent_sleeping'] += retry_state.idle_for
+
+        # This is a lot of data to send to telemetry, not sending by default for now due to fears of maxxing out daily ingress cap.
+        # if 'retries' not in activity_logger.activity_info:
+        #     activity_logger.activity_info['retries'] = []
+        #     activity_logger.activity_info['first_retry'] = datetime.utcnow()
+        # activity_logger.activity_info['retries'] += [json.dumps({'idx': retry_state.attempt_number, 'timestamp': str(datetime.utcnow()), 'sleep': retry_state.next_action.sleep})]
+
+    class stop_after_delay_that_works(stop_base):
+        """Stop when the time from the first attempt >= limit."""
+
+        def __init__(self, max_delay, activity_logger) -> None:
+            self.max_delay = max_delay
+            self.activity_logger = activity_logger
+
+        def __call__(self, retry_state) -> bool:
+            first_retry = self.activity_logger.activity_info.get('first_retry', None)
+            if first_retry:
+                return (datetime.utcnow() - first_retry).seconds >= self.max_delay
+            else:
+                False
+
+    # Copied from https://github.com/hwchase17/langchain/blob/511c12dd3985ce682226371c12f8fa70d8c9a8e1/langchain/embeddings/openai.py#L34
+    def _create_retry_decorator(embeddings):
+        import openai
+
+        min_seconds = 4
+        max_seconds = 10
+        # Wait 2^x * 1 second between each retry starting with
+        # 4 seconds, then up to 10 seconds, then 10 seconds afterwards
+        return retry(
+            reraise=True,
+            # stop=stop_after_attempt(embeddings.max_retries),
+            stop=stop_after_delay_that_works(max_seconds_retrying, activity_logger),
+            wait=wait_exponential(multiplier=1, min=min_seconds, max=max_seconds),
+            retry=(
+                retry_if_exception_type(openai.error.Timeout)
+                | retry_if_exception_type(openai.error.APIError)
+                | retry_if_exception_type(openai.error.APIConnectionError)
+                | retry_if_exception_type(openai.error.RateLimitError)
+                | retry_if_exception_type(openai.error.ServiceUnavailableError)
+            ),
+            before_sleep=_log_it,
+        )
+
+    langchain_openai._create_retry_decorator = _create_retry_decorator
+
+
 def _parse_open_ai_args(arguments: dict):
     from langchain.embeddings.openai import OpenAIEmbeddings
     import openai
 
     logger.info('OpenAI arguments: \n')
     logger.info('\n'.join(f'{k}={v}' for k, v in arguments.items()))
 
@@ -120,18 +195,32 @@
             del args["model_name"]
         elif "model" in arguments:
             model_name = arguments["model"]
             del args["model"]
         else:
             raise ValueError("HuggingFace embeddings require a model name.")
 
-        return HuggingFaceEmbeddings(model_name=model_name)
+        class ActivitySafeHuggingFaceEmbeddings(Embedder):
+            """HuggingFaceEmbeddings with kwargs argument to embed_doceuments to support loggers being passed in."""
+            def __init__(self, embeddings):
+                """Initialize the ActivitySafeHuggingFaceEmbeddings."""
+                self.embeddings = embeddings
+
+            def embed_documents(self, documents: List[str], **kwargs) -> List[List[float]]:
+                """Embed the given documents."""
+                return self.embeddings.embed_documents(documents)
+
+            def embed_query(self, query: str) -> List[float]:
+                """Embed the given query."""
+                return self.embeddings.embed_query(query)
+
+        return ActivitySafeHuggingFaceEmbeddings(HuggingFaceEmbeddings(model_name=model_name))
     elif embedding_kind == "none":
         class NoneEmbeddings(Embedder):
-            def embed_documents(self, documents: List[str]) -> List[List[float]]:
+            def embed_documents(self, documents: List[str], **kwargs) -> List[List[float]]:
                 return [[]] * len(documents)
 
             def embed_query(self, query: str) -> List[float]:
                 return []
 
         return NoneEmbeddings()
     elif embedding_kind == "custom":
@@ -139,30 +228,32 @@
 
 
 def get_embed_fn(embedding_kind: str, arguments: dict) -> Callable[[List[str]], List[List[float]]]:
     """Get an embedding function from the given arguments."""
     if "open_ai" in embedding_kind:
         embedder = _parse_open_ai_args(arguments)
 
-        def embed(texts: List[str]) -> List[List[float]]:
+        def embed(texts: List[str], activity_logger=None) -> List[List[float]]:
             # AOAI doesn't allow batch_size > 1 so we serialize embedding here to improve error handling
+            patch_openai_embedding_retries(logger, activity_logger)
             embeddings = []
             pre_batch = None
             for i in range(0, len(texts), embedder.chunk_size):
                 texts_chunk = texts[i:i + embedder.chunk_size]
                 try:
                     pre_batch = time.time()
                     embeddings.extend(embedder.embed_documents(texts_chunk))
                 except Exception as e:
                     if pre_batch:
                         duration = time.time() - pre_batch
                     else:
                         duration = 0
                     logger.error(f'Failed to embed after {duration}s:\n{e}.', exc_info=e, extra={'print': True})
-                    track_error(logger, 'open_ai.embed', {'batch_size': embedder.chunk_size, 'duration': duration})
+                    if activity_logger:
+                        activity_logger.error("Failed to embed", extra={'properties': {'batch_size': embedder.chunk_size, 'duration': duration, 'embedding_kind': embedding_kind}})
                     print(f'Failed texts: {texts_chunk}\nlengths: {[len(t) for t in texts_chunk]}\n')
                     raise e
             return embeddings
 
         return embed
     elif embedding_kind == "hugging_face":
         embedder = get_langchain_embeddings(embedding_kind, arguments)
@@ -612,16 +703,16 @@
                 'Embeddings.embed',
                 custom_dimensions={
                     'documents_to_embed': len(documents_to_embed),
                     'reused_documents': len(documents_embedded.keys()),
                     'kind': self.kind,
                     'model': self.arguments.get('model', ''),
                 }
-            ):
-                embeddings = self._embed_fn(data_to_embed)
+            ) as activity_logger:
+                embeddings = self._embed_fn(data_to_embed, activity_logger=activity_logger)
         except Exception as e:
             logger.error(f'Failed to get embeddings with error: {e}')
             raise
 
         for ((doc_id, mtime, document_data, document_hash, document_metadata), embeddings) in zip(documents_to_embed, embeddings):
             documents_embedded[doc_id] = \
                 DataEmbeddedDocument(doc_id, mtime, document_hash,
@@ -632,22 +723,22 @@
     def as_faiss_index(self):
         """Returns a FAISS index that can be used to query the embeddings."""
         from langchain.docstore.in_memory import InMemoryDocstore
         from langchain.vectorstores import FAISS
         from langchain.vectorstores.faiss import dependable_faiss_import
         import numpy as np
 
-        logger.info("Building index", extra={'print': True})
+        logger.info("Building index")
         t1 = time.time()
         num_source_docs = 0
         documents = []
         embeddings = []
         for doc_id, emb_doc in self._document_embeddings.items():
-            logger.info(f'Adding document: {doc_id}', extra={'print': True})
-            logger.debug(f'{doc_id},{emb_doc.document_hash},{emb_doc.get_embeddings()[0:20]}', extra={'print': True})
+            logger.info(f'Adding document: {doc_id}')
+            logger.debug(f'{doc_id},{emb_doc.document_hash},{emb_doc.get_embeddings()[0:20]}')
             embeddings.append(emb_doc.get_embeddings())
             # TODO: LazyDocument/RefDocument gets uri to page_content
             documents.append(
                 Document(
                     page_content=emb_doc.get_data(),
                     metadata={
                         "source_doc_id": doc_id,
@@ -664,23 +755,23 @@
             {index_to_id[i]: doc for i, doc in enumerate(documents)}
         )
 
         faiss = dependable_faiss_import()
         index = faiss.IndexFlatL2(len(embeddings[0]))
         index.add(np.array(embeddings, dtype=np.float32))
 
-        logger.info(f"Built index from {num_source_docs} documents and {len(embeddings)} chunks, took {time.time()-t1:.4f} seconds", extra={'print': True})
+        logger.info(f"Built index from {num_source_docs} documents and {len(embeddings)} chunks, took {time.time()-t1:.4f} seconds")
 
         return FAISS(self.get_query_embed_fn(), index, docstore, index_to_id)
 
     def write_as_faiss_mlindex(self, output_path: Path):
         """Writes the embeddings to a FAISS MLIndex file."""
         faiss_index = self.as_faiss_index()
 
-        logger.info("Saving index", extra={'print': True})
+        logger.info("Saving index")
         faiss_index.save_local(str(output_path))
 
         mlindex_config = {
             "embeddings": self.get_metadata()
         }
         mlindex_config["index"] = {
             "kind": "faiss",
```

## azureml/rag/tasks/build_faiss.py

```diff
@@ -21,30 +21,30 @@
     args = parser.parse_args()
 
     print('\n'.join(f'{k}={v}' for k, v in vars(args).items()))
 
     enable_stdout_logging()
     enable_appinsights_logging()
 
-    raw_embeddings_uri = args.embeddings
-    logger.info(f'got embeddings uri as input: {raw_embeddings_uri}', extra={'print': True})
-    splits = raw_embeddings_uri.split('/')
-    embeddings_dir_name = splits.pop(len(splits)-2)
-    logger.info(f'extracted embeddings directory name: {embeddings_dir_name}', extra={'print': True})
-    parent = '/'.join(splits)
-    logger.info(f'extracted embeddings container path: {parent}', extra={'print': True})
-
-    # Mock OPENAI_API_KEY being set so that loading Embeddings doesn't fail, we don't need to do any embedding so should be fine
-    os.environ['OPENAI_API_KEY'] = 'nope'
-
-    from azureml.dataprep.fuse.dprepfuse import (MountOptions, rslex_uri_volume_mount)
-    mnt_options = MountOptions(
-        default_permission=0o555, allow_other=False, read_only=True)
-    logger.info(f'mounting embeddings container from: \n{parent} \n   to: \n{os.getcwd()}/raw_embeddings', extra={'print': True})
-    with rslex_uri_volume_mount(parent, f'{os.getcwd()}/raw_embeddings', options=mnt_options) as mount_context:
-        logger.info("Loading Embeddings", extra={'print': True})
-        emb = Embeddings.load(embeddings_dir_name, mount_context.mount_point)
-
-        with track_activity(logger, 'update_acs', custom_dimensions={'num_embeddings': len(emb._document_embeddings)}) as activity_logger:
+    with track_activity(logger, 'build_faiss') as activity_logger:
+        raw_embeddings_uri = args.embeddings
+        logger.info(f'got embeddings uri as input: {raw_embeddings_uri}')
+        splits = raw_embeddings_uri.split('/')
+        embeddings_dir_name = splits.pop(len(splits)-2)
+        logger.info(f'extracted embeddings directory name: {embeddings_dir_name}')
+        parent = '/'.join(splits)
+        logger.info(f'extracted embeddings container path: {parent}')
+
+        # Mock OPENAI_API_KEY being set so that loading Embeddings doesn't fail, we don't need to do any embedding so should be fine
+        os.environ['OPENAI_API_KEY'] = 'nope'
+
+        from azureml.dataprep.fuse.dprepfuse import (MountOptions, rslex_uri_volume_mount)
+        mnt_options = MountOptions(
+            default_permission=0o555, allow_other=False, read_only=True)
+        logger.info(f'mounting embeddings container from: \n{parent} \n   to: \n{os.getcwd()}/raw_embeddings', extra={'print': True})
+        with rslex_uri_volume_mount(parent, f'{os.getcwd()}/raw_embeddings', options=mnt_options) as mount_context:
+            logger.info("Loading Embeddings")
+            emb = Embeddings.load(embeddings_dir_name, mount_context.mount_point)
+            activity_logger.activity_info["num_documents"] = len(emb._document_embeddings)
             emb.write_as_faiss_mlindex(Path(args.output))
 
-    logger.info('Generated FAISS index', extra={'print': True})
+        logger.info('Generated FAISS index')
```

## azureml/rag/tasks/crack_and_chunk.py

```diff
@@ -1,27 +1,28 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
+import json
 import openai
 import pandas as pd
 from pathlib import Path
 import time
+from typing import Iterator
+import re
 
 from azureml.rag.utils.logging import get_logger, enable_stdout_logging, enable_appinsights_logging, track_activity
-from azureml.rag.documents import SUPPORTED_EXTENSIONS, DocumentChunksIterator, split_documents, crack_documents
+from azureml.rag.documents import SUPPORTED_EXTENSIONS, DocumentChunksIterator, DocumentSource, split_documents, crack_documents
 from azureml.rag.models import parse_model_uri
 from azureml.rag.utils.azureml import get_secret_from_workspace
 
 
 logger = get_logger('crack_and_chunk')
 
 
 def chunks_to_dataframe(chunks) -> pd.DataFrame:
-    import json
-
     metadata = []
     data = []
     for chunk in chunks:
         metadata.append(json.dumps(chunk.get_metadata()))
         data.append(chunk.load_data())
     #(metadata, data) = [(json.dumps(chunk.metadata), chunk.load_data()) for chunk in chunks]
     chunks_dict = {
@@ -73,65 +74,92 @@
 
     enable_stdout_logging()
     enable_appinsights_logging()
 
     summary_model_config = None
     include_summary = (args.include_summary == "True" or args.include_summary == "true")
     if include_summary:
-        import json
-
         try:
             summary_model_config = json.loads(args.summary_model_config)
             # TODO: For back compat, remove in a few weeks.
             summary_model_config['kind'] = summary_model_config['type']
             del summary_model_config['type']
             summary_model_config['model'] = summary_model_config['model_name']
             del summary_model_config['model_name']
             if 'deployment_name' in summary_model_config:
                 summary_model_config['deployment'] = summary_model_config['deployment_name']
                 del summary_model_config['deployment_name']
         except json.decoder.JSONDecodeError:
             # Try parse as uri
             summary_model_config = parse_model_uri(args.summary_model)
 
-        logger.info(f"Using summary_model: {json.dumps(summary_model_config, indent=2)}", extra={'print': True})
+        logger.info(f"Using summary_model: {json.dumps(summary_model_config, indent=2)}")
         if summary_model_config.get("kind") == "azure_open_ai" or summary_model_config.get("api_type") == "azure":
             summary_model_config["key"] = get_secret_from_workspace("OPENAI-API-KEY")
             summary_model_config["kind"] = "open_ai"
             summary_model_config["api_type"] = "azure"
             summary_model_config["api_version"] = args.openai_api_version
             summary_model_config["api_base"] = summary_model_config.get('endpoint') if summary_model_config.get('endpoint') is not None else get_secret_from_workspace("OPENAI-API-BASE")
             openai.api_version = summary_model_config["api_version"]
             openai.api_type = summary_model_config["api_type"]
             openai.api_base = summary_model_config["api_base"]
             openai.api_key = summary_model_config["key"]
 
     splitter_args = {'chunk_size': args.chunk_size, 'chunk_overlap': args.chunk_overlap, 'use_rcts': args.use_rcts}
 
     with track_activity(logger, 'crack_and_chunk', custom_dimensions={**splitter_args}) as activity_logger:
+
+        def filter_and_log_extensions(sources: Iterator[DocumentSource], allowed_extensions=SUPPORTED_EXTENSIONS) -> Iterator[DocumentSource]:
+            """Filter out sources with extensions not in allowed_extensions."""
+            total_files = 0
+            skipped_files = 0
+            skipped_extensions = {}
+            kept_extension = {}
+            for source in sources:
+                total_files += 1
+                if allowed_extensions is not None:
+                    if source.path.suffix not in allowed_extensions:
+                        skipped_files += 1
+                        ext_skipped = skipped_extensions.get(source.path.suffix, 0)
+                        skipped_extensions[source.path.suffix] = ext_skipped + 1
+                        logger.debug(f'Filtering out extension "{source.path.suffix}" source: {source.filename}')
+                        continue
+                ext_kept = kept_extension.get(source.path.suffix, 0)
+                kept_extension[source.path.suffix] = ext_kept + 1
+                yield source
+            logger.info(f"[DocumentChunksIterator::filter_extensions] Filtered {skipped_files} files out of {total_files}")
+            activity_logger.activity_info['total_files'] = total_files
+            activity_logger.activity_info['skipped_files'] = skipped_files
+            activity_logger.activity_info['skipped_extensions'] = json.dumps(skipped_extensions)
+            activity_logger.activity_info['kept_extensions'] = json.dumps(kept_extension)
+
         chunked_documents = DocumentChunksIterator(
             files_source=args.input_data,
             glob=args.input_glob,
             base_url=args.data_source_url,
             document_path_replacement_regex=args.document_path_replacement_regex,
+            file_filter=filter_and_log_extensions,
             chunked_document_processors = [lambda docs: split_documents(docs, splitter_args=splitter_args)],
         )
         file_count = 0
         for document in chunked_documents:
             file_count += 1
             logger.info(f'Processing file: {document.source.filename}', extra={'print': True})
             # TODO: Ideally make it easy to limit number of files with a `- take: n` operation on input URI in MLTable
             if (args.max_sample_files != -1 and file_count >= args.max_sample_files):
                 logger.info(f"file count: {file_count} - reached max sample file count: {args.max_sample_files}", extra={'print': True})
                 break
             write_chunks_to_csv(chunks_to_dataframe(document.chunks), Path(args.output_title_chunk) / f"Chunks_{Path(document.source.filename).name}.csv")
         logger.info(f"Processed {file_count} files", extra={'print': True})
+        activity_logger.activity_info["file_count"] = file_count
 
         if file_count == 0:
             logger.info(f"No files found in {args.input_data} with glob {args.input_glob}", extra={'print': True})
+            activity_logger.activity_info["error"] = "No files found"
+            activity_logger.activity_info["glob"] = args.input_glob if re.match("^[*/\\\"']+$", args.input_glob) is not None else "[REDACTED]"
             raise ValueError(f"No files found in {args.input_data} with glob {args.input_glob}, no chunks produced.")
 
         file_count = 0
         if include_summary:
             chunked_documents = DocumentChunksIterator(
                 files_source=args.input_data,
                 glob=args.input_glob,
```

## azureml/rag/tasks/embed_prs.py

```diff
@@ -47,32 +47,32 @@
                     # list all folders in embeddings_container and find the latest one
                     try:
                         embeddings_container_dir_name = str(max([dir for dir in pathlib.Path(
                             mount_context.mount_point).glob('*') if dir.is_dir() and dir.name != os.environ['AZUREML_RUN_ID']], key=os.path.getmtime).name)
                     except Exception as e:
                         activity_logger.warn('Failed to get latest folder from embeddings_container.')
                         logger.warn(
-                            f'failed to get latest folder from {mount_context.mount_point} with {e}.', extra={'print': True})
+                            f'failed to get latest folder from {mount_context.mount_point} with {e}.')
                         pass
 
                     if embeddings_container_dir_name is not None:
                         logger.info(
-                            f'loading from previous embeddings from {embeddings_container_dir_name} in {mount_context.mount_point}', extra={'print': True})
+                            f'loading from previous embeddings from {embeddings_container_dir_name} in {mount_context.mount_point}')
                         try:
                             embeddings_container = Embeddings.load(
                                 embeddings_container_dir_name, mount_context.mount_point)
                             if hasattr(activity_logger, 'activity_info'):
                                 activity_logger.activity_info["completionStatus"] = "Success"
                         except Exception as e:
                             activity_logger.warn('Failed to load from embeddings_container_dir_name. Creating new Embeddings.')
                             logger.warn(
-                                f'Failed to load from previous embeddings with {e}.\nCreating new Embeddings.', extra={'print': True})
+                                f'Failed to load from previous embeddings with {e}.\nCreating new Embeddings.')
             except Exception as e:
                 activity_logger.warn('Failed to load from embeddings_container. Creating new Embeddings.')
-                logger.warn(f'Failed to load previous embeddings from mount with {e}, proceeding to create new embeddings.', extra={'print': True})
+                logger.warn(f'Failed to load previous embeddings from mount with {e}, proceeding to create new embeddings.')
 
     connection_args = {}
     connection_id = os.environ.get('AZUREML_WORKSPACE_CONNECTION_ID_AOAI')
     if connection_id is not None:
         connection_args['connection_type'] = 'workspace_connection'
         connection_args['connection'] = {'id': connection_id}
     else:
@@ -94,28 +94,28 @@
     """
     Embed minibatch of chunks.
 
     :param mini_batch: The list of files to be processed.
     :param output_data: The output folder to save data to.
     :param embeddings: The Embeddings object that should be used to embed new data.
     """
-    logger.info(f'run method start: {__file__}, run({mini_batch})', extra={'print': True})
-    logger.info(f'Task id: {mini_batch.task_id}', extra={'print': True})
+    logger.info(f'run method start: {__file__}, run({mini_batch})')
+    logger.info(f'Task id: {mini_batch.task_id}')
 
     # read chunks
     pre_embed = time.time()
     embeddings = embeddings.embed_and_create_new_instance(read_chunks_into_documents((pathlib.Path(p) for p in mini_batch)))
     post_embed = time.time()
-    logger.info(f"Embedding took {post_embed - pre_embed} seconds", extra={'print': True})
+    logger.info(f"Embedding took {post_embed - pre_embed} seconds")
 
     save_metadata = str(mini_batch.task_id) == '0'
     if save_metadata:
-        logger.info('Metadata will be saved', extra={'print': True})
+        logger.info('Metadata will be saved')
     else:
-        logger.info('Only data will be saved', extra={'print': True})
+        logger.info('Only data will be saved')
     embeddings.save(output_data, with_metadata=save_metadata, suffix=mini_batch.task_id)
 
 
 def run(mini_batch):
     """Embed minibatch of chunks."""
     _run_internal(mini_batch, output_data, embeddings_container)
     return pd.DataFrame({"Files": [os.path.split(file)[-1] for file in mini_batch]})
```

## azureml/rag/tasks/git_clone.py

```diff
@@ -1,11 +1,12 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 import argparse
+import git
 import os
 
 from azureml.rag.utils.git import clone_repo, get_keyvault_authentication
 from azureml.rag.utils.logging import get_logger, enable_stdout_logging, enable_appinsights_logging, track_activity
 
 logger = get_logger('git_clone')
 
@@ -17,25 +18,42 @@
     parser.add_argument("--authentication-key-prefix", type=str, required=False, default=None, help="<PREFIX>-USER and <PREFIX>-PASS are the expected names of two Secrets in the Workspace Key Vault which will be used for authenticated when pulling the given git repo.")
     parser.add_argument("--output-data", type=str, required=True, dest='output_data')
     args = parser.parse_args()
 
     enable_stdout_logging()
     enable_appinsights_logging()
 
-    connection_id = os.environ.get('AZUREML_WORKSPACE_CONNECTION_ID_GIT')
-    if connection_id is not None and connection_id != '':
-        from azureml.rag.utils.connections import get_connection_by_id_v2
-
-        connection = get_connection_by_id_v2(connection_id)
-        if args.git_repository != connection['properties']['target']:
-            logger.warning(f"Given git repository '{args.git_repository}' does not match the git repository '{connection['properties']['target']}' specified in the Workspace Connection '{connection_id}'. Using the Workspace Connection git repository.")
-        args.git_repository = connection['properties']['target']
-        authentication = {'username': connection['properties']['metadata']['username'], 'password': connection['properties']['credentials']['pat']}
-    elif args.authentication_key_prefix is not None:
-        authentication = get_keyvault_authentication(args.authentication_key_prefix)
-    else:
-        authentication = None
-
     with track_activity(logger, 'git_clone') as activity_logger:
-        clone_repo(args.git_repository, args.output_data, args.branch_name, authentication)
+        try:
+            connection_id = os.environ.get('AZUREML_WORKSPACE_CONNECTION_ID_GIT')
+            if connection_id is not None and connection_id != '':
+                from azureml.rag.utils.connections import get_connection_by_id_v2
+
+                connection = get_connection_by_id_v2(connection_id)
+                if args.git_repository != connection['properties']['target']:
+                    logger.warning(f"Given git repository '{args.git_repository}' does not match the git repository '{connection['properties']['target']}' specified in the Workspace Connection '{connection_id}'. Using the Workspace Connection git repository.")
+                args.git_repository = connection['properties']['target']
+                authentication = {'username': connection['properties']['metadata']['username'], 'password': connection['properties']['credentials']['pat']}
+            elif args.authentication_key_prefix is not None:
+                authentication = get_keyvault_authentication(args.authentication_key_prefix)
+            else:
+                authentication = None
+        except Exception as e:
+            logger.error(f"Failed to get authentication information from the Workspace Connection '{connection_id}'.")
+            activity_logger.activity_info['error'] = f"{e.__class__.__name__}: Failed to get authentication information from the Workspace Connection."
+
+            raise e
+
+        activity_logger.activity_info['authentication_used'] = str(authentication is not None)
+
+        try:
+            clone_repo(args.git_repository, args.output_data, args.branch_name, authentication)
+        except git.exc.GitError as e:
+            activity_logger.activity_info['error'] = f"{e.__class__.__name__}: Failed with GitError."
+
+            raise e
+        except Exception as e:
+            activity_logger.activity_info['error'] = f"{e.__class__.__name__}: Failed to clone git repository."
+
+            raise e
 
     logger.info('Finished cloning.')
```

## azureml/rag/tasks/register_mlindex.py

```diff
@@ -4,15 +4,15 @@
 """File for registering ML Indexes."""
 import argparse
 from azureml.core import Run
 import fsspec
 import re
 import yaml
 
-from azureml.rag.utils.logging import get_logger, enable_stdout_logging, enable_appinsights_logging, track_info
+from azureml.rag.utils.logging import get_logger, enable_stdout_logging, enable_appinsights_logging, track_activity
 from azureml.rag._asset_client.client import get_rest_client, register_new_data_asset_version
 
 logger = get_logger('register_mlindex')
 
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser()
@@ -23,50 +23,52 @@
 
     print('\n'.join(f'{k}={v}' for k, v in vars(args).items()))
 
     enable_stdout_logging()
     enable_appinsights_logging()
 
     run: Run = Run.get_context()
-    ws = run.experiment.workspace
+    with track_activity(logger, 'register_mlindex', custom_dimensions={'source': run.properties.get('azureml.mlIndexAssetSource', 'Unknown')}) as activity_logger:
+        ws = run.experiment.workspace
 
-    logger.info(f'Checking for MLIndex at: {args.storage_uri.strip("/")}/MLIndex')
-    index_kind = None
-    mlindex_yaml = None
-    try:
-        mlindex_file = fsspec.open(f"{args.storage_uri}/MLIndex", 'r')
-        # parse yaml to dict
-        with mlindex_file as f:
-            mlindex_yaml = yaml.safe_load(f)
-            index_kind = mlindex_yaml.get('index', {}).get('kind', None)
-    except Exception as e:
-        logger.error(f"Could not find MLIndex: {e}")
-        raise e
-
-    if index_kind is None:
-        logger.error(f"Could not find index.kind in MLIndex: {mlindex_yaml}")
-        raise ValueError(f"Could not find index.kind in MLIndex: {mlindex_yaml}")
-
-    client = get_rest_client(ws)
-    data_version = register_new_data_asset_version(
-        client,
-        run,
-        args.asset_name,
-        args.storage_uri,
-        properties={
-            'azureml.mlIndexAssetKind': index_kind,
-            'azureml.mlIndexAsset': 'true',
-            'azureml.mlIndexAssetSource': run.properties.get('azureml.mlIndexAssetSource', 'Unknown'),
-            'azureml.mlIndexAssetPipelineRunId': run.properties.get('azureml.pipelinerunid', 'Unknown')
-        })
-
-    print(data_version.asset_id)
-
-    asset_id = re.sub('azureml://locations/(.*)/workspaces/(.*)/data', f'azureml://subscriptions/{ws._subscription_id}/resourcegroups/{ws._resource_group}/providers/Microsoft.MachineLearningServices/workspaces/{ws._workspace_name}/data', data_version.asset_id)
+        logger.info(f'Checking for MLIndex at: {args.storage_uri.strip("/")}/MLIndex')
+        index_kind = None
+        mlindex_yaml = None
+        try:
+            mlindex_file = fsspec.open(f"{args.storage_uri}/MLIndex", 'r')
+            # parse yaml to dict
+            with mlindex_file as f:
+                mlindex_yaml = yaml.safe_load(f)
+                index_kind = mlindex_yaml.get('index', {}).get('kind', None)
+        except Exception as e:
+            logger.error(f"Could not find MLIndex: {e}")
+            raise e
+
+        if index_kind is None:
+            logger.error(f"Could not find index.kind in MLIndex: {mlindex_yaml}")
+            activity_logger.activity_info['error'] = 'Could not find index.kind in MLIndex yaml'
+            raise ValueError(f"Could not find index.kind in MLIndex: {mlindex_yaml}")
+        activity_logger.activity_info['kind'] = index_kind
+
+        client = get_rest_client(ws)
+        data_version = register_new_data_asset_version(
+            client,
+            run,
+            args.asset_name,
+            args.storage_uri,
+            properties={
+                'azureml.mlIndexAssetKind': index_kind,
+                'azureml.mlIndexAsset': 'true',
+                'azureml.mlIndexAssetSource': run.properties.get('azureml.mlIndexAssetSource', 'Unknown'),
+                'azureml.mlIndexAssetPipelineRunId': run.properties.get('azureml.pipelinerunid', 'Unknown')
+            })
+
+        print(data_version.asset_id)
 
-    print(asset_id)
+        asset_id = re.sub('azureml://locations/(.*)/workspaces/(.*)/data', f'azureml://subscriptions/{ws._subscription_id}/resourcegroups/{ws._resource_group}/providers/Microsoft.MachineLearningServices/workspaces/{ws._workspace_name}/data', data_version.asset_id)
 
-    with open(args.output_asset_id, 'w') as f:
-        f.write(asset_id)
+        print(asset_id)
 
-    track_info(logger, 'register_mlindex', {'kind': index_kind, 'source': run.properties.get('azureml.mlIndexAssetSource', 'Unknown')})
-    logger.info(f"Finished Registering MLIndex Asset '{args.asset_name}', version = {data_version.version_id}")
+        with open(args.output_asset_id, 'w') as f:
+            f.write(asset_id)
+
+        logger.info(f"Finished Registering MLIndex Asset '{args.asset_name}', version = {data_version.version_id}")
```

## azureml/rag/tasks/update_acs.py

```diff
@@ -147,20 +147,21 @@
                             "metric": "cosine",
                             "efSearch": 500
                         }
                     }
                 ]
             }
 
+        logger.info(f"Creating {acs_config['index_name']} search index with embeddings", extra={'print': True})
         try:
             response = send_put_request(base_url, headers, payload)
             logger.info(response.text)
         except requests.exceptions.RequestException as e:
             logger.error(f"Request failed: {e}\nResponse: {e.response.text}")
-            raise e
+            raise
     else:
         logger.info(f"Search index {acs_config['index_name']} already exists", extra={'print': True})
 
 
 @tenacity.retry(
     wait=tenacity.wait_fixed(5),  # wait 5 seconds between retries
     stop=tenacity.stop_after_attempt(3),  # stop after 3 attempts
@@ -191,21 +192,21 @@
     }
 
     try:
         response = send_post_request(base_url, headers, payload)
         logger.info(response.text)
     except requests.exceptions.RequestException as e:
         logger.error(f"Request failed: {e}\nResponse: {e.response.text}")
-        raise e
+        raise
 
     return response.json()['value']
 
 
 def create_index_from_raw_embeddings(emb: Embeddings, acs_config={}, connection={}, output_path: Optional[str] = None):
-    with track_activity(logger, 'update_acs', custom_dimensions={'num_embeddings': len(emb._document_embeddings)}) as activity_logger:
+    with track_activity(logger, 'update_acs', custom_dimensions={'num_documents': len(emb._document_embeddings)}) as activity_logger:
         logger.info("Updating ACS index", extra={'print': True})
 
         credential = get_connection_credential(connection)
 
         if str(acs_config.get('push_embeddings')).lower() == "false":
             create_search_index_sdk(acs_config, credential)
             search_client = search_client_from_config(acs_config, credential)
@@ -232,15 +233,15 @@
                 else:
                     if r.succeeded:
                         succeeded.append(r)
                     else:
                         failed.append(r)
             duration = time.time() - start_time
             logger.info(f"Uploaded {len(succeeded)} documents to ACS in {duration:.4f} seconds, {len(failed)} failed", extra={'print': True})
-            activity_logger.info("Uploaded documents", extra={'succeeded': len(succeeded), 'failed': len(failed), 'duration': duration})
+            activity_logger.info("Uploaded documents", extra={'properties': {'succeeded': len(succeeded), 'failed': len(failed), 'duration': duration}})
             if len(failed) > 0:
                 for r in failed:
                     if isinstance(r, dict):
                         error = r['errorMessage']
                     else:
                         error = r.error_message
                     logger.error(f"Failed document reason: {error}", extra={'print': True})
@@ -298,15 +299,15 @@
                 if len(failed) > 0:
                     raise RuntimeError(f"Failed to upload {len(failed)} documents.")
 
             num_source_docs += len(batch)
 
         duration = time.time()-t1
         logger.info(f"Built index from {num_source_docs} documents and {len(emb._document_embeddings)} chunks, took {duration:.4f} seconds", extra={'print': True})
-        activity_logger.info("Built index", extra={'num_source_docs': num_source_docs, 'duration': duration})
+        activity_logger.info("Built index", extra={'properties': {'num_source_docs': num_source_docs, 'duration': duration}})
 
         if output_path is not None:
             logger.info('Writing MLIndex yaml', extra={'print': True})
             mlindex_config = {
                 "embeddings": emb.get_metadata()
             }
             mlindex_config["index"] = {
@@ -348,48 +349,73 @@
     args = parser.parse_args()
 
     print('\n'.join(f'{k}={v}' for k, v in vars(args).items()))
 
     enable_stdout_logging()
     enable_appinsights_logging()
 
-    raw_embeddings_uri = args.embeddings
-    logger.info(f'got embeddings uri as input: {raw_embeddings_uri}', extra={'print': True})
-    splits = raw_embeddings_uri.split('/')
-    embeddings_dir_name = splits.pop(len(splits)-2)
-    logger.info(f'extracted embeddings directory name: {embeddings_dir_name}', extra={'print': True})
-    parent = '/'.join(splits)
-    logger.info(f'extracted embeddings container path: {parent}', extra={'print': True})
-
-    acs_config = json.loads(args.acs_config)
-
-    connection_args = {}
-    connection_id = os.environ.get('AZUREML_WORKSPACE_CONNECTION_ID_ACS')
-    if connection_id is not None:
-        connection_args['connection_type'] = 'workspace_connection'
-        connection_args['connection'] = {'id': connection_id}
-        from azureml.rag.utils.connections import get_connection_by_id_v2
-
-        connection = get_connection_by_id_v2(connection_id)
-        acs_config['endpoint'] = connection['properties']['target']
-        acs_config['api_version'] = connection['properties'].get('metadata', {}).get('apiVersion', "2023-07-01-preview")
-    elif 'endpoint_key_name' in acs_config:
-        connection_args['connection_type'] = 'workspace_keyvault'
-        from azureml.core import Run
-        run = Run.get_context()
-        ws = run.experiment.workspace
-        connection_args['connection'] = {
-            'key': acs_config['endpoint_key_name'],
-            "subscription": ws.subscription_id,
-            "resource_group": ws.resource_group,
-            "workspace": ws.name,
-        }
-
-    from azureml.dataprep.fuse.dprepfuse import (MountOptions, rslex_uri_volume_mount)
-    mnt_options = MountOptions(
-        default_permission=0o555, allow_other=False, read_only=True)
-    logger.info(f'mounting embeddings container from: \n{parent} \n   to: \n{os.getcwd()}/embeddings_mount', extra={'print': True})
-    with rslex_uri_volume_mount(parent, f'{os.getcwd()}/embeddings_mount', options=mnt_options) as mount_context:
-        emb = Embeddings.load(embeddings_dir_name, mount_context.mount_point)
-        create_index_from_raw_embeddings(emb, acs_config=acs_config, connection=connection_args, output_path=args.output)
+    with track_activity(logger, "main") as activity_logger:
+        try:
+            raw_embeddings_uri = args.embeddings
+            logger.info(f'got embeddings uri as input: {raw_embeddings_uri}', extra={'print': True})
+            splits = raw_embeddings_uri.split('/')
+            embeddings_dir_name = splits.pop(len(splits)-2)
+            logger.info(f'extracted embeddings directory name: {embeddings_dir_name}', extra={'print': True})
+            parent = '/'.join(splits)
+            logger.info(f'extracted embeddings container path: {parent}', extra={'print': True})
+
+            acs_config = json.loads(args.acs_config)
+
+            connection_args = {}
+            connection_id = os.environ.get('AZUREML_WORKSPACE_CONNECTION_ID_ACS')
+            if connection_id is not None:
+                connection_args['connection_type'] = 'workspace_connection'
+                connection_args['connection'] = {'id': connection_id}
+                from azureml.rag.utils.connections import get_connection_by_id_v2
+
+                connection = get_connection_by_id_v2(connection_id)
+                acs_config['endpoint'] = connection['properties']['target']
+                acs_config['api_version'] = connection['properties'].get('metadata', {}).get('apiVersion', "2023-07-01-preview")
+            elif 'endpoint_key_name' in acs_config:
+                connection_args['connection_type'] = 'workspace_keyvault'
+                from azureml.core import Run
+                run = Run.get_context()
+                ws = run.experiment.workspace
+                connection_args['connection'] = {
+                    'key': acs_config['endpoint_key_name'],
+                    "subscription": ws.subscription_id,
+                    "resource_group": ws.resource_group,
+                    "workspace": ws.name,
+                }
+
+            from azureml.dataprep.fuse.dprepfuse import (MountOptions, rslex_uri_volume_mount)
+            mnt_options = MountOptions(
+                default_permission=0o555, allow_other=False, read_only=True)
+            logger.info(f'mounting embeddings container from: \n{parent} \n   to: \n{os.getcwd()}/embeddings_mount', extra={'print': True})
+            with rslex_uri_volume_mount(parent, f'{os.getcwd()}/embeddings_mount', options=mnt_options) as mount_context:
+                emb = Embeddings.load(embeddings_dir_name, mount_context.mount_point)
+                create_index_from_raw_embeddings(emb, acs_config=acs_config, connection=connection_args, output_path=args.output)
+        except Exception as e:
+            logger.error('Failed to update ACS index')
+            exception_str = str(e)
+            if isinstance(e, requests.exceptions.RequestException):
+                activity_logger.activity_info['error'] = 'Failed request to ACS'
+                activity_logger.activity_info['response_code'] = e.response.status_code
+                activity_logger.activity_info['error_classification'] = 'SystemError'
+                if 'Floats quota has been exceeded for this service.' in e.response.text:
+                    logger.error('Floats quota exceeded on Azure Cognitive Search Service. For more information check these docs: https://github.com/Azure/cognitive-search-vector-pr#storage-and-vector-index-size-limits')
+                    logger.error('The usage statistic of an index can be checked using this REST API: https://learn.microsoft.com/en-us/rest/api/searchservice/get-index-statistics ')
+                    activity_logger.activity_info['error_classification'] = 'UserError'
+                    activity_logger.activity_info['error'] += ": Floats quota has been exceeded for this service."
+                elif 'Cannot find nested property' in e.response.text:
+                    logger.error(f'The vector index provided "{acs_config["index_name"]}" has a different schema than outlined in this components description. This can happen if a different embedding model was used previously when updating this index.')
+                    activity_logger.activity_info['error_classification'] = 'UserError'
+                    activity_logger.activity_info['error'] += ": Cannot find nested property"
+            elif 'Failed to upload' in exception_str:
+                activity_logger.activity_info['error'] = str(e)
+                activity_logger.activity_info['error_classification'] = 'SystemError'
+            else:
+                activity_logger.activity_info['error'] = str(e.__class__.__name__)
+                activity_logger.activity_info['error_classification'] = 'SystemError'
+            raise
 
-    logger.info('Updated ACS index', extra={'print': True})
+    logger.info('Updated ACS index')
```

## azureml/rag/utils/logging.py

```diff
@@ -47,17 +47,49 @@
 except Exception:
     from contextlib import contextmanager
 
     verbosity = None
     ActivityLoggerAdapter = None
     telemetry_enabled = False
 
+    class ActivityLoggerAdapter(logging.LoggerAdapter):
+        """Make logger look like Activity Logger"""
+
+        def __init__(self, logger):
+            """Initialize a new instance of the class
+            """
+            self._activity_info = {}
+            super(ActivityLoggerAdapter, self).__init__(logger, None)
+
+        @property
+        def activity_info(self):
+            """Return current activity info."""
+            return self._activity_info
+
+        def process(self, msg, kwargs):
+            """Process the log message.
+            """
+            return msg, kwargs
+
     @contextmanager
     def _run_without_logging(logger, activity_name, activity_type, custom_dimensions):
-        yield logger
+        yield ActivityLoggerAdapter(logger)
+
+
+known_modules = [
+    "crack_and_chunk",
+    "create_faiss_index",
+    "create_promptflow",
+    "data_import_acs",
+    "git_clone",
+    "generate_embeddings_parallel",
+    "qa_data_generation",
+    "register_mlindex_asset",
+    "update_acs_index"
+]
 
 
 class LoggerFactory:
     """Factory for creating loggers"""
     def __init__(self, stdout=False):
         """Initialize the logger factory"""
         self.loggers = {}
@@ -75,15 +107,18 @@
                 stdout_handler.setFormatter(logging.Formatter('[%(asctime)s] %(levelname)-8s %(name)s - %(message)s (%(filename)s:%(lineno)s)', "%Y-%m-%d %H:%M:%S"))
                 logger.addHandler(stdout_handler)
         return self
 
     def with_appinsights(self):
         """Set whether to log track_* events to appinsights"""
         if telemetry_enabled:
+            import atexit
+
             self.appinsights = get_telemetry_log_handler(component_name=COMPONENT_NAME, path=telemetry_config_path)
+            atexit.register(self.appinsights.flush)
 
     def get_logger(self, name, level=logging.INFO):
         """Get a logger with the given name and level"""
         if name not in self.loggers:
             logger = logging.getLogger(f'azureml.rag.{name}')
             logger.setLevel(level)
             if self.stdout:
@@ -172,14 +207,20 @@
             location = os.environ.get("AZUREML_SERVICE_ENDPOINT", "")
         info["location"] = location
         try:
             from azureml.core import Run
             run: Run = Run.get_context()
             if hasattr(run, 'experiment'):
                 info["parent_run_id"] = run.properties.get('azureml.pipelinerunid', 'Unknown')
+                info["mlIndexAssetKind"] = run.properties.get('azureml.mlIndexAssetKind', 'Unknown')
+                info["mlIndexAssetSource"] = run.properties.get('azureml.mlIndexAssetSource', 'Unknown')
+                module_name = run.properties.get('azureml.moduleName', 'Unknown')
+                info["moduleName"] = module_name if module_name in known_modules or module_name.startswith('llm_') else 'Unknown'
+                if info["moduleName"] != 'Unknown':
+                    info["moduleVersion"] = run.properties.get('azureml.moduleVersion', 'Unknown')
         except Exception:
             pass
         return info
 
 
 _logger_factory = LoggerFactory()
```

## Comparing `azureml_rag-0.1.6.dist-info/LICENSE.txt` & `azureml_rag-0.1.7.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `azureml_rag-0.1.6.dist-info/METADATA` & `azureml_rag-0.1.7.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: azureml-rag
-Version: 0.1.6
+Version: 0.1.7
 Summary: Contains Retrieval Augmented Generation related utilities for Azure Machine Learning and OSS interoperability.
 Home-page: https://docs.microsoft.com/python/api/overview/azure/ml/?view=azure-ml-py
 Author: Microsoft Corporation
 License: Proprietary https://aka.ms/azureml-preview-sdk-license 
 Platform: UNKNOWN
 Classifier: Development Status :: 5 - Production/Stable
 Classifier: Intended Audience :: Developers
@@ -18,15 +18,15 @@
 Classifier: Programming Language :: Python :: 3.10
 Classifier: Operating System :: MacOS
 Classifier: Operating System :: Microsoft :: Windows
 Classifier: Operating System :: POSIX :: Linux
 Classifier: Topic :: Scientific/Engineering
 Requires-Python: >=3.7,<4.0
 Description-Content-Type: text/markdown
-Requires-Dist: azureml-dataprep[parquet] (<4.11.0a,>=4.10.0a)
+Requires-Dist: azureml-dataprep[parquet] (<4.12.0a,>=4.11.1)
 Requires-Dist: azureml-core
 Requires-Dist: azureml-telemetry
 Requires-Dist: azureml-mlflow
 Requires-Dist: azureml-fsspec
 Requires-Dist: fsspec (~=2023.3)
 Requires-Dist: openai (~=0.27.4)
 Requires-Dist: tiktoken (~=0.3.0)
```

## Comparing `azureml_rag-0.1.6.dist-info/RECORD` & `azureml_rag-0.1.7.dist-info/RECORD`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 azureml/rag/__init__.py,sha256=zgRl-az58He0no_ZUWab0AuVQSOTmgCLlfwbEcnwKS0,246
 azureml/rag/documents.py,sha256=PTxrleeRwsp_fQd7wCNpNWbtGSh5PBGNCyw97If7bUE,39881
-azureml/rag/embeddings.py,sha256=PgirYai7PmQkborZlyVLCNJe4oXkNq525Dpetj7XBN0,29244
+azureml/rag/embeddings.py,sha256=LPJo0dcNxPsM4Z6XWaROddyK-ERIWVAVe9C7T9mi56U,33664
 azureml/rag/mlindex.py,sha256=DNb-Z15nTzjmGZH5h0RGjqxQRvnIDg756W8gJDbObhA,5058
 azureml/rag/models.py,sha256=Ga9tvV0FsVhXprEeWpPVakNJ9q96byc0OjDESqkMuCw,4149
 azureml/rag/_asset_client/client.py,sha256=LpsDsedlQRxytezdvnxx1zfEQUb9DcYDY6RfkRSlXT0,4646
 azureml/rag/_asset_client/_restclient/__init__.py,sha256=38lKUIqL59KqhES7ZGBUGcrELWICWet0VFLxwY4W0fo,893
 azureml/rag/_asset_client/_restclient/_azure_machine_learning_workspaces.py,sha256=7XqTcSuvXHcKXu4FE71E4rBQAQ4Fmpdq8SXrhU7_AAg,4381
 azureml/rag/_asset_client/_restclient/_configuration.py,sha256=E52K3BmA4ZqAE6oP5VjRK32HsBrl6W9Y1oOsWCJu8Ig,3538
 azureml/rag/_asset_client/_restclient/_patch.py,sha256=wuqrJGWK488sJvWwSq6iwPTqil7TPaRadoxE7BMK0tA,1561
@@ -34,25 +34,25 @@
 azureml/rag/_asset_client/_restclient/runhistory/models/_azure_machine_learning_workspaces_enums.py,sha256=cyvWolLt_kCOalE8BvtnI_msfmAAlTYzdob0QDp1KZ0,2566
 azureml/rag/_asset_client/_restclient/runhistory/models/_models.py,sha256=CqQccTVrcm5_9uBZOJ6AMubF9K8GphuIzmF1j46G84Q,175170
 azureml/rag/_asset_client/_restclient/runhistory/models/_models_py3.py,sha256=G5x7IlIzWN98TsXpY80M5nyLjXIwBWWeL9Fk6Piwpzw,188776
 azureml/rag/_asset_client/_restclient/runhistory/operations/__init__.py,sha256=FFrjEyJdO53e7YDI8o_8W9BgTTEVt1ZzS8FwwnNp7-g,563
 azureml/rag/_asset_client/_restclient/runhistory/operations/_runs_operations.py,sha256=1nl-ETYrSQSZH3B-a2QXTKIUCx-Hx3XeQPmgAwKSGoA,162796
 azureml/rag/langchain/acs.py,sha256=xyJ5u7RlZ8FUVAQZg7z4LJv5pXACddLcRLihUL6sEQI,10785
 azureml/rag/tasks/__init__.py,sha256=JlCCObuOLZyNhIZlsoL51KtFxiY4xKIIzIRDBcdeu6Y,183
-azureml/rag/tasks/build_faiss.py,sha256=9qJFbSVgnbP-87j1lmBhUeZUrfZpe_NLPulYPijYQS0,2318
-azureml/rag/tasks/crack_and_chunk.py,sha256=wYnzd_lvcE9w63NTjyOt-fsirvr2wWJqH-SLFUqiFtk,7945
+azureml/rag/tasks/build_faiss.py,sha256=MLgVOgg2vpfq9BgFJL3O69oN2G3mD3R2sNqpASrmllM,2289
+azureml/rag/tasks/crack_and_chunk.py,sha256=pord6kqwlxlwJ5U-kIlbewMSvAnm-HJ3jkx-P8BfqSo,9822
 azureml/rag/tasks/embed.py,sha256=gCjmL9-CG3ygpwxDZX1_8MtfnUwYGMHWnb1kicwPsAQ,7502
-azureml/rag/tasks/embed_prs.py,sha256=DkK9I2kT1qIetgYGo-Bx6fPrDDbe3RrFJdVHrVadPGM,6328
-azureml/rag/tasks/git_clone.py,sha256=L7-gEP5r24AAq-aLxomURtDL7qfuWpusoRC9cm6iBjc,2340
-azureml/rag/tasks/register_mlindex.py,sha256=Gu2KUsyFBTfl0O_PxMOFNU-ROz-EinqowG_kkoiWxqY,3002
-azureml/rag/tasks/update_acs.py,sha256=S-HJD-dzUr-K51m3fuovOgcDKgt_htQ5m3JvXeGVfhI,19067
+azureml/rag/tasks/embed_prs.py,sha256=h1liQenbyr5pynrQjwoRDqkF5WqRI_gWrW8oyMBUMrg,6121
+azureml/rag/tasks/git_clone.py,sha256=T81sYAIh82DIZYIJVCAZB4NY-FsBf3qqswYM3pw8CxY,3239
+azureml/rag/tasks/register_mlindex.py,sha256=7favzOb15gFuh1Cg8RE1igLGQpcujOxMRaxIT4Nh6hc,3327
+azureml/rag/tasks/update_acs.py,sha256=Z8ii7Xh5MVShkVRFW-2dbR8F8CN7Eue-6zcIMn2iayQ,21619
 azureml/rag/utils/__init__.py,sha256=FTY5BaTKeythd7R1SXsf3midWHdshZj-bdFZLmZ7J50,208
 azureml/rag/utils/_telemetry.json,sha256=VlnC81iHUOx9NYj_BNpMzI2Y8Q3D8z832w31TQl9qI4,98
 azureml/rag/utils/azureml.py,sha256=dOfvDTgqjTWHg-I9Ho7_o6e7I8kNcVJdl53O0Ts7xQs,1588
 azureml/rag/utils/connections.py,sha256=zInLr6twjfHW4fF2zgChtNSI7YqnxY3eOVnu74xpq-Y,7435
 azureml/rag/utils/git.py,sha256=oumlex0PWWt8ppmL-0Sxli6cpRWpBR9THR0fqN2thFo,2634
-azureml/rag/utils/logging.py,sha256=0pnj9K8j7KC7IlJmu-HIDZETW1JKiWT2PjEzscycdis,8529
-azureml_rag-0.1.6.dist-info/LICENSE.txt,sha256=FOfkEEz4uS7g278F9Rq12rqKF3lLyBUZhVpLetuZrTg,1021
-azureml_rag-0.1.6.dist-info/METADATA,sha256=EGM4mcLN3I4It4l3CJLAF87_3bhgBBmclzb0YlucFu8,5572
-azureml_rag-0.1.6.dist-info/WHEEL,sha256=YUYzQ6UQdoqxXjimOitTqynltBCkwY6qlTfTh2IzqQU,97
-azureml_rag-0.1.6.dist-info/top_level.txt,sha256=ZOeEa0TAXo6i5wOjwBoqfIGEuxOcKuscGgNSpizqREY,8
-azureml_rag-0.1.6.dist-info/RECORD,,
+azureml/rag/utils/logging.py,sha256=TR6B6GssB3G-BSfpSXhIka3Ru73R4NgpR2k3F0mBwew,10082
+azureml_rag-0.1.7.dist-info/LICENSE.txt,sha256=FOfkEEz4uS7g278F9Rq12rqKF3lLyBUZhVpLetuZrTg,1021
+azureml_rag-0.1.7.dist-info/METADATA,sha256=95q_Jp43ppD-oXmMLBx4lJ8Gos08IGPkIUPBqprAelY,5571
+azureml_rag-0.1.7.dist-info/WHEEL,sha256=YUYzQ6UQdoqxXjimOitTqynltBCkwY6qlTfTh2IzqQU,97
+azureml_rag-0.1.7.dist-info/top_level.txt,sha256=ZOeEa0TAXo6i5wOjwBoqfIGEuxOcKuscGgNSpizqREY,8
+azureml_rag-0.1.7.dist-info/RECORD,,
```

