# Comparing `tmp/bigdl_llm-2.4.0b20230608-py3-none-win_amd64.whl.zip` & `tmp/bigdl_llm-2.4.0b20230609-py3-none-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,46 +1,47 @@
-Zip file size: 2859609 bytes, number of entries: 44
--rw-rw-r--  2.0 unx      956 b- defN 23-May-31 07:26 bigdl/__init__.py
--rw-rw-r--  2.0 unx      874 b- defN 23-Jun-02 07:24 bigdl/llm/__init__.py
--rwxrwxr-x  2.0 unx     1780 b- defN 23-Jun-08 07:55 bigdl/llm/cli/llm-cli
--rwxrwxr-x  2.0 unx     1565 b- defN 23-Jun-08 07:55 bigdl/llm/cli/llm-cli.ps1
--rw-rw-r--  2.0 unx      985 b- defN 23-Jun-05 03:22 bigdl/llm/ggml/__init__.py
--rw-rw-r--  2.0 unx     4991 b- defN 23-Jun-07 06:07 bigdl/llm/ggml/convert.py
--rw-rw-r--  2.0 unx     5431 b- defN 23-Jun-07 06:07 bigdl/llm/ggml/convert_model.py
--rw-rw-r--  2.0 unx     4334 b- defN 23-Jun-07 06:07 bigdl/llm/ggml/quantize.py
--rw-rw-r--  2.0 unx      874 b- defN 23-Jun-02 03:52 bigdl/llm/ggml/model/__init__.py
--rw-rw-r--  2.0 unx      900 b- defN 23-Jun-05 03:22 bigdl/llm/ggml/model/bloom/__init__.py
--rw-rw-r--  2.0 unx     8702 b- defN 23-Jun-08 07:55 bigdl/llm/ggml/model/bloom/bloom.py
--rw-rw-r--  2.0 unx     5241 b- defN 23-Jun-06 01:57 bigdl/llm/ggml/model/bloom/bloom_cpp.py
--rw-rw-r--  2.0 unx      910 b- defN 23-Jun-06 08:26 bigdl/llm/ggml/model/generation/__init__.py
--rw-rw-r--  2.0 unx     6106 b- defN 23-Jun-08 08:00 bigdl/llm/ggml/model/generation/utils.py
--rw-rw-r--  2.0 unx      925 b- defN 23-Jun-02 03:52 bigdl/llm/ggml/model/gptneox/__init__.py
--rw-rw-r--  2.0 unx    47025 b- defN 23-Jun-08 02:02 bigdl/llm/ggml/model/gptneox/gptneox.py
--rw-rw-r--  2.0 unx    26490 b- defN 23-Jun-06 01:57 bigdl/llm/ggml/model/gptneox/gptneox_cpp.py
--rw-rw-r--  2.0 unx     4194 b- defN 23-Jun-02 03:52 bigdl/llm/ggml/model/gptneox/gptneox_types.py
--rw-rw-r--  2.0 unx      921 b- defN 23-Jun-01 06:05 bigdl/llm/ggml/model/llama/__init__.py
--rw-rw-r--  2.0 unx    54627 b- defN 23-Jun-07 06:07 bigdl/llm/ggml/model/llama/llama.py
--rw-rw-r--  2.0 unx    31490 b- defN 23-Jun-02 07:51 bigdl/llm/ggml/model/llama/llama_cpp.py
--rw-rw-r--  2.0 unx     4207 b- defN 23-Jun-02 03:52 bigdl/llm/ggml/model/llama/llama_types.py
--rw-rw-r--  2.0 unx      915 b- defN 23-Jun-07 06:07 bigdl/llm/ggml/transformers/__init__.py
--rw-rw-r--  2.0 unx     5354 b- defN 23-Jun-07 06:07 bigdl/llm/ggml/transformers/model.py
--rw-rw-r--  2.0 unx   660992 b- defN 23-Jun-09 09:01 bigdl/llm/libs/bloom.dll
--rw-rw-r--  2.0 unx   819712 b- defN 23-Jun-09 09:01 bigdl/llm/libs/gptneox.dll
--rw-rw-r--  2.0 unx   807424 b- defN 23-Jun-09 09:01 bigdl/llm/libs/llama.dll
--rw-rw-r--  2.0 unx  2941913 b- defN 23-Jun-09 09:02 bigdl/llm/libs/main-bloom.exe
--rw-rw-r--  2.0 unx  2955796 b- defN 23-Jun-09 09:02 bigdl/llm/libs/main-gptneox.exe
--rw-rw-r--  2.0 unx  2909074 b- defN 23-Jun-09 09:01 bigdl/llm/libs/main-llama.exe
--rw-rw-r--  2.0 unx    82432 b- defN 23-Jun-09 09:01 bigdl/llm/libs/quantize-bloom.exe
--rw-rw-r--  2.0 unx    65024 b- defN 23-Jun-09 09:01 bigdl/llm/libs/quantize-gptneox.exe
--rw-rw-r--  2.0 unx    25088 b- defN 23-Jun-09 09:01 bigdl/llm/libs/quantize-llama.exe
--rw-rw-r--  2.0 unx      896 b- defN 23-Jun-02 07:39 bigdl/llm/utils/__init__.py
--rw-rw-r--  2.0 unx    56427 b- defN 23-Jun-08 07:55 bigdl/llm/utils/convert_util.py
--rw-rw-r--  2.0 unx     1043 b- defN 23-Jun-06 01:57 bigdl/llm/utils/utils.py
--rw-rw-r--  2.0 unx      939 b- defN 23-Jun-02 07:43 bigdl/llm/utils/common/__init__.py
--rw-rw-r--  2.0 unx     1378 b- defN 23-May-31 07:26 bigdl/llm/utils/common/log4Error.py
--rwxrwxr-x  2.0 unx     1565 b- defN 23-Jun-08 07:55 bigdl_llm-2.4.0b20230608.data/scripts/llm-cli.ps1
--rw-rw-r--  2.0 unx      804 b- defN 23-Jun-09 09:02 bigdl_llm-2.4.0b20230608.dist-info/METADATA
--rw-rw-r--  2.0 unx       98 b- defN 23-Jun-09 09:02 bigdl_llm-2.4.0b20230608.dist-info/WHEEL
--rw-rw-r--  2.0 unx       68 b- defN 23-Jun-09 09:02 bigdl_llm-2.4.0b20230608.dist-info/entry_points.txt
--rw-rw-r--  2.0 unx        6 b- defN 23-Jun-09 09:02 bigdl_llm-2.4.0b20230608.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     3959 b- defN 23-Jun-09 09:02 bigdl_llm-2.4.0b20230608.dist-info/RECORD
-44 files, 11554435 bytes uncompressed, 2853241 bytes compressed:  75.3%
+Zip file size: 2860748 bytes, number of entries: 45
+-rw-------  2.0 unx      956 b- defN 23-Jun-09 07:52 bigdl/__init__.py
+-rw-------  2.0 unx      874 b- defN 23-Jun-09 07:52 bigdl/llm/__init__.py
+-rw-------  2.0 unx     1014 b- defN 23-Jun-09 07:52 bigdl/llm/models.py
+-rwxrwxr-x  2.0 unx     1565 b- defN 23-Jun-09 07:52 bigdl/llm/cli/llm-cli.ps1
+-rw-------  2.0 unx      994 b- defN 23-Jun-09 07:52 bigdl/llm/ggml/__init__.py
+-rw-------  2.0 unx     4991 b- defN 23-Jun-09 07:52 bigdl/llm/ggml/convert.py
+-rw-------  2.0 unx     5431 b- defN 23-Jun-09 07:52 bigdl/llm/ggml/convert_model.py
+-rw-------  2.0 unx     4334 b- defN 23-Jun-09 07:52 bigdl/llm/ggml/quantize.py
+-rw-------  2.0 unx      874 b- defN 23-Jun-09 07:52 bigdl/llm/ggml/model/__init__.py
+-rw-------  2.0 unx      900 b- defN 23-Jun-09 07:52 bigdl/llm/ggml/model/bloom/__init__.py
+-rw-------  2.0 unx     8635 b- defN 23-Jun-09 07:52 bigdl/llm/ggml/model/bloom/bloom.py
+-rw-------  2.0 unx     5241 b- defN 23-Jun-09 07:52 bigdl/llm/ggml/model/bloom/bloom_cpp.py
+-rw-------  2.0 unx      910 b- defN 23-Jun-09 07:52 bigdl/llm/ggml/model/generation/__init__.py
+-rw-------  2.0 unx     6106 b- defN 23-Jun-09 07:52 bigdl/llm/ggml/model/generation/utils.py
+-rw-------  2.0 unx      925 b- defN 23-Jun-09 07:52 bigdl/llm/ggml/model/gptneox/__init__.py
+-rw-------  2.0 unx    46923 b- defN 23-Jun-09 07:52 bigdl/llm/ggml/model/gptneox/gptneox.py
+-rw-------  2.0 unx    26490 b- defN 23-Jun-09 07:52 bigdl/llm/ggml/model/gptneox/gptneox_cpp.py
+-rw-------  2.0 unx     4194 b- defN 23-Jun-09 07:52 bigdl/llm/ggml/model/gptneox/gptneox_types.py
+-rw-------  2.0 unx      921 b- defN 23-Jun-09 07:52 bigdl/llm/ggml/model/llama/__init__.py
+-rw-------  2.0 unx    54525 b- defN 23-Jun-09 07:52 bigdl/llm/ggml/model/llama/llama.py
+-rw-------  2.0 unx    31490 b- defN 23-Jun-09 07:52 bigdl/llm/ggml/model/llama/llama_cpp.py
+-rw-------  2.0 unx     4207 b- defN 23-Jun-09 07:52 bigdl/llm/ggml/model/llama/llama_types.py
+-rw-------  2.0 unx      915 b- defN 23-Jun-09 07:52 bigdl/llm/ggml/transformers/__init__.py
+-rw-------  2.0 unx     5382 b- defN 23-Jun-09 07:52 bigdl/llm/ggml/transformers/model.py
+-rw-------  2.0 unx   660992 b- defN 23-Jun-09 11:32 bigdl/llm/libs/bloom.dll
+-rw-------  2.0 unx   819712 b- defN 23-Jun-09 11:32 bigdl/llm/libs/gptneox.dll
+-rw-------  2.0 unx   807424 b- defN 23-Jun-09 11:32 bigdl/llm/libs/llama.dll
+-rw-------  2.0 unx  2941913 b- defN 23-Jun-09 11:32 bigdl/llm/libs/main-bloom.exe
+-rw-------  2.0 unx  2955796 b- defN 23-Jun-09 11:32 bigdl/llm/libs/main-gptneox.exe
+-rw-------  2.0 unx  2909074 b- defN 23-Jun-09 11:32 bigdl/llm/libs/main-llama.exe
+-rw-------  2.0 unx    82432 b- defN 23-Jun-09 11:32 bigdl/llm/libs/quantize-bloom.exe
+-rw-------  2.0 unx    65024 b- defN 23-Jun-09 11:32 bigdl/llm/libs/quantize-gptneox.exe
+-rw-------  2.0 unx    25088 b- defN 23-Jun-09 11:32 bigdl/llm/libs/quantize-llama.exe
+-rw-------  2.0 unx      896 b- defN 23-Jun-09 07:52 bigdl/llm/utils/__init__.py
+-rw-------  2.0 unx    56520 b- defN 23-Jun-09 07:52 bigdl/llm/utils/convert_util.py
+-rw-------  2.0 unx     1043 b- defN 23-Jun-09 07:52 bigdl/llm/utils/utils.py
+-rw-------  2.0 unx      974 b- defN 23-Jun-09 07:52 bigdl/llm/utils/common/__init__.py
+-rw-------  2.0 unx     2874 b- defN 23-Jun-09 07:52 bigdl/llm/utils/common/lazyimport.py
+-rw-------  2.0 unx     1378 b- defN 23-Jun-09 07:52 bigdl/llm/utils/common/log4Error.py
+-rwxrwxr-x  2.0 unx     1565 b- defN 23-Jun-09 07:52 bigdl_llm-2.4.0b20230609.data/scripts/llm-cli.ps1
+-rw-------  2.0 unx      802 b- defN 23-Jun-09 11:32 bigdl_llm-2.4.0b20230609.dist-info/METADATA
+-rw-------  2.0 unx       98 b- defN 23-Jun-09 11:32 bigdl_llm-2.4.0b20230609.dist-info/WHEEL
+-rw-------  2.0 unx       69 b- defN 23-Jun-09 11:32 bigdl_llm-2.4.0b20230609.dist-info/entry_points.txt
+-rw-------  2.0 unx        6 b- defN 23-Jun-09 11:32 bigdl_llm-2.4.0b20230609.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     4050 b- defN 23-Jun-09 11:32 bigdl_llm-2.4.0b20230609.dist-info/RECORD
+45 files, 11556527 bytes uncompressed, 2854236 bytes compressed:  75.3%
```

## zipnote {}

```diff
@@ -1,14 +1,14 @@
 Filename: bigdl/__init__.py
 Comment: 
 
 Filename: bigdl/llm/__init__.py
 Comment: 
 
-Filename: bigdl/llm/cli/llm-cli
+Filename: bigdl/llm/models.py
 Comment: 
 
 Filename: bigdl/llm/cli/llm-cli.ps1
 Comment: 
 
 Filename: bigdl/llm/ggml/__init__.py
 Comment: 
@@ -105,29 +105,32 @@
 
 Filename: bigdl/llm/utils/utils.py
 Comment: 
 
 Filename: bigdl/llm/utils/common/__init__.py
 Comment: 
 
+Filename: bigdl/llm/utils/common/lazyimport.py
+Comment: 
+
 Filename: bigdl/llm/utils/common/log4Error.py
 Comment: 
 
-Filename: bigdl_llm-2.4.0b20230608.data/scripts/llm-cli.ps1
+Filename: bigdl_llm-2.4.0b20230609.data/scripts/llm-cli.ps1
 Comment: 
 
-Filename: bigdl_llm-2.4.0b20230608.dist-info/METADATA
+Filename: bigdl_llm-2.4.0b20230609.dist-info/METADATA
 Comment: 
 
-Filename: bigdl_llm-2.4.0b20230608.dist-info/WHEEL
+Filename: bigdl_llm-2.4.0b20230609.dist-info/WHEEL
 Comment: 
 
-Filename: bigdl_llm-2.4.0b20230608.dist-info/entry_points.txt
+Filename: bigdl_llm-2.4.0b20230609.dist-info/entry_points.txt
 Comment: 
 
-Filename: bigdl_llm-2.4.0b20230608.dist-info/top_level.txt
+Filename: bigdl_llm-2.4.0b20230609.dist-info/top_level.txt
 Comment: 
 
-Filename: bigdl_llm-2.4.0b20230608.dist-info/RECORD
+Filename: bigdl_llm-2.4.0b20230609.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## bigdl/llm/ggml/__init__.py

```diff
@@ -15,10 +15,9 @@
 #
 
 # This would makes sure Python is aware there is more than one sub-package within bigdl,
 # physically located elsewhere.
 # Otherwise there would be module not found error in non-pip's setting as Python would
 # only search the first bigdl package and end up finding only one sub-package.
 
-from .quantize import quantize
-from .convert import _convert_to_ggml
-from .convert_model import convert_model
+from bigdl.llm.utils.common import LazyImport
+convert_model = LazyImport('bigdl.llm.ggml.convert_model.convert_model')
```

## bigdl/llm/ggml/model/bloom/bloom.py

```diff
@@ -56,28 +56,27 @@
     """High-level Python wrapper for a bloom.cpp model."""
 
     def __init__(self,
                  model_path: str,
                  n_ctx: int = 512,
                  seed: int = 1337,
                  logits_all: bool = False,
-                 n_threads: int = -1,
+                 n_threads: int = 2,
                  n_batch: int = 8,
                  last_n_tokens_size: int = 64,
                  verbose: bool = True,
                  ):
         """Load a bloom.cpp model from `model_path`.
 
         Args:
             model_path: Path to the model.
             n_ctx: Maximum context size.
             seed: Random seed. 0 for random.
             logits_all: Return logits for all tokens, not just the last token.
-            n_threads: Number of threads to use.
-                       If None, the number of threads is automatically determined.
+            n_threads: Number of threads to use. Default to be 2.
             n_batch: Maximum number of prompt tokens to batch together when calling llama_eval.
             last_n_tokens_size: Maximum number of tokens to keep in the last_n_tokens deque.
             verbose: Print verbose output to stderr.
 
         Raises:
             ValueError: If the model path does not exist.
```

## bigdl/llm/ggml/model/gptneox/gptneox.py

```diff
@@ -135,15 +135,15 @@
         seed: int = 1337,
         f16_kv: bool = True,
         logits_all: bool = False,
         vocab_only: bool = False,
         use_mmap: bool = True,
         use_mlock: bool = False,
         embedding: bool = False,
-        n_threads: Optional[int] = None,
+        n_threads: Optional[int] = 2,
         n_batch: int = 512,
         last_n_tokens_size: int = 64,
         lora_base: Optional[str] = None,
         lora_path: Optional[str] = None,
         verbose: bool = True,
     ):
         """Load a gptneox.cpp model from `model_path`.
@@ -156,16 +156,15 @@
             seed: Random seed. 0 for random.
             f16_kv: Use half-precision for key/value cache.
             logits_all: Return logits for all tokens, not just the last token.
             vocab_only: Only load the vocabulary no weights.
             use_mmap: Use mmap if possible.
             use_mlock: Force the system to keep the model in RAM.
             embedding: Embedding mode only.
-            n_threads: Number of threads to use. If None, the number of threads
-            is automatically determined.
+            n_threads: Number of threads to use. Default to be 2.
             n_batch: Maximum number of prompt tokens to batch together when calling gptneox_eval.
             last_n_tokens_size: Maximum number of tokens to keep in the last_n_tokens deque.
             lora_base: Optional path to base model, useful if using a quantized base model and
             you want to apply LoRA to an f16 model.
             lora_path: Path to a LoRA file to apply to the model.
             verbose: Print verbose output to stderr.
 
@@ -193,15 +192,15 @@
         self.last_n_tokens_size = last_n_tokens_size
         self.n_batch = min(n_ctx, n_batch)
         self.eval_tokens: Deque[gptneox_cpp.gptneox_token] = deque(maxlen=n_ctx)
         self.eval_logits: Deque[List[float]] = deque(maxlen=n_ctx if logits_all else 1)
 
         self.cache: Optional[GptneoxCache] = None
 
-        self.n_threads = n_threads or max(multiprocessing.cpu_count() // 2, 1)
+        self.n_threads = n_threads
 
         self.lora_base = lora_base
         self.lora_path = lora_path
 
         invalidInputError(os.path.exists(model_path), f"Model path does not exist: {model_path}.")
 
         self.ctx = gptneox_cpp.gptneox_init_from_file(
```

## bigdl/llm/ggml/model/llama/llama.py

```diff
@@ -133,15 +133,15 @@
         seed: int = 1337,
         f16_kv: bool = True,
         logits_all: bool = False,
         vocab_only: bool = False,
         use_mmap: bool = True,
         use_mlock: bool = False,
         embedding: bool = False,
-        n_threads: Optional[int] = None,
+        n_threads: Optional[int] = 2,
         n_batch: int = 512,
         last_n_tokens_size: int = 64,
         lora_base: Optional[str] = None,
         lora_path: Optional[str] = None,
         verbose: bool = True,
     ):
         """Load a llama.cpp model from `model_path`.
@@ -154,16 +154,15 @@
             seed: Random seed. 0 for random.
             f16_kv: Use half-precision for key/value cache.
             logits_all: Return logits for all tokens, not just the last token.
             vocab_only: Only load the vocabulary no weights.
             use_mmap: Use mmap if possible.
             use_mlock: Force the system to keep the model in RAM.
             embedding: Embedding mode only.
-            n_threads: Number of threads to use. If None, the number of threads is
-            automatically determined.
+            n_threads: Number of threads to use. Default to be 2.
             n_batch: Maximum number of prompt tokens to batch together when calling llama_eval.
             last_n_tokens_size: Maximum number of tokens to keep in the last_n_tokens deque.
             lora_base: Optional path to base model, useful if using a quantized base model and
             you want to apply LoRA to an f16 model.
             lora_path: Path to a LoRA file to apply to the model.
             verbose: Print verbose output to stderr.
 
@@ -190,15 +189,15 @@
         self.last_n_tokens_size = last_n_tokens_size
         self.n_batch = min(n_ctx, n_batch)
         self.eval_tokens: Deque[int] = deque(maxlen=n_ctx)
         self.eval_logits: Deque[List[float]] = deque(maxlen=n_ctx if logits_all else 1)
 
         self.cache: Optional[LlamaCache] = None
 
-        self.n_threads = n_threads or max(multiprocessing.cpu_count() // 2, 1)
+        self.n_threads = n_threads
 
         self.lora_base = lora_base
         self.lora_path = lora_path
 
         # DEPRECATED
         self.n_parts = n_parts
         # DEPRECATED
```

## bigdl/llm/ggml/transformers/model.py

```diff
@@ -17,17 +17,15 @@
 # This would makes sure Python is aware there is more than one sub-package within bigdl,
 # physically located elsewhere.
 # Otherwise there would be module not found error in non-pip's setting as Python would
 # only search the first bigdl package and end up finding only one sub-package.
 
 import os
 import traceback
-from huggingface_hub import snapshot_download
 from bigdl.llm.utils.common import invalidInputError
-from bigdl.llm.ggml import convert_model
 
 
 class AutoModelForCausalLM:
     """
     A generic model class that mimics the behavior of
     ``transformers.AutoModelForCausalLM.from_pretrained`` API
     """
@@ -67,14 +65,15 @@
                           "Now we only support int4 as date type for weight")
 
         # check whether pretrained_model_name_or_path exists.
         # if not, it is likely that the user wants to pass in the repo id.
         if not os.path.exists(pretrained_model_name_or_path):
             try:
                 # download from huggingface based on repo id
+                from huggingface_hub import snapshot_download
                 pretrained_model_name_or_path = snapshot_download(
                     repo_id=pretrained_model_name_or_path)
             except Exception as e:
                 traceback.print_exc()
                 # if downloading fails, it could be the case that repo id is invalid,
                 # or the user pass in the wrong path for checkpoint
                 invalidInputError(False,
@@ -86,14 +85,15 @@
 
         ggml_model_path = pretrained_model_name_or_path
         # check whether pretrained_model_name_or_path is a file.
         # if not, it is likely that pretrained_model_name_or_path
         # points to a huggingface checkpoint
         if not os.path.isfile(pretrained_model_name_or_path):
             # huggingface checkpoint
+            from bigdl.llm.ggml import convert_model
             ggml_model_path = convert_model(input_path=pretrained_model_name_or_path,
                                             output_path=cache_dir,
                                             model_family=model_family,
                                             dtype=dtype)
 
         if model_family == 'llama':
             from bigdl.llm.ggml.model.llama import Llama
```

## bigdl/llm/utils/convert_util.py

```diff
@@ -62,14 +62,16 @@
 from dataclasses import dataclass
 from pathlib import Path
 from typing import (IO, TYPE_CHECKING, Any, Callable, Dict, Iterable, List,
                     Literal, Optional, Sequence, Tuple, TypeVar, Union)
 import numpy as np
 from sentencepiece import SentencePieceProcessor
 from bigdl.llm.utils.common import invalidInputError
+import os
+from pathlib import Path
 
 if TYPE_CHECKING:
     from typing_extensions import TypeAlias
 
 if hasattr(faulthandler, 'register') and hasattr(signal, 'SIGUSR1'):
     faulthandler.register(signal.SIGUSR1)
 
@@ -1244,15 +1246,16 @@
                                                  if outtype == "f16" else torch.float32)
 
     model.eval()
     for p in model.parameters():
         p.requires_grad = False
     hparams = model.config.to_dict()
 
-    fn_out = outfile_dir + f"/ggml-{model_path.split('/')[-1]}-{outtype}.bin"
+    filestem = Path(model_path).stem
+    fn_out = os.path.join(outfile_dir, f"ggml-{filestem}-{outtype}.bin")
     fout = open(fn_out, "wb")
 
     ggml_file_magic = 0x67676d66  # 0x67676d6c is unversioned
     ggml_file_version = 0x00000001  # v1
 
     if outtype == "f16":
         ftype = 1
@@ -1335,15 +1338,16 @@
     config = AutoConfig.from_pretrained(model_path)
     hparams = config.to_dict()
     model = AutoModelForCausalLM.from_pretrained(model_path, config=config,
                                                  torch_dtype=torch.float16
                                                  if outtype == "f16" else torch.float32,
                                                  low_cpu_mem_usage=True)
 
-    fn_out = outfile_dir + f"/ggml-model-{model_path.split('/')[-1]}-{outtype}.bin"
+    filestem = Path(model_path).stem
+    fn_out = os.path.join(outfile_dir, f"ggml-{filestem}-{outtype}.bin")
     fout = open(fn_out, "wb")
 
     if outtype == "f16":
         ftype = 1
     else:
         ftype = 0
```

## bigdl/llm/utils/common/__init__.py

```diff
@@ -16,7 +16,8 @@
 
 # This would makes sure Python is aware there is more than one sub-package within bigdl,
 # physically located elsewhere.
 # Otherwise there would be module not found error in non-pip's setting as Python would
 # only search the first bigdl package and end up finding only one sub-package.
 
 from .log4Error import invalidInputError, invalidOperationError
+from .lazyimport import LazyImport
```

## Comparing `bigdl_llm-2.4.0b20230608.data/scripts/llm-cli.ps1` & `bigdl_llm-2.4.0b20230609.data/scripts/llm-cli.ps1`

 * *Files identical despite different names*

## Comparing `bigdl_llm-2.4.0b20230608.dist-info/METADATA` & `bigdl_llm-2.4.0b20230609.dist-info/METADATA`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: bigdl-llm
-Version: 2.4.0b20230608
+Version: 2.4.0b20230609
 Summary: Large Language Model Develop Toolkit
 Home-page: https://github.com/intel-analytics/BigDL
 Author: BigDL Authors
 Author-email: bigdl-user-group@googlegroups.com
 License: Apache License, Version 2.0
 Platform: windows
 Classifier: License :: OSI Approved :: Apache Software License
@@ -16,8 +16,10 @@
 Requires-Dist: numpy ; extra == 'all'
 Requires-Dist: torch ; extra == 'all'
 Requires-Dist: transformers ; extra == 'all'
 Requires-Dist: sentencepiece ; extra == 'all'
 Requires-Dist: accelerate ; extra == 'all'
 
 
-    BigDL LLM
+BigDL LLM
+
+
```

## Comparing `bigdl_llm-2.4.0b20230608.dist-info/RECORD` & `bigdl_llm-2.4.0b20230609.dist-info/RECORD`

 * *Files 11% similar despite different names*

```diff
@@ -1,44 +1,45 @@
 bigdl/__init__.py,sha256=9RhNOlx9rGC96ZLIJFCFSApoifpjHoayBKx7N9ZKSno,956
 bigdl/llm/__init__.py,sha256=T-EbRT6GJ_8RCu-iLmSzcftOimXSPQf2d5X72AUAy2Y,874
-bigdl/llm/cli/llm-cli,sha256=GXatwN8lwSLuBYcs4D0t_33cz3yAuImthp5gCivWzxU,1780
+bigdl/llm/models.py,sha256=KwzU9MnflAlyPPDDcEAWwcEuORedoKOAB-maekBnn-o,1014
 bigdl/llm/cli/llm-cli.ps1,sha256=r0bBuj3v912WKb4NQ9q807heEykIeRHzxOgd01QFsfc,1565
-bigdl/llm/ggml/__init__.py,sha256=7ultjf6Js_rAuNmCYS1vfIVNOdedpXq7-2l3MAJ9nPk,985
+bigdl/llm/ggml/__init__.py,sha256=Flc31gOq42SIKILIniO8i7Zp_is1A3uKRcqBM7TFK1g,994
 bigdl/llm/ggml/convert.py,sha256=HYiNMcIGVKXq4fpojr51P7gwku5eY5kzhOkdTtpls0s,4991
 bigdl/llm/ggml/convert_model.py,sha256=MEC9I7Ap8x0E5AbpJWn-NfNwJGuVtzhokVUHp-F4QYo,5431
 bigdl/llm/ggml/quantize.py,sha256=sqMiHwphbq9NXI_Z1NpPTjWw8umP8RHKEYVuikrI1xY,4334
 bigdl/llm/ggml/model/__init__.py,sha256=T-EbRT6GJ_8RCu-iLmSzcftOimXSPQf2d5X72AUAy2Y,874
 bigdl/llm/ggml/model/bloom/__init__.py,sha256=291QHI19FMw7Z1oaKBAf2YJ0M51iYqWC4IT1ejI-OGg,900
-bigdl/llm/ggml/model/bloom/bloom.py,sha256=UURKde60On4ZTI3ghYJ6AC96zyygXCiOtvEdUBZ_TiE,8702
+bigdl/llm/ggml/model/bloom/bloom.py,sha256=e9_6-P-F5taIC7HMP_8jF-_UHQNAZNuiS07hGeIViko,8635
 bigdl/llm/ggml/model/bloom/bloom_cpp.py,sha256=BiSGP6vl-10ltCzlQEE3ekUfuFC-X952KIeXK19vjc8,5241
 bigdl/llm/ggml/model/generation/__init__.py,sha256=Yr9QhfkBaoqd6WHmGVeWi16InwbBUrqSDphSQw__R38,910
 bigdl/llm/ggml/model/generation/utils.py,sha256=pNmsHNlYs2Yr-XNL__hlRG7DiTV174AMaP8eigttINc,6106
 bigdl/llm/ggml/model/gptneox/__init__.py,sha256=7AwRAlz1bpOj8L4yFy7zp3KSG9WTxLdSD6PmFmr76MA,925
-bigdl/llm/ggml/model/gptneox/gptneox.py,sha256=8JIh93-6S4WfXDbGpGilQdfnkgv7YyhaYMHTq8smdJ0,47025
+bigdl/llm/ggml/model/gptneox/gptneox.py,sha256=fwtOAKd9PwfHNBdIOY1Tess2YR1Lw6nP724HJBBDCO0,46923
 bigdl/llm/ggml/model/gptneox/gptneox_cpp.py,sha256=zIadcSBku_HjMpuqKi3apjsM36J3YM_Fw9Ar3fjuo4E,26490
 bigdl/llm/ggml/model/gptneox/gptneox_types.py,sha256=UXBo-BjTJLyaB68ZnJyDDpEATuYaTlbN_0zbef4rVAg,4194
 bigdl/llm/ggml/model/llama/__init__.py,sha256=RuXaQsQZDvprNMCF0hySt13IpwjnPGrlRqaQfpZT4Ig,921
-bigdl/llm/ggml/model/llama/llama.py,sha256=kJnL31DDRsCnq3sEHAbbEfr_C_obZd8Dj0WwPUXNpJE,54627
+bigdl/llm/ggml/model/llama/llama.py,sha256=E96Ylt6bVo1ZUD67bAZWpZ-Vs6swEzcnvJ_i6wadfP4,54525
 bigdl/llm/ggml/model/llama/llama_cpp.py,sha256=aIJW1TE53pfTBFFierrobFCe_3qm3I4zsd11FWVaxNQ,31490
 bigdl/llm/ggml/model/llama/llama_types.py,sha256=cesJvcZxWe8hp0MU5BP-iBwKzoUqS67yiQZnCa71p48,4207
 bigdl/llm/ggml/transformers/__init__.py,sha256=4yboXMiDVybwGmqJ4Nv2-tj5eCPHLL6K7O27KZNhO1g,915
-bigdl/llm/ggml/transformers/model.py,sha256=y787OCMFsHwS5ieLTQPJPl0x9JcPZ84WW4CR9voNdcI,5354
+bigdl/llm/ggml/transformers/model.py,sha256=3D1X_PvF4vc1fMRLcEg6bmRDNm88rTdQIBrWDIr1BY8,5382
 bigdl/llm/libs/bloom.dll,sha256=mNIujaN3IkRQuEK6hVvJ5RQa1o1VTayD7bSbm_HxImM,660992
 bigdl/llm/libs/gptneox.dll,sha256=-Ok-uIFw2Bfq_ibW55D8eHa21qiYNSilfXup0vUeK8k,819712
 bigdl/llm/libs/llama.dll,sha256=OWkctBdXBuIpIyR2sTr17giSb417T0FTqDUgcaCCFyk,807424
 bigdl/llm/libs/main-bloom.exe,sha256=JiSeL4P9vc5_HdI_AWy321C2EPhwzfnzPnPlI_US5LQ,2941913
 bigdl/llm/libs/main-gptneox.exe,sha256=eMhADQ3pQY4FtGhj1Kw0PTTebkY3YKP2wXya24QIUn8,2955796
 bigdl/llm/libs/main-llama.exe,sha256=dNNTRlQnwgWFPFTC13SeWTzUcEUYSiPxKdf7fQcNpTA,2909074
 bigdl/llm/libs/quantize-bloom.exe,sha256=MtbBGtT9UDP5gT_Be3oOxAkXNYgWPIBBIZ6R3aUyH-4,82432
 bigdl/llm/libs/quantize-gptneox.exe,sha256=BoWu6AO8yzm1xZpmAucVCrJdpiH6ZaHcjRtnr6Jx69c,65024
 bigdl/llm/libs/quantize-llama.exe,sha256=VcN-2C14_YOUf1OQA8L0Rc4sqL-i6uvR4xnScB5c76s,25088
 bigdl/llm/utils/__init__.py,sha256=f_oavwQog8Wd1fSIebb-KgA61UKSzcOgw4cBxMHyS_w,896
-bigdl/llm/utils/convert_util.py,sha256=QO-kRE83e16GkDGvOSLuYIV6FmdobwXYcS4Gbo6g3f4,56427
+bigdl/llm/utils/convert_util.py,sha256=p0_cARIfLKyUJGJ5EXRIjq04pEv1pLiTb5IGy8mrjJ4,56520
 bigdl/llm/utils/utils.py,sha256=hKzroKKjlMead-Xtj3D2oWKG-Sjw4BJ2MR0-U3I-tPc,1043
-bigdl/llm/utils/common/__init__.py,sha256=bfBIGO0D7DqnCHEtlLq-3nA-MkyfkdnJpCEXXxOcAQI,939
+bigdl/llm/utils/common/__init__.py,sha256=4RHJ8vO-C1rhOMeU2X7ZSY_9RSCYXdM2tceIIcdy7u4,974
+bigdl/llm/utils/common/lazyimport.py,sha256=TVDp-rtc6IHopmlS9WEIgP5GKg5i0iWGm8khPwG4jTo,2874
 bigdl/llm/utils/common/log4Error.py,sha256=tVYLVKyPvqOphUBdV9hZRr6yJgQpDLAAq8OlW-5Xcto,1378
-bigdl_llm-2.4.0b20230608.data/scripts/llm-cli.ps1,sha256=r0bBuj3v912WKb4NQ9q807heEykIeRHzxOgd01QFsfc,1565
-bigdl_llm-2.4.0b20230608.dist-info/METADATA,sha256=NZ7hTLUuHSC6A22fppTn5MSr9dGQn7aOb_9b-nsemCA,804
-bigdl_llm-2.4.0b20230608.dist-info/WHEEL,sha256=AiyVnrmrEuW_cPqhPlarzpb5qtD8ITcBeZKvhXXgyN4,98
-bigdl_llm-2.4.0b20230608.dist-info/entry_points.txt,sha256=FVpeY6-F9uXHJNtQjlqTEMk_J4R-3i7BLfc1SnzGLag,68
-bigdl_llm-2.4.0b20230608.dist-info/top_level.txt,sha256=iGuLfZARD_qANcIMfy0tbbrC3EtCg6BSiH8icc3dLWs,6
-bigdl_llm-2.4.0b20230608.dist-info/RECORD,,
+bigdl_llm-2.4.0b20230609.data/scripts/llm-cli.ps1,sha256=r0bBuj3v912WKb4NQ9q807heEykIeRHzxOgd01QFsfc,1565
+bigdl_llm-2.4.0b20230609.dist-info/METADATA,sha256=QPvU6Tg3sAHDHvszRcgi4eRVRUhk4AE-GSjrKlFyzbc,802
+bigdl_llm-2.4.0b20230609.dist-info/WHEEL,sha256=bC8mYJUOJCh5KnyEeT6W_BCQYi3v39D3z64Vy_sFvVg,98
+bigdl_llm-2.4.0b20230609.dist-info/entry_points.txt,sha256=YUVSS5d4e1iCyW1aLzD73Mm822jg8xQcm2pcAF1V2oY,69
+bigdl_llm-2.4.0b20230609.dist-info/top_level.txt,sha256=iGuLfZARD_qANcIMfy0tbbrC3EtCg6BSiH8icc3dLWs,6
+bigdl_llm-2.4.0b20230609.dist-info/RECORD,,
```

