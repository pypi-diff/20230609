# Comparing `tmp/towhee-1.0.0rc1-py3-none-any.whl.zip` & `tmp/towhee-1.1.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,146 +1,154 @@
-Zip file size: 207434 bytes, number of entries: 144
--rw-r--r--  2.0 unx     2855 b- defN 23-Mar-21 08:26 towhee/__init__.py
--rw-r--r--  2.0 unx      665 b- defN 23-Mar-21 08:26 towhee/__main__.py
--rw-r--r--  2.0 unx      619 b- defN 23-Mar-21 08:26 towhee/command/__init__.py
--rw-r--r--  2.0 unx     1627 b- defN 23-Mar-21 08:26 towhee/command/cmdline.py
--rw-r--r--  2.0 unx     4668 b- defN 23-Mar-21 08:26 towhee/command/develop.py
--rw-r--r--  2.0 unx     4223 b- defN 23-Mar-21 08:26 towhee/command/execute.py
--rw-r--r--  2.0 unx     4908 b- defN 23-Mar-21 08:26 towhee/command/repo.py
--rw-r--r--  2.0 unx     5240 b- defN 23-Mar-21 08:26 towhee/command/user.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/data/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/data/dataset/__init__.py
--rw-r--r--  2.0 unx      662 b- defN 23-Mar-21 08:26 towhee/data/dataset/dataset.py
--rw-r--r--  2.0 unx     2918 b- defN 23-Mar-21 08:26 towhee/data/dataset/image_datasets.py
--rw-r--r--  2.0 unx      714 b- defN 23-Mar-21 08:26 towhee/datacollection/__init__.py
--rw-r--r--  2.0 unx     7581 b- defN 23-Mar-21 08:26 towhee/datacollection/data_collection.py
--rw-r--r--  2.0 unx     3448 b- defN 23-Mar-21 08:26 towhee/datacollection/entity.py
--rw-r--r--  2.0 unx      627 b- defN 23-Mar-21 08:26 towhee/datacollection/mixins/__init__.py
--rw-r--r--  2.0 unx    11315 b- defN 23-Mar-21 08:26 towhee/datacollection/mixins/display.py
--rw-r--r--  2.0 unx      934 b- defN 23-Mar-21 08:26 towhee/hub/__init__.py
--rw-r--r--  2.0 unx     3201 b- defN 23-Mar-21 08:26 towhee/hub/cache_manager.py
--rw-r--r--  2.0 unx     5748 b- defN 23-Apr-27 08:00 towhee/hub/downloader.py
--rw-r--r--  2.0 unx     4697 b- defN 23-Mar-21 08:26 towhee/hub/operator_manager.py
--rw-r--r--  2.0 unx     3755 b- defN 23-Mar-21 08:26 towhee/hub/pipeline_manager.py
--rw-r--r--  2.0 unx    10289 b- defN 23-Mar-21 08:26 towhee/hub/repo_manager.py
--rw-r--r--  2.0 unx      777 b- defN 23-Mar-21 08:26 towhee/operator/__init__.py
--rw-r--r--  2.0 unx     7758 b- defN 23-Mar-21 08:26 towhee/operator/base.py
--rw-r--r--  2.0 unx     2031 b- defN 23-Mar-21 08:26 towhee/operator/concat_operator.py
--rw-r--r--  2.0 unx     1444 b- defN 23-Mar-21 08:26 towhee/operator/nop.py
--rw-r--r--  2.0 unx      975 b- defN 23-Mar-21 08:26 towhee/pipelines/__init__.py
--rw-r--r--  2.0 unx      947 b- defN 23-Mar-21 08:26 towhee/pipelines/_builtin_pipeline.py
--rw-r--r--  2.0 unx     1471 b- defN 23-Mar-21 08:26 towhee/pipelines/insert_milvus.py
--rw-r--r--  2.0 unx     1548 b- defN 23-Mar-21 08:26 towhee/pipelines/search_milvus.py
--rw-r--r--  2.0 unx     2871 b- defN 23-Mar-21 08:26 towhee/pipelines/sentence_embedding.py
--rw-r--r--  2.0 unx     2306 b- defN 23-Mar-21 08:26 towhee/pipelines/text_image_embedding.py
--rw-r--r--  2.0 unx     6886 b- defN 23-Mar-21 08:26 towhee/pipelines/video_copy_detection.py
--rw-r--r--  2.0 unx     4850 b- defN 23-Mar-21 08:26 towhee/pipelines/video_embedding.py
--rw-r--r--  2.0 unx     1003 b- defN 23-Mar-21 08:26 towhee/runtime/__init__.py
--rw-r--r--  2.0 unx     8984 b- defN 23-Mar-21 08:26 towhee/runtime/auto_config.py
--rw-r--r--  2.0 unx     2280 b- defN 23-Mar-21 08:26 towhee/runtime/auto_pipes.py
--rw-r--r--  2.0 unx     5091 b- defN 23-Mar-21 08:26 towhee/runtime/check_utils.py
--rw-r--r--  2.0 unx     1413 b- defN 23-Mar-21 08:26 towhee/runtime/constants.py
--rw-r--r--  2.0 unx    13063 b- defN 23-Mar-21 08:26 towhee/runtime/dag_repr.py
--rw-r--r--  2.0 unx     9260 b- defN 23-Mar-21 08:26 towhee/runtime/data_queue.py
--rw-r--r--  2.0 unx     3571 b- defN 23-Mar-21 08:26 towhee/runtime/factory.py
--rw-r--r--  2.0 unx     6619 b- defN 23-Mar-21 08:26 towhee/runtime/node_config.py
--rw-r--r--  2.0 unx     7149 b- defN 23-Mar-21 08:26 towhee/runtime/node_repr.py
--rw-r--r--  2.0 unx    12539 b- defN 23-Mar-21 08:26 towhee/runtime/performance_profiler.py
--rw-r--r--  2.0 unx    16957 b- defN 23-Mar-21 08:26 towhee/runtime/pipeline.py
--rw-r--r--  2.0 unx     2018 b- defN 23-Mar-21 08:26 towhee/runtime/pipeline_loader.py
--rw-r--r--  2.0 unx     2821 b- defN 23-Mar-21 08:26 towhee/runtime/runtime_conf.py
--rw-r--r--  2.0 unx     6648 b- defN 23-Mar-21 08:26 towhee/runtime/runtime_pipeline.py
--rw-r--r--  2.0 unx     2335 b- defN 23-Mar-21 08:26 towhee/runtime/schema_repr.py
--rw-r--r--  2.0 unx     2572 b- defN 23-Mar-21 08:26 towhee/runtime/nodes/__init__.py
--rw-r--r--  2.0 unx     2377 b- defN 23-Mar-21 08:26 towhee/runtime/nodes/_concat.py
--rw-r--r--  2.0 unx     2722 b- defN 23-Mar-21 08:26 towhee/runtime/nodes/_filter.py
--rw-r--r--  2.0 unx     2879 b- defN 23-Mar-21 08:26 towhee/runtime/nodes/_flat_map.py
--rw-r--r--  2.0 unx     3551 b- defN 23-Mar-21 08:26 towhee/runtime/nodes/_map.py
--rw-r--r--  2.0 unx     1630 b- defN 23-Mar-21 08:26 towhee/runtime/nodes/_output.py
--rw-r--r--  2.0 unx     4481 b- defN 23-Mar-21 08:26 towhee/runtime/nodes/_time_window.py
--rw-r--r--  2.0 unx     5737 b- defN 23-Mar-21 08:26 towhee/runtime/nodes/_window.py
--rw-r--r--  2.0 unx     3310 b- defN 23-Mar-21 08:26 towhee/runtime/nodes/_window_all.py
--rw-r--r--  2.0 unx     6165 b- defN 23-Mar-21 08:26 towhee/runtime/nodes/node.py
--rw-r--r--  2.0 unx      872 b- defN 23-Mar-21 08:26 towhee/runtime/operator_manager/__init__.py
--rw-r--r--  2.0 unx     3016 b- defN 23-Mar-21 08:26 towhee/runtime/operator_manager/operator_action.py
--rw-r--r--  2.0 unx     6478 b- defN 23-Mar-21 08:26 towhee/runtime/operator_manager/operator_loader.py
--rw-r--r--  2.0 unx     3634 b- defN 23-Mar-21 08:26 towhee/runtime/operator_manager/operator_pool.py
--rw-r--r--  2.0 unx     2867 b- defN 23-Mar-21 08:26 towhee/runtime/operator_manager/operator_registry.py
--rw-r--r--  2.0 unx     1951 b- defN 23-Mar-21 08:26 towhee/runtime/operator_manager/uri.py
--rw-r--r--  2.0 unx      592 b- defN 23-Mar-21 08:26 towhee/serve/__init__.py
--rw-r--r--  2.0 unx     2005 b- defN 23-Mar-21 08:26 towhee/serve/server_builder.py
--rw-r--r--  2.0 unx      773 b- defN 23-Mar-21 08:26 towhee/serve/triton/__init__.py
--rw-r--r--  2.0 unx      816 b- defN 23-Mar-21 08:26 towhee/serve/triton/constant.py
--rw-r--r--  2.0 unx     2330 b- defN 23-Mar-21 08:26 towhee/serve/triton/docker_image_builder.py
--rw-r--r--  2.0 unx     4500 b- defN 23-Apr-27 08:00 towhee/serve/triton/model_to_triton.py
--rw-r--r--  2.0 unx     3483 b- defN 23-Apr-27 08:00 towhee/serve/triton/pipe_to_triton.py
--rw-r--r--  2.0 unx     4286 b- defN 23-Mar-21 08:26 towhee/serve/triton/pipeline_builder.py
--rw-r--r--  2.0 unx     4051 b- defN 23-Mar-21 08:26 towhee/serve/triton/pipeline_client.py
--rw-r--r--  2.0 unx     1729 b- defN 23-Mar-21 08:26 towhee/serve/triton/serializer.py
--rw-r--r--  2.0 unx     2408 b- defN 23-Mar-21 08:26 towhee/serve/triton/triton_client.py
--rw-r--r--  2.0 unx     2547 b- defN 23-Mar-21 08:26 towhee/serve/triton/triton_config_builder.py
--rw-r--r--  2.0 unx     1499 b- defN 23-Mar-21 08:26 towhee/serve/triton/triton_files.py
--rw-r--r--  2.0 unx      706 b- defN 23-Mar-21 08:26 towhee/serve/triton/bls/__init__.py
--rw-r--r--  2.0 unx     2228 b- defN 23-Mar-21 08:26 towhee/serve/triton/bls/pipeline_model.py
--rw-r--r--  2.0 unx      934 b- defN 23-Mar-21 08:26 towhee/serve/triton/bls/python_backend_wrapper.py
--rw-r--r--  2.0 unx      592 b- defN 23-Mar-21 08:26 towhee/serve/triton/bls/caller/__init__.py
--rw-r--r--  2.0 unx     3437 b- defN 23-Mar-21 08:26 towhee/serve/triton/bls/caller/local_caller.py
--rw-r--r--  2.0 unx      592 b- defN 23-Mar-21 08:26 towhee/serve/triton/bls/mock/__init__.py
--rw-r--r--  2.0 unx     4727 b- defN 23-Mar-21 08:26 towhee/serve/triton/bls/mock/mock_pb_util.py
--rw-r--r--  2.0 unx     2699 b- defN 23-Mar-21 08:26 towhee/serve/triton/bls/mock/mock_triton_client.py
--rw-r--r--  2.0 unx      750 b- defN 23-Apr-27 08:00 towhee/serve/triton/dockerfiles/DockerfileCuda113
--rw-r--r--  2.0 unx      748 b- defN 23-Apr-27 08:00 towhee/serve/triton/dockerfiles/DockerfileCuda114
--rw-r--r--  2.0 unx      748 b- defN 23-Apr-27 08:00 towhee/serve/triton/dockerfiles/DockerfileCuda116
--rw-r--r--  2.0 unx      748 b- defN 23-Apr-27 08:00 towhee/serve/triton/dockerfiles/DockerfileCuda117
--rw-r--r--  2.0 unx      956 b- defN 23-Apr-27 08:00 towhee/serve/triton/dockerfiles/DockerfileCuda117dev
--rw-r--r--  2.0 unx     1317 b- defN 23-Mar-21 08:26 towhee/serve/triton/dockerfiles/__init__.py
--rw-r--r--  2.0 unx      940 b- defN 23-Mar-21 08:26 towhee/trainer/__init__.py
--rw-r--r--  2.0 unx    26591 b- defN 23-Mar-21 08:26 towhee/trainer/callback.py
--rw-r--r--  2.0 unx    10668 b- defN 23-Mar-21 08:26 towhee/trainer/metrics.py
--rw-r--r--  2.0 unx     8861 b- defN 23-Mar-21 08:26 towhee/trainer/modelcard.py
--rw-r--r--  2.0 unx    14904 b- defN 23-Mar-21 08:26 towhee/trainer/scheduler.py
--rw-r--r--  2.0 unx    34675 b- defN 23-Mar-21 08:26 towhee/trainer/trainer.py
--rw-r--r--  2.0 unx    14667 b- defN 23-Mar-21 08:26 towhee/trainer/training_config.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/trainer/optimization/__init__.py
--rw-r--r--  2.0 unx     7830 b- defN 23-Mar-21 08:26 towhee/trainer/optimization/adafactor.py
--rw-r--r--  2.0 unx     5148 b- defN 23-Mar-21 08:26 towhee/trainer/optimization/adamw.py
--rw-r--r--  2.0 unx    12006 b- defN 23-Mar-21 08:26 towhee/trainer/optimization/optimization.py
--rw-r--r--  2.0 unx      592 b- defN 23-Mar-21 08:26 towhee/trainer/utils/__init__.py
--rw-r--r--  2.0 unx      970 b- defN 23-Mar-21 08:26 towhee/trainer/utils/file_utils.py
--rw-r--r--  2.0 unx     5007 b- defN 23-Mar-21 08:26 towhee/trainer/utils/layer_freezer.py
--rw-r--r--  2.0 unx    16453 b- defN 23-Mar-21 08:26 towhee/trainer/utils/plot_utils.py
--rw-r--r--  2.0 unx     9017 b- defN 23-Mar-21 08:26 towhee/trainer/utils/trainer_utils.py
--rw-r--r--  2.0 unx      872 b- defN 23-Mar-21 08:26 towhee/types/__init__.py
--rw-r--r--  2.0 unx     2341 b- defN 23-Mar-21 08:26 towhee/types/arg.py
--rw-r--r--  2.0 unx     2141 b- defN 23-Mar-21 08:26 towhee/types/audio_frame.py
--rw-r--r--  2.0 unx     2443 b- defN 23-Mar-21 08:26 towhee/types/image.py
--rw-r--r--  2.0 unx     2016 b- defN 23-Mar-21 08:26 towhee/types/image_utils.py
--rw-r--r--  2.0 unx     2649 b- defN 23-Mar-21 08:26 towhee/types/video_frame.py
--rw-r--r--  2.0 unx      592 b- defN 23-Mar-21 08:26 towhee/utils/__init__.py
--rw-r--r--  2.0 unx     1178 b- defN 23-Mar-21 08:26 towhee/utils/cv2_utils.py
--rw-r--r--  2.0 unx     1403 b- defN 23-Mar-21 08:26 towhee/utils/dependency_control.py
--rw-r--r--  2.0 unx      829 b- defN 23-Mar-21 08:26 towhee/utils/empty_format.py
--rw-r--r--  2.0 unx     7744 b- defN 23-Mar-21 08:26 towhee/utils/git_utils.py
--rw-r--r--  2.0 unx     1381 b- defN 23-Mar-21 08:26 towhee/utils/hub_file_utils.py
--rw-r--r--  2.0 unx    13148 b- defN 23-Mar-21 08:26 towhee/utils/hub_utils.py
--rw-r--r--  2.0 unx     1301 b- defN 23-Mar-21 08:26 towhee/utils/lazy_import.py
--rw-r--r--  2.0 unx      894 b- defN 23-Mar-21 08:26 towhee/utils/log.py
--rw-r--r--  2.0 unx     2975 b- defN 23-Mar-21 08:26 towhee/utils/matplotlib_utils.py
--rw-r--r--  2.0 unx     4385 b- defN 23-Mar-21 08:26 towhee/utils/ndarray_utils.py
--rw-r--r--  2.0 unx     2414 b- defN 23-Mar-21 08:26 towhee/utils/np_format.py
--rw-r--r--  2.0 unx     1156 b- defN 23-Mar-21 08:26 towhee/utils/onnx_utils.py
--rw-r--r--  2.0 unx     2571 b- defN 23-Mar-21 08:26 towhee/utils/pil_utils.py
--rw-r--r--  2.0 unx     8448 b- defN 23-Mar-21 08:26 towhee/utils/repo_normalize.py
--rw-r--r--  2.0 unx     1102 b- defN 23-Mar-21 08:26 towhee/utils/singleton.py
--rw-r--r--  2.0 unx     1538 b- defN 23-Mar-21 08:26 towhee/utils/triton_httpclient.py
--rw-r--r--  2.0 unx     2535 b- defN 23-Mar-21 08:26 towhee/utils/yaml_utils.py
--rw-r--r--  2.0 unx      592 b- defN 23-Mar-21 08:26 towhee/utils/thirdparty/__init__.py
--rw-r--r--  2.0 unx      845 b- defN 23-Mar-21 08:26 towhee/utils/thirdparty/dill_util.py
--rw-r--r--  2.0 unx     1230 b- defN 23-Mar-21 08:26 towhee/utils/thirdparty/ipython_utils.py
--rw-r--r--  2.0 unx     1130 b- defN 23-Mar-21 08:26 towhee/utils/thirdparty/pandas_utils.py
--rw-r--r--  2.0 unx    11357 b- defN 23-May-04 08:04 towhee-1.0.0rc1.dist-info/LICENSE
--rw-r--r--  2.0 unx    17248 b- defN 23-May-04 08:04 towhee-1.0.0rc1.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-May-04 08:04 towhee-1.0.0rc1.dist-info/WHEEL
--rw-r--r--  2.0 unx      114 b- defN 23-May-04 08:04 towhee-1.0.0rc1.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        7 b- defN 23-May-04 08:04 towhee-1.0.0rc1.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    12830 b- defN 23-May-04 08:04 towhee-1.0.0rc1.dist-info/RECORD
-144 files, 604582 bytes uncompressed, 187054 bytes compressed:  69.1%
+Zip file size: 221205 bytes, number of entries: 152
+-rw-r--r--  2.0 unx     5836 b- defN 23-Jun-09 07:02 towhee/__init__.py
+-rw-r--r--  2.0 unx      665 b- defN 22-Aug-19 02:52 towhee/__main__.py
+-rw-r--r--  2.0 unx     2527 b- defN 23-Jun-09 07:02 towhee/data_loader.py
+-rw-r--r--  2.0 unx      619 b- defN 22-Aug-19 02:52 towhee/command/__init__.py
+-rw-r--r--  2.0 unx     1627 b- defN 23-Mar-13 07:00 towhee/command/cmdline.py
+-rw-r--r--  2.0 unx     4668 b- defN 22-Oct-08 02:27 towhee/command/develop.py
+-rw-r--r--  2.0 unx     4223 b- defN 23-Mar-13 07:00 towhee/command/execute.py
+-rw-r--r--  2.0 unx     4908 b- defN 22-Oct-08 02:27 towhee/command/repo.py
+-rw-r--r--  2.0 unx     5240 b- defN 22-Oct-08 02:27 towhee/command/user.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/data/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/data/dataset/__init__.py
+-rw-r--r--  2.0 unx     1255 b- defN 23-May-17 03:38 towhee/data/dataset/dataset.py
+-rw-r--r--  2.0 unx     2918 b- defN 22-Oct-08 02:27 towhee/data/dataset/image_datasets.py
+-rw-r--r--  2.0 unx      714 b- defN 22-Nov-03 09:21 towhee/datacollection/__init__.py
+-rw-r--r--  2.0 unx     8283 b- defN 23-May-04 09:35 towhee/datacollection/data_collection.py
+-rw-r--r--  2.0 unx     3448 b- defN 22-Nov-03 09:21 towhee/datacollection/entity.py
+-rw-r--r--  2.0 unx      627 b- defN 22-Oct-12 06:36 towhee/datacollection/mixins/__init__.py
+-rw-r--r--  2.0 unx     1900 b- defN 23-May-17 03:38 towhee/datacollection/mixins/display.py
+-rw-r--r--  2.0 unx     1093 b- defN 23-May-04 09:35 towhee/hub/__init__.py
+-rw-r--r--  2.0 unx     5178 b- defN 23-May-04 09:35 towhee/hub/cache_manager.py
+-rw-r--r--  2.0 unx     6414 b- defN 23-Jun-09 07:02 towhee/hub/downloader.py
+-rw-r--r--  2.0 unx     4697 b- defN 22-Oct-08 02:27 towhee/hub/operator_manager.py
+-rw-r--r--  2.0 unx     3755 b- defN 22-Oct-08 02:27 towhee/hub/pipeline_manager.py
+-rw-r--r--  2.0 unx    10289 b- defN 23-Mar-03 09:49 towhee/hub/repo_manager.py
+-rw-r--r--  2.0 unx      777 b- defN 23-Mar-13 07:00 towhee/operator/__init__.py
+-rw-r--r--  2.0 unx     7747 b- defN 23-Jun-09 07:02 towhee/operator/base.py
+-rw-r--r--  2.0 unx      944 b- defN 23-Jun-09 07:02 towhee/operator/nop.py
+-rw-r--r--  2.0 unx      975 b- defN 23-Feb-06 09:20 towhee/pipelines/__init__.py
+-rw-r--r--  2.0 unx      947 b- defN 23-Mar-13 07:00 towhee/pipelines/_builtin_pipeline.py
+-rw-r--r--  2.0 unx     1547 b- defN 23-Jun-09 07:02 towhee/pipelines/insert_milvus.py
+-rw-r--r--  2.0 unx     1654 b- defN 23-Jun-09 07:02 towhee/pipelines/search_milvus.py
+-rw-r--r--  2.0 unx     2953 b- defN 23-Jun-09 07:02 towhee/pipelines/sentence_embedding.py
+-rw-r--r--  2.0 unx     2388 b- defN 23-Jun-09 07:02 towhee/pipelines/text_image_embedding.py
+-rw-r--r--  2.0 unx     6891 b- defN 23-Jun-09 07:02 towhee/pipelines/video_copy_detection.py
+-rw-r--r--  2.0 unx     4858 b- defN 23-Jun-09 07:02 towhee/pipelines/video_embedding.py
+-rw-r--r--  2.0 unx     1004 b- defN 23-Mar-13 07:00 towhee/runtime/__init__.py
+-rw-r--r--  2.0 unx     9344 b- defN 23-Jun-09 07:02 towhee/runtime/auto_config.py
+-rw-r--r--  2.0 unx     2854 b- defN 23-Jun-09 07:02 towhee/runtime/auto_pipes.py
+-rw-r--r--  2.0 unx     3771 b- defN 23-Jun-09 07:02 towhee/runtime/check_utils.py
+-rw-r--r--  2.0 unx     1508 b- defN 23-May-04 09:35 towhee/runtime/constants.py
+-rw-r--r--  2.0 unx    21806 b- defN 23-Jun-09 07:02 towhee/runtime/dag_repr.py
+-rw-r--r--  2.0 unx    10047 b- defN 23-May-04 09:35 towhee/runtime/data_queue.py
+-rw-r--r--  2.0 unx     4392 b- defN 23-Jun-09 07:02 towhee/runtime/factory.py
+-rw-r--r--  2.0 unx     3420 b- defN 23-Jun-09 07:02 towhee/runtime/node_config.py
+-rw-r--r--  2.0 unx     2838 b- defN 23-Jun-09 07:02 towhee/runtime/node_repr.py
+-rw-r--r--  2.0 unx    18802 b- defN 23-Jun-09 07:02 towhee/runtime/pipeline.py
+-rw-r--r--  2.0 unx     2420 b- defN 23-May-04 09:35 towhee/runtime/pipeline_loader.py
+-rw-r--r--  2.0 unx     2821 b- defN 23-Jun-09 07:02 towhee/runtime/runtime_conf.py
+-rw-r--r--  2.0 unx    10298 b- defN 23-Jun-09 07:02 towhee/runtime/runtime_pipeline.py
+-rw-r--r--  2.0 unx     2314 b- defN 23-Jun-09 07:02 towhee/runtime/schema_repr.py
+-rw-r--r--  2.0 unx     1563 b- defN 23-May-04 09:35 towhee/runtime/time_profiler.py
+-rw-r--r--  2.0 unx     2776 b- defN 23-May-04 09:35 towhee/runtime/nodes/__init__.py
+-rw-r--r--  2.0 unx     2246 b- defN 23-May-04 09:35 towhee/runtime/nodes/_concat.py
+-rw-r--r--  2.0 unx     2323 b- defN 23-May-04 09:35 towhee/runtime/nodes/_filter.py
+-rw-r--r--  2.0 unx     2066 b- defN 23-May-04 09:35 towhee/runtime/nodes/_flat_map.py
+-rw-r--r--  2.0 unx     3093 b- defN 23-May-04 09:35 towhee/runtime/nodes/_map.py
+-rw-r--r--  2.0 unx     1383 b- defN 23-May-04 09:35 towhee/runtime/nodes/_output.py
+-rw-r--r--  2.0 unx     3246 b- defN 23-May-04 09:35 towhee/runtime/nodes/_reduce.py
+-rw-r--r--  2.0 unx     1266 b- defN 23-May-04 09:35 towhee/runtime/nodes/_single_input.py
+-rw-r--r--  2.0 unx     3692 b- defN 23-May-04 09:35 towhee/runtime/nodes/_time_window.py
+-rw-r--r--  2.0 unx     2632 b- defN 23-May-04 09:35 towhee/runtime/nodes/_window.py
+-rw-r--r--  2.0 unx     2630 b- defN 23-May-04 09:35 towhee/runtime/nodes/_window_all.py
+-rw-r--r--  2.0 unx     3666 b- defN 23-May-04 09:35 towhee/runtime/nodes/_window_base.py
+-rw-r--r--  2.0 unx     6468 b- defN 23-Jun-09 07:02 towhee/runtime/nodes/node.py
+-rw-r--r--  2.0 unx      872 b- defN 22-Oct-28 02:52 towhee/runtime/operator_manager/__init__.py
+-rw-r--r--  2.0 unx     4604 b- defN 23-May-04 09:35 towhee/runtime/operator_manager/operator_action.py
+-rw-r--r--  2.0 unx     6712 b- defN 23-Jun-09 07:02 towhee/runtime/operator_manager/operator_loader.py
+-rw-r--r--  2.0 unx     3953 b- defN 23-Jun-09 07:02 towhee/runtime/operator_manager/operator_pool.py
+-rw-r--r--  2.0 unx     2867 b- defN 23-Mar-13 07:00 towhee/runtime/operator_manager/operator_registry.py
+-rw-r--r--  2.0 unx     1951 b- defN 22-Oct-28 02:52 towhee/runtime/operator_manager/uri.py
+-rw-r--r--  2.0 unx      592 b- defN 22-Oct-08 02:27 towhee/serve/__init__.py
+-rw-r--r--  2.0 unx     4190 b- defN 23-Jun-09 07:02 towhee/serve/server_builder.py
+-rw-r--r--  2.0 unx      773 b- defN 23-Feb-06 09:20 towhee/serve/triton/__init__.py
+-rw-r--r--  2.0 unx      816 b- defN 23-Mar-13 07:00 towhee/serve/triton/constant.py
+-rw-r--r--  2.0 unx     2330 b- defN 23-Mar-13 07:00 towhee/serve/triton/docker_image_builder.py
+-rw-r--r--  2.0 unx     4495 b- defN 23-Jun-09 07:02 towhee/serve/triton/model_to_triton.py
+-rw-r--r--  2.0 unx     3483 b- defN 23-Mar-15 09:15 towhee/serve/triton/pipe_to_triton.py
+-rw-r--r--  2.0 unx     4284 b- defN 23-Jun-09 07:02 towhee/serve/triton/pipeline_builder.py
+-rw-r--r--  2.0 unx     4044 b- defN 23-May-17 03:38 towhee/serve/triton/pipeline_client.py
+-rw-r--r--  2.0 unx     2408 b- defN 23-Feb-06 09:20 towhee/serve/triton/triton_client.py
+-rw-r--r--  2.0 unx     2547 b- defN 23-Mar-13 07:00 towhee/serve/triton/triton_config_builder.py
+-rw-r--r--  2.0 unx     1499 b- defN 22-Nov-30 03:44 towhee/serve/triton/triton_files.py
+-rw-r--r--  2.0 unx      706 b- defN 22-Nov-30 03:44 towhee/serve/triton/bls/__init__.py
+-rw-r--r--  2.0 unx     2813 b- defN 23-May-17 03:38 towhee/serve/triton/bls/pipeline_model.py
+-rw-r--r--  2.0 unx      934 b- defN 22-Aug-19 02:52 towhee/serve/triton/bls/python_backend_wrapper.py
+-rw-r--r--  2.0 unx      592 b- defN 22-Aug-19 02:52 towhee/serve/triton/bls/caller/__init__.py
+-rw-r--r--  2.0 unx     3437 b- defN 22-Oct-08 02:27 towhee/serve/triton/bls/caller/local_caller.py
+-rw-r--r--  2.0 unx      592 b- defN 22-Aug-19 02:52 towhee/serve/triton/bls/mock/__init__.py
+-rw-r--r--  2.0 unx     4727 b- defN 23-Feb-06 09:20 towhee/serve/triton/bls/mock/mock_pb_util.py
+-rw-r--r--  2.0 unx     2699 b- defN 22-Nov-30 03:44 towhee/serve/triton/bls/mock/mock_triton_client.py
+-rw-r--r--  2.0 unx      750 b- defN 23-May-04 09:35 towhee/serve/triton/dockerfiles/DockerfileCuda113
+-rw-r--r--  2.0 unx      748 b- defN 23-May-04 09:35 towhee/serve/triton/dockerfiles/DockerfileCuda114
+-rw-r--r--  2.0 unx      748 b- defN 23-May-04 09:35 towhee/serve/triton/dockerfiles/DockerfileCuda116
+-rw-r--r--  2.0 unx      748 b- defN 23-Jun-09 07:02 towhee/serve/triton/dockerfiles/DockerfileCuda117
+-rw-r--r--  2.0 unx      956 b- defN 23-May-04 09:35 towhee/serve/triton/dockerfiles/DockerfileCuda117dev
+-rw-r--r--  2.0 unx     1317 b- defN 23-Mar-13 07:00 towhee/serve/triton/dockerfiles/__init__.py
+-rw-r--r--  2.0 unx      754 b- defN 23-May-04 09:35 towhee/tools/__init__.py
+-rw-r--r--  2.0 unx     5942 b- defN 23-May-17 03:38 towhee/tools/data_visualizer.py
+-rw-r--r--  2.0 unx     1983 b- defN 23-May-04 09:35 towhee/tools/graph_visualizer.py
+-rw-r--r--  2.0 unx    11912 b- defN 23-May-04 09:35 towhee/tools/profilers.py
+-rw-r--r--  2.0 unx     6863 b- defN 23-May-17 03:38 towhee/tools/visualizer.py
+-rw-r--r--  2.0 unx      940 b- defN 23-Mar-01 09:42 towhee/trainer/__init__.py
+-rw-r--r--  2.0 unx    26591 b- defN 23-Feb-06 09:20 towhee/trainer/callback.py
+-rw-r--r--  2.0 unx    10668 b- defN 23-Feb-17 09:42 towhee/trainer/metrics.py
+-rw-r--r--  2.0 unx     8861 b- defN 22-Aug-19 02:52 towhee/trainer/modelcard.py
+-rw-r--r--  2.0 unx    14904 b- defN 22-Aug-19 02:52 towhee/trainer/scheduler.py
+-rw-r--r--  2.0 unx    34675 b- defN 23-Feb-17 09:42 towhee/trainer/trainer.py
+-rw-r--r--  2.0 unx    14667 b- defN 23-Feb-17 09:42 towhee/trainer/training_config.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/trainer/optimization/__init__.py
+-rw-r--r--  2.0 unx     7830 b- defN 22-Aug-19 02:52 towhee/trainer/optimization/adafactor.py
+-rw-r--r--  2.0 unx     5148 b- defN 22-Aug-19 02:52 towhee/trainer/optimization/adamw.py
+-rw-r--r--  2.0 unx    12006 b- defN 22-Aug-19 02:52 towhee/trainer/optimization/optimization.py
+-rw-r--r--  2.0 unx      592 b- defN 22-Aug-19 02:52 towhee/trainer/utils/__init__.py
+-rw-r--r--  2.0 unx     1563 b- defN 23-May-17 03:38 towhee/trainer/utils/file_utils.py
+-rw-r--r--  2.0 unx     5600 b- defN 23-May-17 03:38 towhee/trainer/utils/layer_freezer.py
+-rw-r--r--  2.0 unx    16453 b- defN 22-Oct-08 02:27 towhee/trainer/utils/plot_utils.py
+-rw-r--r--  2.0 unx     9017 b- defN 22-Aug-19 02:52 towhee/trainer/utils/trainer_utils.py
+-rw-r--r--  2.0 unx      872 b- defN 23-Mar-13 07:00 towhee/types/__init__.py
+-rw-r--r--  2.0 unx     2341 b- defN 22-Aug-19 02:52 towhee/types/arg.py
+-rw-r--r--  2.0 unx     2141 b- defN 22-Aug-19 02:52 towhee/types/audio_frame.py
+-rw-r--r--  2.0 unx     2443 b- defN 22-Aug-19 02:52 towhee/types/image.py
+-rw-r--r--  2.0 unx     2016 b- defN 22-Oct-08 02:27 towhee/types/image_utils.py
+-rw-r--r--  2.0 unx     2649 b- defN 22-Aug-19 02:52 towhee/types/video_frame.py
+-rw-r--r--  2.0 unx      592 b- defN 22-Oct-08 02:27 towhee/utils/__init__.py
+-rw-r--r--  2.0 unx     3928 b- defN 23-May-17 03:38 towhee/utils/console_table.py
+-rw-r--r--  2.0 unx     1178 b- defN 22-Aug-19 02:52 towhee/utils/cv2_utils.py
+-rw-r--r--  2.0 unx     1403 b- defN 23-Feb-06 09:20 towhee/utils/dependency_control.py
+-rw-r--r--  2.0 unx     7744 b- defN 22-Aug-19 02:52 towhee/utils/git_utils.py
+-rw-r--r--  2.0 unx     7243 b- defN 23-Jun-09 07:02 towhee/utils/html_table.py
+-rw-r--r--  2.0 unx     1381 b- defN 22-Aug-19 02:52 towhee/utils/hub_file_utils.py
+-rw-r--r--  2.0 unx    13166 b- defN 23-May-04 09:35 towhee/utils/hub_utils.py
+-rw-r--r--  2.0 unx     1301 b- defN 23-Feb-06 09:20 towhee/utils/lazy_import.py
+-rw-r--r--  2.0 unx      894 b- defN 22-Aug-19 02:52 towhee/utils/log.py
+-rw-r--r--  2.0 unx     2975 b- defN 22-Oct-08 02:27 towhee/utils/matplotlib_utils.py
+-rw-r--r--  2.0 unx     4385 b- defN 22-Oct-08 02:27 towhee/utils/ndarray_utils.py
+-rw-r--r--  2.0 unx     1156 b- defN 23-Feb-06 09:20 towhee/utils/onnx_utils.py
+-rw-r--r--  2.0 unx     2571 b- defN 22-Aug-19 02:52 towhee/utils/pil_utils.py
+-rw-r--r--  2.0 unx     8448 b- defN 22-Aug-19 02:52 towhee/utils/repo_normalize.py
+-rw-r--r--  2.0 unx     3628 b- defN 23-May-17 03:38 towhee/utils/serializer.py
+-rw-r--r--  2.0 unx     1102 b- defN 22-Oct-08 02:27 towhee/utils/singleton.py
+-rw-r--r--  2.0 unx     1538 b- defN 23-Feb-06 09:20 towhee/utils/triton_httpclient.py
+-rw-r--r--  2.0 unx     2535 b- defN 22-Oct-08 02:27 towhee/utils/yaml_utils.py
+-rw-r--r--  2.0 unx      592 b- defN 22-Aug-19 02:52 towhee/utils/thirdparty/__init__.py
+-rw-r--r--  2.0 unx      845 b- defN 23-Feb-17 09:42 towhee/utils/thirdparty/dill_util.py
+-rw-r--r--  2.0 unx     1230 b- defN 22-Oct-08 02:27 towhee/utils/thirdparty/ipython_utils.py
+-rw-r--r--  2.0 unx     1130 b- defN 22-Oct-08 02:27 towhee/utils/thirdparty/pandas_utils.py
+-rw-r--r--  2.0 unx    11357 b- defN 23-Jun-09 07:05 towhee-1.1.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx    13967 b- defN 23-Jun-09 07:05 towhee-1.1.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jun-09 07:05 towhee-1.1.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx      114 b- defN 23-Jun-09 07:05 towhee-1.1.0.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx        7 b- defN 23-Jun-09 07:05 towhee-1.1.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    13488 b- defN 23-Jun-09 07:05 towhee-1.1.0.dist-info/RECORD
+152 files, 642862 bytes uncompressed, 199819 bytes compressed:  68.9%
```

## zipnote {}

```diff
@@ -1,13 +1,16 @@
 Filename: towhee/__init__.py
 Comment: 
 
 Filename: towhee/__main__.py
 Comment: 
 
+Filename: towhee/data_loader.py
+Comment: 
+
 Filename: towhee/command/__init__.py
 Comment: 
 
 Filename: towhee/command/cmdline.py
 Comment: 
 
 Filename: towhee/command/develop.py
@@ -69,17 +72,14 @@
 
 Filename: towhee/operator/__init__.py
 Comment: 
 
 Filename: towhee/operator/base.py
 Comment: 
 
-Filename: towhee/operator/concat_operator.py
-Comment: 
-
 Filename: towhee/operator/nop.py
 Comment: 
 
 Filename: towhee/pipelines/__init__.py
 Comment: 
 
 Filename: towhee/pipelines/_builtin_pipeline.py
@@ -129,17 +129,14 @@
 
 Filename: towhee/runtime/node_config.py
 Comment: 
 
 Filename: towhee/runtime/node_repr.py
 Comment: 
 
-Filename: towhee/runtime/performance_profiler.py
-Comment: 
-
 Filename: towhee/runtime/pipeline.py
 Comment: 
 
 Filename: towhee/runtime/pipeline_loader.py
 Comment: 
 
 Filename: towhee/runtime/runtime_conf.py
@@ -147,14 +144,17 @@
 
 Filename: towhee/runtime/runtime_pipeline.py
 Comment: 
 
 Filename: towhee/runtime/schema_repr.py
 Comment: 
 
+Filename: towhee/runtime/time_profiler.py
+Comment: 
+
 Filename: towhee/runtime/nodes/__init__.py
 Comment: 
 
 Filename: towhee/runtime/nodes/_concat.py
 Comment: 
 
 Filename: towhee/runtime/nodes/_filter.py
@@ -165,23 +165,32 @@
 
 Filename: towhee/runtime/nodes/_map.py
 Comment: 
 
 Filename: towhee/runtime/nodes/_output.py
 Comment: 
 
+Filename: towhee/runtime/nodes/_reduce.py
+Comment: 
+
+Filename: towhee/runtime/nodes/_single_input.py
+Comment: 
+
 Filename: towhee/runtime/nodes/_time_window.py
 Comment: 
 
 Filename: towhee/runtime/nodes/_window.py
 Comment: 
 
 Filename: towhee/runtime/nodes/_window_all.py
 Comment: 
 
+Filename: towhee/runtime/nodes/_window_base.py
+Comment: 
+
 Filename: towhee/runtime/nodes/node.py
 Comment: 
 
 Filename: towhee/runtime/operator_manager/__init__.py
 Comment: 
 
 Filename: towhee/runtime/operator_manager/operator_action.py
@@ -222,17 +231,14 @@
 
 Filename: towhee/serve/triton/pipeline_builder.py
 Comment: 
 
 Filename: towhee/serve/triton/pipeline_client.py
 Comment: 
 
-Filename: towhee/serve/triton/serializer.py
-Comment: 
-
 Filename: towhee/serve/triton/triton_client.py
 Comment: 
 
 Filename: towhee/serve/triton/triton_config_builder.py
 Comment: 
 
 Filename: towhee/serve/triton/triton_files.py
@@ -276,14 +282,29 @@
 
 Filename: towhee/serve/triton/dockerfiles/DockerfileCuda117dev
 Comment: 
 
 Filename: towhee/serve/triton/dockerfiles/__init__.py
 Comment: 
 
+Filename: towhee/tools/__init__.py
+Comment: 
+
+Filename: towhee/tools/data_visualizer.py
+Comment: 
+
+Filename: towhee/tools/graph_visualizer.py
+Comment: 
+
+Filename: towhee/tools/profilers.py
+Comment: 
+
+Filename: towhee/tools/visualizer.py
+Comment: 
+
 Filename: towhee/trainer/__init__.py
 Comment: 
 
 Filename: towhee/trainer/callback.py
 Comment: 
 
 Filename: towhee/trainer/metrics.py
@@ -345,24 +366,27 @@
 
 Filename: towhee/types/video_frame.py
 Comment: 
 
 Filename: towhee/utils/__init__.py
 Comment: 
 
+Filename: towhee/utils/console_table.py
+Comment: 
+
 Filename: towhee/utils/cv2_utils.py
 Comment: 
 
 Filename: towhee/utils/dependency_control.py
 Comment: 
 
-Filename: towhee/utils/empty_format.py
+Filename: towhee/utils/git_utils.py
 Comment: 
 
-Filename: towhee/utils/git_utils.py
+Filename: towhee/utils/html_table.py
 Comment: 
 
 Filename: towhee/utils/hub_file_utils.py
 Comment: 
 
 Filename: towhee/utils/hub_utils.py
 Comment: 
@@ -375,26 +399,26 @@
 
 Filename: towhee/utils/matplotlib_utils.py
 Comment: 
 
 Filename: towhee/utils/ndarray_utils.py
 Comment: 
 
-Filename: towhee/utils/np_format.py
-Comment: 
-
 Filename: towhee/utils/onnx_utils.py
 Comment: 
 
 Filename: towhee/utils/pil_utils.py
 Comment: 
 
 Filename: towhee/utils/repo_normalize.py
 Comment: 
 
+Filename: towhee/utils/serializer.py
+Comment: 
+
 Filename: towhee/utils/singleton.py
 Comment: 
 
 Filename: towhee/utils/triton_httpclient.py
 Comment: 
 
 Filename: towhee/utils/yaml_utils.py
@@ -408,26 +432,26 @@
 
 Filename: towhee/utils/thirdparty/ipython_utils.py
 Comment: 
 
 Filename: towhee/utils/thirdparty/pandas_utils.py
 Comment: 
 
-Filename: towhee-1.0.0rc1.dist-info/LICENSE
+Filename: towhee-1.1.0.dist-info/LICENSE
 Comment: 
 
-Filename: towhee-1.0.0rc1.dist-info/METADATA
+Filename: towhee-1.1.0.dist-info/METADATA
 Comment: 
 
-Filename: towhee-1.0.0rc1.dist-info/WHEEL
+Filename: towhee-1.1.0.dist-info/WHEEL
 Comment: 
 
-Filename: towhee-1.0.0rc1.dist-info/entry_points.txt
+Filename: towhee-1.1.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: towhee-1.0.0rc1.dist-info/top_level.txt
+Filename: towhee-1.1.0.dist-info/top_level.txt
 Comment: 
 
-Filename: towhee-1.0.0rc1.dist-info/RECORD
+Filename: towhee-1.1.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## towhee/__init__.py

```diff
@@ -14,14 +14,15 @@
 
 # pylint: disable=redefined-builtin
 # pylint: disable=import-outside-toplevel
 
 import sys
 
 from towhee.runtime import register, pipe, ops, accelerate, AutoConfig, AutoPipes
+from towhee.data_loader import DataLoader
 from towhee.serve.triton import triton_client
 from towhee.utils.lazy_import import LazyImport
 
 # Legacy towhee._types
 from towhee import types
 _types = types  # pylint: disable=protected-access
 sys.modules['towhee._types'] = sys.modules['towhee.types']
@@ -35,39 +36,127 @@
     'dataset',
     'pipe',
     'triton_client',
     'AutoConfig',
     'build_docker_image',
     'build_pipeline_model',
     'AutoConfig',
-    'AutoPipes'
+    'AutoPipes',
+    'DataLoader'
 ]
 
 __import__('pkg_resources').declare_namespace(__name__)
 
 
-def build_docker_image(*args, **kwargs):
+def build_docker_image(
+        dc_pipeline: 'towhee.RuntimePipeline',
+        image_name: str,
+        cuda_version: str,
+        format_priority: list,
+        parallelism: int = 8,
+        inference_server: str = 'triton',
+    ):
     """
-    Wrapper for lazy import build_docker_image
-    """
-    return server_builder.build_docker_image(*args, **kwargs)
-
+    Wrapper for lazy import build_docker_image.
 
-def build_pipeline_model(*args, **kwargs):
+    Args:
+        dc_pipeline ('towhee.RuntimPipeline'):
+            The pipeline to build as a model in the docker image.
+        image_name (`str`):
+            The name of the docker image.
+        cuda_version (`str`):
+            Cuda version.
+        format_priority (`list`):
+            The priority order of the model format.
+        parallelism (`int`):
+            The parallel number.
+        inference_server (`str`):
+            The inference server.
+
+    Examples:
+        >>> import towhee
+        >>> from towhee import pipe, ops
+
+        >>> p = (
+        ...     pipe.input('url')
+        ...         .map('url', 'image', ops.image_decode.cv2_rgb())
+        ...         .map('image', 'vec', ops.image_embedding.timm(model_name='resnet50'))
+        ...         .output('vec')
+        ... )
+
+        >>> towhee.build_docker_image(
+        ...     dc_pipeline=p,
+        ...     image_name='clip:v1',
+        ...     cuda_version='11.7',
+        ...     format_priority=['onnx'],
+        ...     parallelism=4,
+        ...     inference_server='triton'
+        ... )
+    """
+    return server_builder.build_docker_image(dc_pipeline, image_name, cuda_version, format_priority, parallelism, inference_server)
+
+
+def build_pipeline_model(
+        dc_pipeline: 'towhee.RuntimePipeline',
+        model_root: str,
+        format_priority: list,
+        parallelism: int = 8,
+        server: str = 'triton'
+    ):
     """
-    Wrapper for lazy import build_pipeline_model
+    Wrapper for lazy import build_pipeline_model.
+
+    Args:
+        dc_pipeline ('towhee.RuntimePipeline'):
+            The piepline to build as a model.
+        model_root (`str`):
+            The model root path.
+        format_priority (`list`):
+            The priority order of the model format.
+        parallelism (`int`):
+            The parallel number.
+        server (`str`):
+            The server type.
+
+    Examples:
+        >>> import towhee
+        >>> from towhee import pipe, ops
+
+        >>> p = (
+        ...     pipe.input('url')
+        ...         .map('url', 'image', ops.image_decode.cv2_rgb())
+        ...         .map('image', 'vec', ops.image_embedding.timm(model_name='resnet50'))
+        ...         .output('vec')
+        ... )
+
+        >>> towhee.build_pipeline_model(
+        ...     dc_pipeline=p,
+        ...     model_root='models',
+        ...     format_priority=['onnx'],
+        ...     parallelism=4,
+        ...     server='triton'
+        ... )
     """
-    return server_builder.build_pipeline_model(*args, **kwargs)
+    return server_builder.build_pipeline_model(dc_pipeline, model_root, format_priority, parallelism, server)
 
 
-def DataCollection(*args, **kwargs):  # pylint: disable=invalid-name
+def DataCollection(data):  # pylint: disable=invalid-name
     """
     Wrapper for lazy import DataCollection
+
+    DataCollection is a pythonic computation and processing framework for unstructured
+    data in machine learning and data science. It allows a data scientist or researcher
+    to assemble data processing pipelines and do their model work (embedding,
+    transforming, or classification) with a method-chaining style API.
+
+    Args:
+        data ('towhee.runtime.DataQueue'):
+            The data to be stored in DataColletion in the form of DataQueue.
     """
-    return datacollection.DataCollection(*args, **kwargs)
+    return datacollection.DataCollection(data)
 
 
 def dataset(name: str, *args, **kwargs) -> 'TorchDataSet':
     """Get a dataset by name, and pass into the custom params.
     Args:
         name (str): Name of a dataset.
         *args (any): Arguments of the dataset construct method.
```

## towhee/data/dataset/dataset.py

```diff
@@ -1,7 +1,21 @@
+# Copyright 2021 Zilliz. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 from torch.utils.data.dataset import Dataset
 
 
 class TowheeDataSet:
     """
     TowheeDataSet is a kind of dataset wrapper, where the `self.dataset` is the true dataset.
     """
```

## towhee/datacollection/data_collection.py

```diff
@@ -11,14 +11,15 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import copy
 from typing import Any
 
 from towhee.datacollection.entity import Entity
+from towhee.runtime.data_queue import ColumnType
 from towhee.datacollection.mixins.display import DisplayMixin
 
 
 # pylint: disable=protected-access
 class DataCollection(DisplayMixin):
     """
     A pythonic computation and processing framework.
@@ -38,17 +39,22 @@
         >>> dq = DataQueue([('a', ColumnType.SCALAR), ('b', ColumnType.QUEUE)])
         >>> dq.put(('a', 'b1'))
         True
         >>> DataCollection(dq)
         <DataCollection Schema[a: ColumnType.SCALAR, b: ColumnType.QUEUE] SIZE 1>
     """
     def __init__(self, data):
-        self._schema = data.schema
-        self._type_schema = data.type_schema
-        self._iterable = [Entity.from_dict(dict(zip(self._schema, data.get()))) for _ in range(data.size)]
+        if isinstance(data, dict):
+            self._schema = data['schema']
+            self._type_schema = [ColumnType[type] for type in data['type_schema']]
+            self._iterable = [Entity.from_dict(dict(zip(data['schema'], entity))) for entity in data['iterable']]
+        else:
+            self._schema = data.schema
+            self._type_schema = data.type_schema
+            self._iterable = [Entity.from_dict(dict(zip(self._schema, data.get()))) for _ in range(data.size)]
 
     def __iter__(self):
         """
         Iterate the DataCollection in the form of Entity.
 
         Examples:
             >>> from towhee.runtime.data_queue import DataQueue, ColumnType
@@ -200,7 +206,19 @@
             >>> id(dc[0]) == id(dc_dcopy[0])
             False
         """
         if deep:
             return copy.deepcopy(self)
         else:
             return copy.copy(self)
+
+    def to_dict(self):
+        ret = {}
+        ret['schema'] = self._schema
+        ret['type_schema'] = [type.name for type in self._type_schema]
+        ret['iterable'] = [[getattr(entity, col) for col in self._schema] for entity in self._iterable]
+
+        return ret
+
+    @staticmethod
+    def from_dict(data):
+        return DataCollection(data)
```

## towhee/datacollection/mixins/display.py

```diff
@@ -7,302 +7,43 @@
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-import numpy
-
-from towhee.types import Image, VideoFrame, AudioFrame
-from towhee.datacollection.entity import Entity
-# pylint: disable=dangerous-default-value
 
 
 class DisplayMixin: # pragma: no cover
     """
     Mixin for displaying data.
     """
 
-    def as_str(self):
-        return self._factory(map(str, self._iterable))
+    def prepare_table_data(self, limit=5):
+        if limit > 0:
+            data = [list(x.__dict__.values()) for i, x in enumerate(self) if i < limit]
+        else:
+            data = [list(x.__dict__.values()) for i, x in enumerate(self)]
+        return {'data': data, 'headers': self._schema}
 
-    def show(self, limit=5, header=None, tablefmt='html', formatter={}):
+    def show(self, limit=5, tablefmt=None):
         """Print the first n lines of a DataCollection.
 
         Args:
             limit (int, optional): The number of lines to print. Prints all if limit is negative. Defaults to 5.
-            header (_type_, optional): The field names. Defaults to None.
-            tablefmt (str, optional): The format of the output, supports html, plain, grid.. Defaults to 'html'.
+            tablefmt (str, optional): The format of the output, supports html, grid.
         """
-        # pylint: disable=protected-access
-        contents = [x for i, x in enumerate(self) if i < limit]
-
-        if all(isinstance(x, Entity) for x in contents):
-            header = self._schema
-            data = [list(x.__dict__.values()) for x in contents]
-        else:
-            data = [[x] for x in contents]
-
-        table_display(to_printable_table(data, header, tablefmt, formatter), tablefmt)
-
-
-def table_display(table, tablefmt='html'):  # pragma: no cover
-    """_summary_
-
-    Args:
-        table (str): Table in printable format, such as HTML.
-        tablefmt (str, optional): The format of the output, supports html, plain, grid. Defaults to 'html'.
-
-    Raises:
-        ValueError: Unsupported table format.
-    """
-    # pylint: disable=import-outside-toplevel
-    from towhee.utils.thirdparty.ipython_utils import display, HTML
-
-    if tablefmt == 'html':
-        display(HTML(table))
-    elif tablefmt in ('plain', 'grid'):
-        print(table)
-    else:
-        raise ValueError('unsupported table format %s' % tablefmt)
-
-
-def to_printable_table(data, header=None, tablefmt='html', formatter={}):  # pragma: no cover
-    """Convert two dimensional data structure into printable table
-
-    Args:
-        data (List[List], or List[Dict]): The data filled into table. If a list of dict is given, keys are used as column names.
-        header (Iterable[str]), optional): The names of columns defined by user. Defaults to None.
-        tablefmt (str, optional): The format of the output, supports html, plain, grid.. Defaults to 'html'.
-
-    Raises:
-        ValueError: Unsupported table format
-
-    Returns:
-        str: The table.
-    """
-    header = [] if not header else header
-
-    if tablefmt == 'html':
-        return to_html_table(data, header, formatter)
-    elif tablefmt in ('plain', 'grid'):
-        return to_plain_table(data, header, tablefmt)
-
-    raise ValueError('unsupported table format %s' % tablefmt)
-
-
-def to_plain_table(data, header, tablefmt):  # pragma: no cover
-    """Convert two dimensional data structure into plain table
-
-    Args:
-        data (List[List] or List[Dict]): The data filled into table. If a list of dict is given, keys are used as column names.
-        header (Iterable[str]): The names of columns defined by user.
-        tablefmt (str): The format of the output, supports plain, grid.
-
-    Returns:
-        str: The table in plain format.
-    """
-    # pylint: disable=import-outside-toplevel
-    from tabulate import tabulate
-
-    tb_contents = [[_to_plain_cell(x) for x in r] for r in data]
-    return tabulate(tb_contents, headers=header, tablefmt=tablefmt)
-
-
-def _to_plain_cell(data):  # pragma: no cover
-    if isinstance(data, str):
-        return _text_brief(data)
-    if isinstance(data, (Image, VideoFrame)):
-        return _image_brief(data)
-    elif isinstance(data, AudioFrame):
-        return _audio_frame_brief(data)
-    elif isinstance(data, numpy.ndarray):
-        return _ndarray_brief(data)
-
-    elif isinstance(data, (list, tuple)):
-        if all(isinstance(x, str) for x in data):
-            return _list_brief(data, _text_brief)
-        elif all(isinstance(x, (Image, VideoFrame)) for x in data):
-            return _list_brief(data, _image_brief)
-        elif all(isinstance(x, AudioFrame) for x in data):
-            return _list_brief(data, _audio_frame_brief)
-        elif all(isinstance(x, numpy.ndarray) for x in data):
-            return _list_brief(data, _ndarray_brief)
-    return _default_brief(data)
-
-
-def to_html_table(data, header, formatter={}):  # pragma: no cover
-    """Convert two dimensional data structure into html table
-
-    Args:
-        data (List[List] or List[Dict]): The data filled into table. If a list of dict is given, keys are used as column names.
-        header (Iterable[str]): The names of columns defined by user.
-
-    Returns:
-        str: The table in html format.
-    """
-    tb_style = 'style="border-collapse: collapse;"'
-    th_style = 'style="text-align: center; font-size: 130%; border: none;"'
-
-    str_2_callback = {
-        'text': _text_brief,
-        'image': _image_to_html_cell,
-        'audio_frame': _audio_frame_to_html_cell,
-        'video_frame': _image_to_html_cell,
-        'video_path': _video_path_to_html_cell,
-    }
 
-    trs = []
-    trs.append('<tr>' + ' '.join(['<th ' + th_style + '>' + x + '</th>' for x in header]) + '</tr>')
-
-    to_html_callback = {}
-    for i, field in enumerate(header):
-        cb = formatter.get(field, None)
-        if isinstance(cb, str):
-            cb = str_2_callback.get(cb, None)
-        to_html_callback[i] = cb
-    for r in data:
-        trs.append(
-            '<tr>' + ' '.join([_to_html_td(x, to_html_callback.get(i, None)) for i, x in enumerate(r)]) + '</tr>'
-        )
-    return '<table ' + tb_style + '>' + ' '.join(trs) + '</table>'
-
-
-def _to_html_td(data, callback=None):  # pragma: no cover
-
-    def wrap_td_tag(content, align='center', vertical_align='center'):
-        td_style = 'style="' \
-            + 'text-align: ' + align + '; ' \
-            + 'vertical-align: ' + vertical_align + '; ' \
-            + 'border-right: solid 1px #D3D3D3; ' \
-            + 'border-left: solid 1px #D3D3D3; ' \
-            + '"'
-        return '<td ' + td_style + '>' + content + '</td>'
-
-    if callback is not None:
-        return wrap_td_tag(callback(data))
-
-    if isinstance(data, str):
-        return wrap_td_tag(_text_brief(data))
-    if isinstance(data, (Image, VideoFrame)):
-        return wrap_td_tag(_image_to_html_cell(data))
-    elif isinstance(data, AudioFrame):
-        return wrap_td_tag(_audio_frame_to_html_cell(data))
-    elif isinstance(data, numpy.ndarray):
-        return wrap_td_tag(_ndarray_brief(data), align='left')
-
-    elif isinstance(data, (list, tuple)):
-        if all(isinstance(x, str) for x in data):
-            return wrap_td_tag(_text_list_brief(data))
-        elif all(isinstance(x, (Image, VideoFrame)) for x in data):
-            return wrap_td_tag(_images_to_html_cell(data), vertical_align='top')
-        elif all(isinstance(x, AudioFrame) for x in data):
-            return wrap_td_tag(_audio_frames_to_html_cell(data), vertical_align='top')
-        elif all(isinstance(x, numpy.ndarray) for x in data):
-            return wrap_td_tag(_list_brief(data, _ndarray_brief), align='left')
+        if not tablefmt:
+            try:
+                _ = get_ipython().__class__.__name__
+                tablefmt = 'html'
+            except NameError:
+                tablefmt = 'grid'
+
+        if tablefmt == 'html':
+            from towhee.utils.html_table import NestedHTMLTable # pylint: disable=import-outside-toplevel
+            NestedHTMLTable(self.prepare_table_data(limit)).show()
         else:
-            return wrap_td_tag(_list_brief(data, _default_brief), align='left')
-    return wrap_td_tag(_default_brief(data))
-
-
-def _image_to_html_cell(img, width=128, height=128):  # pragma: no cover
-    # pylint: disable=import-outside-toplevel
-    from towhee.utils.cv2_utils import cv2
-    from towhee.utils.matplotlib_utils import plt
-    import base64
-    from io import BytesIO
-    plt.ioff()
-    fig = plt.figure(figsize=(width / 100, height / 100))
-    img = cv2.resize(img, (width, height))
-    fig.figimage(img)
-    plt.ion()
-    tmpfile = BytesIO()
-    fig.savefig(tmpfile, format='png')
-    data = base64.b64encode(tmpfile.getvalue()).decode('ascii')
-    src = 'src="data:image/png;base64,' + data + '" '
-    w = '128 = "' + str(128) + 'px" '
-    h = '128 = "' + str(128) + 'px" '
-    style = 'style = "float:left; padding:2px"'
-    return '<img ' + src + w + h + style + '>'
-
-
-def _images_to_html_cell(imgs, width=128, height=128):  # pragma: no cover
-    return ' '.join([_image_to_html_cell(x, width, height) for x in imgs])
-
-
-def _audio_frame_to_html_cell(frame, width=128, height=128):  # pragma: no cover
-    # pylint: disable=import-outside-toplevel
-    from towhee.utils.matplotlib_utils import plt
-
-    signal = frame[0, ...]
-    fourier = numpy.fft.fft(signal)
-    freq = numpy.fft.fftfreq(signal.shape[-1], 1)
-    fig = plt.figure()
-    plt.plot(freq, fourier.real)
-    fig.canvas.draw()
-    data = numpy.frombuffer(fig.canvas.tostring_rgb(), dtype=numpy.uint8)
-    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))
-    img = Image(data, 'RGB')
-    plt.close()
-    return _image_to_html_cell(img, width, height)
-
-
-def _audio_frames_to_html_cell(frames, width=128, height=128):  # pragma: no cover
-    return ' '.join([_audio_frame_to_html_cell(x, width, height) for x in frames])
-
-
-def _video_path_to_html_cell(path, width=128, height=128):
-    src = 'src="' + path + '" '
-    type_suffix_idx = path.rfind('.')
-    if type_suffix_idx == -1:
-        raise ValueError('unsupported video format %s' % path)
-
-    video_type = 'type="video/' + path[path.rfind('.') + 1:].lower() + '" '
-    w = 'width = "' + str(width) + 'px" '
-    h = 'height = "' + str(height) + 'px" '
-    style = 'style = "float:left; padding:2px"'
-
-    return '<video ' + w + h + style + 'controls><source ' + src + video_type + '></source></video>'
-
-
-def _ndarray_brief(array, maxlen=3):  # pragma: no cover
-    head_vals = [repr(v) for i, v in enumerate(array.flatten()) if i < maxlen]
-    if len(array.flatten()) > maxlen:
-        head_vals.append('...')
-    shape = 'shape=' + repr(array.shape)
-
-    return '[' + ', '.join(head_vals) + '] ' + shape
-
-
-def _image_brief(img):  # pragma: no cover
-    return str(img)
-
-
-def _audio_frame_brief(frame):  # pragma: no cover
-    return str(frame)
-
-
-def _text_brief(text, maxlen=128):  # pragma: no cover
-    if len(text) > maxlen:
-        return text[:maxlen] + '...'
-    else:
-        return text
-
-
-def _text_list_brief(data, maxlen=16):  # pragma: no cover
-    head_vals = ['<br>' + _text_brief(x) + '</br>' for i, x in enumerate(data) if i < maxlen]
-    if len(data) > maxlen:
-        head_vals.append('<br>...</br>')
-    return ' '.join(head_vals)
-
-
-def _list_brief(data, str_method, maxlen=4):  # pragma: no cover
-    head_vals = [str_method(x) for i, x in enumerate(data) if i < maxlen]
-    if len(data) > maxlen:
-        head_vals.append('...')
-    return '[' + ','.join(head_vals) + ']' + ' len=' + str(len(data))
-
+            from towhee.utils.console_table import NestedConsoleTable # pylint: disable=import-outside-toplevel
+            NestedConsoleTable(self.prepare_table_data(limit)).show()
 
-def _default_brief(data, maxlen=128):  # pragma: no cover
-    s = str(data)
-    return s[:maxlen] + '...' if len(s) > maxlen else s
```

## towhee/hub/__init__.py

```diff
@@ -15,16 +15,20 @@
 
 from .cache_manager import CacheManager, set_local_dir
 from .downloader import set_hub_url
 
 _CACHE_MANAGER = CacheManager()
 
 
-def get_operator(operator: str, tag: str, install_reqs: bool = True):
-    return _CACHE_MANAGER.get_operator(operator, tag, install_reqs)
+def get_operator(operator: str, tag: str, install_reqs: bool = True, latest: bool = False):
+    return _CACHE_MANAGER.get_operator(operator, tag, install_reqs, latest)
+
+
+def get_pipeline(operator: str, tag: str, latest: bool = False):
+    return _CACHE_MANAGER.get_pipeline(operator, tag, latest)
 
 
 __all__ = [
     'set_local_dir',
     'set_hub_url',
     'get_operator'
 ]
```

## towhee/hub/cache_manager.py

```diff
@@ -12,15 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import os
 from pathlib import Path
 import threading
 
-from .downloader import download_operator, operator_tag_path
+from .downloader import download_operator, download_pipeline, repo_tag_path
 
 DEFAULT_CACHE_DIR = '~/.towhee'
 ENV_TOWHEE_HOME = 'TOWHEE_HOME'
 _HUB_ROOT = None
 
 def set_local_dir(d):
     global _HUB_ROOT
@@ -39,34 +39,44 @@
     Downloading from hub
     """
     def __init__(self):
         self._download_lock = threading.Lock()
 
     def _op_cache_name(self, author: str, repo: str, tag: str):
         if author == 'local':
-            cache_root = os.environ.get('TEST_CACHE')
+            cache_root = os.environ.get('TEST_OP_CACHE')
             return Path(cache_root) / repo.replace('-', '_')
         else:
             cache_root = get_local_dir()
-            return operator_tag_path(Path(cache_root) / 'operators' / author / repo, tag)
+            return repo_tag_path(Path(cache_root) / 'operators' / author / repo, tag)
 
-    def get_operator(self, operator: str, tag: str, install_reqs: bool) -> Path:
+    def _pipe_cache_name(self, author: str, repo: str, tag: str):
+        if author == 'local':
+            cache_root = os.environ.get('TEST_PIPE_CACHE')
+            return Path(cache_root) / repo.replace('-', '_')
+        else:
+            cache_root = get_local_dir()
+            return repo_tag_path(Path(cache_root) / 'pipelines' / author / repo, tag)
+
+    def get_operator(self, operator: str, tag: str, install_reqs: bool, latest: bool) -> Path:
         """Obtain the path to the requested operator.
 
         This function will obtain the first reference to the operator from the cache locations.
         If no opeartor is found, this function will download it to the default cache location
         from the Towhee hub.
 
         Args:
             operator (`str`):
                 The operator in 'author/repo' format. Author will be 'local' if locally imported.
             tag (`str`):
                 Which tag version to use of the opeartor. Will use 'main' if locally imported.
             install_reqs (`bool`):
                 Whether to download the python packages if a requirements.txt file is included in the repo.
+            latest (`bool`):
+                Whether to download the latest files.
 
         Returns:
             (Path | None)
                 Returns the path of the operator, None if a local operator isnt found.
 
         Raises:
             (`ValueError`):
@@ -75,16 +85,52 @@
         operator_split = operator.split('/')
         # For now assuming all piplines will be classifed as 'author/repo'.
         if len(operator_split) != 2:
             raise ValueError('''Incorrect operator format, should be '<author>/<operator_repo>'.''')
         author, repo = operator_split
         op_path = self._op_cache_name(author, repo, tag)
 
-        if op_path.is_dir():
+        if op_path.is_dir() and not latest:
             return op_path
 
         with self._download_lock:
-            if op_path.is_dir():
-                return op_path
             download_path = op_path.parent.parent
-            download_operator(author, repo, tag, download_path, install_reqs)
+            download_operator(author, repo, tag, download_path, install_reqs, latest)
             return op_path
+
+    def get_pipeline(self, pipeline: str, tag: str, latest: bool) -> Path:
+        """Obtain the path to the requested pipeline.
+
+        This function will obtain the first reference to the pipeline from the cache locations.
+        If no pipeline is found, this function will download it to the default cache location
+        from the Towhee hub.
+
+        Args:
+            pipeline (`str`):
+                The pipeline in 'author/repo' format. Author will be 'local' if locally imported.
+            tag (`str`):
+                Which tag version to use of the pipeline. Will use 'main' if locally imported.
+            latest (`bool`):
+                Whether to download the latest files.
+
+        Returns:
+            (Path | None)
+                Returns the path of the operator, None if a local operator isnt found.
+
+        Raises:
+            (`ValueError`):
+                Incorrect pipeline format.
+        """
+        operator_split = pipeline.split('/')
+        # For now assuming all piplines will be classifed as 'author/repo'.
+        if len(operator_split) != 2:
+            raise ValueError('''Incorrect operator format, should be '<author>/<operator_repo>'.''')
+        author, repo = operator_split
+        pipe_path = self._pipe_cache_name(author, repo, tag)
+
+        if pipe_path.is_dir() and not latest:
+            return pipe_path
+
+        with self._download_lock:
+            download_path = pipe_path.parent.parent
+            download_pipeline(author, repo, tag, download_path, latest)
+            return pipe_path
```

## towhee/hub/downloader.py

```diff
@@ -13,19 +13,24 @@
 # limitations under the License.
 
 import os
 import sys
 import tempfile
 import shutil
 from pathlib import Path
-import threading
 from concurrent.futures import ThreadPoolExecutor
 import subprocess
+
 import requests
 from tqdm import tqdm
+from tenacity import (
+    retry,
+    stop_after_attempt,
+    wait_random_exponential,
+)
 
 from towhee.utils.hub_utils import HubUtils
 
 
 ENV_TOWHEE_URL = 'TOWHEE_URL'
 _HUB_URL = 'https://towhee.io'
 
@@ -86,21 +91,24 @@
     @property
     def requirements(self):
         for item in self._meta_infos:
             if item['type'] == 'blob' and item['path'] == 'requirements.txt':
                 return self.get_tag_path() / item['path']
         return None
 
-    def symlink_files(self):
+    def symlink_files(self, latest):
         tmp_dir = Path(tempfile.mkdtemp(dir=self._root))
         for dst, src in self.symlink_pair():
             dst_file = tmp_dir / dst
             dst_file.parent.mkdir(parents=True, exist_ok=True)
             dst_file.symlink_to(src)
-        os.renames(tmp_dir, self.get_tag_path())
+        tag_dir = self.get_tag_path()
+        if latest and tag_dir.exists():
+            shutil.rmtree(tag_dir)
+        os.renames(tmp_dir, tag_dir)
 
     def symlink_pair(self):
         pair = []
         for item in self._meta_infos:
             if item['type'] == 'tree':
                 continue
             pair.append((Path(item['path']), self.file_path / item['sha']))
@@ -119,14 +127,15 @@
     """
     Download op
     """
     def __init__(self, hub_files: 'HubFiles'):
         self._hub_files = hub_files
 
     @staticmethod
+    @retry(wait=wait_random_exponential(min=1, max=5), stop=stop_after_attempt(3))
     def download_url_to_file(url: str, dst: str, file_size: int = None) -> bool:
         """Download file at the given URL to a local path.
         Args:
             url (str): URL of the file to download
             dst (str): Full path where file will be saved
         """
 
@@ -151,25 +160,34 @@
         with ThreadPoolExecutor(max_workers=5) as pool:
             for local_file, url in self._hub_files.local_url_pair():
                 if not local_file.is_file():
                     futures.append(pool.submit(_Downloader.download_url_to_file, url, local_file))
             _ = [i.result() for i in futures]
 
 
-_DOWNLOAD_LOCK = threading.Lock()
-
-def download_operator(author: str, repo: str, tag: str, op_path: Path, install_reqs: bool = True):
+def download_operator(author: str, repo: str, tag: str, op_path: Path, install_reqs: bool = True, latest: bool = False):
     hub_url = get_hub_url()
     ht = HubUtils(author, repo, hub_url)
     meta = ht.branch_tree(tag)
     if meta is None:
         raise RuntimeError('Fetch op {}/{}:{} info failed'.format(author, repo, tag))
     fs = _HubFiles(op_path, tag, meta)
     _Downloader(fs).download()
-    fs.symlink_files()
+    fs.symlink_files(latest)
 
     if install_reqs and fs.requirements:
         subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-r', fs.requirements])
 
 
-def operator_tag_path(root: Path, tag: str):
+def download_pipeline(author: str, repo: str, tag: str, pipe_path: Path, latest: bool = False):
+    hub_url = get_hub_url()
+    ht = HubUtils(author, repo, hub_url)
+    meta = ht.branch_tree(tag)
+    if meta is None:
+        raise RuntimeError('Fetch pipeline {}/{}:{} info failed'.format(author, repo, tag))
+    fs = _HubFiles(pipe_path, tag, meta)
+    _Downloader(fs).download()
+    fs.symlink_files(latest)
+
+
+def repo_tag_path(root: Path, tag: str):
     return _HubFiles(root, tag).get_tag_path()
```

## towhee/operator/base.py

```diff
@@ -8,16 +8,15 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from abc import abstractmethod
-from abc import ABC
+from abc import abstractmethod, ABC
 from enum import Enum
 from enum import Flag, auto
 
 
 # NotShareable:
 #    Stateful & reusable operator.
 
@@ -26,14 +25,15 @@
 
 # Shareable:
 #    Stateless operator
 
 SharedType = Enum('SharedType', ('NotShareable', 'NotReusable', 'Shareable'))
 
 
+# legacy class
 class OperatorFlag(Flag):
     EMPTYFLAG = auto()
     STATELESS = auto()
     REUSEABLE = auto()
 
 
 class Operator(ABC):
@@ -41,17 +41,16 @@
     Operator base class, implements __init__ and __call__,
 
     Examples:
         class AddOperator(Operator):
             def __init__(self, factor: int):
                 self._factor = factor
 
-            def __call__(self, num) -> NamedTuple("Outputs", [("sum", int)]):
-                Outputs = NamedTuple("Outputs", [("sum", int)])
-                return Outputs(self._factor + num)
+            def __call__(self, num)
+                return self._factor + num
     """
 
     @abstractmethod
     def __init__(self):
         """
         Init operator, before a graph starts, the framework will call Operator __init__ function.
 
@@ -91,17 +90,22 @@
     def shared_type(self):
         return SharedType.NotShareable
 
     @key.setter
     def key(self, value):
         self._key = value
 
-    @property
-    def flag(self):
-        return OperatorFlag.STATELESS|OperatorFlag.REUSEABLE
+    def flush(self):
+        """
+        This method is triggered by RuntimePipeline.flush().
+
+        After the pipeline runs, some operators may need to trigger some additional operations,
+        such as the to_faiss operator needing to trigger data storage.
+        """
+        pass
 
 
 class NNOperator(Operator):
     """
     Neural Network related operators that involve machine learning frameworks.
 
     Args:
@@ -114,18 +118,14 @@
         self._framework = framework
         self.operator_name = type(self).__name__
         self.model = None
         self.model_card = None
         self._trainer = None
 
     @property
-    def flag(self):
-        return OperatorFlag.STATELESS|OperatorFlag.REUSEABLE
-
-    @property
     def framework(self):
         return self._framework
 
     @framework.setter
     def framework(self, framework: str):
         self._framework = framework
 
@@ -243,9 +243,9 @@
     """
 
     def __init__(self):
         super().__init__()
         pass
 
     @property
-    def flag(self):
-        return OperatorFlag.EMPTYFLAG
+    def shared_type(self):
+        return SharedType.NotShareable
```

## towhee/operator/nop.py

```diff
@@ -9,33 +9,17 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
-from typing import Any, Dict, NamedTuple
-
 from towhee.operator import PyOperator
 
 
-class NOPOperator(PyOperator):
-    """No-op operator. Input arguments are redefined as a `NamedTuple` and returned as
-    outputs.
-    """
-
-    def __init__(self):
-        #pylint: disable=useless-super-delegation
-        super().__init__()
-
-    def __call__(self, **args: Dict[str, Any]) -> NamedTuple:
-        fields = [(name, type(val)) for name, val in args.items()]
-        return NamedTuple('Outputs', fields)(**args)  # pylint: disable=not-callable
-
-
 class NOPNodeOperator(PyOperator):
     """
     DefaultOperator for _input and _output nodes.
     """
 
     def __init__(self):
         #pylint: disable=useless-super-delegation
```

## towhee/pipelines/insert_milvus.py

```diff
@@ -8,29 +8,30 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from typing import Optional
+from pydantic import BaseModel
 
 from towhee import ops, pipe, AutoPipes, AutoConfig
 
 
 @AutoConfig.register
-class MilvusInsertConfig:
+class MilvusInsertConfig(BaseModel):
     """
     Config of pipeline
     """
-    def __init__(self):
-        self.host = '127.0.0.1'
-        self.port = '19530'
-        self.collection_name = None
-        self.user = None
-        self.password = None
+    host: Optional[str] = '127.0.0.1'
+    port: Optional[str] = '19530'
+    collection_name: Optional[str] = None
+    user: Optional[str] = None
+    password: Optional[str] = None
 
 
 @AutoPipes.register
 def milvus_insert_pipe(config):
     return (
         pipe.input('row')
         .map('row', 'mr', ops.ann_insert.milvus_client(host=config.host,
```

## towhee/pipelines/search_milvus.py

```diff
@@ -8,30 +8,31 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from typing import Dict, Optional, Any
+from pydantic import BaseModel
 
 from towhee import ops, pipe, AutoPipes, AutoConfig
 
 
 @AutoConfig.register
-class MilvusSearchConfig:
+class MilvusSearchConfig(BaseModel):
     """
     Config of pipeline
     """
-    def __init__(self):
-        self.host= '127.0.0.1'
-        self.port= '19530'
-        self.collection_name = None
-        self.search_params = {}
-        self.user = None
-        self.password = None
+    host: Optional[str] = '127.0.0.1'
+    port: Optional[str] = '19530'
+    collection_name: Optional[str] = None
+    search_params: Optional[Dict[str, Any]] = {}
+    user: Optional[str] = None
+    password: Optional[str] = None
 
 
 @AutoPipes.register
 def milvus_insert_pipe(config):
     return (
         pipe.input('vec')
         .map('vec', 'rows', ops.ann_search.milvus_client(host=config.host,
```

## towhee/pipelines/sentence_embedding.py

```diff
@@ -8,29 +8,30 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from typing import Optional, Any
+from pydantic import BaseModel
 
 from towhee import ops, pipe, AutoPipes, AutoConfig
 
 
 @AutoConfig.register
-class SentenceSimilarityConfig:
+class SentenceSimilarityConfig(BaseModel):
     """
     Config of pipeline
     """
-    def __init__(self):
-        self.model = 'all-MiniLM-L6-v2'
-        self.openai_api_key = None
-        self.customize_embedding_op = None
-        self.normalize_vec = True
-        self.device = -1
+    model: Optional[str] = 'all-MiniLM-L6-v2'
+    openai_api_key: Optional[str] = None
+    customize_embedding_op: Optional[Any] = None
+    normalize_vec: Optional[bool] = True
+    device: Optional[int] = -1
 
 
 _hf_models = ops.sentence_embedding.transformers().get_op().supported_model_names()
 _sbert_models = ops.sentence_embedding.sbert().get_op().supported_model_names()
 _openai_models = ['text-embedding-ada-002', 'text-similarity-davinci-001',
                   'text-similarity-curie-001', 'text-similarity-babbage-001',
                   'text-similarity-ada-001']
```

## towhee/pipelines/text_image_embedding.py

```diff
@@ -8,26 +8,27 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from typing import Optional, Any
+from pydantic import BaseModel
 
 from towhee import pipe, ops, AutoPipes, AutoConfig
 
 
 @AutoConfig.register
-class TextImageEmbeddingConfig:
-    def __init__(self):
-        self.model = 'clip_vit_base_patch16'
-        self.modality = 'image'
-        self.normalize_vec = True
-        self.customize_embedding_op = None
-        self.device = -1
+class TextImageEmbeddingConfig(BaseModel):
+    model: Optional[str] = 'clip_vit_base_patch16'
+    modality: Optional[str] = 'image'
+    customize_embedding_op: Optional[Any] = None
+    normalize_vec: Optional[bool] = True
+    device: Optional[int] = -1
 
 
 def _get_embedding_op(config):
     if config.device == -1:
         device = 'cpu'
     else:
         device = config.device
```

## towhee/pipelines/video_copy_detection.py

```diff
@@ -10,14 +10,17 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 # pylint: disable = import-outside-toplevel
 
+from typing import Optional
+from pydantic import BaseModel
+
 from towhee import ops, pipe, AutoPipes, AutoConfig
 
 
 def merge_ndarray(x):
     import numpy as np
     return np.concatenate(x).reshape(-1, x[0].shape[0])
 
@@ -36,59 +39,55 @@
             for idx in range(i[0], i[2] + 1):
                 weights[idx] = max(weights[idx], j)
 
         return (sum(weights) / duration) > self._threshold
 
 
 @AutoConfig.register
-class VideoCopyDetectionConfig:
+class VideoCopyDetectionConfig(BaseModel):
     """
     Config of pipeline
     """
-    def __init__(self):
-        # decode op
-        # The range in the video to deocode
-        self.start_time = None
-        self.end_time = None
-
-        # emb op
-        self.model = 'isc'
-        self.img_size = 512
-
-        # milvus op
-        self.milvus_host = '127.0.0.1'
-        self.milvus_port = '19530'
-        self.collection = None
-        # The number of embeddings return from milvus search op
-        self.milvus_search_limit = 64
-
-        # kv op
-        self.hbase_host = '127.0.0.1'
-        self.hbase_port = 9090
-        self.hbase_table = None
-        self.leveldb_path = None
-
-        # select video op
-        # The number of nearest videos to return by the pipeline
-        self.top_k = 100
-
-        # tn op
-        # The minimal similar frame(s) that the return videos should contain
-        self.min_similar_length = 1
-
-        # filter op
-        self.threshold = None
+    # decode op
+    # The range in the video to decode
+    start_time: Optional[float] = None
+    end_time: Optional[float] = None
+
+    # emb op
+    model: Optional[str] = 'isc'
+    img_size: Optional[int] = 512
+
+    # milvus op
+    milvus_host: Optional[str] = '127.0.0.1'
+    milvus_port: Optional[str] = '19530'
+    collection: Optional[str] = None
+    # The number of embeddings return from milvus search op
+    milvus_search_limit: Optional[int] = 64
+
+    # kv op
+    hbase_host: Optional[str] = '127.0.0.1'
+    hbase_port: Optional[int] = 9090
+    hbase_table: Optional[str] = None
+    leveldb_path: Optional[str] = None
+
+    # select video op
+    # The number of nearest videos to return by the pipeline
+    top_k: Optional[int] = 100
+
+    # tn op
+    # The minimal similar frame(s) that the return videos should contain
+    min_similar_length: Optional[int] = 1
 
-        # tracer
-        self.tracer = False
+    # filter op
+    threshold: Optional[float] = None
 
-        self.device = -1
+    device: Optional[int] = -1
 
 
-def _video_copy_detection(decode_op, emb_op, milvus_op, kv_op, select_op, tn_op, norm_op, filter_op, tracer, allow_triton=False, device=-1):
+def _video_copy_detection(decode_op, emb_op, milvus_op, kv_op, select_op, tn_op, norm_op, filter_op, allow_triton=False, device=-1):
     op_config = {}
     if allow_triton:
         if device >= 0:
             op_config = AutoConfig.TritonGPUConfig(device_ids=[device], max_batch_size=128)
         else:
             op_config = AutoConfig.TritonCPUConfig()
     if norm_op:
@@ -118,20 +117,20 @@
             search_pipe.map(('video_emb', 'retrieved_emb'), ('similar_segment', 'segment_score'), tn_op)
                 .filter(
                     ('candidates', 'similar_segment', 'segment_score'),
                     ('candidates', 'similar_segment', 'segment_score'),
                     ('similar_segment', 'segment_score', 'video_emb'),
                     filter_op
                 )
-                .output('url', 'candidates', 'similar_segment', 'segment_score', tracer=tracer)
+                .output('url', 'candidates', 'similar_segment', 'segment_score')
         )
     else:
         detect_pipe = (
             search_pipe.map(('video_emb', 'retrieved_emb'), ('similar_segment', 'segment_score'), tn_op)
-                .output('url', 'candidates', 'similar_segment', 'segment_score', tracer=tracer)
+                .output('url', 'candidates', 'similar_segment', 'segment_score')
         )
 
     return detect_pipe
 
 
 def _get_embedding_op(config):
     if config.device == -1:
@@ -195,9 +194,9 @@
     select_op = ops.video_copy_detection.select_video(top_k=config.top_k, reduce_function='sum', reverse=True)
     tn_op = ops.video_copy_detection.temporal_network(min_length=config.min_similar_length)
     norm_op = None if config.model == 'isc' else ops.towhee.np_normalize()
     filter_op = DuplicateFilter(config.threshold) if config.threshold else None
 
 
     return _video_copy_detection(
-        decode_op, emb_op, milvus_op, kv_op, select_op, tn_op, norm_op, filter_op, config.tracer, allow_triton, config.device
+        decode_op, emb_op, milvus_op, kv_op, select_op, tn_op, norm_op, filter_op, allow_triton, config.device
     )
```

## towhee/pipelines/video_embedding.py

```diff
@@ -10,55 +10,54 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 # pylint: disable = import-outside-toplevel
 
+from typing import Optional
+from pydantic import BaseModel
+
 from towhee import ops, pipe, AutoPipes, AutoConfig
 
 
 def merge_ndarray(x):
     import numpy as np
     return np.concatenate(x).reshape(-1, x[0].shape[0])
 
 
 @AutoConfig.register
-class VideoEmbeddingConfig:
+class VideoEmbeddingConfig(BaseModel):
     """
     Config of pipeline
     """
-    def __init__(self):
-        # decode op
-        # The range in the video to deocode
-        self.start_time = None
-        self.end_time = None
-
-        # emb op
-        self.model = 'isc'
-        self.img_size = 512
-
-        # milvus op
-        self.milvus_host = '127.0.0.1'
-        self.milvus_port = '19530'
-        self.collection = None
-
-        # kv op
-        self.hbase_host = '127.0.0.1'
-        self.hbase_port = 9090
-        self.hbase_table = None
-        self.leveldb_path = None
-
-        # tracer
-        self.tracer = False
+    # decode op
+    # The range in the video to deocode
+    start_time: Optional[float] = None
+    end_time: Optional[float] = None
+
+    # emb op
+    model: Optional[str] = 'isc'
+    img_size: Optional[int] = 512
+
+    # milvus op
+    milvus_host: Optional[str] = '127.0.0.1'
+    milvus_port: Optional[str] = '19530'
+    collection: Optional[str] = None
+
+    # kv op
+    hbase_host: Optional[str] = '127.0.0.1'
+    hbase_port: Optional[int] = 9090
+    hbase_table: Optional[str] = None
+    leveldb_path: Optional[str] = None
 
-        self.device = -1
+    device: Optional[int] = -1
 
 
-def _video_embedding(decode_op, emb_op, milvus_op, kv_op, norm_op, tracer, allow_triton=False, device=-1):
+def _video_embedding(decode_op, emb_op, milvus_op, kv_op, norm_op, allow_triton=False, device=-1):
     op_config = {}
     if allow_triton:
         if device >= 0:
             op_config = AutoConfig.TritonGPUConfig(device_ids=[device], max_batch_size=128)
         else:
             op_config = AutoConfig.TritonCPUConfig()
 
@@ -67,26 +66,26 @@
             pipe.input('url')
                 .flat_map('url', 'frames', decode_op)
                 .map('frames', 'emb', emb_op, config=op_config)
                 .map('emb', 'emb', norm_op)
                 .map(('url', 'emb'), 'milvus_res', milvus_op)
                 .window_all('emb', 'video_emb', merge_ndarray)
                 .map(('url', 'video_emb'), ('insert_status'), kv_op)
-                .output(tracer=tracer)
+                .output()
         )
 
     def _unnorm():
         return (
             pipe.input('url')
                 .flat_map('url', 'frames', decode_op)
                 .map('frames', 'emb', emb_op, config=op_config)
                 .map(('url', 'emb'), 'milvus_res', milvus_op)
                 .window_all('emb', 'video_emb', merge_ndarray)
                 .map(('url', 'video_emb'), ('insert_status'), kv_op)
-                .output(tracer=tracer)
+                .output()
         )
 
     if norm_op:
         return _norm()
     return _unnorm()
 
 def _get_embedding_op(config):
@@ -139,8 +138,8 @@
     milvus_op = ops.ann_insert.milvus_client(host=config.milvus_host, port=config.milvus_port, collection_name=config.collection)
     if config.hbase_table:
         kv_op = ops.kvstorage.insert_hbase(host=config.hbase_host, port=config.hbase_port, table=config.hbase_table)
     if config.leveldb_path:
         kv_op = ops.kvstorage.insert_leveldb(path=config.leveldb_path)
     norm_op = None if config.model == 'isc' else ops.towhee.np_normalize()
 
-    return _video_embedding(decode_op, emb_op, milvus_op, kv_op, norm_op, config.tracer, allow_triton, config.device)
+    return _video_embedding(decode_op, emb_op, milvus_op, kv_op, norm_op, allow_triton, config.device)
```

## towhee/runtime/__init__.py

```diff
@@ -27,9 +27,9 @@
     'register',
     'ops',
     'get_sys_config',
     'accelerate',
     'AcceleratorConf',
     'AutoConfig',
     'AutoPipes',
-    'AutoConfig'
+    'AutoConfig',
 ]
```

## towhee/runtime/auto_config.py

```diff
@@ -63,14 +63,22 @@
         name = get_config_name()
         if name is not None:
             AutoConfig._REGISTERED_CONFIG[name] = wrapper
         return wrapper
 
     @staticmethod
     def load_config(name: str, *args, **kwargs):
+        """
+        Load config from pre-defined pipeline.
+
+        Examples:
+            >>> from towhee import AutoConfig
+            >>> config = AutoConfig.load_config('sentence_embedding')
+            SentenceSimilarityConfig(model='all-MiniLM-L6-v2', openai_api_key=None, customize_embedding_op=None, normalize_vec=True, device=-1)
+        """
         with AutoConfig._lock:
             if name in AutoConfig._REGISTERED_CONFIG:
                 return AutoConfig._REGISTERED_CONFIG[name](*args, **kwargs)
 
             with set_config_name(name):
                 pipe_loader.PipelineLoader.load_pipeline(name)
             if name in AutoConfig._REGISTERED_CONFIG:
@@ -132,14 +140,15 @@
             >>> p = (pipe.input('url')
             ...          .map('url', 'image', ops.image_decode.cv2())
             ...          .map('image', 'vec', ops.image_embedding.timm(model_name='resnet50'), config=AutoConfig.TritonCPUConfig())
             ...          .output('vec')
             ... )
 
             You can also to set the configuration:
+
             >>> from towhee import pipe, AutoConfig
             >>> config = AutoConfig.TritonCPUConfig(num_instances_per_device=3,
             ...                                     max_batch_size=128,
             ...                                     batch_latency_micros=100000,
             ...                                     preferred_batch_size=[8, 16])
             >>> p = (pipe.input('url')
             ...          .map('url', 'image', ops.image_decode.cv2())
@@ -147,15 +156,15 @@
             ...          .output('vec')
             ... )
         """
         return nd_conf.TowheeConfig.set_triton_config(device_ids=None,
                                                       num_instances_per_device=num_instances_per_device,
                                                       max_batch_size=max_batch_size,
                                                       batch_latency_micros=batch_latency_micros,
-                                              preferred_batch_size=preferred_batch_size)
+                                                      preferred_batch_size=preferred_batch_size)
 
     @staticmethod
     def TritonGPUConfig(device_ids: list = None,
                         num_instances_per_device: int = 1,
                         max_batch_size: int = None,
                         batch_latency_micros: int = None,
                         preferred_batch_size: list = None):
@@ -179,14 +188,15 @@
             >>> p = (pipe.input('url')
             ...          .map('url', 'image', ops.image_decode.cv2())
             ...          .map('image', 'vec', ops.image_embedding.timm(model_name='resnet50'), config=AutoConfig.TritonGPUConfig())
             ...          .output('vec')
             ... )
 
             You can also to set the configuration:
+
             >>> from towhee import pipe, AutoConfig
             >>> config = AutoConfig.TritonGPUConfig(device_ids=[0, 1],
             ...                                     num_instances_per_device=3,
             ...                                     max_batch_size=128,
             ...                                     batch_latency_micros=100000,
             ...                                     preferred_batch_size=[8, 16])
             >>> p = (pipe.input('url')
```

## towhee/runtime/auto_pipes.py

```diff
@@ -38,15 +38,24 @@
         return _AUTO_PIPES_VAR.get()
     except:  # pylint: disable=bare-except
         return None
 
 
 class AutoPipes:
     """
-    Load Predefined pipeines.
+    Load pre-defined pipelines. Some available pipelines are under [towhee/pipelines](https://github.com/towhee-io/towhee/tree/main/towhee/pipelines).
+    And also put the predefined pipelines on the [Towhee Hub](https://towhee.io/).
+
+    Examples:
+        >>> from towhee import AutoPipes, AutoConfig
+        >>> # config for sentence_embedding pipeline
+        >>> config = AutoConfig.load_config('sentence_embedding')
+        >>> config.model = 'all-MiniLM-L6-v2'
+        >>> embed_pipe = AutoPipes.pipeline('sentence_embedding', config=config)
+        >>> print(embed_pipe('How are you?').to_list())
     """
     _PIPES_DEF = {}
     _lock = threading.Lock()
 
     def __init__(self):
         raise EnvironmentError(
             'AutoPipes is designed to be instantiated, please using the `AutoPipes.load_pipeline(pipeline_name)` etc.'
```

## towhee/runtime/check_utils.py

```diff
@@ -8,67 +8,81 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Dict, Any, Set, Tuple
+from typing import Dict, Any, Set, Tuple, Optional
+from pydantic import BaseModel, constr, validator
 
-from towhee.utils.log import engine_log
 from towhee.runtime.constants import (
     WindowConst,
     FilterConst,
     TimeWindowConst
 )
 
 
-def check_keys(info: Dict[str, Any], essentials: Set[str]):
-    """
-    Check if the src is a valid node dictionary to describe.
-
-    Args:
-        info (`Dict[str, Any]`): The info dictionary.
-        essentials (`Set[str]`): The essential keys that node dictionary should contain.
-    """
-    if not isinstance(info, dict):
-        raise ValueError(f'Config {str(info)} is not valid, it must be dict.')
-    info_keys = set(info.keys())
-    if not essentials.issubset(info_keys):
-        raise ValueError(f'Node {str(info)} is not valid, lack attr {essentials - info_keys}')
+# pylint: disable=no-self-argument
+class IntForm(BaseModel):
+    data: int
+
+    @validator('data')
+    def must_larger_than_zero(cls, v):
+        if v <= 0:
+            raise ValueError(f'The iteration param is not valid, the [{v}]<=0.')
+        return v
+
+
+class TupleForm(BaseModel):
+    data: Optional[Tuple[str, ...]]
+    schema_data: Optional[Tuple[constr(regex='^[a-z][a-z0-9_]*$'), ...]]
+
+    @validator('*', pre=True)
+    def must_be_tuple(cls, v):
+        if isinstance(v, str):
+            return (v,)
+        return v
+
+
+class SetForm(BaseModel):
+    data: Set[str]
+
+    @validator('data', pre=True)
+    def must_be_set(cls, v):
+        if isinstance(v, str):
+            return {v}
+        if not isinstance(v, set):
+            return set(v)
+        return v
 
 
 def check_set(inputs: Tuple, all_inputs: Set[str]):
     """
     Check if the inputs in all_inputs.
 
     Args:
         inputs (`Tuple[str]`): The inputs schema.
         all_inputs (`Set[str]`): The all inputs schema in the DAG util the node.
     """
-    if isinstance(inputs, str):
-        inputs = (inputs,)
-    inputs = set(inputs)
+    inputs = SetForm(data=inputs).data
     if not inputs.issubset(all_inputs):
         raise ValueError(f'The DAG Nodes inputs {str(inputs)} is not valid, which is not declared: {inputs - all_inputs}.')
 
 
 def check_int(info: Dict[str, Any], checks: list):
     """
     Check if the info is type of int.
 
     Args:
         info (`Dict[str, Any]`): The essential set will be check.
         checks (`list`): The list to check.
     """
     for name in checks:
-        if not isinstance(info[name], int):
-            raise ValueError(f'The iteration param:{info} is not valid, the type of {name} is not int.')
-        if info[name] <= 0:
-            raise ValueError(f'The iteration param:{info} is not valid, the [{name}]<=0.')
+        IntForm(data=info[name])
 
 
 def check_length(inputs: Tuple, outputs: Tuple):
     """
     Check if the length of inputs and outputs is equal.
 
     Args:
@@ -96,41 +110,7 @@
     elif iter_type == TimeWindowConst.name:
         check_set(iter_param[TimeWindowConst.param.timestamp_col], all_inputs)
         check_int(iter_param, [TimeWindowConst.param.time_range_sec,
                                TimeWindowConst.param.time_step_sec])
     elif iter_type == WindowConst.name:
         check_int(iter_param, [WindowConst.param.size,
                                WindowConst.param.step])
-
-
-def check_config(info: Dict[str, Any], essentials: Set[str]):
-    """
-    Check if the src covers all the needed config info.
-
-    Args:
-        info (`Dict[str, Any]`):
-            The info dictionary.
-        essentials (`Set[str]`):
-            The essential keys that node config dictionary should contain.
-    """
-    if not isinstance(info, dict):
-        raise ValueError(f'Config {str(info)} is not valid, it must be dict.')
-    info_keys = set(info.keys())
-    if not essentials.issubset(info_keys):
-        raise ValueError(f'Config {str(info)} is not valid, lack attr {essentials - info_keys}')
-
-
-def check_supported(info: Dict[str, Any], essentials: Set[str]):
-    """
-    Check if the src is supported and logging warning.
-
-    Args:
-        info (`Dict[str, Any]`):
-            The info dictionary.
-        essentials (`Set[str]`):
-            The essential keys that node config dictionary can contain.
-    """
-    if not isinstance(info, dict):
-        raise ValueError(f'Config {str(info)} is not valid, it must be dict.')
-    info_keys = set(info.keys())
-    if not info_keys.issubset(essentials):
-        engine_log.warning('Config %s is not supported, please make sure the config is in %s', str(info_keys - essentials), essentials)
```

## towhee/runtime/constants.py

```diff
@@ -44,14 +44,18 @@
     param = _Param()
 
 
 class WindowAllConst:
     name = 'window_all'
 
 
+class ReduceConst:
+    name = 'reduce'
+
+
 class ConcatConst:
     name = 'concat'
 
 
 class FlatMapConst:
     name = 'flat_map'
 
@@ -64,11 +68,13 @@
     name = '_output'
 
 
 class OPType:
     HUB = 'hub'
     LAMBDA = 'lambda'
     CALLABLE = 'callable'
+    BUILTIN = 'built_in'
+    PIPELINE = 'pipeline'
 
 
-class TracerConst:
-    name = 'tracer'
+class OPName:
+    NOP = 'NOPNodeOperator'
```

## towhee/runtime/dag_repr.py

```diff
@@ -9,55 +9,72 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import types
-from typing import Dict, Any, Set, List, Tuple
+import uuid
+import json
+from copy import deepcopy
+from typing import Dict, Any, Set, List, Tuple, Optional
+from pydantic import BaseModel
 
 from towhee.runtime.check_utils import check_set, check_node_iter
 from towhee.runtime.node_repr import NodeRepr
 from towhee.runtime.schema_repr import SchemaRepr
-from towhee.runtime.constants import FilterConst, TimeWindowConst, OPType, InputConst, OutputConst
 from towhee.runtime.node_config import TowheeConfig
+from towhee.runtime.constants import (
+    WindowAllConst,
+    WindowConst,
+    ConcatConst,
+    MapConst,
+    ReduceConst,
+    FilterConst,
+    TimeWindowConst,
+    FlatMapConst,
+    InputConst,
+    OutputConst,
+    OPType,
+)
 
 
-class DAGRepr:
+class DAGRepr(BaseModel):
     """
     A `DAGRepr` represents a complete DAG.
 
     Args:
         nodes (`Dict[str, NodeRepr]`): All nodes in the dag, which start with _input and end with _output node.
-        edges (`Dict[str, List]`): The edges about data queue schema, such as:
+        edges (`Dict[int, List]`): The edges about data queue schema, such as:
                             { 0: {'data': [(a, ColumnType.SCALAR), (b, ColumnType.SCALAR)], 'schema': {'a', SchemaRepr, 'b', SchemaRepr}}
                               1: {'data': [(b, ColumnType.SCALAR), (c, ColumnType.SCALAR)], 'schema': {'b', SchemaRepr, 'c', SchemaRepr}}
                               2: {'data': [(a, ColumnType.SCALAR), (c, ColumnType.SCALAR)], 'schema': {'a', SchemaRepr, 'c', SchemaRepr}}
                             }
+        dag_dict(`Dict`): The dag dict.
     """
-    def __init__(self, nodes: Dict[str, NodeRepr], edges: Dict[int, Dict]):
-        self._nodes = nodes
-        self._edges = edges
+    nodes: Dict[str, NodeRepr]
+    edges: Dict[int, Dict]
+    dag_dict: Optional[Dict[str, Any]]
+    top_list: Optional[List[str]]
 
     @property
-    def nodes(self) -> Dict:
-        return self._nodes
-
-    @property
-    def edges(self) -> Dict:
-        return self._edges
+    def top_sort(self) -> list:
+        if self.top_list:
+            return self.top_list
+        else:
+            return self.get_top_sort(self.nodes)
 
     @staticmethod
-    def check_nodes(nodes: Dict[str, NodeRepr]):
+    def check_nodes(nodes: Dict[str, NodeRepr], top_sort: list):
         """Check nodes if start with _input and ends with _output, and the schema has declared before using.
 
         Args:
             nodes (`Dict[str, NodeRepr]`): All the nodes from DAG.
+            top_sort (`list`): Topological list.
         """
-        top_sort = DAGRepr.get_top_sort(nodes)
         if len(top_sort) != len(nodes):
             raise ValueError('The DAG is not valid, it has a circle.')
         if top_sort[0] != InputConst.name:
             raise ValueError('The DAG is not valid, it does not started with `_input`.')
         if top_sort[-1] != OutputConst.name:
             raise ValueError('The DAG is not valid, it does not ended with `_output`.')
 
@@ -100,15 +117,15 @@
 
         Returns:
            Dict[str, Tuple]: Dict of the node and the all inputs of this node.
         """
         all_nodes = [InputConst.name]
         all_inputs = dict((name, nodes[InputConst.name].outputs) for name in nodes)
         for name in top_sort[1:]:
-            if nodes[name].iter_info.type == 'concat':
+            if nodes[name].iter_info.type == ConcatConst.name:
                 pre_name = all_nodes.pop()
                 while name in nodes[pre_name].next_nodes:
                     nodes[name].inputs += nodes[pre_name].outputs
                     if len(all_nodes) == 0:
                         break
                     pre_name = all_nodes.pop()
                 nodes[name].outputs = nodes[name].inputs
@@ -198,42 +215,42 @@
 
         edge_schemas = {}
         for d in schema:
             if d not in ahead_schemas:
                 inputs_type = [ahead_schemas[inp].type for inp in inputs]
                 edge_schemas[d] = SchemaRepr.from_dag(d, iter_type, inputs_type)
             elif d in outputs:
-                if iter_type == 'concat':
+                if iter_type == ConcatConst.name:
                     inputs_type = [ahead_schemas[d].type]
                 else:
                     inputs_type = [ahead_schemas[inp].type for inp in inputs]
                     inputs_type.append(ahead_schemas[d].type)
                 edge_schemas[d] = SchemaRepr.from_dag(d, iter_type, inputs_type)
             else:
-                edge_schemas[d] = SchemaRepr.from_dag(d, 'map', [ahead_schemas[d].type])
+                edge_schemas[d] = SchemaRepr.from_dag(d, MapConst.name, [ahead_schemas[d].type])
         edge = {'schema': edge_schemas, 'data': [(s, t.type) for s, t in edge_schemas.items()]}
         return edge
 
     @staticmethod
-    def set_edges(nodes: Dict[str, NodeRepr]):
+    def set_edges(nodes: Dict[str, NodeRepr], top_sort: list):
         """Set in_edges and out_edges for the node, and return the nodes and edge.
 
         Args:
             nodes (`Dict[str, NodeRepr]`): All the nodes repr from DAG.
+            top_sort (`list`): Topological list.
 
         Returns:
             Dict[str, NodeRepr]: The nodes update in_edges and out_edges.
             Dict[str, Dict]: The edges for the DAG.
         """
         out_id = 0
         edges = {out_id: DAGRepr.get_edge_from_schema(nodes[InputConst.name].outputs, nodes[InputConst.name].inputs, nodes[InputConst.name].outputs,
                                                       nodes[InputConst.name].iter_info.type, None)}
         nodes[InputConst.name].in_edges = [out_id]
 
-        top_sort = DAGRepr.get_top_sort(nodes)
         for name in top_sort[:-1]:
             ahead_schema = set(nodes[name].outputs)
             for i in nodes[name].in_edges:
                 ahead_schema = ahead_schema | edges[i]['schema'].keys()
             for next_name in nodes[name].next_nodes:
                 out_id += 1
                 out_schema = DAGRepr.dfs_used_schema(nodes, next_name, ahead_schema)
@@ -254,33 +271,74 @@
         final_edge = nodes[OutputConst.name].in_edges[0]
         final_schema = edges[final_edge]['schema']
         edges[out_id] = {'schema': final_schema, 'data': [(s, final_schema[s].type) for s in nodes[OutputConst.name].inputs]}
 
         nodes[OutputConst.name].out_edges = [out_id]
         return nodes, edges
 
+    def to_dict(self):
+        def get_op(node):
+            if node.op_info.type == OPType.LAMBDA:
+                return 'lambda'
+            if node.op_info.type == OPType.CALLABLE:
+                return node.op_info.operator.__name__
+            return node.op_info.operator
+
+        info = {}
+        info['edges'] = {}
+        info['nodes'] = {}
+
+        for k, v in self.edges.items():
+            info['edges'][k] = []
+            for (name, ctype) in v['data']:
+                info['edges'][k].append({'name': name, 'type': ctype.name})
+
+        for k, v in self.nodes.items():
+            info['nodes'][k] = {}
+            info['nodes'][k]['name'] = v.name
+            info['nodes'][k]['iter_info'] = {'type': v.iter_info.type, 'param': v.iter_info.param}
+            info['nodes'][k]['op_info'] = {
+                'operator': get_op(v),
+                'type': v.op_info.type
+            }
+            info['nodes'][k]['next_nodes'] = v.next_nodes
+            info['nodes'][k]['inputs'] = v.in_edges
+            info['nodes'][k]['outputs'] = v.out_edges
+            info['nodes'][k]['op_input'] = v.inputs
+            info['nodes'][k]['op_output'] = v.outputs
+
+        return info
+
+    def to_json(self, **kws):
+        return json.dumps(self.to_dict(), **kws)
+
     @staticmethod
     def from_dict(dag: Dict[str, Any]):
         """Return a DAGRepr from a dag dictionary.
 
         Args:
             dag (`str`): The dag dictionary.
 
         Returns:
             DAGRepr
         """
+        dag_dict = deepcopy(dag)
+        for uid, node in dag_dict.items():
+            if node['op_info']['type'] == OPType.PIPELINE:
+                DAGRepr.rebuild_dag(dag, uid, node['op_info'], node['iter_info'], node['inputs'], node['outputs'])
+
         def _get_name(val):
             if val['op_info']['type'] == OPType.CALLABLE:
                 if isinstance(val['op_info']['operator'], types.FunctionType):
                     name = val['op_info']['operator'].__name__
                 else:
                     name = type(val['op_info']['operator']).__name__
             elif val['op_info']['type'] == OPType.LAMBDA:
                 name = 'lambda'
-            elif val['op_info']['type'] == OPType.HUB:
+            elif val['op_info']['type'] in [OPType.HUB, OPType.BUILTIN]:
                 fn = val['op_info']['operator']
                 if isinstance(fn, str):
                     name = fn
                 else:
                     name = fn.__class__.__name__
 
             return name
@@ -294,28 +352,158 @@
                 assert isinstance(val['config'], dict)
 
             # Deal with input and output.
             if key in [InputConst.name, OutputConst.name]:
                 val['config'] = {'name': key}
 
             # Concat nodes does not have op_info.
-            elif val['iter_info']['type'] == 'concat':
+            elif val['iter_info']['type'] == ConcatConst.name:
                 val['config'] = {'name': 'concat-' + str(node_index)}
                 node_index += 1
 
             # If config does not specified.
             elif 'config' not in val or not val['config']:
                 name = _get_name(val)
                 val['config'] = {'name': name + '-' + str(node_index)}
                 node_index += 1
 
             # Process dict config.
             elif isinstance(val['config'], dict):
                 if 'name' not in val['config']:
                     name = _get_name(val)
                     val['config']['name'] = name + '-' + str(node_index)
-                    node_index += 1
+                elif val['config']['name'] in [InputConst.name, OutputConst.name]:
+                    val['config']['name'] = val['config']['name'] + '-' + str(node_index)
+                node_index += 1
+
+            nodes[key] = NodeRepr(uid=key, **val)
+        top_sort = DAGRepr.get_top_sort(nodes)
+        DAGRepr.check_nodes(nodes, top_sort)
+        dag_nodes, schema_edges = DAGRepr.set_edges(nodes, top_sort)
+        return DAGRepr(nodes=dag_nodes, edges=schema_edges, dag_dict=dag, top_list=top_sort)
+
+    @staticmethod
+    def rebuild_dag(dag, sub_uid, op_info, iter_info, input_schema, output_schema):
+        sub_dag = op_info['dag']
+        sub_top_sort = op_info['top_sort']
+        iter_type = iter_info['type']
+        param = iter_info['param']
+
+        used_schemas = ()
+        for _, node in dag.items():
+            used_schemas += node['inputs'] + node['outputs']
+        if iter_type == FilterConst.name:
+            filter_cols = param[FilterConst.param.filter_by]
+            new_dag, sub_uid_out = DAGRepr.rebuild_sub_dag(sub_dag, sub_uid, sub_top_sort, filter_cols, filter_cols, used_schemas)
+        else:
+            new_dag, sub_uid_out = DAGRepr.rebuild_sub_dag(sub_dag, sub_uid, sub_top_sort, input_schema, output_schema, used_schemas)
+        new_dag[sub_uid_out]['next_nodes'] += dag[sub_uid]['next_nodes']
+
+        if iter_type == FlatMapConst.name:
+            new_dag[sub_uid_out]['iter_info'] = {'type': FlatMapConst.name,
+                                                 'param': None}
+        elif iter_type == FilterConst.name:
+            new_dag[sub_uid_out]['iter_info'] = {'type': FilterConst.name,
+                                                 'param': {FilterConst.param.filter_by: new_dag[sub_uid_out]['inputs']}}
+            new_dag[sub_uid_out]['inputs'] = input_schema
+            new_dag[sub_uid_out]['outputs'] = output_schema
+        elif iter_type == WindowConst.name:
+            new_dag[sub_uid]['iter_info'] = {'type': WindowConst.name,
+                                             'param': {WindowConst.param.size: param[WindowConst.param.size],
+                                                       WindowConst.param.step: param[WindowConst.param.step]}}
+        elif iter_type == TimeWindowConst.name:
+            new_dag[sub_uid]['iter_info'] = {'type': TimeWindowConst.name,
+                                             'param': {TimeWindowConst.param.time_range_sec: param[TimeWindowConst.param.time_range_sec],
+                                                       TimeWindowConst.param.time_step_sec: param[TimeWindowConst.param.time_step_sec],
+                                                       TimeWindowConst.param.timestamp_col: param[TimeWindowConst.param.timestamp_col]}}
+        elif iter_type == WindowAllConst.name:
+            new_dag[sub_uid]['iter_info'] = {'type': WindowAllConst.name,
+                                             'param': None}
+        elif iter_type == ReduceConst.name:
+            new_dag[sub_uid]['iter_info'] = {'type': ReduceConst.name,
+                                             'param': None}
+
+        for _, node in new_dag.items():
+            if node['config']['name'].startswith(InputConst.name) or node['config']['name'].startswith(OutputConst.name):
+                node['config']['name'] = node['config']['name'].split('-')[0]
+            else:
+                node['config'].pop('name')
+        dag.update(new_dag)
+
+    @staticmethod
+    def rebuild_sub_dag(sub_dag, uid_in, sub_top_sort, input_schema, output_schema, used_schemas):
+        pipe_dag = deepcopy(sub_dag)
+
+        # update the duplicate schemas in pipe_dag
+        DAGRepr._rename_schemas(pipe_dag, set(used_schemas))
+        if len(input_schema) != len(pipe_dag['_input']['inputs']):
+            DAGRepr._rename_group_schemas(pipe_dag, sub_top_sort, pipe_dag['_input']['inputs'], input_schema)
+        # update input node to pipe_dag
+        DAGRepr._update_input(pipe_dag, input_schema, uid_in)
+        # update output node to pipe_dag
+        uid_out = uuid.uuid4().hex
+        DAGRepr._update_output(pipe_dag, output_schema, uid_out, sub_top_sort[-2])
+
+        return pipe_dag, uid_out
+
+    @staticmethod
+    def _update_input(dag, input_schema, uid):
+        input_info = dag.pop('_input')
+        input_info['outputs'] = input_info['inputs']
+        input_info['inputs'] = input_schema
+        dag[uid] = input_info
 
-            nodes[key] = NodeRepr.from_dict(key, val)
-        DAGRepr.check_nodes(nodes)
-        dag_nodes, schema_edges = DAGRepr.set_edges(nodes)
-        return DAGRepr(dag_nodes, schema_edges)
+    @staticmethod
+    def _update_output(dag, output_schema, uid, mark_node):
+        output_info = dag.pop('_output')
+        output_info['inputs'] = output_info['outputs']
+        output_info['outputs'] = output_schema
+        dag[uid] = output_info
+        dag[mark_node]['next_nodes'].remove('_output')
+        dag[mark_node]['next_nodes'].append(uid)
+
+    @staticmethod
+    def _rename_group_schemas(dag, top_sort, ori_input_schema, input_schema):
+        assert len(input_schema) == 1 or len(ori_input_schema) == 1
+        for node_uid in top_sort:
+            dag[node_uid]['inputs'] = DAGRepr._replace_schema(dag[node_uid]['inputs'], ori_input_schema, input_schema)
+            DAGRepr._replace_cols(dag[node_uid]['iter_info']['type'], dag[node_uid]['iter_info']['param'], ori_input_schema, input_schema)
+
+            ori_name = DAGRepr._to_string(ori_input_schema)
+            new_name = DAGRepr._to_string(dag[node_uid]['outputs'])
+            if ori_name in new_name and node_uid != '_input':
+                break
+
+    @staticmethod
+    def _rename_schemas(dag, schemas):
+        for schema in schemas:
+            for _, node in dag.items():
+                node['inputs'] = DAGRepr._replace_schema(node['inputs'], schema)
+                node['outputs'] = DAGRepr._replace_schema(node['outputs'], schema)
+                DAGRepr._replace_cols(node['iter_info']['type'], node['iter_info']['param'], schema)
+
+    @staticmethod
+    def _replace_cols(node_type, node_param, schema, new_schema=None):
+        if node_type == FilterConst.name:
+            node_param[FilterConst.param.filter_by] = DAGRepr._replace_schema(node_param[FilterConst.param.filter_by], schema, new_schema)
+        elif node_type == TimeWindowConst.name:
+            node_param[TimeWindowConst.param.timestamp_col] = DAGRepr._replace_schema((node_param[TimeWindowConst.param.timestamp_col],),
+                                                                                      schema, new_schema)[0]
+
+    @staticmethod
+    def _replace_schema(schema, ori_name, new_name=None):
+        if not new_name:
+            new_name = ori_name + '_bak'
+            new_schema = [new_name if name == ori_name else name for name in schema]
+        else:
+            ori_name = DAGRepr._to_string(ori_name)
+            new_name = DAGRepr._to_string(new_name)
+            new_schema = DAGRepr._to_string(schema).replace(ori_name, new_name)
+            new_schema = new_schema.split(',')[1:-1]
+        return tuple(new_schema)
+
+    @staticmethod
+    def _to_string(schema):
+        str_schema = ','
+        for x in schema:
+            str_schema += x + ','
+        return str_schema
```

## towhee/runtime/data_queue.py

```diff
@@ -9,36 +9,37 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import threading
+import copy
 from enum import Enum, auto
 from typing import List, Tuple, Union, Dict, Optional
 
 from collections import deque, namedtuple
 
 
 class DataQueue:
     """
     Col-based storage.
     """
 
-    def __init__(self, schema_info, max_size=1000):
+    def __init__(self, schema_info, max_size=1000, keep_data=False):
         self._max_size = max_size
         self._schema = _Schema(schema_info)
         self._data = []
         self._queue_index = []
         self._scalar_index = []
         self._has_all_scalars = False
         for index in range(len(self._schema.col_types)):
             col_type = self._schema.col_types[index]
             if col_type == ColumnType.QUEUE:
-                self._data.append(_QueueColumn())
+                self._data.append(_QueueColumn() if not keep_data else _ListColumn())
                 self._queue_index.append(index)
             else:
                 self._data.append(_ScalarColumn())
                 self._scalar_index.append(index)
 
         self._sealed = False
         self._size = 0
@@ -174,16 +175,15 @@
     def seal(self):
         with self._lock:
             if self._sealed:
                 return
 
             self._sealed = True
             if self._queue_index:
-                self._size = max([self._data[index].size()
-                                  for index in self._queue_index])
+                self._size = self._get_size()
             self._not_empty.notify_all()
             self._not_full.notify_all()
 
     @property
     def sealed(self) -> bool:
         return self._sealed
 
@@ -221,21 +221,31 @@
         if not self._has_all_scalars:
             for index in self._scalar_index:
                 if not self._data[index].has_data():
                     return 0
             self._has_all_scalars = True
 
         que_size = [self._data[index].size() for index in self._queue_index]
+
         if que_size:
-            return min(que_size)
+            return max(que_size) if self._sealed else min(que_size)
         return 1 if len(self._scalar_index) > 0 else 0
 
     def col_type(self, col_name):
         return self.type_schema[self.schema.index(col_name)]
 
+    def reset_size(self):
+        """
+        For debug, read data repeatedly.
+        """
+        for col in self._data:
+            if isinstance(col, _ListColumn):
+                col.reset_size()
+        self._size = self._get_size()
+
 
 class ColumnType(Enum):
     """
     ColumnType
     """
     QUEUE = auto()
     SCALAR = auto()
@@ -312,14 +322,42 @@
         if data is Empty():
             return
         self._q.append(data)
 
     def size(self):
         return len(self._q)
 
+
+class _ListColumn:
+    """
+    List column, for debug.
+    """
+
+    def __init__(self):
+        self._q = []
+        self._index = 0
+
+    def get(self):
+        if len(self._q) == self._index:
+            return Empty()
+        self._index += 1
+        return copy.deepcopy(self._q[self._index - 1])
+
+    def put(self, data) -> bool:
+        if data is Empty():
+            return
+        self._q.append(data)
+
+    def size(self):
+        return len(self._q) - self._index
+
+    def reset_size(self):
+        self._index = 0
+
+
 class _ScalarColumn:
     """
     Scalar column
     """
 
     def __init__(self):
         self._data = None
```

## towhee/runtime/factory.py

```diff
@@ -63,17 +63,19 @@
     Operator wrapper for initialization.
     """
 
     def __init__(self,
                  name: str,
                  init_args: Tuple = None,
                  init_kws: Dict[str, Any] = None,
-                 tag: str = 'main'):
+                 tag: str = 'main',
+                 latest: bool = False):
         self._name = name.replace('.', '/').replace('_', '-')
         self._tag = tag
+        self._latest = latest
         self._init_args = init_args
         self._init_kws = init_kws
         self._op = None
 
     @property
     def name(self):
         return self._name
@@ -86,23 +88,35 @@
     def init_args(self):
         return self._init_args
 
     @property
     def init_kws(self):
         return self._init_kws
 
+    @property
+    def is_latest(self):
+        return self._latest
+
+    def revision(self, tag: str = 'main'):
+        self._tag = tag
+        return self
+
+    def latest(self):
+        self._latest = True
+        return self
+
     def get_op(self):
         if self._op is None:
             self.preload_op()
         return self._op
 
     def preload_op(self):
         try:
             loader = OperatorLoader()
-            self._op = loader.load_operator(self._name, self._init_args, self._init_kws, tag=self._tag)
+            self._op = loader.load_operator(self._name, self._init_args, self._init_kws, tag=self._tag, latest=self._latest)
         except Exception as e:
             err = f'Loading operator with error:{e}'
             engine_log.error(err)
             raise RuntimeError(err) from e
 
     def __call__(self, *args, **kws):
         if self._op is None:
@@ -115,14 +129,28 @@
     def callback(name: str, *args, **kws):
         return _OperatorWrapper(name, args, kws)
 
 
 class _OperatorParser:
     """
     Class to loading operator with _OperatorWrapper.
+    An operator is usually referred to with its full name: namespace/name.
+
+    Examples:
+        >>> from towhee import ops
+        >>> op = ops.towhee.image_decode()
+        >>> img = op('./towhee_logo.png')
+
+        We can also specify the version of the operator on the hub via the `revision` method:
+
+        >>> op = ops.towhee.image_decode()
+
+        And the `latest` method is used to update the current version of the operator to the latest:
+
+        >>> op = ops.towhee.image_decode().latest()
     """
     @classmethod
     def __getattr__(cls, name):
         @ops_parse
         def wrapper(name, *args, **kws):
             return _OperatorWrapper.callback(name, *args, **kws)
```

## towhee/runtime/node_config.py

```diff
@@ -8,180 +8,82 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Dict, Any, Optional, List
+from typing import Dict, Optional, List
 
-from towhee.runtime.check_utils import check_config, check_supported
+from pydantic import BaseModel, Extra, validator
 
 
-class NodeConfig:
+class TritonServerConf(BaseModel):
     """
-    The config of nodes.
+    Triton server config.
     """
-    def __init__(self, *, name: str,
-                 device: int,
-                 acc_info: Optional[Dict],
-                 server_info: Optional[Dict]):
-        self._name = name
-        self._device = device
-        self._acc_conf = AcceleratorConf.from_dict(acc_info) if acc_info is not None else None
-        self._server_conf = ServerConf.from_dict(server_info) if server_info is not None else None
-
-    @property
-    def name(self):
-        return self._name
-
-    @property
-    def device(self):
-        return self._device
-
-    @property
-    def acc_conf(self):
-        return self._acc_conf
-
-    @acc_conf.setter
-    def acc_conf(self, acc_conf):
-        self._acc_conf = acc_conf
-
-    @property
-    def server_conf(self):
-        return self._server_conf
+    preferred_batch_size: Optional[List[int]] = None
 
-    @staticmethod
-    def from_dict(conf: Dict[str, Any]):
-        essentials = {'name'}
-        check_config(conf, essentials)
-        return NodeConfig(
-            name=conf['name'],
-            device=conf.get('device', -1),
-            acc_info=conf.get('acc_info'),
-            server_info=conf.get('server')
-        )
 
-
-class AcceleratorConf:
+class ServerConf(BaseModel):
     """
-    AcceleratorConf
+    ServerConf
     """
-    def __init__(self, acc_type: str, conf: Dict):
-        self._type = acc_type
-        if self._type == 'triton':
-            self._conf = TritonClientConf.from_dict(conf)
-        elif self._type == 'mock':
-            pass
-        else:
-            raise ValueError(f'Unkown accelerator: {acc_type}')
-
-    def is_triton(self):
-        return self._type == 'triton'
-
-    def is_mock(self):
-        return self._type == 'mock'
-
-    @property
-    def triton(self):
-        return self._conf
 
-    @staticmethod
-    def from_dict(acc_conf: Dict[str, Any]):
-        return AcceleratorConf(acc_conf['type'], acc_conf['params'])
+    device_ids: Optional[List[int]] = None
+    max_batch_size: Optional[int] = None
+    batch_latency_micros: Optional[int] = None
+    num_instances_per_device: Optional[int] = None
+    triton: TritonServerConf = TritonServerConf()
 
 
-class TritonClientConf:
+class TritonClientConf(BaseModel):
     """
     Triton client config.
     """
-    def __init__(self, model_name: str, inputs: List[str], outputs: List[str]):
-        self._model_name = model_name
-        self._inputs = inputs
-        self._outputs = outputs
+    model_name: str
+    inputs: List[str]
+    outputs: List[str]
 
-    @property
-    def model_name(self):
-        return self._model_name
-
-    @property
-    def inputs(self):
-        return self._inputs
-
-    @property
-    def outputs(self):
-        return self._outputs
 
-    @staticmethod
-    def from_dict(conf):
-        return TritonClientConf(conf['model_name'], conf['inputs'], conf['outputs'])
-
-
-class ServerConf:
+class AcceleratorConf(BaseModel):
     """
-    ServerConf
+    AcceleratorConf
     """
-    def __init__(self, device_ids,
-                 max_batch_size,
-                 batch_latency_micros,
-                 num_instances_per_device,
-                 triton: 'TritonServerConf'):
-        self._device_ids = device_ids
-        self._max_batch_size = max_batch_size
-        self._batch_latency_micros = batch_latency_micros
-        self._num_instances_per_device = num_instances_per_device
-        self._triton = triton
 
-    @property
-    def device_ids(self):
-        return self._device_ids
+    type: str
+    params: Optional[TritonClientConf]
 
-    @property
-    def max_batch_size(self):
-        return self._max_batch_size
+    @validator('type')
+    @classmethod
+    def type_match(cls, v):
+        if v not in ['triton', 'mock']:
+            raise ValueError(f'Unkown accelerator: {v}')
+        return v
 
-    @property
-    def batch_latency_micros(self):
-        return self._batch_latency_micros
+    def is_triton(self):
+        return self.type == 'triton'
 
-    @property
-    def num_instances_per_device(self):
-        return self._num_instances_per_device
+    def is_mock(self):
+        return self.type == 'mock'
 
     @property
     def triton(self):
-        return self._triton
+        return self.params
 
-    @staticmethod
-    def from_dict(server_info: Dict[str, Any]):
-        check_supported(server_info, {'device_ids', 'max_batch_size', 'batch_latency_micros', 'num_instances_per_device', 'triton'})
-        triton_conf = TritonServerConf.from_dict(server_info.get('triton'))
-        return ServerConf(server_info.get('device_ids'), server_info.get('max_batch_size'), server_info.get('batch_latency_micros'),
-                          server_info.get('num_instances_per_device'), triton_conf)
 
-
-class TritonServerConf:
+class NodeConfig(BaseModel, extra=Extra.allow):
     """
-    Triton server config.
+    The config of nodes.
     """
-    def __init__(self, preferred_batch_size: str = None):
-        self._preferred_batch_size = preferred_batch_size
-
-    @property
-    def preferred_batch_size(self):
-        return self._preferred_batch_size
 
-    @staticmethod
-    def from_dict(triton_info: Dict[str, Any]):
-        if triton_info is None:
-            return TritonServerConf()
-        check_supported(triton_info, {'preferred_batch_size'})
-        if 'preferred_batch_size' not in triton_info:
-            return TritonServerConf()
-        return TritonServerConf(triton_info.get('preferred_batch_size'))
+    name: str
+    device: int = -1
+    acc_info: Optional[AcceleratorConf] = None
+    server: Optional[ServerConf] = None
 
 
 class TowheeConfig:
     """
     TowheeConfig mapping for AutoConfig.
     """
     def __init__(self, config: Dict):
```

## towhee/runtime/node_repr.py

```diff
@@ -8,229 +8,74 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Dict, Any, Tuple, Union, Callable, List
+from typing import Dict, Any, Tuple, Union, Callable, List, Optional
+
+from pydantic import BaseModel
 
-from towhee.runtime.check_utils import check_keys
 from towhee.runtime.node_config import NodeConfig
-from towhee.runtime.constants import ConcatConst, InputConst, OutputConst
 
 
-# pylint: disable=redefined-builtin
-class OperatorRepr:
+class OperatorRepr(BaseModel):
     """
     OperatorRepr for operator representations.
 
     Args:
         operator (`Union[str, Callable]`): The operator, such as a callable (lambda, function) or the name of an op on the hub.
         type (`str`): The type of operator, such as 'hub', 'lambda' and 'callable'.
         init_args (`Tuple`): The args to initialize the operator.
         init_kws (`Dict[str, any]`): The kwargs to initialize the operator.
         tag (`str`): The tag for The function, defaults to 'main'.
     """
-    def __init__(
-        self,
-        operator: Union[str, Callable],
-        type: str,
-        init_args: Tuple,
-        init_kws: Dict[str, any],
-        tag: str = 'main'
-    ):
-        self._operator = operator
-        self._type = type
-        self._init_args = init_args
-        self._init_kws = init_kws
-        self._tag = tag
-
-    @property
-    def operator(self):
-        return self._operator
-
-    @property
-    def type(self) -> str:
-        return self._type
-
-    @property
-    def init_args(self) -> Tuple:
-        return self._init_args
-
-    @property
-    def init_kws(self) -> Dict:
-        return self._init_kws
-
-    @property
-    def tag(self) -> str:
-        return self._tag
 
-    @staticmethod
-    def from_dict(op_info: Dict[str, Any]) -> 'OperatorRepr':
-        """Return a OperatorRepr from a description dict.
+    operator: Union[None, str, Callable]
+    type: Optional[str]
+    init_args: Optional[Tuple]
+    init_kws: Optional[Dict[str, Any]]
+    tag: Optional[str] = 'main'
+    latest: bool = False
 
-        Args:
-            op_info (`Dict[str, Any]`): Dictionary about operator information.
 
-        Returns:
-            OperatorRepr object.
-        """
-        check_keys(op_info, {'operator', 'type', 'init_args', 'init_kws', 'tag'})
-        return OperatorRepr(op_info['operator'], op_info['type'], op_info['init_args'], op_info['init_kws'], op_info['tag'])
-
-
-class IterationRepr:
+class IterationRepr(BaseModel):
     """
     IterationRepr for iteration representations.
 
     Args:
         type (`str`): The type of the iteration, such as 'map', 'flat_map', 'filter', 'time_window'.
         param (`Dict[str, any]`): The parameter for the iteration, defaults to None.
     """
-    def __init__(
-        self,
-        type: str,
-        param: Dict[str, any] = None,
-    ):
-        self._type = type
-        self._param = param
-
-    @property
-    def type(self) -> str:
-        return self._type
-
-    @property
-    def param(self) -> Dict[str, any]:
-        return self._param
-
-    @staticmethod
-    def from_dict(iter_info: Dict[str, Any]) -> 'IterationRepr':
-        """Return a IterationRepr from a description dict.
-
-        Args:
-            iter_info (`Dict[str, Any]`): Dictionary about iteration information.
+    type: str
+    param: Optional[Dict[str, Any]] = None
 
-        Returns:
-            IterationRepr object.
-        """
-        check_keys(iter_info, {'type', 'param'})
-        return IterationRepr(iter_info['type'], iter_info['param'])
 
-
-class NodeRepr:
+class NodeRepr(BaseModel):
     """
     NodeRepr for node representations.
 
     Args:
         name (`str`): Name of the node, such as '_input', '_output' and id for the node.
         inputs (`Tuple`): Input schema to this node.
         outputs (`Tuple`): Output schema to this node.
         iter_info (`IterationRepr`): The iteration to this node.
         op_info (`OperatorRepr`): The operator to this node.
-        config (`Dict[str, any]`): The configuration to this node.
+        config (`NodeConfig`): The configuration to this node.
         next_nodes (`List'): The next nodes.
         in_edges (`List`): The input edges about data queue schema, defaults to None.
         out_edges (`List`): The output edges about data queue schema, defaults to None.
     """
-    def __init__(
-            self,
-            uid: str,
-            inputs: Tuple,
-            outputs: Tuple,
-            iter_info: IterationRepr,
-            op_info: OperatorRepr,
-            config: Dict[str, Any],
-            next_nodes: List,
-            in_edges: List = None,
-            out_edges: List = None,
-    ):
-        self._uid = uid
-        self._inputs = inputs
-        self._outputs = outputs
-        self._iter_info = iter_info
-        self._op_info = op_info
-        self._config = config
-        self._next_nodes = next_nodes
-        self._in_edges = in_edges
-        self._out_edges = out_edges
-
-    @property
-    def uid(self):
-        return self._uid
+    uid: str
+    inputs: Optional[Tuple]
+    outputs: Optional[Tuple]
+    iter_info: IterationRepr
+    op_info: OperatorRepr
+    config: NodeConfig
+    next_nodes: Optional[List]
+    in_edges: Optional[List] = None
+    out_edges: Optional[List] = None
 
     @property
     def name(self):
-        return self._config.name
-
-    @property
-    def inputs(self) -> Union[str, Tuple]:
-        return self._inputs
-
-    @inputs.setter
-    def inputs(self, inputs):
-        self._inputs = inputs
-
-    @property
-    def outputs(self) -> Union[str, Tuple]:
-        return self._outputs
-
-    @outputs.setter
-    def outputs(self, outputs):
-        self._outputs = outputs
-
-    @property
-    def iter_info(self) -> IterationRepr:
-        return self._iter_info
-
-    @property
-    def op_info(self) -> OperatorRepr:
-        return self._op_info
-
-    @property
-    def config(self) -> Dict[str, Any]:
-        return self._config
-
-    @property
-    def next_nodes(self) -> List:
-        return self._next_nodes
-
-    @property
-    def in_edges(self) -> List:
-        return self._in_edges
-
-    @in_edges.setter
-    def in_edges(self, in_edges):
-        self._in_edges = in_edges
-
-    @property
-    def out_edges(self) -> List:
-        return self._out_edges
-
-    @out_edges.setter
-    def out_edges(self, out_edges):
-        self._out_edges = out_edges
-
-    @staticmethod
-    def from_dict(uid: str, node: Dict[str, Any]) -> 'NodeRepr':
-        """Return a NodeRepr from a description dict.
-
-        Args:
-            uid (`str`): Node id.
-            node (`Dict[str, Any]`): Dictionary about node info from dag.
-
-        Returns:
-            NodeRepr object.
-        """
-
-        check_keys(node, {'inputs', 'outputs', 'iter_info', 'next_nodes'})
-        iter_repr = IterationRepr.from_dict(node['iter_info'])
-
-        config = NodeConfig.from_dict(node['config'])
-
-        if uid in [InputConst.name, OutputConst.name] or node['iter_info']['type'] == ConcatConst.name:
-            op_repr = OperatorRepr.from_dict({'operator': uid, 'type': 'hub', 'init_args': None, 'init_kws': None, 'tag': 'main'})
-            return NodeRepr(uid, node['inputs'], node['outputs'], iter_repr, op_repr, config, node['next_nodes'])
-        else:
-            check_keys(node, {'op_info', 'config'})
-            op_repr = OperatorRepr.from_dict(node['op_info'])
-            return NodeRepr(uid, node['inputs'], node['outputs'], iter_repr, op_repr, config, node['next_nodes'])
+        return self.config.name
```

## towhee/runtime/pipeline.py

```diff
@@ -8,31 +8,33 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-
 import uuid
 from copy import deepcopy
 
+from towhee.runtime.check_utils import TupleForm
 from towhee.runtime.operator_manager import OperatorAction
 from towhee.runtime.factory import _OperatorWrapper
 from towhee.runtime.runtime_pipeline import RuntimePipeline
 from towhee.runtime.constants import (
     MapConst,
     WindowAllConst,
     WindowConst,
+    ReduceConst,
     FilterConst,
     TimeWindowConst,
     FlatMapConst,
     ConcatConst,
     InputConst,
     OutputConst,
+    OPName,
 )
 
 
 # pylint: disable=protected-access
 class Pipeline:
     """
     Pipeline is a tool to create data transformation chains.
@@ -57,66 +59,49 @@
         Args:
             schema (list): The schema for the values being inputted into the pipeline.
 
         Returns:
             Pipeline: Pipeline ready to be chained.
 
         Examples:
-            >>> import towhee
-            >>> pipe = towhee.pipe.input('a', 'b', 'c')
+            >>> from towhee import pipe
+            >>> p = pipe.input('a', 'b', 'c')
         """
         dag_dict = {}
         output_schema = cls._check_schema(schema)
         uid = InputConst.name
-        dag_dict[uid] = {
-            'inputs': output_schema,
-            'outputs': output_schema,
-            'iter_info': {
-                'type': 'map',
-                'param': None
-            },
-            'next_nodes': [],
-        }
+        dag_dict[uid] = cls._nop_node_dict(output_schema, output_schema)
         return cls(dag_dict)
 
-    # TODO: Run with the configuration.
-    def output(self, *output_schema, **config_kws) -> 'RuntimePipeline':
+    def output(self, *output_schema) -> 'RuntimePipeline':
         """
         Close and preload the pipeline, and ready to run with it.
 
         Args:
             output_schema (tuple): Which columns to output.
             config_kws (dict): The config for this pipeline.
 
 
         Returns:
             RuntimePipeline: The runtime pipeline that can be called on inputs.
 
         Examples:
-            >>> import towhee
-            >>> pipe = towhee.pipe.input('a').map('a', 'b', lambda x: x+1).output('b')
-            >>> pipe(1).get()
+            >>> from towhee import pipe
+            >>> p = pipe.input('a').map('a', 'b', lambda x: x+1).output('b')
+            >>> p(1).get()
             [2]
         """
         output_schema = self._check_schema(output_schema)
 
         uid = OutputConst.name
         dag_dict = deepcopy(self._dag)
-        dag_dict[uid] = {
-            'inputs': output_schema,
-            'outputs': output_schema,
-            'iter_info': {
-                'type': 'map',
-                'param': None
-            },
-            'next_nodes': None,
-        }
+        dag_dict[uid] = self._nop_node_dict(output_schema, output_schema)
         dag_dict[self._clo_node]['next_nodes'].append(uid)
 
-        run_pipe = RuntimePipeline(dag_dict, config=config_kws)
+        run_pipe = RuntimePipeline(dag_dict)
         run_pipe.preload()
         return run_pipe
 
     def map(self, input_schema, output_schema, fn, config=None) -> 'Pipeline':
         """
         One to one map of function on inputs.
 
@@ -126,17 +111,17 @@
             fn (Operation | lambda | callable): The action to perform on the input_schema.
             config (dict, optional): Config for the map. Defaults to None.
 
         Returns:
             Pipeline: Pipeline with action added.
 
         Examples:
-            >>> import towhee
-            >>> pipe = towhee.pipe.input('a').map('a', 'b', lambda x: x+1).output('a', 'b')
-            >>> pipe(1).get()
+            >>> from towhee import pipe
+            >>> p = pipe.input('a').map('a', 'b', lambda x: x+1).output('a', 'b')
+            >>> p(1).get()
             [1, 2]
         """
         output_schema = self._check_schema(output_schema)
         input_schema = self._check_schema(input_schema)
 
         uid = uuid.uuid4().hex
         fn_action = self._to_action(fn)
@@ -162,31 +147,35 @@
         Args:
             pipes : one or more pipelines to concat.
 
         Returns:
             Pipeline: Pipeline to be concated.
 
         Examples:
-            >>> pipe0 = towhee.pipe.input('a', 'b', 'c')
-            >>> pipe1 = pipe0.map('a', 'd', lambda x: x+1)
-            >>> pipe2 = pipe0.map(('b', 'c'), 'e', lambda x, y: x - y)
-            >>> pipe3 = pipe2.concat(pipe1).output('d', 'e')
-            >>> pipe3(1, 2, 3).get()
+            >>> from towhee import pipe
+            >>> p0 = pipe.input('a', 'b', 'c')
+            >>> p1 = p0.map('a', 'd', lambda x: x+1)
+            >>> p2 = p0.map(('b', 'c'), 'e', lambda x, y: x - y)
+            >>> p3 = p2.concat(p1).output('d', 'e')
+            >>> p3(1, 2, 3).get()
             [2, -1]
         """
         self._check_concat_pipe(pipes)
         uid = uuid.uuid4().hex
         dag_dict = self._concat_dag(deepcopy(self._dag), pipes)
+        fn_action = self._to_action(ConcatConst.name)
         dag_dict[uid] = {
             'inputs': (),
             'outputs': (),
+            'op_info': fn_action.serialize(),
             'iter_info': {
                 'type': ConcatConst.name,
-                'param': None
+                'param': None,
             },
+            'config': None,
             'next_nodes': [],
         }
         dag_dict[self._clo_node]['next_nodes'].append(uid)
         for pipe in pipes:
             dag_dict[pipe._clo_node]['next_nodes'].append(uid)
 
         return Pipeline(dag_dict, uid)
@@ -203,19 +192,19 @@
             fn (Operation | lambda | callable): The action to perform on the input_schema.
             config (dict, optional): Config for the flat_map. Defaults to None.
 
         Returns:
             Pipeline: Pipeline with flat_map action added.
 
         Examples:
-            >>> import towhee
-            >>> pipe = (towhee.pipe.input('a')
+            >>> from towhee import pipe
+            >>> p = (pipe.input('a')
             ...         .flat_map('a', 'b', lambda x: [y for y in x])
             ...         .output('b'))
-            >>> res = pipe([1, 2, 3])
+            >>> res = p([1, 2, 3])
             >>> res.get()
             [1]
             >>> res.get()
             [2]
             >>> res.get()
             [3]
         """
@@ -250,23 +239,23 @@
             fn (Operation | lambda | callable): The action to perform on the filter_colums.
             config (dict, optional): Config for the filter. Defaults to None.
 
         Returns:
             Pipeline: Pipeline with filter action added.
 
         Examples:
-            >>> import towhee
+            >>> from towhee import pipe
             >>> def filter_func(num):
             ...     return num > 10
-            >>> pipe = (towhee.pipe.input('a', 'c')
+            >>> p = (pipe.input('a', 'c')
             ...         .filter('c', 'd', 'a', filter_func)
             ...         .output('d'))
-            >>> pipe(1, 12).get()
+            >>> p(1, 12).get()
             None
-            >>> pipe(11, 12).get()
+            >>> p(11, 12).get()
             [12]
         """
         output_schema = self._check_schema(output_schema)
         input_schema = self._check_schema(input_schema)
         filter_columns = self._check_schema(filter_columns)
 
         uid = uuid.uuid4().hex
@@ -298,20 +287,20 @@
             fn (Operation | lambda | callable): The action to perform on the input_schema after window.
             config (dict, optional): Config for the window map. Defaults to None
 
         Returns:
             Pipeline: Pipeline with window action added.
 
         Examples:
-            >>> import towhee
-            >>> pipe = (towhee.pipe.input('n1', 'n2')
+            >>> from towhee import pipe
+            >>> p = (pipe.input('n1', 'n2')
             ...         .flat_map(('n1', 'n2'), ('n1', 'n2'), lambda x, y: [(a, b) for a, b in zip(x, y)])
             ...         .window(('n1', 'n2'), ('s1', 's2'), 2, 1, lambda x, y: (sum(x), sum(y)))
             ...         .output('s1', 's2'))
-            >>> res = pipe([1, 2, 3, 4], [2, 3, 4, 5])
+            >>> res = p([1, 2, 3, 4], [2, 3, 4, 5])
             >>> res.get()
             [3, 5]
             >>> res.get()
             [5, 7]
         """
         output_schema = self._check_schema(output_schema)
         input_schema = self._check_schema(input_schema)
@@ -344,20 +333,20 @@
             fn (Operation | lambda | callable): The action to perform on the input_schema after window all data.
             config (dict, optional): Config for the window_all. Defaults to None
 
         Returns:
             Pipeline: Pipeline with window_all action added.
 
         Examples:
-            >>> import towhee
-            >>> pipe = (towhee.pipe.input('n1', 'n2')
+            >>> from towhee import pipe
+            >>> p = (pipe.input('n1', 'n2')
             ...         .flat_map(('n1', 'n2'), ('n1', 'n2'), lambda x, y: [(a, b) for a, b in zip(x, y)])
             ...         .window_all(('n1', 'n2'), ('s1', 's2'), lambda x, y: (sum(x), sum(y)))
             ...         .output('s1', 's2'))
-            >>> pipe([1, 2, 3, 4], [2, 3, 4, 5]).get()
+            >>> p([1, 2, 3, 4], [2, 3, 4, 5]).get()
             [10, 14]
         """
         output_schema = self._check_schema(output_schema)
         input_schema = self._check_schema(input_schema)
 
         uid = uuid.uuid4().hex
         fn_action = self._to_action(fn)
@@ -372,14 +361,59 @@
             },
             'config': config,
             'next_nodes': [],
         }
         dag_dict[self._clo_node]['next_nodes'].append(uid)
         return Pipeline(dag_dict, uid)
 
+    def reduce(self, input_schema, output_schema, fn, config=None) -> 'Pipeline':
+        """
+        Reduce the sequence to a single value.
+
+        Args:
+            input_schema (tuple): The input column/s of fn.
+            output_schema (tuple): The output column/s of fn.
+            fn (Operation | lambda | callable): The action to perform on the input_schema after window all data.
+            config (dict, optional): Config for the window_all. Defaults to None
+
+        Returns:
+            Pipeline: Pipeline with reduce action added.
+
+        Examples:
+            >>> from towhee import pipe
+            >>> p = (pipe.input('n1', 'n2')
+            ...         .flat_map(('n1', 'n2'), ('n1', 'n2'), lambda x, y: [(a, b) for a, b in zip(x, y)])
+            ...         .reduce(('n1', 'n2'), ('s1', 's2'), lambda x, y: (sum(x), sum(y)))
+            ...         .output('s1', 's2'))
+            >>> p([1, 2, 3, 4], [2, 3, 4, 5]).get()
+            [10, 14]
+        """
+        if isinstance(fn, RuntimePipeline):
+            raise RuntimeError("Reduce node doesn't support pipeline fn")
+
+        output_schema = self._check_schema(output_schema)
+        input_schema = self._check_schema(input_schema)
+
+        uid = uuid.uuid4().hex
+        fn_action = self._to_action(fn)
+        dag_dict = deepcopy(self._dag)
+        dag_dict[uid] = {
+            'inputs': input_schema,
+            'outputs': output_schema,
+            'op_info': fn_action.serialize(),
+            'iter_info': {
+                'type': ReduceConst.name,
+                'param': None,
+            },
+            'config': config,
+            'next_nodes': [],
+        }
+        dag_dict[self._clo_node]['next_nodes'].append(uid)
+        return Pipeline(dag_dict, uid)
+
     def time_window(self, input_schema, output_schema, timestamp_col, size, step, fn, config=None) -> 'Pipeline':
         """
         Perform action on time windows.
 
         Args:
             input_schema (tuple): The input column/s of fn.
             output_schema (tuple): The output columns to fn.
@@ -390,21 +424,21 @@
                                                 after window the date with timestamp_col.
             config (dict, optional): Config for the time window. Defaults to None.
 
         Returns:
             Pipeline: Pipeline with time_window action added.
 
         Examples:
-            >>> import towhee
-            >>> pipe = (towhee.pipe.input('d')
+            >>> from towhee import pipe
+            >>> p = (pipe.input('d')
             ...         .flat_map('d', ('n1', 'n2', 't'), lambda x: ((a, b, c) for a, b, c in x))
             ...         .time_window(('n1', 'n2'), ('s1', 's2'), 't', 3, 3, lambda x, y: (sum(x), sum(y)))
             ...         .output('s1', 's2'))
             >>> data = [(i, i+1, i * 1000) for i in range(11) if i < 3 or i > 7] #[(0, 1), (1, 2), (2, 3), (8, 9), (9, 10), (10, 11)]
-            >>> res = pipe(data)
+            >>> res = p(data)
             >>> res.get() #[(0, 1), (1, 2), (2, 3)]
             [3, 6]
             >>> res.get() #(8, 9)
             [8, 9]
             >>> res.get() #(9, 10), (10, 11)
             [19, 21]
         """
@@ -428,23 +462,43 @@
             'next_nodes': [],
         }
         dag_dict[self._clo_node]['next_nodes'].append(uid)
         return Pipeline(dag_dict, uid)
 
     @staticmethod
     def _to_action(fn):
+        if fn in [OPName.NOP, ConcatConst.name]:
+            return OperatorAction.from_builtin(fn)
         if isinstance(fn, _OperatorWrapper):
-            return OperatorAction.from_hub(fn.name, fn.init_args, fn.init_kws)
+            return OperatorAction.from_hub(fn.name, fn.init_args, fn.init_kws, fn.tag, fn.is_latest)
+        if isinstance(fn, RuntimePipeline):
+            return OperatorAction.from_pipeline(fn)
         if getattr(fn, '__name__', None) == '<lambda>':
             return OperatorAction.from_lambda(fn)
         if callable(fn):
             return OperatorAction.from_callable(fn)
         raise ValueError('Unknown operator, please make sure it is lambda, callable or operator with ops.')
 
     @staticmethod
+    def _nop_node_dict(input_schema, output_schema):
+        fn_action = Pipeline._to_action(OPName.NOP)
+        node_dict = {
+            'inputs': input_schema,
+            'outputs': output_schema,
+            'op_info': fn_action.serialize(),
+            'iter_info': {
+                'type': MapConst.name,
+                'param': None,
+            },
+            'config': None,
+            'next_nodes': [],
+        }
+        return node_dict
+
+    @staticmethod
     def _check_concat_pipe(pipes):
         if len(pipes) == 0:
             raise ValueError('The parameter of concat cannot be None.')
         for pipe in pipes:
             if not isinstance(pipe, Pipeline):
                 raise ValueError(f'{pipe} is invalid, the parameter of concat must be Pipeline.')
 
@@ -456,18 +510,8 @@
             for name in same_nodes:
                 dag2[name]['next_nodes'] = list(set(dag2[name]['next_nodes'] + dag1[name]['next_nodes']))
             dag1.update(dag2)
         return dag1
 
     @staticmethod
     def _check_schema(schema):
-        if isinstance(schema, str):
-            return (schema,)
-        if schema is None:
-            return schema
-        if isinstance(schema, tuple):
-            for s in schema:
-                if not isinstance(s, str):
-                    raise ValueError(f'{s} is invalid, schema must be string.')
-            return schema
-        else:
-            raise ValueError(f'{schema} is invalid, schema must be string.')
+        return TupleForm(schema_data=schema).schema_data
```

## towhee/runtime/pipeline_loader.py

```diff
@@ -16,49 +16,55 @@
 import pathlib
 import hashlib
 import threading
 import importlib
 
 
 from towhee.pipelines import get_builtin_pipe_file
+from towhee.hub import get_pipeline
 
 
 PIPELINE_NAMESPACE = 'towhee.pipeline'
 
 
 class PipelineLoader:
     """
     Load Predefined Pipelines
     """
     _lock = threading.Lock()
 
     @staticmethod
     def module_name(name):
-        name = name.replace('/', '_')
+        name = name.replace('/', '.')
         return PIPELINE_NAMESPACE + '.' + name
 
     @staticmethod
     def _load_pipeline_from_file(name: str, file_path: 'Path'):
         modname = PipelineLoader.module_name(name)
-        spec = importlib.util.spec_from_file_location(modname, file_path.resolve())
+        spec = importlib.util.spec_from_file_location(modname, file_path)
         module = importlib.util.module_from_spec(spec)
         spec.loader.exec_module(module)
 
     @staticmethod
     def _load_builtins(name: str) -> bool:
         file_path = get_builtin_pipe_file(name)
         if file_path is None:
             return False
         PipelineLoader._load_pipeline_from_file(name, file_path)
         return True
 
     @staticmethod
-    def load_pipeline(name: str):
+    def load_pipeline(name: str, tag: str = 'main', latest: bool = False):
         with PipelineLoader._lock:
             file_path = pathlib.Path(name)
             if file_path.is_file():
                 new_name = hashlib.sha256(name.encode('utf-8')).hexdigest()
                 PipelineLoader._load_pipeline_from_file(new_name, file_path)
             else:
                 if not PipelineLoader._load_builtins(name):
-                    # TODO load pipeline from hub
-                    pass
+                    if '/' not in name:
+                        name = 'towhee/' + name
+                    path = get_pipeline(name, tag, latest)
+                    pipe_name = name.replace('-', '_')
+                    file_path = path / (pipe_name.split('/')[-1] + '.py')
+                    new_name = pipe_name + '.' + tag
+                    PipelineLoader._load_pipeline_from_file(new_name, file_path)
```

## towhee/runtime/runtime_conf.py

```diff
@@ -33,15 +33,15 @@
     @property
     def sys_config(self):
         return self._sys_config
 
     @staticmethod
     def from_node_config(node_conf: 'NodeConfig'):
         sys_conf = SysConf(node_conf.device)
-        return RuntimeConf(sys_conf, node_conf.acc_conf)
+        return RuntimeConf(sys_conf, node_conf.acc_info)
 
 
 class SysConf:
     """
     sys conf
     """
```

## towhee/runtime/runtime_pipeline.py

```diff
@@ -8,34 +8,35 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-
+import re
 from typing import Dict, Any, Union, Tuple, List
 from concurrent.futures import ThreadPoolExecutor
 
+from towhee.tools import visualizers
 from towhee.utils.log import engine_log
 from .operator_manager import OperatorPool
 from .data_queue import DataQueue
 from .dag_repr import DAGRepr
 from .nodes import create_node, NodeStatus
 from .node_repr import NodeRepr
-from .performance_profiler import PerformanceProfiler, TimeProfiler, Event
-from .constants import TracerConst
+from .time_profiler import TimeProfiler, Event
 
 
 class _GraphResult:
     def __init__(self, graph: '_Graph'):
         self._graph = graph
 
     def result(self):
         ret = self._graph.result()
+        self._graph.release_op()
         del self._graph
         return ret
 
 
 class _Graph:
     """
     Graph.
@@ -47,45 +48,52 @@
         thread_pool(`OperatorPool`): The ThreadPoolExecutor.
     """
     def __init__(self,
                  nodes: Dict[str, NodeRepr],
                  edges: Dict[str, Any],
                  operator_pool: 'OperatorPool',
                  thread_pool: 'ThreadPoolExecutor',
-                 enable_trance: bool = False):
+                 time_profiler: 'TimeProfiler' = None,
+                 trace_edges: list = None):
         self._nodes = nodes
         self._edges = edges
         self._operator_pool = operator_pool
         self._thread_pool = thread_pool
-        self._time_profiler = TimeProfiler(enable_trance)
+        self._time_profiler = time_profiler
+        self._trace_edges = trace_edges
         self._node_runners = None
         self._data_queues = None
         self.features = None
-        self.time_profiler.record(Event.pipe_name, Event.pipe_in)
-        self.initialize()
+        self._time_profiler.record(Event.pipe_name, Event.pipe_in)
+        self._initialize()
         self._input_queue = self._data_queues[0]
 
-    def initialize(self):
+    def _initialize(self):
         self._node_runners = []
-        self._data_queues = dict((name, DataQueue(edge['data'])) for name, edge in self._edges.items())
+        self._data_queues = dict(
+            (
+                name,
+                DataQueue(edge['data'], keep_data=(self._trace_edges and self._trace_edges.get(name, False)))
+            ) for name, edge in self._edges.items()
+        )
         for name in self._nodes:
             in_queues = [self._data_queues[edge] for edge in self._nodes[name].in_edges]
             out_queues = [self._data_queues[edge] for edge in self._nodes[name].out_edges]
             node = create_node(self._nodes[name], self._operator_pool, in_queues, out_queues, self._time_profiler)
             if not node.initialize():
                 raise RuntimeError(node.err_msg)
             self._node_runners.append(node)
 
     def result(self) -> any:
         for f in self.features:
             f.result()
         errs = ''
         for node in self._node_runners:
             if node.status != NodeStatus.FINISHED:
-                if node.status ==NodeStatus.FAILED:
+                if node.status == NodeStatus.FAILED:
                     errs += node.err_msg + '\n'
         if errs:
             raise RuntimeError(errs)
         end_edge_num = self._nodes['_output'].out_edges[0]
         res = self._data_queues[end_edge_num]
         self.time_profiler.record(Event.pipe_name, Event.pipe_out)
         return res
@@ -95,92 +103,178 @@
         self._input_queue.put(inputs)
         self._input_queue.seal()
         self.features = []
         for node in self._node_runners:
             self.features.append(self._thread_pool.submit(node.process))
         return _GraphResult(self)
 
+    def release_op(self):
+        for node in self._node_runners:
+            node.release_op()
+
     def __call__(self, inputs: Union[Tuple, List]):
         f = self.async_call(inputs)
         return f.result()
 
     @property
     def time_profiler(self):
         return self._time_profiler
 
     @property
     def input_col_size(self):
         return self._input_queue.col_size
 
+    @property
+    def data_queues(self):
+        return self._data_queues
+
 
 class RuntimePipeline:
     """
     Manage the pipeline and runs it as a single instance.
 
 
     Args:
         dag_dict(`Dict`): The DAG Dictionary from the user pipeline.
         max_workers(`int`): The maximum number of threads.
     """
 
-    def __init__(self, dag: Union[Dict, DAGRepr], max_workers: int = None, config: dict = None):
+    def __init__(self, dag: Union[Dict, DAGRepr], max_workers: int = None):
         if isinstance(dag, Dict):
             self._dag_repr = DAGRepr.from_dict(dag)
         else:
             self._dag_repr = dag
         self._operator_pool = OperatorPool()
         self._thread_pool = ThreadPoolExecutor(max_workers=max_workers)
-        self._time_profiler_list = []
-        self._config = {} if config is None else config
-        self._enable_trace = self._config.get(TracerConst.name, False)
 
     def preload(self):
         """
         Preload the operators.
         """
-        return _Graph(self._dag_repr.nodes, self._dag_repr.edges, self._operator_pool, self._thread_pool)
+        return _Graph(self._dag_repr.nodes, self._dag_repr.edges, self._operator_pool, self._thread_pool, TimeProfiler(False))
 
     def __call__(self, *inputs):
         """
         Output with ordering matching the input `DataQueue`.
         """
-        graph = _Graph(self._dag_repr.nodes, self._dag_repr.edges, self._operator_pool, self._thread_pool, self._enable_trace)
-        if self._enable_trace:
-            self._time_profiler_list.append(graph.time_profiler)
-        return graph(inputs)
+        return self._call(*inputs, profiler=False, tracer=False)[0]
 
     def batch(self, batch_inputs):
+        return self._batch(batch_inputs, profiler=False, tracer=False)[0]
+
+    def flush(self):
+        """
+        Call the flush interface of ops.
+        """
+        self._operator_pool.flush()
+
+    def _call(self, *inputs, profiler: bool, tracer: bool, trace_edges: list = None):
+        """
+        Run pipeline with debug option.
+        """
+        time_profiler = TimeProfiler(True) if profiler else TimeProfiler(False)
+        graph = _Graph(self._dag_repr.nodes, self._dag_repr.edges, self._operator_pool, self._thread_pool, time_profiler, trace_edges)
+
+        return graph(inputs), [graph.time_profiler] if profiler else None, [graph.data_queues] if tracer else None
+
+    def _batch(self, batch_inputs, profiler: bool, tracer: bool, trace_edges: list = None):
+        """
+        Run batch call with debug option.
+        """
         graph_res = []
+        time_profilers = []
+        data_queues = []
         for inputs in batch_inputs:
-            gh = _Graph(self._dag_repr.nodes, self._dag_repr.edges, self._operator_pool, self._thread_pool, self._enable_trace)
-            if self._enable_trace:
-                self._time_profiler_list.append(gh.time_profiler)
+            time_profiler = TimeProfiler(False) if time_profilers is None else TimeProfiler(True)
+            gh = _Graph(self._dag_repr.nodes, self._dag_repr.edges, self._operator_pool, self._thread_pool, time_profiler, trace_edges)
+
+            if profiler:
+                time_profilers.append(gh.time_profiler)
+            if tracer:
+                data_queues.append(gh.data_queues)
             if gh.input_col_size == 1:
                 inputs = (inputs, )
             graph_res.append(gh.async_call(inputs))
 
         rets = []
         for gf in graph_res:
             ret = gf.result()
             rets.append(ret)
-        return rets
+        return rets, time_profilers if time_profilers else None, data_queues if data_queues else None
 
     @property
     def dag_repr(self):
         return self._dag_repr
 
-    def profiler(self):
+    def _get_trace_nodes(self, include, exclude):
+        def _find_match(patterns, x):
+            return any(re.search(pattern, x) for pattern in patterns)
+
+        include = [include] if isinstance(include, str) else include
+        exclude = [exclude] if isinstance(exclude, str) else exclude
+        include = [node.name for node in self._dag_repr.nodes.values() if _find_match(include, node.name)] if include else []
+        exclude = [node.name for node in self._dag_repr.nodes.values() if _find_match(exclude, node.name)] if exclude else []
+        trace_nodes = list(set(include) - set(exclude)) if include \
+            else list(set(node.name for node in self._dag_repr.nodes.values()) - set(exclude))
+
+        return trace_nodes
+
+    def _get_trace_edges(self, trace_nodes):
+        def _set_false(idx):
+            trace_edges[idx] = False
+
+        trace_edges = dict((id, True) for id in self.dag_repr.edges)
+        for node in self.dag_repr.nodes.values():
+            if node.name not in trace_nodes:
+                _ = [_set_false(i) for i in node.out_edges]
+
+        return trace_edges
+
+    def debug(
+        self,
+        *inputs,
+        batch: bool = False,
+        profiler: bool = False,
+        tracer: bool = False,
+        include: Union[List[str], str] = None,
+        exclude: Union[List[str], str] = None
+    ):
         """
-        Report the performance results after running the pipeline, and please note that you need to set tracer to True when you declare a pipeline.
+        Run pipeline in debug mode.
+
+        One can record the running time of each operator by setting `profiler` to `True`, or record the data of itermediate nodes
+        by setting `tracer` to True. Note that one should at least specify one of `profiler` and `tracer` options to True.
+        When debug with `tracer` option, one can specify which nodes to include or exclude.
+
+        Args:
+            batch (`bool):
+                Whether to run in batch mode.
+            profiler (`bool`):
+                Whether to record the performance of the pipeline.
+            tracer (`bool`):
+                Whether to record the data from intermediate nodes.
+            include (`Union[List[str], str]`):
+                The nodes not to trace.
+            exclude (`Union[List[str], str]`):
+                The nodes to trace.
         """
-        if not self._enable_trace or not self._time_profiler_list:
-            engine_log.warning('Please set tracer to True or you need to run it first, there is nothing to report.')
-            return None
+        if not profiler and not tracer:
+            e_msg = 'You should set at least one of `profiler` or `tracer` to `True` when debug.'
+            engine_log.error(e_msg)
+            raise ValueError(e_msg)
 
-        performance_profiler = PerformanceProfiler(self._time_profiler_list, self._dag_repr)
-        return performance_profiler
+        trace_nodes = self._get_trace_nodes(include, exclude)
+        trace_edges = self._get_trace_edges(trace_nodes)
 
-    def reset_tracer(self):
-        """
-        Reset the tracer, reset the record to None.
-        """
-        self._time_profiler_list = []
+        time_profilers = [] if profiler else None
+        data_queues = [] if tracer else None
+
+        if not batch:
+            res, time_profilers, data_queues = self._call(*inputs, profiler=profiler, tracer=tracer, trace_edges=trace_edges)
+        else:
+            res, time_profilers, data_queues = self._batch(inputs[0], profiler=profiler, tracer=tracer, trace_edges=trace_edges)
+
+        v = visualizers.Visualizer(
+            result=res, time_profiler=time_profilers, data_queues=data_queues ,nodes=self._dag_repr.to_dict().get('nodes'), trace_nodes=trace_nodes
+        )
+
+        return v
```

## towhee/runtime/schema_repr.py

```diff
@@ -9,60 +9,60 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from typing import List
+from pydantic import BaseModel
 
 from towhee.runtime.data_queue import ColumnType
-from towhee.utils.log import engine_log
+from towhee.runtime.constants import (
+    WindowAllConst,
+    WindowConst,
+    ReduceConst,
+    FilterConst,
+    TimeWindowConst,
+    FlatMapConst,
+    ConcatConst,
+    MapConst
+)
 
 
 # pylint: disable=redefined-builtin
-class SchemaRepr:
+class SchemaRepr(BaseModel):
     """
     A `SchemaRepr` represents the data queue schema.
 
     Args:
         name (`str`): The name column data.
         type (`ColumnType`): The type of the column data, such as ColumnType.SCALAR or ColumnType.QUEUE.
     """
-    def __init__(self, name: str, type: 'ColumnType'):
-        self._name = name
-        self._type = type
-
-    @property
-    def name(self) -> str:
-        return self._name
-
-    @property
-    def type(self) -> 'ColumnType':
-        return self._type
+    name: str
+    type: ColumnType
 
     @staticmethod
     def from_dag(col_name: str, iter_type: str, inputs_type: List = None):
         """Return a SchemaRepr from the dag info.
 
         Args:
             col_name (`str`): Schema name.
             iter_type (`Dict[str, Any]`): The iteration type of this node.
             inputs_type (`List`): A list of the inputs schema type.
 
         Returns:
             SchemaRepr object.
         """
-        if iter_type in ['flat_map', 'window', 'time_window']:
+        if iter_type in [FlatMapConst.name, WindowConst.name, TimeWindowConst.name]:
             col_type = ColumnType.QUEUE
-        elif iter_type == 'concat':
+        elif iter_type == ConcatConst.name:
             col_type = inputs_type[0]
-        elif iter_type == 'window_all':
+        elif iter_type in [WindowAllConst.name, ReduceConst.name]:
             col_type = ColumnType.SCALAR
-        elif iter_type in ['map', 'filter']:
+        elif iter_type in [MapConst.name, FilterConst.name]:
             if inputs_type is not None and ColumnType.QUEUE in inputs_type:
                 col_type = ColumnType.QUEUE
             else:
                 col_type = ColumnType.SCALAR
         else:
-            engine_log.error('Unknown iteration type: %s', iter_type)
             raise ValueError(f'Unknown iteration type: {iter_type}')
-        return SchemaRepr(col_name, col_type)
+        return SchemaRepr(name=col_name, type=col_type)
```

## towhee/runtime/nodes/__init__.py

```diff
@@ -13,26 +13,28 @@
 # limitations under the License.
 
 
 from towhee.runtime.constants import (
     MapConst,
     WindowAllConst,
     WindowConst,
+    ReduceConst,
     FilterConst,
     TimeWindowConst,
     FlatMapConst,
     ConcatConst,
     OutputConst
 )
 from towhee.utils.log import engine_log
 
 from ._map import Map
 from ._window import Window
 from ._time_window import TimeWindow
 from ._window_all import WindowAll
+from ._reduce import Reduce
 from ._concat import Concat
 from ._filter import Filter
 from ._flat_map import FlatMap
 from ._output import Output
 from .node import NodeStatus
 
 
@@ -51,14 +53,17 @@
         return Filter(node_repr, op_pool, inputs, outputs, time_profiler)
     if node_repr.iter_info.type == TimeWindowConst.name:
         assert len(inputs) == 1
         return TimeWindow(node_repr, op_pool, inputs, outputs, time_profiler)
     if node_repr.iter_info.type == WindowAllConst.name:
         assert len(inputs) == 1
         return WindowAll(node_repr, op_pool, inputs, outputs, time_profiler)
+    if node_repr.iter_info.type == ReduceConst.name:
+        assert len(inputs) == 1
+        return Reduce(node_repr, op_pool, inputs, outputs, time_profiler)
     if node_repr.iter_info.type == ConcatConst.name:
         return Concat(node_repr, op_pool, inputs, outputs, time_profiler)
     if node_repr.iter_info.type == FlatMapConst.name:
         assert len(inputs) == 1
         return FlatMap(node_repr, op_pool, inputs, outputs, time_profiler)
     else:
         engine_log.error('Unknown node iteration type: %s', str(node_repr.iter_info.type))
```

## towhee/runtime/nodes/_concat.py

```diff
@@ -10,15 +10,15 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
 from .node import Node
-from towhee.runtime.performance_profiler import Event
+from towhee.runtime.time_profiler import Event
 
 
 class Concat(Node):
     """Concatenates all the pipelins
 
        Examples:
            p1 = towhee.pipe.input('url').map('url', 'image', ops.image_decode.cv2())
@@ -42,28 +42,25 @@
                     cols.append(col)
             self.cols_every_que.append(cols)
             all_cols.extend(cols)
             q_nums -= 1
         self.cols_every_que.reverse()
         return True
 
-    def process_step(self) -> bool:
+    def process_step(self):
         self._time_profiler.record(self.uid, Event.queue_in)
         all_data = {}
         for i, q in enumerate(self._in_ques):
             data = q.get_dict(self.cols_every_que[i])
             if data:
                 all_data.update(data)
 
         if not all_data:
             self._set_finished()
-            return True
+            return
 
         self._time_profiler.record(self.uid, Event.process_in)
         self._time_profiler.record(self.uid, Event.process_out)
 
-        for out_que in self._output_ques:
-            if not out_que.put_dict(all_data):
-                self._set_stopped()
-                return True
+        if not self.data_to_next(all_data):
+            return
         self._time_profiler.record(self.uid, Event.queue_out)
-        return False
```

## towhee/runtime/nodes/_filter.py

```diff
@@ -12,19 +12,21 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from typing import List
 
 from towhee.runtime.constants import FilterConst
 from towhee.runtime.data_queue import Empty
-from towhee.runtime.performance_profiler import Event
+from towhee.runtime.time_profiler import Event
+
 from .node import Node
+from ._single_input import SingleInputMixin
 
 
-class Filter(Node):
+class Filter(Node, SingleInputMixin):
     """
     Filter Operator.
 
     Filter the input columns based on the selected filter_columns and filter.
 
     i.e.
             ---1---2---3---4--->
@@ -33,40 +35,29 @@
     """
     def __init__(self, node_repr: 'NodeRepr',
                  op_pool: 'OperatorPool',
                  in_ques: List['DataQueue'],
                  out_ques: List['DataQueue'],
                  time_profiler: 'TimeProfiler'):
         super().__init__(node_repr, op_pool, in_ques, out_ques, time_profiler)
-        self._input_q = self._in_ques[0]
         self._key_map = dict(zip(self._node_repr.outputs, self._node_repr.inputs))
-        self._side_by_keys = list(set(self._input_q.schema) - set(self._node_repr.outputs))
 
-    def process_step(self) -> bool:
+    def process_step(self):
         self._time_profiler.record(self.uid, Event.queue_in)
-        data = self._input_q.get_dict()
-        if data is None:
-            self._set_finished()
-            return True
-        side_by = dict((k, data[k]) for k in self._side_by_keys)
+        data = self.read_row()
+        if data is None or not self.side_by_to_next(data):
+            return None
 
         process_data = [data.get(key) for key in self._node_repr.iter_info.param[FilterConst.param.filter_by]]
-        if not any((i is Empty() for i in process_data)):
-            self._time_profiler.record(self.uid, Event.process_in)
-            succ, is_need, msg = self._call(process_data)
-            self._time_profiler.record(self.uid, Event.process_out)
-            if not succ:
-                self._set_failed(msg)
-                return True
-
-            if is_need:
-                output_map = {new_key: data[old_key] for new_key, old_key in self._key_map.items()}
-                side_by.update(output_map)
+        if any((i is Empty() for i in process_data)):
+            return None
 
+        self._time_profiler.record(self.uid, Event.process_in)
+        succ, is_need, msg = self._call(process_data)
+        self._time_profiler.record(self.uid, Event.process_out)
+        assert succ, msg
         self._time_profiler.record(self.uid, Event.queue_out)
+        if is_need:
+            output_map = {new_key: data[old_key] for new_key, old_key in self._key_map.items()}
+            self.data_to_next(output_map)
 
-        for out_que in self._output_ques:
-            if not out_que.put_dict(side_by):
-                self._set_stopped()
-                return True
 
-        return False
```

## towhee/runtime/nodes/_flat_map.py

```diff
@@ -7,71 +7,51 @@
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import List, Any
 
-from .node import Node
 from towhee.runtime.data_queue import Empty
-from towhee.runtime.performance_profiler import Event
+from towhee.runtime.time_profiler import Event
+
+from .node import Node
+from ._single_input import SingleInputMixin
 
 
-class FlatMap(Node):
+class FlatMap(Node, SingleInputMixin):
     """
     FlatMap Operator.
 
     FlatMap transforms the iterable/nested outputs into one or more elements, i.e. split elements, unnest iterables.
 
     i.e.
             ---[0, 1, 2, 3]--->
         [    FlatMap('input', 'output', lambda i: i)    ]
             ---0---1---2---3--->
     """
-    def __init__(self, node_repr, op_pool, in_ques, out_ques, time_profiler):
-        super().__init__(node_repr, op_pool, in_ques, out_ques, time_profiler)
-        self._input_q = self._in_ques[0]
-        self._side_by_keys = list(set(self._input_q.schema) - set(self._node_repr.outputs))
-
-    def process_step(self) -> List[Any]:
+    def process_step(self):
         self._time_profiler.record(self.uid, Event.queue_in)
-        data = self._in_ques[0].get_dict()
-        if data is None:
-            self._set_finished()
-            return True
-
-        side_by = dict((k, data[k]) for k in self._side_by_keys)
+        data = self.read_row()
+        if data is None or not self.side_by_to_next(data):
+            return None
         process_data = [data.get(key) for key in self._node_repr.inputs]
-        if any((i is Empty() for i in process_data)):
-            for out_que in self._output_ques:
-                if not out_que.put_dict(side_by):
-                    self._set_stopped()
-                    return True
-        else:
-            self._time_profiler.record(self.uid, Event.process_in)
-            succ, outputs, msg = self._call(process_data)
-            if not succ:
-                self._set_failed(msg)
-                return True
-
-            size = len(self._node_repr.outputs)
-
-            for output in outputs:
-                if size > 1:
-                    output_map = {self._node_repr.outputs[i]: output[i] for i in range(size)}
-                else:
-                    output_map = {self._node_repr.outputs[0]: output}
-
-                side_by.update(output_map)
-
-                for out_que in self._output_ques:
-                    if not out_que.put_dict(side_by):
-                        self._set_stopped()
-                        return True
-
-                side_by = {}
-            self._time_profiler.record(self.uid, Event.process_out)
-            self._time_profiler.record(self.uid, Event.queue_out)
 
-        return False
+        if any((item is Empty() for item in process_data)):
+            return None
+
+        self._time_profiler.record(self.uid, Event.process_in)
+        succ, outputs, msg = self._call(process_data)
+        assert succ, msg
+
+        size = len(self._node_repr.outputs)
+        for output in outputs:
+            if size > 1:
+                output_map = {self._node_repr.outputs[i]: output[i] for i in range(size)}
+            else:
+                output_map = {self._node_repr.outputs[0]: output}
+            if not self.data_to_next(output_map):
+                return None
+
+        self._time_profiler.record(self.uid, Event.process_out)
+        self._time_profiler.record(self.uid, Event.queue_out)
```

## towhee/runtime/nodes/_map.py

```diff
@@ -8,23 +8,24 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-
 from typing import Generator
 
-from .node import Node
 from towhee.runtime.data_queue import Empty
-from towhee.runtime.performance_profiler import Event
+from towhee.runtime.time_profiler import Event
 
+from .node import Node
+from ._single_input import SingleInputMixin
 
-class Map(Node):
+
+class Map(Node, SingleInputMixin):
     """Map operator.
 
         Project each element of an input sequence into a new form.
         two cases:
            1. The operator return normal element.
 
                ---1---2---3---4--->
@@ -40,64 +41,50 @@
                     num += 1
 
                ---1---2---3---4--->
            [   map('input', 'output', func)    ]
                ---[0]---[0, 1]---[0, 1, 2]---[0, 1, 2, 3]--->
     """
 
-    def __init__(self, node_repr, op_pool, in_ques, out_ques, time_profiler):
-        super().__init__(node_repr, op_pool, in_ques, out_ques, time_profiler)
-        self._input_q = self._in_ques[0]
-        self._side_by_keys = list(set(self._input_q.schema) - set(self._node_repr.outputs))
-
-    def process_step(self) -> bool:
+    def process_step(self):
         """
         Called for each element.
         """
         self._time_profiler.record(self.uid, Event.queue_in)
-        data = self._in_ques[0].get_dict()
-        if data is None:
-            self._set_finished()
-            return True
-
-        side_by = dict((k, data[k]) for k in self._side_by_keys)
-
+        data = self.read_row()
+        if data is None or not self.side_by_to_next(data):
+            return None
         process_data = [data.get(key) for key in self._node_repr.inputs]
-        if not any((i is Empty() for i in process_data)):
-            self._time_profiler.record(self.uid, Event.process_in)
-            succ, outputs, msg = self._call(process_data)
-            if not succ:
-                self._set_failed(msg)
-                return True
-            if isinstance(outputs, Generator):
-                outputs = self._get_from_generator(outputs, len(self._node_repr.outputs))
-            self._time_profiler.record(self.uid, Event.process_out)
-
-            size = len(self._node_repr.outputs)
-            if size > 1:
-                output_map = dict((self._node_repr.outputs[i], outputs[i])
-                                  for i in range(size))
-            elif size == 0:
-                output_map = {}
-            else:
-                output_map = {}
-                output_map[self._node_repr.outputs[0]] = outputs
 
-            side_by.update(output_map)
-        self._time_profiler.record(self.uid, Event.queue_out)
+        if any((item is Empty() for item in process_data)):
+            return None
 
-        if not side_by:
-            return False
+        self._time_profiler.record(self.uid, Event.process_in)
+        succ, outputs, msg = self._call(process_data)
+        assert succ, msg
+        if isinstance(outputs, Generator):
+            outputs = self._get_from_generator(outputs, len(self._node_repr.outputs))
+        self._time_profiler.record(self.uid, Event.process_out)
+
+        size = len(self._node_repr.outputs)
+        if size > 1:
+            output_map = dict((self._node_repr.outputs[i], outputs[i])
+                              for i in range(size))
+        elif size == 0:
+            # ignore the op result
+            # eg: ignore the milvus result
+            # .map('vec', (), ops.ann_insert.milvus()),
+            output_map = {}
+        else:
+            # Use one col to store all op result.
+            output_map = {}
+            output_map[self._node_repr.outputs[0]] = outputs
 
-        for out_que in self._output_ques:
-            if not out_que.put_dict(side_by):
-                self._set_stopped()
-                return True
-
-        return False
+        self._time_profiler.record(self.uid, Event.queue_out)
+        self.data_to_next(output_map)
 
     def _get_from_generator(self, gen, size):
         if size == 1:
             return list(gen)
         ret = [ [] for _ in range(size)]
         for data in gen:
             for i in range(size):
```

## towhee/runtime/nodes/_output.py

```diff
@@ -10,42 +10,36 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
 from .node import Node
-from towhee.runtime.performance_profiler import Event
+from ._single_input import SingleInputMixin
+from towhee.runtime.time_profiler import Event
 
 
-class Output(Node):
+class Output(Node, SingleInputMixin):
     """Output the data as input
 
        Examples:
            p1 = towhee.pipe.input('url').output('url')
     """
-    def initialize(self) -> bool:
-        for q in self._output_ques:
-            q.max_size = 0
+    def initialize(self):
+        self._output_ques[0].max_size = 0
         return super().initialize()
 
-    def process_step(self) -> bool:
+    def process_step(self):
         self._time_profiler.record(self.uid, Event.queue_in)
-        all_data = {}
-        for q in self._in_ques:
-            data = q.get_dict()
-            if data:
-                all_data.update(data)
-
-        if not all_data:
-            self._set_finished()
-            return True
+
+        data = self.read_row()
+        if data is None:
+            return
 
         self._time_profiler.record(self.uid, Event.process_in)
         self._time_profiler.record(self.uid, Event.process_out)
 
-        for out_que in self._output_ques:
-            if not out_que.put_dict(all_data):
-                self._set_stopped()
-                return True
+        if not self.data_to_next(data):
+            return
+
         self._time_profiler.record(self.uid, Event.queue_out)
-        return False
+
```

## towhee/runtime/nodes/_time_window.py

```diff
@@ -12,18 +12,18 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
 from towhee.runtime.constants import TimeWindowConst
 from towhee.runtime.data_queue import Empty
 
-from ._window import Window
+from ._window_base import WindowBase
 
 
-class TimeWindow(Window):
+class TimeWindow(WindowBase):
     """Window operator
 
       inputs:       ---1--2-3-----4-5-6--->
       timestamp:    0 ------1000ms-------2000ms---->
 
       node definition:
 
@@ -37,43 +37,22 @@
 
       outputs:
         ----6-----15------->
     """
     def _init(self):
         self._time_range_sec = self._node_repr.iter_info.param[TimeWindowConst.param.time_range_sec]
         self._time_step_sec = self._node_repr.iter_info.param[TimeWindowConst.param.time_step_sec]
-        self._input_que = self._in_ques[0]
-        self._schema = self._input_que.schema
-        self._timestamp_index = self._schema.index(self._node_repr.iter_info.param[TimeWindowConst.param.timestamp_col])
+        self._timestamp_index = self._node_repr.iter_info.param[TimeWindowConst.param.timestamp_col]
         self._buffer = _TimeWindowBuffer(self._time_range_sec, self._time_step_sec)
-        self._row_buffer = []
 
-    def _get_buffer(self):
-        while True:
-            data = self._input_que.get()
-            if data is None:
-                # end of the data_queue
-                if self._buffer is not None and self._buffer.data:
-                    ret = self._buffer.data
-                    self._buffer = self._buffer.next()
-                    return self._to_cols(ret)
-                else:
-                    return None
-
-            self._row_buffer.append(data)
-
-            timestamp = data[self._timestamp_index]
-            if timestamp is Empty():
-                # end of the timestamp col
-                continue
-
-            if self._buffer(data, timestamp) and self._buffer.data:
-                ret = self._buffer.data
-                self._buffer = self._buffer.next()
-                return self._to_cols(ret)
+    def _window_index(self, data):
+        timestamp = data[self._timestamp_index]
+        if timestamp is Empty():
+            return -1
+        return timestamp
 
 
 class _TimeWindowBuffer:
     '''
     TimeWindow
     The unit of timestamp is milliseconds, the unit of window(range, step) is seconds.
     '''
```

## towhee/runtime/nodes/_window.py

```diff
@@ -12,20 +12,20 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
 from typing import List
 
 from towhee.runtime.constants import WindowConst
-from towhee.runtime.data_queue import Empty
-from towhee.runtime.performance_profiler import Event
-from .node import Node
 
+from ._window_base import WindowBase
 
-class Window(Node):
+
+
+class Window(WindowBase):
     """Window operator
 
       inputs: ---1-2-3-4-5-6--->
 
       node definition:
 
         [    window('input', 'output', size=3, step=2, callable=lambda i: sum(i))     ]
@@ -36,108 +36,23 @@
         callable([3, 4, 5]) -> 12
         callable([5, 6]) -> 11
 
       outputs:
         ----6-12-11---->
     """
 
-    def __init__(self, node_repr: 'NodeRepr',
-                 op_pool: 'OperatorPool',
-                 in_ques: List['DataQueue'],
-                 out_ques: List['DataQueue'],
-                 time_profiler: 'TimeProfiler'):
-
-        super().__init__(node_repr, op_pool, in_ques, out_ques, time_profiler)
-        self._init()
-
     def _init(self):
         self._size = self._node_repr.iter_info.param[WindowConst.param.size]
         self._step = self._node_repr.iter_info.param[WindowConst.param.step]
         self._cur_index = -1
-        self._input_que = self._in_ques[0]
-        self._schema = self._in_ques[0].schema
         self._buffer = _WindowBuffer(self._size, self._step)
-        self._row_buffer = []
 
-    def _get_buffer(self):
-        while True:
-            data = self._input_que.get()
-            if data is None:
-                # end of the data_queue
-                if self._buffer is not None and self._buffer.data:
-                    ret = self._buffer.data
-                    self._buffer = self._buffer.next()
-                    return self._to_cols(ret)
-                else:
-                    return None
-
-            self._cur_index += 1
-            self._row_buffer.append(data)
-            if self._buffer(data, self._cur_index) and self._buffer.data:
-                ret = self._buffer.data
-                self._buffer = self._buffer.next()
-                return self._to_cols(ret)
-
-    def _to_cols(self, rows: List[List]):
-        cols = [[] for _ in self._schema]
-        for row in rows:
-            for i in range(len(self._schema)):
-                if row[i] is not Empty():
-                    cols[i].append(row[i])
-        ret = {}
-        for i in range(len(self._schema)):
-            ret[self._schema[i]] = cols[i]
-        return ret
-
-    def process_step(self) -> bool:
-        """
-        Process each window data.
-        """
-        self._time_profiler.record(self.uid, Event.queue_in)
-        in_buffer = self._get_buffer()
-        if in_buffer is None:
-            if self._row_buffer:
-                cols = self._to_cols(self._row_buffer)
-                for out_que in self._output_ques:
-                    if not out_que.batch_put_dict(cols):
-                        self._set_stopped()
-                        return True
-                self._row_buffer = []
-            self._set_finished()
-            return True
-
-        process_data = [in_buffer.get(key) for key in self._node_repr.inputs]
-        if not any(process_data):
-            return False
-
-        self._time_profiler.record(self.uid, Event.process_in)
-        succ, outputs, msg = self._call(process_data)
-        self._time_profiler.record(self.uid, Event.process_out)
-        if not succ:
-            self._set_failed(msg)
-            return True
-
-        size = len(self._node_repr.outputs)
-        if size > 1:
-            output_map = dict((self._node_repr.outputs[i], [outputs[i]])
-                              for i in range(size))
-        else:
-            output_map = {}
-            output_map[self._node_repr.outputs[0]] = [outputs]
-
-        cols = self._to_cols(self._row_buffer)
-        cols.update(output_map)
-        self._time_profiler.record(self.uid, Event.queue_out)
-
-        for out_que in self._output_ques:
-            if not out_que.batch_put_dict(cols):
-                self._set_stopped()
-                return True
-        self._row_buffer = []
-        return False
+    def _window_index(self, data):  # pylint: disable=unused-argument
+        self._cur_index += 1
+        return self._cur_index
 
 
 class _WindowBuffer:
     '''
     Collect data by size and step.
     '''
     def __init__(self, size: int, step: int, start_index: int = 0):
```

## towhee/runtime/nodes/_window_all.py

```diff
@@ -8,24 +8,22 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-
-from typing import List
-
 from towhee.runtime.data_queue import Empty
+from towhee.runtime.time_profiler import Event
 
 from .node import Node
-from towhee.runtime.performance_profiler import Event
+from ._single_input import SingleInputMixin
 
 
-class WindowAll(Node):
+class WindowAll(Node, SingleInputMixin):
     """Window operator
 
       inputs: ---1-2-3-4-5-6--->
 
       node definition:
 
         [    window_all('input', 'output', callable=lambda i: sum(i))     ]
@@ -33,77 +31,55 @@
       step:
 
         callable([1, 2, 3, 4, 5, 6]) -> 21
 
       outputs:
         ----21---->
     """
-    def __init__(self, node_info: 'NodeInfo',
-                 op_pool: 'OperatorPool',
-                 in_ques: List['DataQueue'],
-                 out_ques: List['DataQueue'],
-                 time_profiler: 'TimeProfiler'):
-
-        super().__init__(node_info, op_pool, in_ques, out_ques, time_profiler)
-        self._input_que = in_ques[0]
-        self._schema = in_ques[0].schema
-
     def _get_buffer(self):
-        data = self._input_que.get()
-        if not data:
-            return None
-        else:
-            assert len(self._schema) == len(data)
-            cols = [[i] if i is not Empty() else [] for i in data]
-
+        ret = dict((key, []) for key in self._node_repr.inputs)
         while True:
-            data = self._input_que.get()
+            data = self.input_que.get_dict()
             if data is None:
-                break
+                return ret
+
+            if not self.side_by_to_next(data):
+                return None
 
-            for i in range(len(self._schema)):
-                if data[i] is not Empty():
-                    cols[i].append(data[i])
-
-        ret = {}
-        for i in range(len(self._schema)):
-            ret[self._schema[i]] = cols[i]
-        return ret
+            for key in self._node_repr.inputs:
+                if data.get(key) is not Empty():
+                    ret[key].append(data.get(key))
 
-    def process_step(self) -> bool:
+    def process_step(self):
         """
         Process each window data.
         """
         self._time_profiler.record(self.uid, Event.queue_in)
         in_buffer = self._get_buffer()
         if in_buffer is None:
+            return
+
+        if all(map(lambda x: len(x) == 0, in_buffer.values())):
             self._set_finished()
-            return True
+            return
 
         process_data = [in_buffer.get(key) for key in self._node_repr.inputs]
-        if not any(process_data):
-            return False
-
         self._time_profiler.record(self.uid, Event.process_in)
         succ, outputs, msg = self._call(process_data)
         self._time_profiler.record(self.uid, Event.process_out)
-        if not succ:
-            self._set_failed(msg)
-            return True
+        assert succ, msg
 
         size = len(self._node_repr.outputs)
         if size > 1:
-            output_map = dict((self._node_repr.outputs[i], [outputs[i]])
+            output_map = dict((self._node_repr.outputs[i], outputs[i])
                               for i in range(size))
+        elif size == 1:
+            output_map = {}
+            output_map[self._node_repr.outputs[0]] = outputs
         else:
             output_map = {}
-            output_map[self._node_repr.outputs[0]] = [outputs]
 
-        in_buffer.update(output_map)
         self._time_profiler.record(self.uid, Event.queue_out)
-
-        for out_que in self._output_ques:
-            if not out_que.batch_put_dict(in_buffer):
-                self._set_stopped()
-                return True
+        if not self.data_to_next(output_map):
+            return
         self._set_finished()
-        return True
+
```

## towhee/runtime/nodes/node.py

```diff
@@ -17,28 +17,28 @@
 from enum import Enum, auto
 from abc import ABC
 import traceback
 
 from towhee.runtime.data_queue import DataQueue
 from towhee.runtime.runtime_conf import set_runtime_config
 from towhee.runtime.constants import OPType
-from towhee.runtime.performance_profiler import Event, TimeProfiler
+from towhee.runtime.time_profiler import Event, TimeProfiler
 from towhee.utils.log import engine_log
 
 
 class NodeStatus(Enum):
     NOT_RUNNING = auto()
     RUNNING = auto()
     FINISHED = auto()
     FAILED = auto()
     STOPPED = auto()
 
     @staticmethod
     def is_end(status: 'NodeStatus') -> bool:
-        return status in [NodeStatus.FINISHED, NodeStatus.FAILED]
+        return status in [NodeStatus.FINISHED, NodeStatus.FAILED, NodeStatus.STOPPED]
 
 
 class Node(ABC):
     """
     node_info:
         name
         func_type: operator/lambda
@@ -76,25 +76,27 @@
         self._err_msg = None
 
     def initialize(self) -> bool:
         #TODO
         # Create multiple-operators to support parallelism.
         # Read the parallelism info by config.
         op_type = self._node_repr.op_info.type
-        if op_type == OPType.HUB:
+        if op_type in [OPType.HUB, OPType.BUILTIN]:
             try:
                 hub_id = self._node_repr.op_info.operator
                 with set_runtime_config(self._node_repr.config):
                     self._time_profiler.record(self.uid, Event.init_in)
                     self._op = self._op_pool.acquire_op(
                         self.uid,
                         hub_id,
                         self._node_repr.op_info.init_args,
                         self._node_repr.op_info.init_kws,
-                        self._node_repr.op_info.tag)
+                        self._node_repr.op_info.tag,
+                        self._node_repr.op_info.latest,
+                    )
                     self._time_profiler.record(self.uid, Event.init_out)
                     return True
             except Exception as e:  # pylint: disable=broad-except
                 st_err = '{}, {}'.format(str(e), traceback.format_exc())
                 err = 'Create {} operator {}:{} with args {} and kws {} failed, err: {}'.format(
                     self.name,
                     hub_id,
@@ -160,28 +162,35 @@
 
     def process_step(self) -> bool:
         raise NotImplementedError
 
     def process(self):
         engine_log.info('Begin to run %s', str(self))
         self._set_status(NodeStatus.RUNNING)
-        while True:
-            if not self._need_stop:
-                try:
-                    if self.process_step():
-                        break
-                except Exception as e:  # pylint: disable=broad-except
-                    err = '{}, {}'.format(e, traceback.format_exc())
-                    self._set_failed(err)
-                    break
-            else:
-                self._set_finished()
+        while not self._need_stop and not NodeStatus.is_end(self.status):
+            try:
+                self.process_step()
+            except Exception as e:  # pylint: disable=broad-except
+                err = '{}, {}'.format(e, traceback.format_exc())
+                self._set_failed(err)
+
+    def data_to_next(self, data) -> bool:
+        for out_que in self._output_ques:
+            if not out_que.put_dict(data):
+                self._set_stopped()
+                return False
+            pass
+        return True
 
     def _set_status(self, status: NodeStatus) -> None:
         self._status = status
 
     def __str__(self) -> str:
         return 'Node-{}'.format(self.name)
 
-    def __del__(self):
-        if self._node_repr.op_info.type == OPType.HUB and self._op:
+    def release_op(self):
+        if self._op and self._node_repr.op_info.type == OPType.HUB:
             self._op_pool.release_op(self._op)
+            self._op = None
+
+    def __del__(self):
+        self.release_op()
```

## towhee/runtime/operator_manager/operator_action.py

```diff
@@ -7,14 +7,15 @@
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
 from towhee.runtime.constants import OPType
 
 # pylint: disable=protected-access
 class OperatorAction:
     """
     Action wrapper.
 
@@ -27,47 +28,88 @@
         self._tag = 'main'
 
     @property
     def type(self):
         return self._type
 
     @staticmethod
-    def from_hub(name, args, kwargs):
+    def from_hub(name, args, kwargs, tag, latest):
         """Create an Action for hub op.
 
         Args:
             name (str): The op name or the path to an op.
             args (list): The op args.
             kwargs (dict): The op kwargs.
+            tag: (`str`): The tag of op.
+            latest (`bool`): Whether to download the latest op files.
 
         Returns:
             Action: The action.
         """
         action = OperatorAction()
         action._op_name = name
         action._op_args = args
         action._op_kwargs = kwargs
+        action._tag = tag
+        action._latest = latest
         action._type = 'hub'
         return action
 
+    @staticmethod
+    def from_builtin(name, args=None, kwargs=None, tag=None, latest=False):
+        """Create an Action for hub op.
+
+        Args:
+            name (str): The op name or the path to an op.
+            args (list): The op args.
+            kwargs (dict): The op kwargs.
+
+        Returns:
+            Action: The action.
+        """
+        action = OperatorAction()
+        action._op_name = name
+        action._op_args = args
+        action._op_kwargs = kwargs
+        action._type = OPType.BUILTIN
+        action._tag = tag
+        action._latest = latest
+        return action
+
+    @staticmethod
+    def from_pipeline(fn):
+        """Create an Action for callable op.
+
+        Args:
+            fn (callable): The callable function for op.
+
+        Returns:
+            Action: The action.
+        """
+        action = OperatorAction()
+        action._dag_dict = fn.dag_repr.dag_dict
+        action._top_sort = fn.dag_repr.top_sort
+        action._type = OPType.PIPELINE
+        return action
+
     # TODO: Deal with serialized input vs non_serialized
     @staticmethod
     def from_lambda(fn):
         """Create an Action for lambda op.
 
         Args:
             fn (lambda): The lambda function for op.
 
         Returns:
             Action: The action.
         """
         action = OperatorAction()
         action._fn = fn
         action._loaded_fn = fn
-        action._type = 'lambda'
+        action._type = OPType.LAMBDA
         return action
 
     # TODO: Deal with serialized input vs non_serialized
     @staticmethod
     def from_callable(fn):
         """Create an Action for callable op.
 
@@ -76,27 +118,36 @@
 
         Returns:
             Action: The action.
         """
         action = OperatorAction()
         action._fn = fn
         action._loaded_fn = fn
-        action._type = 'callable'
+        action._type = OPType.CALLABLE
         return action
 
     def serialize(self):
-        if self._type == OPType.HUB:
+        if self._type in [OPType.HUB, OPType.BUILTIN]:
             return {
                 'operator': self._op_name,
                 'type': self._type,
-                'init_args': self._op_args if len(self._op_args) != 0 else None,
-                'init_kws': self._op_kwargs if len(self._op_kwargs) != 0 else None,
-                'tag': self._tag
+                'init_args': self._op_args if self._op_args and len(self._op_args) != 0 else None,
+                'init_kws': self._op_kwargs if self._op_kwargs and len(self._op_kwargs) != 0 else None,
+                'tag': self._tag,
+                'latest': self._latest
             }
         elif self._type in [OPType.LAMBDA, OPType.CALLABLE]:
             return {
                 'operator': self._fn,
                 'type': self._type,
                 'init_args': None,
                 'init_kws': None,
                 'tag': None
             }
+        elif self._type == OPType.PIPELINE:
+            return {
+                'dag': self._dag_dict,
+                'top_sort': self._top_sort,
+                'type': self._type,
+            }
+        else:
+            raise KeyError(f'Operator type {self._type} is not valid.')
```

## towhee/runtime/operator_manager/operator_loader.py

```diff
@@ -16,48 +16,50 @@
 import sys
 import subprocess
 from pathlib import Path
 from typing import Any, List, Dict, Union
 import re
 import traceback
 import pkg_resources
+import hashlib
 
 from towhee.operator import Operator
 from towhee.operator.nop import NOPNodeOperator
 from towhee.hub import get_operator
-from towhee.runtime.constants import InputConst, OutputConst
+from towhee.runtime.constants import OPName
 from towhee.utils.log import engine_log
 from .operator_registry import OperatorRegistry
 
 
+# pylint: disable=unused-argument
 class OperatorLoader:
     """
     Wrapper class used to load operators from either local cache or a remote
     location.
 
     Args:
         cache_path: (`str`)
             Local cache path to use. If not specified, it will default to
             `$HOME/.towhee/operators`.
     """
 
-    def _load_operator_from_internal(self, function: str, arg: List[Any], kws: Dict[str, Any], tag: str) -> Operator:  # pylint: disable=unused-argument
-        if function in [InputConst.name, OutputConst.name]:
+    def _load_operator_from_internal(self, function: str, arg: List[Any], kws: Dict[str, Any], tag: str, latest: bool) -> Operator:
+        if function == OPName.NOP:
             return NOPNodeOperator()
         else:
             return None
 
-    def _load_operator_from_registry(self, function: str, arg: List[Any], kws: Dict[str, Any], tag: str) -> Operator:  # pylint: disable=unused-argument
+    def _load_operator_from_registry(self, function: str, arg: List[Any], kws: Dict[str, Any], tag: str, latest: bool) -> Operator:
         op = OperatorRegistry.resolve(function)
         return self._instance_operator(op, arg, kws) if op is not None else None
 
     def _load_legacy_op(self, modname, path, fname):
         # support old version operator API
         file_name = path / (fname + '.py')
-        spec = importlib.util.spec_from_file_location(modname, file_name.resolve())
+        spec = importlib.util.spec_from_file_location(modname, file_name)
         # Create the module and then execute the module in its own namespace.
 
         module = importlib.util.module_from_spec(spec)
         sys.modules[modname] = module
         spec.loader.exec_module(module)
         # Instantiate the operator object and return it to the caller for
         # `load_operator`. By convention, the operator class is simply the CamelCase
@@ -75,15 +77,15 @@
         module = importlib.util.module_from_spec(spec)
         sys.modules[modname] = module
         spec.loader.exec_module(module)
         op = getattr(module, fname, None)
 
         return op
 
-    def _load_operator_from_path(self, path: Union[str, Path], function: str, arg: List[Any], kws: Dict[str, Any]) -> Operator:
+    def _load_operator_from_path(self, path: Union[str, Path], function: str, arg: List[Any], kws: Dict[str, Any], tag: str = None) -> Operator:
         """
         Load operator form local path.
         Args:
             path (`Union[str, Path]`):
                 Path to the operator python file.
             arg (`List[str, Any]`):
                 The init args for OperatorClass.
@@ -92,63 +94,68 @@
         Returns
             (`typing.Any`)
                 The `Operator` output.
         """
         path = Path(path)
         fname = function.split('/')[1].replace('-', '_')
         op_name = function.replace('-', '_').replace('/', '.')
-        modname = 'towhee.operator.' + op_name
+        if not tag:
+            tag = hashlib.sha256(fname.encode('utf-8')).hexdigest()
+        modname = 'towhee.operator.' + op_name + '.' + tag
 
         all_pkg = [item.project_name for item in list(pkg_resources.working_set)]
-        if 'requirements.txt' in (i.name for i in path.parent.iterdir()):
-            with open(path.parent / 'requirements.txt', 'r', encoding='utf-8') as f:
+        if 'requirements.txt' in (i.name for i in path.iterdir()):
+            with open(path / 'requirements.txt', 'r', encoding='utf-8') as f:
                 reqs = f.read().split('\n')
             for req in reqs:
+                need_install = []
                 if not req:
                     continue
-                pkg_name = re.split(r'(~|>|<|=|!| )', req)[0]
+                pkg_name = re.split(r'(~|>|<|=|!|\]|\[| )', req)[0]
                 pkg_name = pkg_name.replace('_', '-')
                 if pkg_name not in all_pkg:
-                    subprocess.check_call([sys.executable, '-m', 'pip', 'install', req])
+                    need_install.append(req)
+            if need_install:
+                subprocess.check_call([sys.executable, '-m', 'pip', 'install', *need_install])
 
         op = self._load_op(modname, path, fname)
         if not op:
             engine_log.warning('Load operator %s:%s:%s failed, try to use the legacy type' , modname, path, fname)
             op = self._load_legacy_op(modname, path, fname)
 
         return self._instance_operator(op, arg, kws) if op is not None else None
 
-    def _load_operator_from_hub(self, function: str, arg: List[Any], kws: Dict[str, Any], tag: str) -> Operator:
+    def _load_operator_from_hub(self, function: str, arg: List[Any], kws: Dict[str, Any], tag: str, latest: bool) -> Operator:
         if '/' not in function:
             function = 'towhee/'+function
         try:
-            path = get_operator(operator=function, tag=tag)
+            path = get_operator(operator=function, tag=tag, latest=latest)
         except Exception as e:  # pylint: disable=broad-except
             err = '{}, {}'.format(str(e), traceback.format_exc())
             engine_log.error(err)
             return None
 
-        return self._load_operator_from_path(path, function, arg, kws)
+        return self._load_operator_from_path(path, function, arg, kws, tag)
 
-    def load_operator(self, function: str, arg: List[Any], kws: Dict[str, Any], tag: str) -> Operator:
+    def load_operator(self, function: str, arg: List[Any], kws: Dict[str, Any], tag: str, latest: bool) -> Operator:
         """
         Attempts to load an operator from cache. If it does not exist, looks up the
         operator in a remote location and downloads it to cache instead. By standard
         convention, the operator must be called `Operator` and all associated data must
         be contained within a single directory.
 
         Args:
             function: (`str`)
                 Origin and method/class name of the operator. Used to look up the proper
                 operator in cache.
         """
         for factory in [self._load_operator_from_internal,
                         self._load_operator_from_registry,
                         self._load_operator_from_hub]:
-            op = factory(function, arg, kws, tag)
+            op = factory(function, arg, kws, tag, latest)
             if op is not None:
                 return op
         if op is None:
             raise RuntimeError('Load operator failed')
 
     def _instance_operator(self, op, arg: List[Any], kws: Dict[str, Any]) -> Operator:
         if arg is None:
```

## towhee/runtime/operator_manager/operator_pool.py

```diff
@@ -40,14 +40,19 @@
     def put(self, op: Operator, force_put: bool = False):
         if self._shared_type is None:
             self._shared_type = op.shared_type
 
         if force_put or self._shared_type == SharedType.NotShareable:
             self._ops.append(op)
 
+    def flush(self):
+        for op in self._ops:
+            if hasattr(op, 'flush'):
+                op.flush()
+
     def __len__(self):
         return len(self._ops)
 
 
 class OperatorPool:
     """
     `OperatorPool` manages `Operator` creation, acquisition, release, and garbage
@@ -63,28 +68,30 @@
         for _, op_storage in self._all_ops.items():
             num += len(op_storage)
         return num
 
     def clear(self):
         self._all_ops = {}
 
-    def acquire_op(self, key, hub_op_id: str, op_args: List, op_kws: Dict[str, any], tag: str) -> Operator:
+    def acquire_op(self, key, hub_op_id: str, op_args: List, op_kws: Dict[str, any], tag: str, latest: bool) -> Operator:
         """
         Instruct the `OperatorPool` to reserve and return the
         specified operator for use in the executor.
 
         Args:
             key: (`str`)
             hub_op_id: (`str`)
             op_args: (`List`)
                 Operator init parameters with args
             op_kws: (`Dict[str, any]`)
                 Operator init parameters with kwargs
             tag: (`str`)
                 The tag of operator
+            latest (`bool`):
+                Whether to download the latest files.
 
         Returns:
             (`towhee.operator.Operator`)
                 The operator instance reserved for the caller.
         """
 
         # Load the operator if the computed key does not exist in the operator
@@ -92,15 +99,15 @@
         with self._lock:
             storage = self._all_ops.get(key, None)
             if storage is None:
                 storage = _OperatorStorage()
                 self._all_ops[key] = storage
 
             if not storage.op_available():
-                op = self._op_loader.load_operator(hub_op_id, op_args, op_kws, tag)
+                op = self._op_loader.load_operator(hub_op_id, op_args, op_kws, tag, latest)
                 storage.put(op, True)
                 op.key = key
             return storage.get()
 
     def release_op(self, op: Operator):
         """
         Releases the specified operator and all associated resources back to the
@@ -109,7 +116,11 @@
         Args:
             op: (`towhee.Operator`)
                 `Operator` instance to add back into the operator pool.
         """
         with self._lock:
             storage = self._all_ops[op.key]
             storage.put(op)
+
+    def flush(self):
+        for _, storage in self._all_ops.items():
+            storage.flush()
```

## towhee/serve/server_builder.py

```diff
@@ -17,32 +17,111 @@
 from .triton.docker_image_builder import DockerImageBuilder
 from .triton.pipeline_builder import Builder as TritonModelBuilder
 from .triton import constant
 from towhee.utils.log import engine_log
 
 
 def build_docker_image(
-        dc_pipeline: 'towhee.RuntimePipeline', image_name: str,
-        cuda_version: str, format_priority: list,
+        dc_pipeline: 'towhee.RuntimePipeline',
+        image_name: str,
+        cuda_version: str,
+        format_priority: list,
         parallelism: int = 8,
-        inference_server: str = 'triton'):
+        inference_server: str = 'triton'
+    ):
+    """
+    Build a docker image based on a RuntimePipeline.
+
+    Args:
+        dc_pipeline ('towhee.RuntimPipeline'):
+            The pipeline to build as a model in the docker image.
+        image_name (`str`):
+            The name of the docker image.
+        cuda_version (`str`):
+            Cuda version.
+        format_priority (`list`):
+            The priority order of the model format.
+        parallelism (`int`):
+            The parallel number.
+        inference_server (`str`):
+            The inference server.
+
+    Examples:
+        >>> import towhee
+        >>> from towhee import pipe, ops
+
+        >>> p = (
+        ...     pipe.input('url')
+        ...         .map('url', 'image', ops.image_decode.cv2_rgb())
+        ...         .map('image', 'vec', ops.image_embedding.timm(model_name='resnet50'))
+        ...         .output('vec')
+        ... )
+
+        >>> towhee.build_docker_image(
+        ...     dc_pipeline=p,
+        ...     image_name='clip:v1',
+        ...     cuda_version='11.7',
+        ...     format_priority=['onnx'],
+        ...     parallelism=4,
+        ...     inference_server='triton'
+        ... )
+    """
 
     server_config = {
         constant.FORMAT_PRIORITY : format_priority,
         constant.PARALLELISM: parallelism
     }
     if inference_server == 'triton':
         DockerImageBuilder(dc_pipeline, image_name, server_config, cuda_version).build()
     else:
         engine_log.error('Unknown server type: %s.', inference_server)
         return False
 
 
-def build_pipeline_model(dc_pipeline: 'towhee.RuntimePipeline', model_root: str,
-                         format_priority: list, parallelism: int = 8, server: str = 'triton'):
+def build_pipeline_model(
+        dc_pipeline: 'towhee.RuntimePipeline',
+        model_root: str,
+        format_priority: list,
+        parallelism: int = 8,
+        server: str = 'triton'
+    ):
+    """
+    Build the pipeline as a model.
+
+    Args:
+        dc_pipeline ('towhee.RuntimePipeline'):
+            The piepline to build as a model.
+        model_root (`str`):
+            The model root path.
+        format_priority (`list`):
+            The priority order of the model format.
+        parallelism (`int`):
+            The parallel number.
+        server (`str`):
+            The server type.
+
+    Examples:
+        >>> import towhee
+        >>> from towhee import pipe, ops
+
+        >>> p = (
+        ...     pipe.input('url')
+        ...         .map('url', 'image', ops.image_decode.cv2_rgb())
+        ...         .map('image', 'vec', ops.image_embedding.timm(model_name='resnet50'))
+        ...         .output('vec')
+        ... )
+
+        >>> towhee.build_pipeline_model(
+        ...     dc_pipeline=p,
+        ...     model_root='models',
+        ...     format_priority=['onnx'],
+        ...     parallelism=4,
+        ...     server='triton'
+        ... )
+    """
 
     if server == 'triton':
         dag_repr = copy.deepcopy(dc_pipeline.dag_repr)
         server_conf = {
             constant.FORMAT_PRIORITY: format_priority,
             constant.PARALLELISM: parallelism
         }
```

## towhee/serve/triton/model_to_triton.py

```diff
@@ -26,15 +26,15 @@
     NNOp to triton model.
     """
 
     def __init__(self, model_root: str, op: 'NNOperator', model_name: str, node_conf: 'NodeConfig', server_conf: dict):
         self._model_root = model_root
         self._op = op
         self._name = model_name
-        self._op_conf = node_conf.server_conf
+        self._op_conf = node_conf.server
         self._server_conf = server_conf
         self._triton_files = TritonFiles(model_root, self._name)
         self._model_format_priority = self._server_conf.get(constant.FORMAT_PRIORITY, [])
         self._backend = None
 
     def _create_model_dir(self) -> bool:
         self._triton_files.root.mkdir(parents=True, exist_ok=False)
```

## towhee/serve/triton/pipeline_builder.py

```diff
@@ -68,15 +68,15 @@
             status = converter.to_triton()
             if status == -1:
                 return False
             elif status == 0:
                 return True
             inputs, outputs = converter.get_model_in_out()
             acc_conf = {'model_name': model_name, 'inputs': inputs, 'outputs': outputs}
-            node_repr.config.acc_conf = AcceleratorConf(acc_type='triton', conf=acc_conf)
+            node_repr.config.acc_info = AcceleratorConf(type='triton', params=acc_conf)
         return True
 
     def build(self) -> bool:
         progress_bar = tqdm(total=len(self.dag_repr.nodes)+1, desc='Building model and pipeline')
         for node_id, node in self.dag_repr.nodes.items():
             if node_id in ['_input', '_output'] or node.op_info.type != 'hub':
                 progress_bar.update(1)
```

## towhee/serve/triton/pipeline_client.py

```diff
@@ -12,16 +12,16 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import asyncio
 from typing import List
 
 import numpy as np
-from towhee.serve.triton.serializer import to_triton_data, from_triton_data
 from towhee.serve.triton.constant import PIPELINE_NAME
+from towhee.utils.serializer import to_triton_data, from_triton_data
 from towhee.utils.log import engine_log
 
 from towhee.utils.triton_httpclient import aio_httpclient
 
 
 class Client:
     """
```

## towhee/serve/triton/bls/pipeline_model.py

```diff
@@ -1,19 +1,32 @@
 #coding=utf-8
-# pylint: skip-file
+# Copyright 2023 Zilliz. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
+# pylint: skip-file
 import logging
 from pathlib import Path
 
 import numpy as np
 import dill as pickle
 
 from towhee.serve.triton.bls.python_backend_wrapper import pb_utils
 from towhee.runtime.runtime_pipeline import RuntimePipeline
-from towhee.serve.triton.serializer import to_triton_data, from_triton_data
+from towhee.utils.serializer import to_triton_data, from_triton_data
 
 
 logger = logging.getLogger()
 
 
 class TritonPythonModel:
     '''
```

## towhee/trainer/utils/file_utils.py

```diff
@@ -1,7 +1,21 @@
+# Copyright 2022 Zilliz. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 import importlib
 
 import importlib_metadata
 from towhee.utils.log import trainer_log
 
 _captum_available = importlib.util.find_spec("captum") is not None
 try:
```

## towhee/trainer/utils/layer_freezer.py

```diff
@@ -1,7 +1,21 @@
+# Copyright 2022 Zilliz. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 from typing import Union
 
 
 class LayerFreezer:
     """
     Utilities to freeze/unfreeze layers.
```

## towhee/utils/hub_utils.py

```diff
@@ -431,14 +431,14 @@
             file_text = file_text.replace(ori_str, tar_str)
         with open(new_file, 'w', encoding='utf-8') as f2:
             f2.write(file_text)
 
     def branch_tree(self, tag):
         url = f'{self._root}/towhee-api/v1/repos/{self._author}/{self._repo}/tree?recursive=true&ref={tag}'
         try:
-            r = requests.get(url)
+            r = requests.get(url, timeout=(10, 10))
             r.raise_for_status()
             return r.json()
         except Exception as e:  # pylint: disable=broad-except
             err = '{}, {}'.format(str(e), traceback.format_exc())
             engine_log.error(err)
             return None
```

## Comparing `towhee/runtime/performance_profiler.py` & `towhee/tools/profilers.py`

 * *Files 6% similar despite different names*

```diff
@@ -8,36 +8,37 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import json
-import time
+from typing import Dict, List
 from tabulate import tabulate
 from copy import deepcopy
 from pathlib import Path
 
-from .constants import WindowAllConst
+from towhee.runtime.constants import WindowAllConst
+from towhee.runtime.time_profiler import Event
 from towhee.utils.log import engine_log
 
 
 class PipelineProfiler:
     """
     PipelineProfiler to trace one pipeline.
     """
-    def __init__(self, dag: 'DAGRepr'):
+    def __init__(self, nodes: Dict[str, str]):
         self.time_in = None
         self.time_out = None
         self.data = None
         self.node_tracer = {}
         self.node_report = {}
-        for uid, node in dag.nodes.items():
-            self.node_tracer[uid] = dict(name=node.name, iter=node.iter_info.type, init_in=[], init_out=[], queue_in=[], queue_out=[],
-                                         process_in=[], process_out=[])
+        for uid, node in nodes.items():
+            self.node_tracer[uid] = dict(name=node.get('name'), iter=node.get('iter_info').get('type'), init_in=[], init_out=[], queue_in=[],
+                                         queue_out=[], process_in=[], process_out=[])
 
     def add_node_tracer(self, name, event, ts):
         ts = int(ts) / 1000000
         if event == Event.pipe_in:
             self.time_in = ts
         elif event == Event.pipe_out:
             self.time_out = ts
@@ -127,15 +128,15 @@
                 if node['iter'] != WindowAllConst.name:
                     node['queue_in'].pop()
                 assert len(node['queue_in']) >= len(node['queue_out'])
                 assert len(node['process_in']) == len(node['process_out']) <= len(node['queue_in'])
                 assert len(node['init_in']) == len(node['init_out'])
                 assert len(node['init_in']) == 0 or len(node['init_in']) == 1
         except Exception as e:
-            engine_log.error('Node:{%s} failed, please reset the tracer with `pipe.reset_tracer` and rerun it.', node['name'])
+            engine_log.error('Node:{%s} failed, please reset the tracer with `pipe.reset_profiler` and rerun it.', node['name'])
             raise e
 
     @staticmethod
     def cal_time(list_in, list_out):
         if len(list_in) == 0:  # concat/func is no init
             return 0
         num = min(len(list_in), len(list_out))
@@ -144,25 +145,25 @@
 
 
 class PerformanceProfiler:
     """
     PerformanceProfiler to analysis the time profiler.
     """
 
-    def __init__(self, time_prfilers: list, dag: 'DAGRepr'):
+    def __init__(self, time_prfilers: List['TimeProfiler'], nodes: Dict[str, str]):
         self._time_prfilers = time_prfilers
-        self.dag = dag
+        self._nodes = nodes
         self.timing = None
         self.pipes_profiler = []
         self.node_report = {}
         self.make_report()
 
     def make_report(self):
         for tf in self._time_prfilers:
-            p_tracer = PipelineProfiler(self.dag)
+            p_tracer = PipelineProfiler(self._nodes)
             p_tracer.data = tf.inputs
             for ts_info in tf.time_record:
                 name, event, ts = ts_info.split('::')
                 p_tracer.add_node_tracer(name, event, ts)
             self.pipes_profiler.append(p_tracer)
         self.set_node_report()
         self.timing = self.get_timing_report()
@@ -201,62 +202,28 @@
         timing_list = []
         for p_tracer in self.pipes_profiler:
             timing_list.append(p_tracer.time_out - p_tracer.time_in)
         sorted_id = sorted(range(len(timing_list)), key=lambda k: timing_list[k])
         return [self.pipes_profiler[i] for i in sorted_id]
 
     def max(self):
-        sorted_pipe_tracer = self.sort()
-        return sorted_pipe_tracer[-1]
+        sorted_pipe_profiler = self.sort()
+        return sorted_pipe_profiler[-1]
 
     def __getitem__(self, item):
         return self.pipes_profiler[item]
 
+    def __len__(self):
+        return len(self.pipes_profiler)
+
     def dump(self, file_path):
         file_path = Path(file_path)
         profiler_json = self.gen_profiler_json()
         with open(file_path, 'w', encoding='utf-8') as f:
             json.dump(profiler_json, f)
         print(f'You can open chrome://tracing/ in your browser and load the file: {file_path}.')
 
     def gen_profiler_json(self):
         profiler_json = []
         for i, p_profiler in enumerate(self.pipes_profiler):
             profiler_json += p_profiler.gen_profiler_json(i)
         return profiler_json
-
-
-class Event:
-    pipe_name = '_run_pipe'
-    pipe_in = 'pipe_in'
-    pipe_out = 'pipe_out'
-    init_in = 'init_in'
-    init_out = 'init_out'
-    process_in = 'process_in'
-    process_out = 'process_out'
-    queue_in = 'queue_in'
-    queue_out = 'queue_out'
-
-
-class TimeProfiler:
-    """
-    TimeProfiler to record the event and timestamp.
-    """
-    def __init__(self, enable=False):
-        self._enable = enable
-        self.time_record = []
-        self.inputs = None
-
-    def record(self, uid, event):
-        if not self._enable:
-            return
-        timestamp = int(round(time.time() * 1000000))
-        self.time_record.append(f'{uid}::{event}::{timestamp}')
-
-    def enable(self):
-        self._enable = True
-
-    def disable(self):
-        self._enable = False
-
-    def reset(self):
-        self.time_record = []
```

## Comparing `towhee-1.0.0rc1.dist-info/LICENSE` & `towhee-1.1.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `towhee-1.0.0rc1.dist-info/METADATA` & `towhee-1.1.0.dist-info/METADATA`

 * *Files 22% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: towhee
-Version: 1.0.0rc1
+Version: 1.1.0
 Summary: Towhee is a framework that helps you encode your unstructured data into embeddings.
 Home-page: https://github.com/towhee-io/towhee
 Author: Towhee Team
 Author-email: towhee-team@zilliz.com
 License: http://www.apache.org/licenses/LICENSE-2.0
 Platform: unix
 Platform: linux
@@ -13,14 +13,16 @@
 Description-Content-Type: text/markdown
 License-File: LICENSE
 Requires-Dist: requests (>=2.12.5)
 Requires-Dist: tqdm (>=4.59.0)
 Requires-Dist: tabulate
 Requires-Dist: numpy
 Requires-Dist: twine
+Requires-Dist: tenacity
+Requires-Dist: pydantic
 Requires-Dist: contextvars ; python_version <= "3.6"
 Requires-Dist: importlib-resources ; python_version<'3.7'
 
 &nbsp;
 
 <p align="center">
     <img src="towhee_logo.png#gh-light-mode-only" width="60%"/>
@@ -45,202 +47,143 @@
   <a href="https://twitter.com/towheeio">
     <img src="https://img.shields.io/badge/follow-twitter-blue?style=flat" alt="twitter"/>
   </a>
   <a href="https://www.apache.org/licenses/LICENSE-2.0">
     <img src="https://img.shields.io/badge/license-apache2.0-green?style=flat" alt="license"/>
   </a>
   <a href="https://github.com/towhee-io/towhee/actions/workflows/pylint.yml">
-    <img src="https://img.shields.io/github/workflow/status/towhee-io/towhee/Workflow%20for%20pylint/main?label=pylint&style=flat" alt="github actions"/>
+    <img src="https://github.com/towhee-io/towhee/actions/workflows/pylint.yml/badge.svg" alt="github actions"/>
+  </a>
+  <a href="https://pypi.org/project/towhee/">
+    <img src="https://img.shields.io/pypi/v/towhee?label=Release&color&logo=Python" alt="github actions"/>
   </a>
   <a href="https://app.codecov.io/gh/towhee-io/towhee">
     <img src="https://img.shields.io/codecov/c/github/towhee-io/towhee?style=flat" alt="coverage"/>
   </a>
 </div>
 
 &nbsp;
 
-[Towhee](https://towhee.io) makes it easy to build neural data processing pipelines for AI applications.
-We provide hundreds of models, algorithms, and transformations that can be used as standard pipeline building blocks.
-You can use Towhee's Pythonic API to build a prototype of your pipeline and
-automatically optimize it for production-ready environments.
-
-:art:&emsp;**Various Modalities:** Towhee supports data processing on a variety of modalities, including images, videos, text, audio, molecular structures, etc.
-
-:mortar_board:&emsp;**SOTA Models:** Towhee provides SOTA models across 5 fields (CV, NLP, Multimodal, Audio, Medical), 15 tasks, and 140+ model architectures. These include BERT, CLIP, ViT, SwinTransformer, MAE, and data2vec, all pretrained and ready to use.
-
-:package:&emsp;**Data Processing:** Towhee also provides traditional methods alongside neural network models to help you build practical data processing pipelines. We have a rich pool of operators available, such as video decoding, audio slicing, frame sampling, feature vector dimension reduction, ensembling, and database operations.
-
-:snake:&emsp;**Pythonic API:** Towhee includes a Pythonic method-chaining API for describing custom data processing pipelines. We also support schemas, which makes processing unstructured data as easy as handling tabular data.
-
-## What's New
-
-**v0.9.0 Dec. 2, 2022**
-* Added one video classification model:
-[*Vis4mer*](https://github.com/towhee-io/towhee/tree/branch0.9.0/towhee/models/vis4mer)
-* Added three visual backbones:
-[*MCProp*](https://github.com/towhee-io/towhee/tree/branch0.9.0/towhee/models/mcprop), 
-[*RepLKNet*](https://github.com/towhee-io/towhee/tree/branch0.9.0/towhee/models/replknet), 
-[*Shunted Transformer*](https://github.com/towhee-io/towhee/tree/branch0.9.0/towhee/models/shunted_transformer)
-* Add two code search operators:
-[*code_search.codebert*](https://towhee.io/code-search/codebert), 
-[*code_search.unixcoder*](https://towhee.io/code-search/unixcoder)
-* Add five image captioning operators: 
-[*image_captioning.expansionnet-v2*](https://towhee.io/image-captioning/expansionnet-v2), 
-[*image_captioning.magic*](https://towhee.io/image-captioning/magic),
-[*image_captioning.clip_caption_reward*](https://towhee.io/image-captioning/clip-caption-reward), 
-[*image_captioning.blip*](https://towhee.io/image-captioning/blip), 
-[*image_captioning.clipcap*](https://towhee.io/image-captioning/clipcap)
-* Add five image-text embedding operators: 
-[*image_text_embedding.albef*](https://towhee.io/image-text-embedding/albef), 
-[*image_text_embedding.ru_clip*](https://towhee.io/image-text-embedding/ru-clip), 
-[*image_text_embedding.japanese_clip*](https://towhee.io/image-text-embedding/japanese-clip),
-[*image_text_embedding.taiyi*](https://towhee.io/image-text-embedding/taiyi),
-[*image_text_embedding.slip*](https://towhee.io/image-text-embedding/slip)
-* Add one machine-translation operator: 
-[*machine_translation.opus_mt*](https://towhee.io/machine-translation/opus-mt)
-* Add one filter-tiny-segments operator:
-[*video-copy-detection.filter-tiny-segments*](https://towhee.io/video-copy-detection/filter-tiny-segments)
-* Add an advanced tutorial for audio fingerprinting: 
-[*Audio Fingerprint II: Music Detection with Temporal Localization*](https://github.com/towhee-io/examples/blob/main/audio/audio_fingerprint/audio_fingerprint_advanced.ipynb) (increased accuracy from 84% to 90%)
-
-**v0.8.1 Sep. 30, 2022**
-
-* Added four visual backbones:
-[*ISC*](https://github.com/towhee-io/towhee/tree/branch0.8.1/towhee/models/isc),
-[*MetaFormer*](https://github.com/towhee-io/towhee/tree/branch0.8.1/towhee/models/metaformer),
-[*ConvNext*](https://github.com/towhee-io/towhee/tree/branch0.8.1/towhee/models/convnext),
-[*HorNet*](https://github.com/towhee-io/towhee/tree/branch0.8.1/towhee/models/hornet)
-* Add two video de-copy operators:
-[*select-video*](https://towhee.io/video-copy-detection/select-video), 
-[*temporal-network*](https://towhee.io/video-copy-detection/temporal-network)
-* Add one image embedding operator specifically designed for image retrieval and video de-copy with SOTA performance on VCSL dataset:
-[*isc*](https://towhee.io/image-embedding/isc)
-* Add one audio embedding operator specified for audio fingerprint:
-[*audio_embedding.nnfp*](https://towhee.io/audio-embedding/nnfp) (with pretrained weights)
-* Add one tutorial for video de-copy: 
-[*How to Build a Video Segment Copy Detection System*](https://github.com/towhee-io/examples/blob/main/video/video_deduplication/segment_level/video_deduplication_at_segment_level.ipynb)
-* Add one beginner tutorial for audio fingerprint:
-[*Audio Fingerprint I: Build a Demo with Towhee & Milvus*](https://github.com/towhee-io/examples/blob/main/audio/audio_fingerprint/audio_fingerprint_beginner.ipynb)
-
-
-**v0.8.0 Aug. 16, 2022**
-
-* Towhee now supports generating an Nvidia Triton Server from a Towhee pipeline, with aditional support for GPU image decoding.
-* Added one audio fingerprinting model: 
-[*nnfp*](https://github.com/towhee-io/towhee/tree/branch0.8.0/towhee/models/nnfp)
-* Added two image embedding models: 
-[*RepMLP*](https://github.com/towhee-io/towhee/tree/branch0.8.0/towhee/models/repmlp), [**WaveViT**](https://github.com/towhee-io/towhee/tree/branch0.8.0/towhee/models/wave_vit)
-
-**v0.7.3 Jul. 27, 2022**
-* Added one multimodal (text/image) model:
-[*CoCa*](https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/coca).
-* Added two video models for grounded situation recognition & repetitive action counting:
-[*CoFormer*](https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/coformer),
-[*TransRAC*](https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/transrac).
-* Added two SoTA models for image tasks (image retrieval, image classification, etc.):
-[*CVNet*](https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/cvnet),
-[*MaxViT*](https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/max_vit)
-
-**v0.7.1 Jul. 1, 2022**
-* Added one image embedding model:
-[*MPViT*](https://towhee.io/image-embedding/mpvit).
-* Added two video retrieval models:
-[*BridgeFormer*](https://towhee.io/video-text-embedding/bridge-former),
-[*collaborative-experts*](https://towhee.io/video-text-embedding/collaborative-experts).
-* Added FAISS-based ANNSearch operators: *to_faiss*, *faiss_search*.
-
-**v0.7.0 Jun. 24, 2022**
-
-* Added six video understanding/classification models:
-[*Video Swin Transformer*](https://towhee.io/action-classification/video-swin-transformer), 
-[*TSM*](https://towhee.io/action-classification/tsm), 
-[*Uniformer*](https://towhee.io/action-classification/uniformer), 
-[*OMNIVORE*](https://towhee.io/action-classification/omnivore), 
-[*TimeSformer*](https://towhee.io/action-classification/timesformer), 
-[*MoViNets*](https://towhee.io/action-classification/movinet).
-* Added four video retrieval models:
-[*CLIP4Clip*](https://towhee.io/video-text-embedding/clip4clip), 
-[*DRL*](https://towhee.io/video-text-embedding/drl), 
-[*Frozen in Time*](https://towhee.io/video-text-embedding/frozen-in-time), 
-[*MDMMT*](https://towhee.io/video-text-embedding/mdmmt).
-
-**v0.6.1  May. 13, 2022**
-
-* Added three text-image retrieval models:
-[*CLIP*](https://towhee.io/image-text-embedding/clip),
-[*BLIP*](https://towhee.io/image-text-embedding/blip),
-[*LightningDOT*](https://towhee.io/image-text-embedding/lightningdot).
-* Added six video understanding/classification models from PyTorchVideo:
-[*I3D*](https://towhee.io/action-classification/pytorchvideo),
-[*C2D*](https://towhee.io/action-classification/pytorchvideo),
-[*Slow*](https://towhee.io/action-classification/pytorchvideo),
-[*SlowFast*](https://towhee.io/action-classification/pytorchvideo),
-[*X3D*](https://towhee.io/action-classification/pytorchvideo),
-[*MViT*](https://towhee.io/action-classification/pytorchvideo).
+[Towhee](https://towhee.io) is a cutting-edge framework designed to streamline the processing of unstructured data through the use of Large Language Model (LLM) based pipeline orchestration. It is uniquely positioned to extract invaluable insights from diverse unstructured data types, including lengthy text, images, audio and video files. Leveraging the capabilities of generative AI and the SOTA deep learning models, Towhee is capable of transforming this unprocessed data into specific formats such as text, image, or embeddings. These can then be efficiently loaded into an appropriate storage system like a vector database. Developers can initially build an intuitive data processing pipeline prototype with user friendly Pythonic APU, then optimize it for production environments.
+
+Multi Modalities: Towhee is capable of handling a wide range of data types. Whether it's image data, video clips, text, audio files, or even molecular structures, Towhee can process them all. 
+
+    LLM Pipeline orchestration:  Towhee offers flexibility to adapt to different Large Language Models (LLMs). Additionally, it allows for hosting open-source large models locally. Moreover, Towhee provides features like prompt management and knowledge retrieval, making the interaction with these LLMs more efficient and effective.
+
+Rich Operators: Towhee provides a wide range of ready-to-use state-of-the-art models across five domains: CV, NLP, multimodal, audio, and medical. With over 140 models like BERT and CLIP and rich functionalities like video decoding, audio slicing, frame sampling,  and dimensionality reduction, it assists in efficiently building data processing pipelines. 
+
+   Prebuilt ETL Pipelines: Towhee offers ready-to-use ETL (Extract, Transform, Load) pipelines for common tasks such as Retrieval-Augmented Generation, Text Image search, and Video copy detection. This means you don't need to be an AI expert to build applications using these features. 
+  High performance backend: Leveraging the power of the Triton Inference Server, Towhee can speed up model serving on both CPU and GPU using platforms like TensorRT, Pytorch, and ONNX. Moreover, you can transform your Python pipeline into a high-performance docker container with just a few lines of code, enabling efficient deployment and scaling.
+
+Pythonic API: Towhee includes a Pythonic method-chaining API for describing custom data processing pipelines. We also support schemas, which makes processing unstructured data as easy as handling tabular data.
 
 ## Getting started
 
 Towhee requires Python 3.6+. You can install Towhee via `pip`:
 
 ```bash
 pip install towhee towhee.models
 ```
 
-If you run into any pip-related install problems, please try to upgrade pip with `pip install -U pip`.
+### Pipeline
+
+### Pre-defined Pipeline
 
-Let's try your first Towhee pipeline. Below is an example for how to create a CLIP-based cross modal retrieval pipeline with only 15 lines of code.
+Towhee provides some pre-defined pipelines to help users quickly implement some functions. 
+Currently implemented are: 
+- [Sentence Embedding](https://towhee.io/tasks/detail/pipeline/sentence-similarity)
+- [Image Embedding](https://towhee.io/tasks/detail/pipeline/text-image-search)
+- [Video deduplication](https://towhee.io/tasks/detail/pipeline/video-copy-detection)
+- [Question Answer with Docs](https://towhee.io/tasks/detail/pipeline/retrieval-augmented-generation)
 
+All pipelines can be found on Towhee Hub. Here is an example of using the sentence_embedding pipeline: 
+
+```python
+from towhee import AutoPipes, AutoConfig
+# get the built-in sentence_similarity pipeline
+config = AutoConfig.load_config('sentence_embedding')
+config.model = 'paraphrase-albert-small-v2'
+config.device = 0
+sentence_embedding = AutoPipes.pipeline('sentence_embedding', config=config)
+
+# generate embedding for one sentence
+embedding = sentence_embedding('how are you?').get()
+# batch generate embeddings for multi-sentences
+embeddings = sentence_embedding.batch(['how are you?', 'how old are you?'])
+embeddings = [e.get() for e in embeddings]
+```
+### Custom pipelines 
+
+If you can't find the pipeline you want in towhee hub, you can also implement custom pipelines through the towhee Python API. In the following example, we will create a cross-modal retrieval pipeline based on CLIP.
 ```python
-import towhee
 
+from towhee import ops, pipe, DataCollection
 # create image embeddings and build index
-(
-    towhee.glob['file_name']('./*.png')
-          .image_decode['file_name', 'img']()
-          .image_text_embedding.clip['img', 'vec'](model_name='clip_vit_base_patch32', modality='image')
-          .tensor_normalize['vec','vec']()
-          .to_faiss[('file_name', 'vec')](findex='./index.bin')
+p = (
+    pipe.input('file_name')
+    .map('file_name', 'img', ops.image_decode.cv2())
+    .map('img', 'vec', ops.image_text_embedding.clip(model_name='clip_vit_base_patch32', modality='image'))
+    .map('vec', 'vec', ops.towhee.np_normalize())
+    .map(('vec', 'file_name'), (), ops.ann_insert.faiss_index('./faiss', 512))
+    .output()
 )
 
-# search image by text
-results = (
-    towhee.dc['text'](['puppy Corgi'])
-          .image_text_embedding.clip['text', 'vec'](model_name='clip_vit_base_patch32', modality='text')
-          .tensor_normalize['vec', 'vec']()
-          .faiss_search['vec', 'results'](findex='./index.bin', k=3)
-          .select['text', 'results']()
+for f_name in ['https://raw.githubusercontent.com/towhee-io/towhee/main/assets/dog1.png',
+               'https://raw.githubusercontent.com/towhee-io/towhee/main/assets/dog2.png',
+               'https://raw.githubusercontent.com/towhee-io/towhee/main/assets/dog3.png']:
+    p(f_name)
+
+# Flush faiss data into disk. 
+p.flush()
+# search image by textdecode = ops.image_decode.cv2('rgb')
+p = (
+    pipe.input('text')
+    .map('text', 'vec', ops.image_text_embedding.clip(model_name='clip_vit_base_patch32', modality='text'))
+    .map('vec', 'vec', ops.towhee.np_normalize())
+    # faiss op result format:  [[id, score, [file_name], ...]
+    .map('vec', 'row', ops.ann_search.faiss_index('./faiss', 3))
+    .map('row', 'images', lambda x: [decode(item[2][0]) for item in x])
+    .output('text', 'images')
 )
+
+DataCollection(p('a cat')).show()
+
 ```
 <img src="assets/towhee_example.png" style="width: 60%; height: 60%">
 
-Learn more examples from the [Towhee Bootcamp](https://codelabs.towhee.io/).
 
 ## Core Concepts
 
 Towhee is composed of four main building blocks - `Operators`, `Pipelines`, `DataCollection API` and `Engine`.
 
 - __Operators__: An operator is a single building block of a neural data processing pipeline. Different implementations of operators are categorized by tasks, with each task having a standard interface. An operator can be a deep learning model, a data processing method, or a Python function.
 
 - __Pipelines__: A pipeline is composed of several operators interconnected in the form of a DAG (directed acyclic graph). This DAG can direct complex functionalities, such as embedding feature extraction, data tagging, and cross modal data analysis.
 
-- __DataCollection API__: A Pythonic and method-chaining style API for building custom pipelines. A pipeline defined by the DataColltion API can be run locally on a laptop for fast prototyping and then be converted to a docker image, with end-to-end optimizations, for production-ready environments.
+- __DataCollection API__: A Pythonic and method-chaining style API for building custom pipelines, providing multiple data conversion interfaces: map, filter, flat_map, concat, window, time_window, and window_all. Through these interfaces, complex data processing pipelines can be built quickly to process unstructured data such as video, audio, text, images, etc.
 
 - __Engine__: The engine sits at Towhee's core. Given a pipeline, the engine will drive dataflow among individual operators, schedule tasks, and monitor compute resource usage (CPU/GPU/etc). We provide a basic engine within Towhee to run pipelines on a single-instance machine and a Triton-based engine for docker containers.
 
+## Resource
+- TowheeHub: https://towhee.io/
+- docs: https://towhee.readthedocs.io/en/latest/
+- examples: https://github.com/towhee-io/examples
+
 ## Contributing
 
 Writing code is not the only way to contribute! Submitting issues, answering questions, and improving documentation are just some of the many ways you can help our growing community. Check out our [contributing page](https://github.com/towhee-io/towhee/blob/main/CONTRIBUTING.md) for more information.
 
 Special thanks goes to these folks for contributing to Towhee, either on Github, our Towhee Hub, or elsewhere:
 <br><!-- Do not remove start of hero-bot --><br>
 <img src="https://img.shields.io/badge/all--contributors-33-orange"><br>
 <a href="https://github.com/AniTho"><img src="https://avatars.githubusercontent.com/u/34787227?v=4" width="30px" /></a>
 <a href="https://github.com/Chiiizzzy"><img src="https://avatars.githubusercontent.com/u/72550076?v=4" width="30px" /></a>
 <a href="https://github.com/GuoRentong"><img src="https://avatars.githubusercontent.com/u/57477222?v=4" width="30px" /></a>
 <a href="https://github.com/NicoYuan1986"><img src="https://avatars.githubusercontent.com/u/109071306?v=4" width="30px" /></a>
+<a href="https://github.com/Opdoop"><img src="https://avatars.githubusercontent.com/u/21202514?v=4" width="30px" /></a>
 <a href="https://github.com/Tumao727"><img src="https://avatars.githubusercontent.com/u/20420181?v=4" width="30px" /></a>
 <a href="https://github.com/YuDongPan"><img src="https://avatars.githubusercontent.com/u/88148730?v=4" width="30px" /></a>
 <a href="https://github.com/binbinlv"><img src="https://avatars.githubusercontent.com/u/83755740?v=4" width="30px" /></a>
 <a href="https://github.com/derekdqc"><img src="https://avatars.githubusercontent.com/u/11754703?v=4" width="30px" /></a>
 <a href="https://github.com/dreamfireyu"><img src="https://avatars.githubusercontent.com/u/47691077?v=4" width="30px" /></a>
 <a href="https://github.com/filip-halt"><img src="https://avatars.githubusercontent.com/u/81822489?v=4" width="30px" /></a>
 <a href="https://github.com/fzliu"><img src="https://avatars.githubusercontent.com/u/6334158?v=4" width="30px" /></a>
@@ -261,11 +204,10 @@
 <a href="https://github.com/soulteary"><img src="https://avatars.githubusercontent.com/u/1500781?v=4" width="30px" /></a>
 <a href="https://github.com/sre-ci-robot"><img src="https://avatars.githubusercontent.com/u/56469371?v=4" width="30px" /></a>
 <a href="https://github.com/sutcalag"><img src="https://avatars.githubusercontent.com/u/83750738?v=4" width="30px" /></a>
 <a href="https://github.com/wxywb"><img src="https://avatars.githubusercontent.com/u/5432721?v=4" width="30px" /></a>
 <a href="https://github.com/zc277584121"><img src="https://avatars.githubusercontent.com/u/17022025?v=4" width="30px" /></a>
 <a href="https://github.com/zengxiang68"><img src="https://avatars.githubusercontent.com/u/68835157?v=4" width="30px" /></a>
 <a href="https://github.com/zhousicong"><img src="https://avatars.githubusercontent.com/u/7541863?v=4" width="30px" /></a>
-<a href="https://github.com/zhujiming"><img src="https://avatars.githubusercontent.com/u/18031320?v=4" width="30px" /></a>
 <br><!-- Do not remove end of hero-bot --><br>
 
 Looking for a database to store and index your embedding vectors? Check out [Milvus](https://github.com/milvus-io/milvus).
```

### html2text {}

```diff
@@ -1,168 +1,126 @@
-Metadata-Version: 2.1 Name: towhee Version: 1.0.0rc1 Summary: Towhee is a
+Metadata-Version: 2.1 Name: towhee Version: 1.1.0 Summary: Towhee is a
 framework that helps you encode your unstructured data into embeddings. Home-
 page: https://github.com/towhee-io/towhee Author: Towhee Team Author-email:
 towhee-team@zilliz.com License: http://www.apache.org/licenses/LICENSE-2.0
 Platform: unix Platform: linux Platform: osx Platform: win32 Description-
 Content-Type: text/markdown License-File: LICENSE Requires-Dist: requests
 (>=2.12.5) Requires-Dist: tqdm (>=4.59.0) Requires-Dist: tabulate Requires-
-Dist: numpy Requires-Dist: twine Requires-Dist: contextvars ; python_version <=
-"3.6" Requires-Dist: importlib-resources ; python_version<'3.7' 
+Dist: numpy Requires-Dist: twine Requires-Dist: tenacity Requires-Dist:
+pydantic Requires-Dist: contextvars ; python_version <= "3.6" Requires-Dist:
+importlib-resources ; python_version<'3.7' 
 [towhee_logo.png#gh-light-mode-only] [assets/towhee_logo_dark.png#gh-dark-mode-
                                      only]
                    **** x2vec, Towhee is all you need! ****
                        **** ENGLISH |  ****
-[join-slack] [twitter] [license] [github_actions] [coverage]
- [Towhee](https://towhee.io) makes it easy to build neural data processing
-pipelines for AI applications. We provide hundreds of models, algorithms, and
-transformations that can be used as standard pipeline building blocks. You can
-use Towhee's Pythonic API to build a prototype of your pipeline and
-automatically optimize it for production-ready environments. :art:
-&emsp;**Various Modalities:** Towhee supports data processing on a variety of
-modalities, including images, videos, text, audio, molecular structures, etc. :
-mortar_board:&emsp;**SOTA Models:** Towhee provides SOTA models across 5 fields
-(CV, NLP, Multimodal, Audio, Medical), 15 tasks, and 140+ model architectures.
-These include BERT, CLIP, ViT, SwinTransformer, MAE, and data2vec, all
-pretrained and ready to use. :package:&emsp;**Data Processing:** Towhee also
-provides traditional methods alongside neural network models to help you build
-practical data processing pipelines. We have a rich pool of operators
-available, such as video decoding, audio slicing, frame sampling, feature
-vector dimension reduction, ensembling, and database operations. :snake:
-&emsp;**Pythonic API:** Towhee includes a Pythonic method-chaining API for
-describing custom data processing pipelines. We also support schemas, which
-makes processing unstructured data as easy as handling tabular data. ## What's
-New **v0.9.0 Dec. 2, 2022** * Added one video classification model: [*Vis4mer*]
-(https://github.com/towhee-io/towhee/tree/branch0.9.0/towhee/models/vis4mer) *
-Added three visual backbones: [*MCProp*](https://github.com/towhee-io/towhee/
-tree/branch0.9.0/towhee/models/mcprop), [*RepLKNet*](https://github.com/towhee-
-io/towhee/tree/branch0.9.0/towhee/models/replknet), [*Shunted Transformer*]
-(https://github.com/towhee-io/towhee/tree/branch0.9.0/towhee/models/
-shunted_transformer) * Add two code search operators: [*code_search.codebert*]
-(https://towhee.io/code-search/codebert), [*code_search.unixcoder*](https://
-towhee.io/code-search/unixcoder) * Add five image captioning operators:
-[*image_captioning.expansionnet-v2*](https://towhee.io/image-captioning/
-expansionnet-v2), [*image_captioning.magic*](https://towhee.io/image-
-captioning/magic), [*image_captioning.clip_caption_reward*](https://towhee.io/
-image-captioning/clip-caption-reward), [*image_captioning.blip*](https://
-towhee.io/image-captioning/blip), [*image_captioning.clipcap*](https://
-towhee.io/image-captioning/clipcap) * Add five image-text embedding operators:
-[*image_text_embedding.albef*](https://towhee.io/image-text-embedding/albef),
-[*image_text_embedding.ru_clip*](https://towhee.io/image-text-embedding/ru-
-clip), [*image_text_embedding.japanese_clip*](https://towhee.io/image-text-
-embedding/japanese-clip), [*image_text_embedding.taiyi*](https://towhee.io/
-image-text-embedding/taiyi), [*image_text_embedding.slip*](https://towhee.io/
-image-text-embedding/slip) * Add one machine-translation operator:
-[*machine_translation.opus_mt*](https://towhee.io/machine-translation/opus-mt)
-* Add one filter-tiny-segments operator: [*video-copy-detection.filter-tiny-
-segments*](https://towhee.io/video-copy-detection/filter-tiny-segments) * Add
-an advanced tutorial for audio fingerprinting: [*Audio Fingerprint II: Music
-Detection with Temporal Localization*](https://github.com/towhee-io/examples/
-blob/main/audio/audio_fingerprint/audio_fingerprint_advanced.ipynb) (increased
-accuracy from 84% to 90%) **v0.8.1 Sep. 30, 2022** * Added four visual
-backbones: [*ISC*](https://github.com/towhee-io/towhee/tree/branch0.8.1/towhee/
-models/isc), [*MetaFormer*](https://github.com/towhee-io/towhee/tree/
-branch0.8.1/towhee/models/metaformer), [*ConvNext*](https://github.com/towhee-
-io/towhee/tree/branch0.8.1/towhee/models/convnext), [*HorNet*](https://
-github.com/towhee-io/towhee/tree/branch0.8.1/towhee/models/hornet) * Add two
-video de-copy operators: [*select-video*](https://towhee.io/video-copy-
-detection/select-video), [*temporal-network*](https://towhee.io/video-copy-
-detection/temporal-network) * Add one image embedding operator specifically
-designed for image retrieval and video de-copy with SOTA performance on VCSL
-dataset: [*isc*](https://towhee.io/image-embedding/isc) * Add one audio
-embedding operator specified for audio fingerprint: [*audio_embedding.nnfp*]
-(https://towhee.io/audio-embedding/nnfp) (with pretrained weights) * Add one
-tutorial for video de-copy: [*How to Build a Video Segment Copy Detection
-System*](https://github.com/towhee-io/examples/blob/main/video/
-video_deduplication/segment_level/video_deduplication_at_segment_level.ipynb) *
-Add one beginner tutorial for audio fingerprint: [*Audio Fingerprint I: Build a
-Demo with Towhee & Milvus*](https://github.com/towhee-io/examples/blob/main/
-audio/audio_fingerprint/audio_fingerprint_beginner.ipynb) **v0.8.0 Aug. 16,
-2022** * Towhee now supports generating an Nvidia Triton Server from a Towhee
-pipeline, with aditional support for GPU image decoding. * Added one audio
-fingerprinting model: [*nnfp*](https://github.com/towhee-io/towhee/tree/
-branch0.8.0/towhee/models/nnfp) * Added two image embedding models: [*RepMLP*]
-(https://github.com/towhee-io/towhee/tree/branch0.8.0/towhee/models/repmlp),
-[**WaveViT**](https://github.com/towhee-io/towhee/tree/branch0.8.0/towhee/
-models/wave_vit) **v0.7.3 Jul. 27, 2022** * Added one multimodal (text/image)
-model: [*CoCa*](https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/
-models/coca). * Added two video models for grounded situation recognition &
-repetitive action counting: [*CoFormer*](https://github.com/towhee-io/towhee/
-tree/branch0.7.3/towhee/models/coformer), [*TransRAC*](https://github.com/
-towhee-io/towhee/tree/branch0.7.3/towhee/models/transrac). * Added two SoTA
-models for image tasks (image retrieval, image classification, etc.): [*CVNet*]
-(https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/cvnet),
-[*MaxViT*](https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/
-max_vit) **v0.7.1 Jul. 1, 2022** * Added one image embedding model: [*MPViT*]
-(https://towhee.io/image-embedding/mpvit). * Added two video retrieval models:
-[*BridgeFormer*](https://towhee.io/video-text-embedding/bridge-former),
-[*collaborative-experts*](https://towhee.io/video-text-embedding/collaborative-
-experts). * Added FAISS-based ANNSearch operators: *to_faiss*, *faiss_search*.
-**v0.7.0 Jun. 24, 2022** * Added six video understanding/classification models:
-[*Video Swin Transformer*](https://towhee.io/action-classification/video-swin-
-transformer), [*TSM*](https://towhee.io/action-classification/tsm),
-[*Uniformer*](https://towhee.io/action-classification/uniformer), [*OMNIVORE*]
-(https://towhee.io/action-classification/omnivore), [*TimeSformer*](https://
-towhee.io/action-classification/timesformer), [*MoViNets*](https://towhee.io/
-action-classification/movinet). * Added four video retrieval models:
-[*CLIP4Clip*](https://towhee.io/video-text-embedding/clip4clip), [*DRL*](https:
-//towhee.io/video-text-embedding/drl), [*Frozen in Time*](https://towhee.io/
-video-text-embedding/frozen-in-time), [*MDMMT*](https://towhee.io/video-text-
-embedding/mdmmt). **v0.6.1 May. 13, 2022** * Added three text-image retrieval
-models: [*CLIP*](https://towhee.io/image-text-embedding/clip), [*BLIP*](https:/
-/towhee.io/image-text-embedding/blip), [*LightningDOT*](https://towhee.io/
-image-text-embedding/lightningdot). * Added six video understanding/
-classification models from PyTorchVideo: [*I3D*](https://towhee.io/action-
-classification/pytorchvideo), [*C2D*](https://towhee.io/action-classification/
-pytorchvideo), [*Slow*](https://towhee.io/action-classification/pytorchvideo),
-[*SlowFast*](https://towhee.io/action-classification/pytorchvideo), [*X3D*]
-(https://towhee.io/action-classification/pytorchvideo), [*MViT*](https://
-towhee.io/action-classification/pytorchvideo). ## Getting started Towhee
-requires Python 3.6+. You can install Towhee via `pip`: ```bash pip install
-towhee towhee.models ``` If you run into any pip-related install problems,
-please try to upgrade pip with `pip install -U pip`. Let's try your first
-Towhee pipeline. Below is an example for how to create a CLIP-based cross modal
-retrieval pipeline with only 15 lines of code. ```python import towhee # create
-image embeddings and build index ( towhee.glob['file_name']('./*.png')
-.image_decode['file_name', 'img']() .image_text_embedding.clip['img', 'vec']
-(model_name='clip_vit_base_patch32', modality='image') .tensor_normalize
-['vec','vec']() .to_faiss[('file_name', 'vec')](findex='./index.bin') ) #
-search image by text results = ( towhee.dc['text'](['puppy Corgi'])
-.image_text_embedding.clip['text', 'vec'](model_name='clip_vit_base_patch32',
-modality='text') .tensor_normalize['vec', 'vec']() .faiss_search['vec',
-'results'](findex='./index.bin', k=3) .select['text', 'results']() ) ```
-[assets/towhee_example.png] Learn more examples from the [Towhee Bootcamp]
-(https://codelabs.towhee.io/). ## Core Concepts Towhee is composed of four main
-building blocks - `Operators`, `Pipelines`, `DataCollection API` and `Engine`.
-- __Operators__: An operator is a single building block of a neural data
-processing pipeline. Different implementations of operators are categorized by
-tasks, with each task having a standard interface. An operator can be a deep
-learning model, a data processing method, or a Python function. -
-__Pipelines__: A pipeline is composed of several operators interconnected in
-the form of a DAG (directed acyclic graph). This DAG can direct complex
-functionalities, such as embedding feature extraction, data tagging, and cross
-modal data analysis. - __DataCollection API__: A Pythonic and method-chaining
-style API for building custom pipelines. A pipeline defined by the DataColltion
-API can be run locally on a laptop for fast prototyping and then be converted
-to a docker image, with end-to-end optimizations, for production-ready
-environments. - __Engine__: The engine sits at Towhee's core. Given a pipeline,
-the engine will drive dataflow among individual operators, schedule tasks, and
-monitor compute resource usage (CPU/GPU/etc). We provide a basic engine within
-Towhee to run pipelines on a single-instance machine and a Triton-based engine
-for docker containers. ## Contributing Writing code is not the only way to
-contribute! Submitting issues, answering questions, and improving documentation
-are just some of the many ways you can help our growing community. Check out
-our [contributing page](https://github.com/towhee-io/towhee/blob/main/
-CONTRIBUTING.md) for more information. Special thanks goes to these folks for
-contributing to Towhee, either on Github, our Towhee Hub, or elsewhere:
+[join-slack] [twitter] [license] [github_actions] [github_actions] [coverage]
+ [Towhee](https://towhee.io) is a cutting-edge framework designed to
+streamline the processing of unstructured data through the use of Large
+Language Model (LLM) based pipeline orchestration. It is uniquely positioned to
+extract invaluable insights from diverse unstructured data types, including
+lengthy text, images, audio and video files. Leveraging the capabilities of
+generative AI and the SOTA deep learning models, Towhee is capable of
+transforming this unprocessed data into specific formats such as text, image,
+or embeddings. These can then be efficiently loaded into an appropriate storage
+system like a vector database. Developers can initially build an intuitive data
+processing pipeline prototype with user friendly Pythonic APU, then optimize it
+for production environments. Multi Modalities: Towhee is capable of
+handling a wide range of data types. Whether it's image data, video clips,
+text, audio files, or even molecular structures, Towhee can process them all.
+ LLM Pipeline orchestration: Towhee offers flexibility to adapt to
+different Large Language Models (LLMs). Additionally, it allows for hosting
+open-source large models locally. Moreover, Towhee provides features like
+prompt management and knowledge retrieval, making the interaction with these
+LLMs more efficient and effective. Rich Operators: Towhee provides a
+wide range of ready-to-use state-of-the-art models across five domains: CV,
+NLP, multimodal, audio, and medical. With over 140 models like BERT and CLIP
+and rich functionalities like video decoding, audio slicing, frame sampling,
+and dimensionality reduction, it assists in efficiently building data
+processing pipelines.  Prebuilt ETL Pipelines: Towhee offers ready-to-use
+ETL (Extract, Transform, Load) pipelines for common tasks such as Retrieval-
+Augmented Generation, Text Image search, and Video copy detection. This means
+you don't need to be an AI expert to build applications using these features.
+ High performance backend: Leveraging the power of the Triton Inference
+Server, Towhee can speed up model serving on both CPU and GPU using platforms
+like TensorRT, Pytorch, and ONNX. Moreover, you can transform your Python
+pipeline into a high-performance docker container with just a few lines of
+code, enabling efficient deployment and scaling. Pythonic API: Towhee
+includes a Pythonic method-chaining API for describing custom data processing
+pipelines. We also support schemas, which makes processing unstructured data as
+easy as handling tabular data. ## Getting started Towhee requires Python 3.6+.
+You can install Towhee via `pip`: ```bash pip install towhee towhee.models ```
+### Pipeline ### Pre-defined Pipeline Towhee provides some pre-defined
+pipelines to help users quickly implement some functions. Currently implemented
+are: - [Sentence Embedding](https://towhee.io/tasks/detail/pipeline/sentence-
+similarity) - [Image Embedding](https://towhee.io/tasks/detail/pipeline/text-
+image-search) - [Video deduplication](https://towhee.io/tasks/detail/pipeline/
+video-copy-detection) - [Question Answer with Docs](https://towhee.io/tasks/
+detail/pipeline/retrieval-augmented-generation) All pipelines can be found on
+Towhee Hub. Here is an example of using the sentence_embedding pipeline:
+```python from towhee import AutoPipes, AutoConfig # get the built-in
+sentence_similarity pipeline config = AutoConfig.load_config
+('sentence_embedding') config.model = 'paraphrase-albert-small-v2'
+config.device = 0 sentence_embedding = AutoPipes.pipeline('sentence_embedding',
+config=config) # generate embedding for one sentence embedding =
+sentence_embedding('how are you?').get() # batch generate embeddings for multi-
+sentences embeddings = sentence_embedding.batch(['how are you?', 'how old are
+you?']) embeddings = [e.get() for e in embeddings] ``` ### Custom pipelines If
+you can't find the pipeline you want in towhee hub, you can also implement
+custom pipelines through the towhee Python API. In the following example, we
+will create a cross-modal retrieval pipeline based on CLIP. ```python from
+towhee import ops, pipe, DataCollection # create image embeddings and build
+index p = ( pipe.input('file_name') .map('file_name', 'img',
+ops.image_decode.cv2()) .map('img', 'vec', ops.image_text_embedding.clip
+(model_name='clip_vit_base_patch32', modality='image')) .map('vec', 'vec',
+ops.towhee.np_normalize()) .map(('vec', 'file_name'), (),
+ops.ann_insert.faiss_index('./faiss', 512)) .output() ) for f_name in ['https:/
+/raw.githubusercontent.com/towhee-io/towhee/main/assets/dog1.png', 'https://
+raw.githubusercontent.com/towhee-io/towhee/main/assets/dog2.png', 'https://
+raw.githubusercontent.com/towhee-io/towhee/main/assets/dog3.png']: p(f_name) #
+Flush faiss data into disk. p.flush() # search image by textdecode =
+ops.image_decode.cv2('rgb') p = ( pipe.input('text') .map('text', 'vec',
+ops.image_text_embedding.clip(model_name='clip_vit_base_patch32',
+modality='text')) .map('vec', 'vec', ops.towhee.np_normalize()) # faiss op
+result format: [[id, score, [file_name], ...] .map('vec', 'row',
+ops.ann_search.faiss_index('./faiss', 3)) .map('row', 'images', lambda x:
+[decode(item[2][0]) for item in x]) .output('text', 'images') ) DataCollection
+(p('a cat')).show() ``` [assets/towhee_example.png] ## Core Concepts Towhee is
+composed of four main building blocks - `Operators`, `Pipelines`,
+`DataCollection API` and `Engine`. - __Operators__: An operator is a single
+building block of a neural data processing pipeline. Different implementations
+of operators are categorized by tasks, with each task having a standard
+interface. An operator can be a deep learning model, a data processing method,
+or a Python function. - __Pipelines__: A pipeline is composed of several
+operators interconnected in the form of a DAG (directed acyclic graph). This
+DAG can direct complex functionalities, such as embedding feature extraction,
+data tagging, and cross modal data analysis. - __DataCollection API__: A
+Pythonic and method-chaining style API for building custom pipelines, providing
+multiple data conversion interfaces: map, filter, flat_map, concat, window,
+time_window, and window_all. Through these interfaces, complex data processing
+pipelines can be built quickly to process unstructured data such as video,
+audio, text, images, etc. - __Engine__: The engine sits at Towhee's core. Given
+a pipeline, the engine will drive dataflow among individual operators, schedule
+tasks, and monitor compute resource usage (CPU/GPU/etc). We provide a basic
+engine within Towhee to run pipelines on a single-instance machine and a
+Triton-based engine for docker containers. ## Resource - TowheeHub: https://
+towhee.io/ - docs: https://towhee.readthedocs.io/en/latest/ - examples: https:/
+/github.com/towhee-io/examples ## Contributing Writing code is not the only way
+to contribute! Submitting issues, answering questions, and improving
+documentation are just some of the many ways you can help our growing
+community. Check out our [contributing page](https://github.com/towhee-io/
+towhee/blob/main/CONTRIBUTING.md) for more information. Special thanks goes to
+these folks for contributing to Towhee, either on Github, our Towhee Hub, or
+elsewhere:
 
 [https://img.shields.io/badge/all--contributors-33-orange]
 [https://avatars.githubusercontent.com/u/34787227?v=4] [https://
 avatars.githubusercontent.com/u/72550076?v=4] [https://
 avatars.githubusercontent.com/u/57477222?v=4] [https://
 avatars.githubusercontent.com/u/109071306?v=4] [https://
+avatars.githubusercontent.com/u/21202514?v=4] [https://
 avatars.githubusercontent.com/u/20420181?v=4] [https://
 avatars.githubusercontent.com/u/88148730?v=4] [https://
 avatars.githubusercontent.com/u/83755740?v=4] [https://
 avatars.githubusercontent.com/u/11754703?v=4] [https://
 avatars.githubusercontent.com/u/47691077?v=4] [https://
 avatars.githubusercontent.com/u/81822489?v=4] [https://
 avatars.githubusercontent.com/u/6334158?v=4] [https://
@@ -182,12 +140,11 @@
 avatars.githubusercontent.com/u/107831450?v=4] [https://
 avatars.githubusercontent.com/u/1500781?v=4] [https://
 avatars.githubusercontent.com/u/56469371?v=4] [https://
 avatars.githubusercontent.com/u/83750738?v=4] [https://
 avatars.githubusercontent.com/u/5432721?v=4] [https://
 avatars.githubusercontent.com/u/17022025?v=4] [https://
 avatars.githubusercontent.com/u/68835157?v=4] [https://
-avatars.githubusercontent.com/u/7541863?v=4] [https://
-avatars.githubusercontent.com/u/18031320?v=4]
+avatars.githubusercontent.com/u/7541863?v=4]
 
 Looking for a database to store and index your embedding vectors? Check out
 [Milvus](https://github.com/milvus-io/milvus).
```

## Comparing `towhee-1.0.0rc1.dist-info/RECORD` & `towhee-1.1.0.dist-info/RECORD`

 * *Files 7% similar despite different names*

```diff
@@ -1,144 +1,152 @@
-towhee/__init__.py,sha256=dt8Awiq_ier5kOMWliI1uccAZOYKVH9M_weqouUvMJg,2855
+towhee/__init__.py,sha256=KFr1jh2pKjXaAs2mseh5GekVyJbmdeGn56sugYbxbSw,5836
 towhee/__main__.py,sha256=hS5mOMatWxqJGYFo9NYdCTs0-0PI4SezBb9UCAijpPU,665
+towhee/data_loader.py,sha256=Uq1JI8vdd_VlJlUjjVNr5_mSNkcJraEdzeF9XKKduCo,2527
 towhee/command/__init__.py,sha256=tahGzF9MVof1DcC0RNgaYDob5AfMrUQtQe9L383JpWA,619
 towhee/command/cmdline.py,sha256=ZIrhaD5VZxp0qrtjZBmENG04whm0Pz6AI6do7Fro1RA,1627
 towhee/command/develop.py,sha256=MEkOD1VTjZzG3vLmMpZTjQ46qH1h1cH0yQ4F1lwsVqQ,4668
 towhee/command/execute.py,sha256=RmQ9_GfmOhpcU5gbuzeHQdR6TnAcU_tzJ9vKI_EvZmY,4223
 towhee/command/repo.py,sha256=Izr2PdUvyN7PebXCvKbTMSyzuAfQoqIOeIsl2RuT76U,4908
 towhee/command/user.py,sha256=dy6nuk1dtnRFfSI3aD3smB4PWZ0_ZVlUwXDhafSREpY,5240
 towhee/data/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 towhee/data/dataset/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-towhee/data/dataset/dataset.py,sha256=JQtZvh18bJdi7ZRMmoxSH0SP7xs3wEuJswuhko_XHPE,662
+towhee/data/dataset/dataset.py,sha256=C_Pj26PuNYIc4hovt867f2ciuRKk2wCj8FScB4j0Hjk,1255
 towhee/data/dataset/image_datasets.py,sha256=v62cec3OADNuZvBq1MVJE8L25yak-ieW6A6G5Ogro_Q,2918
 towhee/datacollection/__init__.py,sha256=6LMGPQBLCUzECUc_t1ATAYzVmlCZkTxPk05_pPfbiuo,714
-towhee/datacollection/data_collection.py,sha256=DSKRaUiini9bldXgYrCW-8dbHdVUCDFnTCbHRAtYwK4,7581
+towhee/datacollection/data_collection.py,sha256=Aq69atwkrLgNgS30Cz_kJeJzay-IZfMIYZb1h6buNVo,8283
 towhee/datacollection/entity.py,sha256=wvGnze_F1EtLajTKQTrPlF79gqyj8WSGj34A6BlQcwg,3448
 towhee/datacollection/mixins/__init__.py,sha256=dtNehiPheZR53LKzbNh6j5L9zxs3dCvC_Vu4SlIX1nM,627
-towhee/datacollection/mixins/display.py,sha256=is6f73nvoEdPRjF3VPPFhlDMIK0TsGOfpE1hx07nDz0,11315
-towhee/hub/__init__.py,sha256=vqHwMKCgYCsZsERBrQtPtfKhXQjYwgGzn2HwOpMa85M,934
-towhee/hub/cache_manager.py,sha256=wTaAZRfl3n0TDaAOdk0ndSbXsVCSq8_KLfubAspDMtw,3201
-towhee/hub/downloader.py,sha256=y9G6PdSdl0UkOgzML3vilzsWHmpPNzyu4-lwbGrzfes,5748
+towhee/datacollection/mixins/display.py,sha256=cpLxeFvKU4LfFRHRU8pY_1O9nA0AJKJpuZZwGOgATBY,1900
+towhee/hub/__init__.py,sha256=YftsDyY89dn2_LTof7Bjh_xBnxxH77nwkgm5pvehJhM,1093
+towhee/hub/cache_manager.py,sha256=IzKR9imdQXGXv0Kp6L7qirMtbSgAUPpkG7sC7Qj7lNs,5178
+towhee/hub/downloader.py,sha256=hZrzrrTDo2s29N-mcLpkh77WUxUzDBwHye15NMNAy0A,6414
 towhee/hub/operator_manager.py,sha256=QwD5M_Ti5z4-l70iUQJSBYt5PNXyNk-dJuiUj979iPQ,4697
 towhee/hub/pipeline_manager.py,sha256=Nz_zL6Tm7EWuCw0Ko-h1SCQBXMneS9R9FhMMP8HYZnM,3755
 towhee/hub/repo_manager.py,sha256=YnNWm_--LzFuMIEHitF1SqeshM8zKHKtH5Vs5Np5khM,10289
 towhee/operator/__init__.py,sha256=tw5-dDFX7EfBRZn73SP5XfM7ijonFcQ1yFbv6Y2UWxU,777
-towhee/operator/base.py,sha256=a1g7C3YhhSK40Rrj0I4zkFRn18SEzC8lHndQinWG1Ww,7758
-towhee/operator/concat_operator.py,sha256=5WEF5z3OuaUg6DB95zcnhY7SeJAHyS6B1wachuMDAn4,2031
-towhee/operator/nop.py,sha256=4W4c1nOY8tVyLL8owdXOKkCCucW6qSxHz-XYQDQmTVU,1444
+towhee/operator/base.py,sha256=Jn83YGOhcAUR6X3o7JbwrdVVKCjPyrRsxmlOp6k90zA,7747
+towhee/operator/nop.py,sha256=1PnJvques2tT2zHgfcwIMJzINo5IeR5lUp4nrnrKky8,944
 towhee/pipelines/__init__.py,sha256=LyB9vbiDBdAoo9SPp5MhwFvHoePGftCqt7tKaIIiO_g,975
 towhee/pipelines/_builtin_pipeline.py,sha256=LyWnNxa1_f8rIaFskOwfNadE-0mqSqWqm8Z4SpL0UGI,947
-towhee/pipelines/insert_milvus.py,sha256=dAlv0HCaf2Aibv0eG4ynKlFPvIj9BHtTEuICwjAldvQ,1471
-towhee/pipelines/search_milvus.py,sha256=96CuJDqBPbss25DUXcZkGhDmbikj59W3dSk1ylCTqYc,1548
-towhee/pipelines/sentence_embedding.py,sha256=d2wRqDIiHmy8i0yMHyrUqfJjTySA978rjM8bZ2SE668,2871
-towhee/pipelines/text_image_embedding.py,sha256=gDIyekqK6Z4iW0XrLSoXEG_ok2VHGM15L_sB5p83iPA,2306
-towhee/pipelines/video_copy_detection.py,sha256=_wNzbEzqd5Ul8ZIHm-FPSlpJTjMjuB3wQpP02TLBaaw,6886
-towhee/pipelines/video_embedding.py,sha256=62VSkRyxhMPCqUSnbJFeSrNhPCAO5A9qslIu0yvdw0E,4850
-towhee/runtime/__init__.py,sha256=z6E2s1-5k9Ku80iRXw1wyvAez0iQW-n48s3QQutwTbk,1003
-towhee/runtime/auto_config.py,sha256=PW6qGbWbXW6lrNxuk_wAhreHMHN1h6JTQNWpcJnSbZg,8984
-towhee/runtime/auto_pipes.py,sha256=QHVCqQepFdGdikFX1pDJ89vMIF_6njE-vmDgnbmjKD8,2280
-towhee/runtime/check_utils.py,sha256=dCKJtfQucSp3nxoeu8EBf9xTOnuH7y90A3vDEQn4zYc,5091
-towhee/runtime/constants.py,sha256=sUZUkEYRFba9G9TOqwp46a2sp0kAJYssRWBUlSF6TPY,1413
-towhee/runtime/dag_repr.py,sha256=6crV2zQGueP44hWV6j9RDMwM49qpvcg6qn5YqTJFuEo,13063
-towhee/runtime/data_queue.py,sha256=FiLmBzd15BgD2f4mxuHe3s8UiWavpS1RgBN5yOQ7THc,9260
-towhee/runtime/factory.py,sha256=mP_9wBAq27jwCbDRZfrjGBfOcvEtVp-ZaSjKjEfzMxM,3571
-towhee/runtime/node_config.py,sha256=s32XJY8BsOD0ryheudHwsIWo7j8Z2akfX55m1XKH0Yg,6619
-towhee/runtime/node_repr.py,sha256=ssi2oHBfFq6_eeffYxhwJ-EnGw2UMnwZmXddafPNqUE,7149
-towhee/runtime/performance_profiler.py,sha256=qPG1pJkSiUlT0_NCTxWXvzGbwjnqu-yMSUe2RScJ9yQ,12539
-towhee/runtime/pipeline.py,sha256=J6KFdVKTuQki5BkzGskWpQJziag-oBY__u9ofXuRNuA,16957
-towhee/runtime/pipeline_loader.py,sha256=o8w2odDHm2fUw06eA0RYYYZ7T1IwqS36v_OTwY44zi8,2018
-towhee/runtime/runtime_conf.py,sha256=EfXSsqo_8Ae7RMRKcQliVKmZpOQkmNu9RxFfyAUMQYk,2821
-towhee/runtime/runtime_pipeline.py,sha256=7br9xMZtvaXq07cXlPKbC3eqY3RwgCGSPrQMO5pf0z0,6648
-towhee/runtime/schema_repr.py,sha256=CSlmtHO2C3iDAkXUrTPhTfFP9LDOAQuIhfb9GPh9Lso,2335
-towhee/runtime/nodes/__init__.py,sha256=Bd-9xBINYAeYqC6lxnMfkiM5CFCo6wmx68TBg7We3Cc,2572
-towhee/runtime/nodes/_concat.py,sha256=whOxzjfo_oiWCGxAv5949uwtKFqMrHQhX3seIF7gO6g,2377
-towhee/runtime/nodes/_filter.py,sha256=d28uGPXb3LHxO0RBz9WpJVSOnVVop-YJYi5Wrlc5-2Q,2722
-towhee/runtime/nodes/_flat_map.py,sha256=ufH5sVvK7Lo1A9Q5xNbSDbu2ZTZoJrAAhC6LBEj0tLg,2879
-towhee/runtime/nodes/_map.py,sha256=lcmCz0due7--imedcZSH48-xIBSj0KDHhp10T6c8h1w,3551
-towhee/runtime/nodes/_output.py,sha256=ywykQ3Qk0Cct5rCGajQ414CYFFkZFRtAQLvWj1xNip8,1630
-towhee/runtime/nodes/_time_window.py,sha256=rZYC1EmrCh2_VQVTGUIcmmP2ZbDSQ7EMzHsp2cfqJ9U,4481
-towhee/runtime/nodes/_window.py,sha256=appfUq3vvmg2-6ILcQHed7DHe8pompnK0cTjoveU0P0,5737
-towhee/runtime/nodes/_window_all.py,sha256=WHyyFd-MbTdusfqe-c1cN4q1JFOdYQTPNNU1gY8IEeY,3310
-towhee/runtime/nodes/node.py,sha256=rfwD7OVy2DFux8aLlBkmTG3dQnMToqTHwQ0tmWJR7Uw,6165
+towhee/pipelines/insert_milvus.py,sha256=xGFme8DjwNmUTAPwqO7a3NDdv_tuy110GYP-EUQtSZ0,1547
+towhee/pipelines/search_milvus.py,sha256=Cvr_moh88GLiFQr1OEeFQtJoekhmuF_bMLQVMJ8UW8A,1654
+towhee/pipelines/sentence_embedding.py,sha256=7TC4qitDMVRDxFqZCilb3mWv2NChCx4RB0UyT6NdHIM,2953
+towhee/pipelines/text_image_embedding.py,sha256=R3lEgbBUEEzRDv2JtH3r4uw_S8bjQ8DvpgOsQTqezew,2388
+towhee/pipelines/video_copy_detection.py,sha256=srL4fZ3kaHIG13dVwLXbRbe-xnlVwY-RdOMdVrEsxRM,6891
+towhee/pipelines/video_embedding.py,sha256=ACI95O2RFDXFa6MqVEmuGsDivDFr-AbsdncF60ws9b8,4858
+towhee/runtime/__init__.py,sha256=XUi6TJf7A-bbkRqmoiIiZgZ_gC_NrK2z99QKG7SS06w,1004
+towhee/runtime/auto_config.py,sha256=WmVtKLuSlgCfxHlq2qFa7KKvwpWmFiha1KqM_Fe5nPg,9344
+towhee/runtime/auto_pipes.py,sha256=euRMuWqOU3UMv_7Czuc-mvc4DuaVp2RF16eLtuSXD2c,2854
+towhee/runtime/check_utils.py,sha256=Q1NtHeQ-WTT19byYLOsDLQIR-3bVl2eN_aKqsb2fYCc,3771
+towhee/runtime/constants.py,sha256=ezf7WMLdiAlyf01sYNaTyqBsfnm2gmO2ORGEY7CaGf4,1508
+towhee/runtime/dag_repr.py,sha256=c6m_Bh8g7HATvPB9Marf_z2MgWcV4BdjrQvBUG78eSs,21806
+towhee/runtime/data_queue.py,sha256=4CGEO5s-ZA-PBmrelI7SbCluejXRYfA23AtnKXoa--A,10047
+towhee/runtime/factory.py,sha256=-KqKMCxq3WcEnYJfb6pyjtlm5FsHCbXfx8xVpf_T060,4392
+towhee/runtime/node_config.py,sha256=X0akBuZckbXajGmZqvTIImvHev93VV_nf41R3I93Iy8,3420
+towhee/runtime/node_repr.py,sha256=DQaGLvcjSES9qqwjubnMbrifNe6-UhTfkTL5vDnMT_U,2838
+towhee/runtime/pipeline.py,sha256=v5_3yTfr_W0gQuuMH1MRKWd7iGaiqkAtzxO8WYZ0fWw,18802
+towhee/runtime/pipeline_loader.py,sha256=f0Am2B_IerwFDQENhTWuJ7aRcFcp3OTEFsFJmpfsHEk,2420
+towhee/runtime/runtime_conf.py,sha256=3w1Vxql1zZiFWLcCwvUadGEEzzOVKVoTo5ESHyx0SeQ,2821
+towhee/runtime/runtime_pipeline.py,sha256=Zpp5nd27AL2CjBfOrjf5h2CnE3b4TEGDNdZ4sgJEJ6g,10298
+towhee/runtime/schema_repr.py,sha256=jcaVyPksv3OQ2HYlOfG1NoRc2HhKCEOKdWopgwOG-2A,2314
+towhee/runtime/time_profiler.py,sha256=66KRXCO2LkpEeFrvvAVhWuqe4k_dczLdhPVmbZfSPXY,1563
+towhee/runtime/nodes/__init__.py,sha256=SaOB6uXuYhSrOuVPS7tf6IxJYdV0DE51iSBGMyG1geM,2776
+towhee/runtime/nodes/_concat.py,sha256=E5KO_BKP5nTR4_AY7yI2t0yXccaK7PEFgWU0Lbl5DAg,2246
+towhee/runtime/nodes/_filter.py,sha256=9RYulL0ldWvjp7YByoy5_T6zRK2WFIn-gRkSfr3qJ5s,2323
+towhee/runtime/nodes/_flat_map.py,sha256=uyCZqUcHuoT6r0quWZ8IxJD8NNicO6qhS2iogIZRWNw,2066
+towhee/runtime/nodes/_map.py,sha256=ydqsBhe9xQimV0sIPEkBTV85RBBOnAAMe2UxJovI_OI,3093
+towhee/runtime/nodes/_output.py,sha256=VRfGNz8iOLv27ueGgreVVzLXhZvtworR64ky64Pjw7U,1383
+towhee/runtime/nodes/_reduce.py,sha256=IZ3Kvc5W-tnrrCDSlWsrrDZnr6ARZonhAzHZqwnJIM8,3246
+towhee/runtime/nodes/_single_input.py,sha256=J_xloNvFURK9Frd-1phjL0gciIm0ejXifOUjCJDQ25g,1266
+towhee/runtime/nodes/_time_window.py,sha256=QAp9AIkWW7B3xox5Hpmpyhk7I2KVDhcFAdgLTy-rvEg,3692
+towhee/runtime/nodes/_window.py,sha256=JOOYUEi06EXTMOz1HVU34rxDyc2cXCBvLm01YFzPdLI,2632
+towhee/runtime/nodes/_window_all.py,sha256=bV3RY1PRYrKlLvf50CapE1XbPIz6dudeYDFJD4s_Ni8,2630
+towhee/runtime/nodes/_window_base.py,sha256=xcSH2sY51q6plMT6OXOOr8Nf450q7jkZ_tLUF0dqPdY,3666
+towhee/runtime/nodes/node.py,sha256=aa3zurTMItcJbxrCcjNeqz0fl_QxXSi6eaK_tQNBJQY,6468
 towhee/runtime/operator_manager/__init__.py,sha256=o23ghCrUc0fGBuokEAEqx5tTBVsh4UlCvN3doGj80Mo,872
-towhee/runtime/operator_manager/operator_action.py,sha256=oqcbyg8lyPVZd5K21fQX-hmkyXmlTVNOPDrv0xUkhFU,3016
-towhee/runtime/operator_manager/operator_loader.py,sha256=w2OgwKuaInKDfJYvB2Ya__4YJ_mEpkF0voRSPPW5hvg,6478
-towhee/runtime/operator_manager/operator_pool.py,sha256=m4h87i3rgfgvjmVXppQY0dcxjffhVoHjkbUmatDI7_A,3634
+towhee/runtime/operator_manager/operator_action.py,sha256=wsccIRJpkN4EgGJqSZIm1oBDj6DEpnRhCu6m4qWdbE8,4604
+towhee/runtime/operator_manager/operator_loader.py,sha256=xV384-K_JJW8bBRJsMhVeayTB8BOXb_ytost8obMSKY,6712
+towhee/runtime/operator_manager/operator_pool.py,sha256=PsxRJBqLKFrir15FGMSX1VY9ygXWc_o5xzuDRwnW6wI,3953
 towhee/runtime/operator_manager/operator_registry.py,sha256=ogVjlDZqTB1BZl9GdGv9gAWqZNkJd6Ol6CS7Udhw7cQ,2867
 towhee/runtime/operator_manager/uri.py,sha256=wzv6lAGlJPPycvprVi45IoDuGJgwUqpFaTlVBKXD4lU,1951
 towhee/serve/__init__.py,sha256=CeCaoChD6XAbWtan8cl1tZ0mD2QokRuoG_IPrSqyvxk,592
-towhee/serve/server_builder.py,sha256=Sh0nPeluZzigmlfGhjgQ3A90tiyL0KbMJ38wY7_ueXI,2005
+towhee/serve/server_builder.py,sha256=ifjL67nAJbc-erHraincuXJrsHo_hl2z5EqaR70D7L4,4190
 towhee/serve/triton/__init__.py,sha256=1IWzWcdn_5G08L9u2wK3d418aw9u55-LwUDHu1CPKC8,773
 towhee/serve/triton/constant.py,sha256=ylfPCrhJTBovSIk-_nOUaRdd-ya_BjlaZfdmQ2nPoBk,816
 towhee/serve/triton/docker_image_builder.py,sha256=Vr6ps4iV9X_JL7yfNx8FsDfGH1WhcBTkwNtjzwbfOxk,2330
-towhee/serve/triton/model_to_triton.py,sha256=sCqNSCy6map0zucmFpgqpgb7Mvk6_OU35IV6JgDa5V4,4500
+towhee/serve/triton/model_to_triton.py,sha256=jd12sKx08C0nzgXLyXhYrBdinZNJvK6y3o3DRlojOoY,4495
 towhee/serve/triton/pipe_to_triton.py,sha256=4e9igNQuNAG8CysqB4GTURSXFYayPfkmIr5ESd9z9sQ,3483
-towhee/serve/triton/pipeline_builder.py,sha256=dm6mOm1ZehfMjV0gDrj69YdS-NROGwjqdYAii4r1TdU,4286
-towhee/serve/triton/pipeline_client.py,sha256=XtiMnvfa-M_MdeJ1bCsni352tzZzebvn7fJCzqLcZnU,4051
-towhee/serve/triton/serializer.py,sha256=HaMtPUWJ_6XjAl7JxksLFQ4CCtnhY0HQleYgW79ouBg,1729
+towhee/serve/triton/pipeline_builder.py,sha256=zSo5mzHVooNt_THc8iOcKfaY9_EHDUnqkfseG4Qhjx8,4284
+towhee/serve/triton/pipeline_client.py,sha256=5yIYza9nxPkHI9kadmEddA3uUbkdLctCljf8KwRbwbk,4044
 towhee/serve/triton/triton_client.py,sha256=GAc2NuUN7PHkTRfTqwzpZ0l7X47xlngmUn_NnugsH0M,2408
 towhee/serve/triton/triton_config_builder.py,sha256=qMv6ycHyqOC1bHYrXgs3y3MzTlDjCGcf3ir67VhA1Dk,2547
 towhee/serve/triton/triton_files.py,sha256=TpdNJKz2Os4EyfIVI2E6pvEm8M3xYlx9S0GFxRXq7vc,1499
 towhee/serve/triton/bls/__init__.py,sha256=SOxtPloSbytyTgieH8fy2dy0eMKSnZGQtklgV-pQ3XY,706
-towhee/serve/triton/bls/pipeline_model.py,sha256=A3FWvxAk7XlCknqNY9jw_vJ4T8352ndh6CUtoxogubY,2228
+towhee/serve/triton/bls/pipeline_model.py,sha256=q7xCwOfp4tnwQ-pBR7RDqyfhUg1quMLcNybVJ7zOLhk,2813
 towhee/serve/triton/bls/python_backend_wrapper.py,sha256=TcuM63k--yJ5QY-u9IAhdHzJxAXAKbR6nul5JCJsCMk,934
 towhee/serve/triton/bls/caller/__init__.py,sha256=CeCaoChD6XAbWtan8cl1tZ0mD2QokRuoG_IPrSqyvxk,592
 towhee/serve/triton/bls/caller/local_caller.py,sha256=0Vjp3FUdDOy2CV7CUJ_oDD-galIhnHTqdKLvgE0KHe8,3437
 towhee/serve/triton/bls/mock/__init__.py,sha256=CeCaoChD6XAbWtan8cl1tZ0mD2QokRuoG_IPrSqyvxk,592
 towhee/serve/triton/bls/mock/mock_pb_util.py,sha256=j03ZNE2GMtrXgNODJk5VxtHd05J5CbO8YE4BiY4JTPs,4727
 towhee/serve/triton/bls/mock/mock_triton_client.py,sha256=BnejCdP3z-_zP0-_ULZU7uuMVvMPfV5EmpvDsG6DHlI,2699
 towhee/serve/triton/dockerfiles/DockerfileCuda113,sha256=Z1yZfp-7AattSBt_KXR2lZ32bIe5feuwoax0dtpvJ44,750
 towhee/serve/triton/dockerfiles/DockerfileCuda114,sha256=B7UgAPRcL4cqOIIq2-o-zPCVghjmoPrMWOx0EnlBwJw,748
 towhee/serve/triton/dockerfiles/DockerfileCuda116,sha256=tkEJM0-5Z0CM1UZFn09YG61-W-mccByPHA-wvlwlI64,748
 towhee/serve/triton/dockerfiles/DockerfileCuda117,sha256=45k4ZcAmOyZm1hky2VIyOdp9eFM0Tef7y5FXfbczAd8,748
 towhee/serve/triton/dockerfiles/DockerfileCuda117dev,sha256=o_N_ugWsfWC1rdG52owA0_mckc0us25qDqnZWsP7wlo,956
 towhee/serve/triton/dockerfiles/__init__.py,sha256=QtTlzrSSnBuLIicj9L9bbfZUjzZbTb-RHzyg8pjd3-g,1317
+towhee/tools/__init__.py,sha256=IKOMFrP-IfwEgml_5nXAzvmpcrMRRKnhJF22OM18Gfo,754
+towhee/tools/data_visualizer.py,sha256=eWb-VLjvBBqjmzAly4QDuurlSzGotZp4V9wxzaMGZKk,5942
+towhee/tools/graph_visualizer.py,sha256=O2NlihAoAc1Fc4ZYkJwzQSflBgPex6Nem7NGF-yAn6s,1983
+towhee/tools/profilers.py,sha256=RFW3lU9SQXtK7WzoeDPMMeECGpqpxkn9CelAQPZFFpY,11912
+towhee/tools/visualizer.py,sha256=5EjM90SKOQ949T81EA0oL4WX7VebM0uZwutCzBR7CkA,6863
 towhee/trainer/__init__.py,sha256=tnnwMilfm6hLypg8n4bk98D_5mT_irqtdRzjAFJGJ-M,940
 towhee/trainer/callback.py,sha256=pYfLCQ8yvOH1r0T4sRXvDZ3SFSVJ2MlcANPSdgx8rsE,26591
 towhee/trainer/metrics.py,sha256=GQgrTZJHfM6j8LY961WFhg3bLs6RTRWiAtXZWSK0i8g,10668
 towhee/trainer/modelcard.py,sha256=v0K_F5pD5hEO58jfq0DqAO0RTKJEr14NgnDMeEoP98A,8861
 towhee/trainer/scheduler.py,sha256=iCgJaEmj1dYhMqJaWiB3O6M5QrcvJrqxDbW7u5eMELk,14904
 towhee/trainer/trainer.py,sha256=fII5llXEdtKhkdI91AGfeUfFUnz_YkETDoNwWPBRuME,34675
 towhee/trainer/training_config.py,sha256=xRKl2SmTK1_4bhqa3vfxaj1Rzi482uB-OTDTg9wXKvQ,14667
 towhee/trainer/optimization/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 towhee/trainer/optimization/adafactor.py,sha256=a4XgT_do-_kcekYKM1oP82jTPagv07hAKlzUEG6_BYI,7830
 towhee/trainer/optimization/adamw.py,sha256=g2NZ_rkJBUCfUt4RZFsR8AwUPXs0MLBAeYZPXPS_M_0,5148
 towhee/trainer/optimization/optimization.py,sha256=J_NSvCD11QcObQMjyqF5zgkzEssQI4_EWdL40g3ko7E,12006
 towhee/trainer/utils/__init__.py,sha256=CeCaoChD6XAbWtan8cl1tZ0mD2QokRuoG_IPrSqyvxk,592
-towhee/trainer/utils/file_utils.py,sha256=Sn3M1Ns49hU21kSo98Id9XH-_NC16_yLd6nnmqpSPmQ,970
-towhee/trainer/utils/layer_freezer.py,sha256=ElaULpjnke5zjP2aPonh2RhREsq5LOIKu2tb_a9nOp8,5007
+towhee/trainer/utils/file_utils.py,sha256=P0jjvggBdq5ZhBiUMQEe86Osxc7r1cBZuevAdq4KHdE,1563
+towhee/trainer/utils/layer_freezer.py,sha256=iddbw6wdeFPs8ZTjLLfjV8YGxt6kh_y_WnY5qmBpbgQ,5600
 towhee/trainer/utils/plot_utils.py,sha256=gbVMLeccR6v1CAUP72VAgR6SetFdHVh0GXKlJbxitDc,16453
 towhee/trainer/utils/trainer_utils.py,sha256=IR9FZp1dwHEfZEz44FtJrnLQnlk-sdtuOVWvlTiLO_k,9017
 towhee/types/__init__.py,sha256=Fp6Yg_pR4m8XjK9XWDQtTaUVdtteHI4_XJjngSAq7qQ,872
 towhee/types/arg.py,sha256=wpwnEEmxcvxy39rWYQRU1DhNtFB904xInkQw3WsjAbY,2341
 towhee/types/audio_frame.py,sha256=RwU4jwtE730s2vn-r0TxtCDnC95T1DsulVg4dikQtZ4,2141
 towhee/types/image.py,sha256=OrrcmabM6esYq4oU7oBBW6f9UjpsKrrm5smuwuAIhnE,2443
 towhee/types/image_utils.py,sha256=0zQiWgXHC0ukyc5NsJxxL9zpw3LvbxpVmpjqtUhNSSE,2016
 towhee/types/video_frame.py,sha256=2-UmqescC6Udp2siDZhEYfXCvBC3k5RBCCgf-n5Zkh0,2649
 towhee/utils/__init__.py,sha256=CeCaoChD6XAbWtan8cl1tZ0mD2QokRuoG_IPrSqyvxk,592
+towhee/utils/console_table.py,sha256=mzOuBdvRbHiUwmeE6QoA9qpNI1KD1nXQCih7qHabn0Q,3928
 towhee/utils/cv2_utils.py,sha256=_mMvNzXIgdq0AViufC9b7lJHksczDXOZ-wL4lnAd7mg,1178
 towhee/utils/dependency_control.py,sha256=F1q6GCyUCm9Mgux5KhozKErTOBwvzqcmy_8TSdwhcCw,1403
-towhee/utils/empty_format.py,sha256=Wdsds-6BQIBR3dubAEupSSEGwedu1JLG8tNhwg9G46Y,829
 towhee/utils/git_utils.py,sha256=hz96k4Z-QJqr5O5oZbLiIEY9PRtckSna5rjl_6Z4nco,7744
+towhee/utils/html_table.py,sha256=1H5Ob7spplcdwQlxKnd9Us-e703_xBGfI7STdzXE0us,7243
 towhee/utils/hub_file_utils.py,sha256=lBrNWj7eEagTlPOtbDL9pjxGGo0i1y15Oeo772hA6lQ,1381
-towhee/utils/hub_utils.py,sha256=8X_Q3tlnfpzwODZp0QhmxS88RcDIv8sa8mSsMG31ejs,13148
+towhee/utils/hub_utils.py,sha256=-6b9QGRaeq7Zb_-kggA-II2SJzykG0QpumE6EiHBKPE,13166
 towhee/utils/lazy_import.py,sha256=kcVml6hHVEZb44AS83yXaL4bq9ZaKlSqR6Cj0diUdag,1301
 towhee/utils/log.py,sha256=5v0l5Vf8Jaexewh_yeJz647uJFf-sZ0xe4UD-WE6NKo,894
 towhee/utils/matplotlib_utils.py,sha256=p6fbp1-0sU0BE8tQsBOLd1-MkFgjidjA5Kk7mATX0Hw,2975
 towhee/utils/ndarray_utils.py,sha256=mb0N13s3XF_BSrMpUzZxSvCzjXfrtfTU9k8kGlw-M_s,4385
-towhee/utils/np_format.py,sha256=j6KL0jiO70hYEVmMexwBNBPidmMaLOi9WF5kmZocamM,2414
 towhee/utils/onnx_utils.py,sha256=RcUIgePepfIpCKYXZXbSay8XsM4Eh1h-YojxKhRbW9o,1156
 towhee/utils/pil_utils.py,sha256=wvCwk7N-ehkzeCbUQOHvXHkVJ-RWh8sLNrzS8FvHjE0,2571
 towhee/utils/repo_normalize.py,sha256=l-om5pytftkWPHoYaz0NxJ3q8lM-fx5v0DbE37ptFS4,8448
+towhee/utils/serializer.py,sha256=OCguK4303oNceg5XNdyU7gQZsPxoTgKz-H2dc5OdJFY,3628
 towhee/utils/singleton.py,sha256=H3n3l8gicm-6udX0-2vNV-QysrNAhTBNut7OzTGJjAc,1102
 towhee/utils/triton_httpclient.py,sha256=qoafN9GXE--tZkqKGsqYoQho6-q7hicFwt9EDYCLCPg,1538
 towhee/utils/yaml_utils.py,sha256=qZ0Mvcbh_qD2ymgRMQyal8Cp6w3d3I4q6ttCoaA7WQ0,2535
 towhee/utils/thirdparty/__init__.py,sha256=CeCaoChD6XAbWtan8cl1tZ0mD2QokRuoG_IPrSqyvxk,592
 towhee/utils/thirdparty/dill_util.py,sha256=YwnUvKa9sm566UHE5mw_8Jmu8YYvaBgVhVr27ckFr-c,845
 towhee/utils/thirdparty/ipython_utils.py,sha256=r0coEqQidnouFbSzVx3Q_dSgu45PPnUgDZwiTcm-FuE,1230
 towhee/utils/thirdparty/pandas_utils.py,sha256=W_1RdRIuyZ95L2x2pv9xbvZRW6hvGMKJ2Y_PLO2hWvE,1130
-towhee-1.0.0rc1.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
-towhee-1.0.0rc1.dist-info/METADATA,sha256=5mNf8ngXj8nQZ8_D4L7a5nqwWbh6whxeox4uz9pjMN8,17248
-towhee-1.0.0rc1.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-towhee-1.0.0rc1.dist-info/entry_points.txt,sha256=Li9rdXM7RZW5O7NR9d9OOQV9D99bK1Fv4BilWvB1btk,114
-towhee-1.0.0rc1.dist-info/top_level.txt,sha256=s8O0-CAA8lmENHKY-FR5ojkSAmqEOEkrpg6ITjplm4A,7
-towhee-1.0.0rc1.dist-info/RECORD,,
+towhee-1.1.0.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
+towhee-1.1.0.dist-info/METADATA,sha256=V883jhOQYlhU7h4Is-dWUx4RDdtFw0sjXeU3Hi4aXyo,13967
+towhee-1.1.0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+towhee-1.1.0.dist-info/entry_points.txt,sha256=Li9rdXM7RZW5O7NR9d9OOQV9D99bK1Fv4BilWvB1btk,114
+towhee-1.1.0.dist-info/top_level.txt,sha256=s8O0-CAA8lmENHKY-FR5ojkSAmqEOEkrpg6ITjplm4A,7
+towhee-1.1.0.dist-info/RECORD,,
```

