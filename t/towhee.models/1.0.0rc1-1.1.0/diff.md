# Comparing `tmp/towhee.models-1.0.0rc1-py3-none-any.whl.zip` & `tmp/towhee.models-1.1.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,282 +1,282 @@
-Zip file size: 1800462 bytes, number of entries: 280
--rw-r--r--  2.0 unx     8596 b- defN 23-Mar-21 08:26 towhee/models/README.md
--rw-r--r--  2.0 unx     8570 b- defN 23-Mar-21 08:26 towhee/models/README_CN.md
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/__init__.py
--rw-r--r--  2.0 unx      678 b- defN 23-Mar-21 08:26 towhee/models/acar_net/__init__.py
--rw-r--r--  2.0 unx    10305 b- defN 23-Mar-21 08:26 towhee/models/acar_net/backbone.py
--rw-r--r--  2.0 unx     9275 b- defN 23-Mar-21 08:26 towhee/models/acar_net/head.py
--rw-r--r--  2.0 unx     3613 b- defN 23-Mar-21 08:26 towhee/models/acar_net/model.py
--rw-r--r--  2.0 unx     4102 b- defN 23-Mar-21 08:26 towhee/models/acar_net/neck.py
--rw-r--r--  2.0 unx     1618 b- defN 23-Mar-21 08:26 towhee/models/acar_net/utils.py
--rw-r--r--  2.0 unx      709 b- defN 23-Mar-21 08:26 towhee/models/action_clip/__init__.py
--rw-r--r--  2.0 unx     4305 b- defN 23-Mar-21 08:26 towhee/models/action_clip/action_clip.py
--rw-r--r--  2.0 unx     2039 b- defN 23-Mar-21 08:26 towhee/models/action_clip/action_clip_utils.py
--rw-r--r--  2.0 unx     1804 b- defN 23-Mar-21 08:26 towhee/models/action_clip/text_prompt.py
--rw-r--r--  2.0 unx     8805 b- defN 23-Mar-21 08:26 towhee/models/action_clip/visual_prompt.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/allinone/__init__.py
--rw-r--r--  2.0 unx     1289 b- defN 23-Mar-21 08:26 towhee/models/allinone/allinone.py
--rw-r--r--  2.0 unx      622 b- defN 23-Mar-21 08:26 towhee/models/bridgeformer/__init__.py
--rw-r--r--  2.0 unx     3300 b- defN 23-Mar-21 08:26 towhee/models/bridgeformer/bridge_former.py
--rw-r--r--  2.0 unx    10421 b- defN 23-Mar-21 08:26 towhee/models/bridgeformer/bridge_former_training.py
--rw-r--r--  2.0 unx    16155 b- defN 23-Mar-21 08:26 towhee/models/bridgeformer/bridge_former_training_block.py
--rw-r--r--  2.0 unx     1560 b- defN 23-Mar-21 08:26 towhee/models/clip/README.md
--rw-r--r--  2.0 unx      639 b- defN 23-Mar-21 08:26 towhee/models/clip/__init__.py
--rw-r--r--  2.0 unx    20204 b- defN 23-Mar-21 08:26 towhee/models/clip/auxilary.py
--rw-r--r--  2.0 unx  1356917 b- defN 23-Mar-21 08:26 towhee/models/clip/bpe_simple_vocab_16e6.txt.gz
--rw-r--r--  2.0 unx    30031 b- defN 23-Mar-21 08:26 towhee/models/clip/clip.py
--rw-r--r--  2.0 unx     9935 b- defN 23-Mar-21 08:26 towhee/models/clip/clip_utils.py
--rw-r--r--  2.0 unx     5618 b- defN 23-Mar-21 08:26 towhee/models/clip/simple_tokenizer.py
--rw-r--r--  2.0 unx       46 b- defN 23-Mar-21 08:26 towhee/models/clip4clip/__init__.py
--rw-r--r--  2.0 unx    10156 b- defN 23-Mar-21 08:26 towhee/models/clip4clip/clip4clip.py
--rw-r--r--  2.0 unx     2605 b- defN 23-Mar-21 08:26 towhee/models/clip4clip/until_module.py
--rw-r--r--  2.0 unx     1837 b- defN 23-Mar-21 08:26 towhee/models/clip4clip/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/coca/__init__.py
--rw-r--r--  2.0 unx    11514 b- defN 23-Mar-21 08:26 towhee/models/coca/coca.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/coformer/__init__.py
--rw-r--r--  2.0 unx     5167 b- defN 23-Mar-21 08:26 towhee/models/coformer/backbone.py
--rw-r--r--  2.0 unx    10222 b- defN 23-Mar-21 08:26 towhee/models/coformer/coformer.py
--rw-r--r--  2.0 unx    10008 b- defN 23-Mar-21 08:26 towhee/models/coformer/config.py
--rw-r--r--  2.0 unx    15471 b- defN 23-Mar-21 08:26 towhee/models/coformer/transformer.py
--rw-r--r--  2.0 unx     2919 b- defN 23-Mar-21 08:26 towhee/models/coformer/utils.py
--rw-r--r--  2.0 unx      630 b- defN 23-Mar-21 08:26 towhee/models/collaborative_experts/__init__.py
--rw-r--r--  2.0 unx    60484 b- defN 23-Mar-21 08:26 towhee/models/collaborative_experts/collaborative_experts.py
--rw-r--r--  2.0 unx     3587 b- defN 23-Mar-21 08:26 towhee/models/collaborative_experts/net_vlad.py
--rw-r--r--  2.0 unx     1612 b- defN 23-Mar-21 08:26 towhee/models/collaborative_experts/util.py
--rw-r--r--  2.0 unx      661 b- defN 23-Mar-21 08:26 towhee/models/convnext/__init__.py
--rw-r--r--  2.0 unx     3245 b- defN 23-Mar-21 08:26 towhee/models/convnext/configs.py
--rw-r--r--  2.0 unx     4425 b- defN 23-Mar-21 08:26 towhee/models/convnext/convnext.py
--rw-r--r--  2.0 unx     4107 b- defN 23-Mar-21 08:26 towhee/models/convnext/utils.py
--rw-r--r--  2.0 unx      614 b- defN 23-Mar-21 08:26 towhee/models/cvnet/__init__.py
--rw-r--r--  2.0 unx     4665 b- defN 23-Mar-21 08:26 towhee/models/cvnet/cvnet.py
--rw-r--r--  2.0 unx     6640 b- defN 23-Mar-21 08:26 towhee/models/cvnet/cvnet_block.py
--rw-r--r--  2.0 unx     2439 b- defN 23-Mar-21 08:26 towhee/models/cvnet/cvnet_utils.py
--rw-r--r--  2.0 unx     8106 b- defN 23-Mar-21 08:26 towhee/models/cvnet/resnet.py
--rw-r--r--  2.0 unx       19 b- defN 23-Mar-21 08:26 towhee/models/drl/__init__.py
--rw-r--r--  2.0 unx    34568 b- defN 23-Mar-21 08:26 towhee/models/drl/drl.py
--rw-r--r--  2.0 unx     7936 b- defN 23-Mar-21 08:26 towhee/models/drl/module_cross.py
--rw-r--r--  2.0 unx     4891 b- defN 23-Mar-21 08:26 towhee/models/drl/until_module.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/embedding/__init__.py
--rw-r--r--  2.0 unx     2061 b- defN 23-Mar-21 08:26 towhee/models/embedding/embedding_extractor.py
--rw-r--r--  2.0 unx      623 b- defN 23-Mar-21 08:26 towhee/models/frozen_in_time/__init__.py
--rw-r--r--  2.0 unx    14921 b- defN 23-Mar-21 08:26 towhee/models/frozen_in_time/frozen_in_time.py
--rw-r--r--  2.0 unx     2394 b- defN 23-Mar-21 08:26 towhee/models/frozen_in_time/frozen_utils.py
--rw-r--r--  2.0 unx    15910 b- defN 23-Mar-21 08:26 towhee/models/frozen_in_time/frozen_video_transformer.py
--rw-r--r--  2.0 unx      659 b- defN 23-Mar-21 08:26 towhee/models/hornet/__init__.py
--rw-r--r--  2.0 unx     4309 b- defN 23-Mar-21 08:26 towhee/models/hornet/configs.py
--rw-r--r--  2.0 unx     5036 b- defN 23-Mar-21 08:26 towhee/models/hornet/hornet.py
--rw-r--r--  2.0 unx     5983 b- defN 23-Mar-21 08:26 towhee/models/hornet/utils.py
--rw-r--r--  2.0 unx      612 b- defN 23-Mar-21 08:26 towhee/models/isc/__init__.py
--rw-r--r--  2.0 unx     3470 b- defN 23-Mar-21 08:26 towhee/models/isc/isc.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/layers/__init__.py
--rw-r--r--  2.0 unx     1638 b- defN 23-Mar-21 08:26 towhee/models/layers/aspp.py
--rw-r--r--  2.0 unx     6693 b- defN 23-Mar-21 08:26 towhee/models/layers/attention.py
--rw-r--r--  2.0 unx     5783 b- defN 23-Mar-21 08:26 towhee/models/layers/cond_conv2d.py
--rw-r--r--  2.0 unx     2706 b- defN 23-Mar-21 08:26 towhee/models/layers/conv2d_same.py
--rw-r--r--  2.0 unx     4180 b- defN 23-Mar-21 08:26 towhee/models/layers/conv2d_separable.py
--rw-r--r--  2.0 unx     4354 b- defN 23-Mar-21 08:26 towhee/models/layers/conv4d.py
--rw-r--r--  2.0 unx     5125 b- defN 23-Mar-21 08:26 towhee/models/layers/conv_bn_activation.py
--rw-r--r--  2.0 unx     1918 b- defN 23-Mar-21 08:26 towhee/models/layers/convmlp.py
--rw-r--r--  2.0 unx     4369 b- defN 23-Mar-21 08:26 towhee/models/layers/cross_attention.py
--rw-r--r--  2.0 unx     5964 b- defN 23-Mar-21 08:26 towhee/models/layers/dropblock2d.py
--rw-r--r--  2.0 unx     2200 b- defN 23-Mar-21 08:26 towhee/models/layers/droppath.py
--rw-r--r--  2.0 unx     2175 b- defN 23-Mar-21 08:26 towhee/models/layers/ffn.py
--rw-r--r--  2.0 unx     2052 b- defN 23-Mar-21 08:26 towhee/models/layers/gatedmlp.py
--rw-r--r--  2.0 unx    10670 b- defN 23-Mar-21 08:26 towhee/models/layers/layers_with_relprop.py
--rw-r--r--  2.0 unx     4330 b- defN 23-Mar-21 08:26 towhee/models/layers/mbconv.py
--rw-r--r--  2.0 unx     2971 b- defN 23-Mar-21 08:26 towhee/models/layers/mixed_conv2d.py
--rw-r--r--  2.0 unx     2566 b- defN 23-Mar-21 08:26 towhee/models/layers/mlp.py
--rw-r--r--  2.0 unx    13234 b- defN 23-Mar-21 08:26 towhee/models/layers/multi_scale_attention.py
--rw-r--r--  2.0 unx     6651 b- defN 23-Mar-21 08:26 towhee/models/layers/multi_scale_transformer_block.py
--rw-r--r--  2.0 unx     3312 b- defN 23-Mar-21 08:26 towhee/models/layers/netvlad.py
--rw-r--r--  2.0 unx     5460 b- defN 23-Mar-21 08:26 towhee/models/layers/non_local.py
--rw-r--r--  2.0 unx     5759 b- defN 23-Mar-21 08:26 towhee/models/layers/padding_functions.py
--rw-r--r--  2.0 unx     4015 b- defN 23-Mar-21 08:26 towhee/models/layers/patch_embed2d.py
--rw-r--r--  2.0 unx     3033 b- defN 23-Mar-21 08:26 towhee/models/layers/patch_embed3d.py
--rw-r--r--  2.0 unx     2458 b- defN 23-Mar-21 08:26 towhee/models/layers/patch_merging.py
--rw-r--r--  2.0 unx     2423 b- defN 23-Mar-21 08:26 towhee/models/layers/patch_merging3d.py
--rw-r--r--  2.0 unx     3343 b- defN 23-Mar-21 08:26 towhee/models/layers/pool_attention.py
--rw-r--r--  2.0 unx     3084 b- defN 23-Mar-21 08:26 towhee/models/layers/position_encoding.py
--rw-r--r--  2.0 unx     4749 b- defN 23-Mar-21 08:26 towhee/models/layers/relative_self_attention.py
--rw-r--r--  2.0 unx     2028 b- defN 23-Mar-21 08:26 towhee/models/layers/resnet_basic_3d_module.py
--rw-r--r--  2.0 unx     3273 b- defN 23-Mar-21 08:26 towhee/models/layers/sam.py
--rw-r--r--  2.0 unx     1500 b- defN 23-Mar-21 08:26 towhee/models/layers/sequence_pool.py
--rw-r--r--  2.0 unx     3663 b- defN 23-Mar-21 08:26 towhee/models/layers/spatial_temporal_cls_positional_encoding.py
--rw-r--r--  2.0 unx      639 b- defN 23-Mar-21 08:26 towhee/models/layers/spp.py
--rw-r--r--  2.0 unx     5347 b- defN 23-Mar-21 08:26 towhee/models/layers/swin_transformer_block3d.py
--rw-r--r--  2.0 unx     1964 b- defN 23-Mar-21 08:26 towhee/models/layers/temporal_cg_avgpool3d.py
--rw-r--r--  2.0 unx     1964 b- defN 23-Mar-21 08:26 towhee/models/layers/tf_avgpool3d.py
--rw-r--r--  2.0 unx     2351 b- defN 23-Mar-21 08:26 towhee/models/layers/time2vec.py
--rw-r--r--  2.0 unx     2054 b- defN 23-Mar-21 08:26 towhee/models/layers/transformer_encoder.py
--rw-r--r--  2.0 unx     3223 b- defN 23-Mar-21 08:26 towhee/models/layers/vision_transformer_basic_head.py
--rw-r--r--  2.0 unx     8064 b- defN 23-Mar-21 08:26 towhee/models/layers/window_attention.py
--rw-r--r--  2.0 unx     4765 b- defN 23-Mar-21 08:26 towhee/models/layers/window_attention3d.py
--rw-r--r--  2.0 unx     1202 b- defN 23-Mar-21 08:26 towhee/models/layers/activations/__init__.py
--rw-r--r--  2.0 unx     1312 b- defN 23-Mar-21 08:26 towhee/models/layers/activations/gelu.py
--rw-r--r--  2.0 unx     1393 b- defN 23-Mar-21 08:26 towhee/models/layers/activations/hardmish.py
--rw-r--r--  2.0 unx     1482 b- defN 23-Mar-21 08:26 towhee/models/layers/activations/hardsigmoid.py
--rw-r--r--  2.0 unx     1424 b- defN 23-Mar-21 08:26 towhee/models/layers/activations/hardswish.py
--rw-r--r--  2.0 unx     1328 b- defN 23-Mar-21 08:26 towhee/models/layers/activations/mish.py
--rw-r--r--  2.0 unx     1320 b- defN 23-Mar-21 08:26 towhee/models/layers/activations/prelu.py
--rw-r--r--  2.0 unx     1378 b- defN 23-Mar-21 08:26 towhee/models/layers/activations/sigmoid.py
--rw-r--r--  2.0 unx      815 b- defN 23-Mar-21 08:26 towhee/models/layers/activations/swiglu.py
--rw-r--r--  2.0 unx     1287 b- defN 23-Mar-21 08:26 towhee/models/layers/activations/swish.py
--rw-r--r--  2.0 unx     1290 b- defN 23-Mar-21 08:26 towhee/models/layers/activations/tanh.py
--rw-r--r--  2.0 unx       26 b- defN 23-Mar-21 08:26 towhee/models/lightning_dot/__init__.py
--rw-r--r--  2.0 unx     5800 b- defN 23-Mar-21 08:26 towhee/models/lightning_dot/bi_encoder.py
--rw-r--r--  2.0 unx      593 b- defN 23-Mar-21 08:26 towhee/models/loss/__init__.py
--rw-r--r--  2.0 unx     2467 b- defN 23-Mar-21 08:26 towhee/models/loss/focal_loss.py
--rw-r--r--  2.0 unx      616 b- defN 23-Mar-21 08:26 towhee/models/max_vit/__init__.py
--rw-r--r--  2.0 unx     1802 b- defN 23-Mar-21 08:26 towhee/models/max_vit/configs.py
--rw-r--r--  2.0 unx     8066 b- defN 23-Mar-21 08:26 towhee/models/max_vit/max_vit.py
--rw-r--r--  2.0 unx    11404 b- defN 23-Mar-21 08:26 towhee/models/max_vit/max_vit_block.py
--rw-r--r--  2.0 unx     5382 b- defN 23-Mar-21 08:26 towhee/models/max_vit/max_vit_utils.py
--rw-r--r--  2.0 unx      702 b- defN 23-Mar-21 08:26 towhee/models/mcprop/__init__.py
--rw-r--r--  2.0 unx     3053 b- defN 23-Mar-21 08:26 towhee/models/mcprop/depthaggregator.py
--rw-r--r--  2.0 unx     2388 b- defN 23-Mar-21 08:26 towhee/models/mcprop/featurefusion.py
--rw-r--r--  2.0 unx     1413 b- defN 23-Mar-21 08:26 towhee/models/mcprop/imageextractor.py
--rw-r--r--  2.0 unx     3070 b- defN 23-Mar-21 08:26 towhee/models/mcprop/loss.py
--rw-r--r--  2.0 unx     6296 b- defN 23-Mar-21 08:26 towhee/models/mcprop/matching.py
--rw-r--r--  2.0 unx     1399 b- defN 23-Mar-21 08:26 towhee/models/mcprop/textextractor.py
--rw-r--r--  2.0 unx     2029 b- defN 23-Mar-21 08:26 towhee/models/mcprop/transformerpooling.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/mdmmt/__init__.py
--rw-r--r--  2.0 unx    15492 b- defN 23-Mar-21 08:26 towhee/models/mdmmt/bert_mmt.py
--rw-r--r--  2.0 unx    13512 b- defN 23-Mar-21 08:26 towhee/models/mdmmt/mmt.py
--rw-r--r--  2.0 unx     1602 b- defN 23-Mar-21 08:26 towhee/models/metaformer/addpositionembed.py
--rw-r--r--  2.0 unx     2453 b- defN 23-Mar-21 08:26 towhee/models/metaformer/attention.py
--rw-r--r--  2.0 unx     1803 b- defN 23-Mar-21 08:26 towhee/models/metaformer/basicblocks.py
--rw-r--r--  2.0 unx    10807 b- defN 23-Mar-21 08:26 towhee/models/metaformer/metaformer.py
--rw-r--r--  2.0 unx     3109 b- defN 23-Mar-21 08:26 towhee/models/metaformer/metaformerblock.py
--rw-r--r--  2.0 unx     1684 b- defN 23-Mar-21 08:26 towhee/models/metaformer/spatialfc.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/movinet/__init__.py
--rw-r--r--  2.0 unx    27214 b- defN 23-Mar-21 08:26 towhee/models/movinet/config.py
--rw-r--r--  2.0 unx     8063 b- defN 23-Mar-21 08:26 towhee/models/movinet/movinet.py
--rw-r--r--  2.0 unx    13711 b- defN 23-Mar-21 08:26 towhee/models/movinet/movinet_block.py
--rw-r--r--  2.0 unx       21 b- defN 23-Mar-21 08:26 towhee/models/mpvit/__init__.py
--rw-r--r--  2.0 unx    28795 b- defN 23-Mar-21 08:26 towhee/models/mpvit/mpvit.py
--rw-r--r--  2.0 unx       47 b- defN 23-Mar-21 08:26 towhee/models/multiscale_vision_transformers/__init__.py
--rw-r--r--  2.0 unx     4509 b- defN 23-Mar-21 08:26 towhee/models/multiscale_vision_transformers/create_mvit.py
--rw-r--r--  2.0 unx    34605 b- defN 23-Mar-21 08:26 towhee/models/multiscale_vision_transformers/mvit.py
--rw-r--r--  2.0 unx      613 b- defN 23-Mar-21 08:26 towhee/models/nnfp/__init__.py
--rw-r--r--  2.0 unx     5235 b- defN 23-Mar-21 08:26 towhee/models/nnfp/nnfp.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/omnivore/__init__.py
--rw-r--r--  2.0 unx    15170 b- defN 23-Mar-21 08:26 towhee/models/omnivore/omnivore.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/perceiver/__init__.py
--rw-r--r--  2.0 unx     1446 b- defN 23-Mar-21 08:26 towhee/models/perceiver/create_cross_attention.py
--rw-r--r--  2.0 unx     1277 b- defN 23-Mar-21 08:26 towhee/models/perceiver/create_self_attention.py
--rw-r--r--  2.0 unx      979 b- defN 23-Mar-21 08:26 towhee/models/perceiver/create_self_attention_block.py
--rw-r--r--  2.0 unx     1233 b- defN 23-Mar-21 08:26 towhee/models/perceiver/cross_attention.py
--rw-r--r--  2.0 unx      344 b- defN 23-Mar-21 08:26 towhee/models/perceiver/mlp.py
--rw-r--r--  2.0 unx     1435 b- defN 23-Mar-21 08:26 towhee/models/perceiver/multi_head_attention.py
--rw-r--r--  2.0 unx      667 b- defN 23-Mar-21 08:26 towhee/models/perceiver/residual.py
--rw-r--r--  2.0 unx     1234 b- defN 23-Mar-21 08:26 towhee/models/perceiver/self_attention.py
--rw-r--r--  2.0 unx      434 b- defN 23-Mar-21 08:26 towhee/models/perceiver/sequential.py
--rw-r--r--  2.0 unx     1767 b- defN 23-Mar-21 08:26 towhee/models/poolformer/basic_blocks.py
--rw-r--r--  2.0 unx      978 b- defN 23-Mar-21 08:26 towhee/models/poolformer/groupnorm.py
--rw-r--r--  2.0 unx     1492 b- defN 23-Mar-21 08:26 towhee/models/poolformer/layernormchannel.py
--rw-r--r--  2.0 unx     1839 b- defN 23-Mar-21 08:26 towhee/models/poolformer/mlp.py
--rw-r--r--  2.0 unx     1558 b- defN 23-Mar-21 08:26 towhee/models/poolformer/patchembed.py
--rw-r--r--  2.0 unx    10490 b- defN 23-Mar-21 08:26 towhee/models/poolformer/poolformer.py
--rw-r--r--  2.0 unx     3034 b- defN 23-Mar-21 08:26 towhee/models/poolformer/poolformerblock.py
--rw-r--r--  2.0 unx     1122 b- defN 23-Mar-21 08:26 towhee/models/poolformer/pooling.py
--rw-r--r--  2.0 unx      661 b- defN 23-Mar-21 08:26 towhee/models/replknet/__init__.py
--rw-r--r--  2.0 unx     2718 b- defN 23-Mar-21 08:26 towhee/models/replknet/configs.py
--rw-r--r--  2.0 unx     8942 b- defN 23-Mar-21 08:26 towhee/models/replknet/replknet.py
--rw-r--r--  2.0 unx    10184 b- defN 23-Mar-21 08:26 towhee/models/replknet/utils.py
--rw-r--r--  2.0 unx      637 b- defN 23-Mar-21 08:26 towhee/models/repmlp/__init__.py
--rw-r--r--  2.0 unx     8448 b- defN 23-Mar-21 08:26 towhee/models/repmlp/blocks.py
--rw-r--r--  2.0 unx     2339 b- defN 23-Mar-21 08:26 towhee/models/repmlp/configs.py
--rw-r--r--  2.0 unx     7259 b- defN 23-Mar-21 08:26 towhee/models/repmlp/repmlp.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/retina_face/__init__.py
--rw-r--r--  2.0 unx     1503 b- defN 23-Mar-21 08:26 towhee/models/retina_face/configs.py
--rw-r--r--  2.0 unx     2552 b- defN 23-Mar-21 08:26 towhee/models/retina_face/heads.py
--rw-r--r--  2.0 unx     2057 b- defN 23-Mar-21 08:26 towhee/models/retina_face/mobilenet_v1.py
--rw-r--r--  2.0 unx     2194 b- defN 23-Mar-21 08:26 towhee/models/retina_face/prior_box.py
--rw-r--r--  2.0 unx     6531 b- defN 23-Mar-21 08:26 towhee/models/retina_face/retinaface.py
--rw-r--r--  2.0 unx     2350 b- defN 23-Mar-21 08:26 towhee/models/retina_face/retinaface_fpn.py
--rw-r--r--  2.0 unx     2140 b- defN 23-Mar-21 08:26 towhee/models/retina_face/ssh.py
--rw-r--r--  2.0 unx     5194 b- defN 23-Mar-21 08:26 towhee/models/retina_face/utils.py
--rw-r--r--  2.0 unx      672 b- defN 23-Mar-21 08:26 towhee/models/shunted_transformer/__init__.py
--rw-r--r--  2.0 unx     2134 b- defN 23-Mar-21 08:26 towhee/models/shunted_transformer/configs.py
--rw-r--r--  2.0 unx     5411 b- defN 23-Mar-21 08:26 towhee/models/shunted_transformer/shunted_transformer.py
--rw-r--r--  2.0 unx    10283 b- defN 23-Mar-21 08:26 towhee/models/shunted_transformer/utils.py
--rw-r--r--  2.0 unx      612 b- defN 23-Mar-21 08:26 towhee/models/svt/__init__.py
--rw-r--r--  2.0 unx     2925 b- defN 23-Mar-21 08:26 towhee/models/svt/svt.py
--rw-r--r--  2.0 unx     1375 b- defN 23-Mar-21 08:26 towhee/models/svt/svt_utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/swin_transformer/__init__.py
--rw-r--r--  2.0 unx     3558 b- defN 23-Mar-21 08:26 towhee/models/swin_transformer/basic_layer.py
--rw-r--r--  2.0 unx     9904 b- defN 23-Mar-21 08:26 towhee/models/swin_transformer/configs.py
--rw-r--r--  2.0 unx     7134 b- defN 23-Mar-21 08:26 towhee/models/swin_transformer/model.py
--rw-r--r--  2.0 unx     6114 b- defN 23-Mar-21 08:26 towhee/models/swin_transformer/swin_transformer_block.py
--rw-r--r--  2.0 unx      620 b- defN 23-Mar-21 08:26 towhee/models/timesformer/__init__.py
--rw-r--r--  2.0 unx     9537 b- defN 23-Mar-21 08:26 towhee/models/timesformer/timesformer.py
--rw-r--r--  2.0 unx     6249 b- defN 23-Mar-21 08:26 towhee/models/timesformer/timesformer_block.py
--rw-r--r--  2.0 unx     9090 b- defN 23-Mar-21 08:26 towhee/models/timesformer/timesformer_utils.py
--rw-r--r--  2.0 unx      638 b- defN 23-Mar-21 08:26 towhee/models/transrac/__init__.py
--rw-r--r--  2.0 unx     5724 b- defN 23-Mar-21 08:26 towhee/models/transrac/transrac.py
--rw-r--r--  2.0 unx     3378 b- defN 23-Mar-21 08:26 towhee/models/transrac/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/tsm/__init__.py
--rw-r--r--  2.0 unx     4683 b- defN 23-Mar-21 08:26 towhee/models/tsm/config.py
--rw-r--r--  2.0 unx     5394 b- defN 23-Mar-21 08:26 towhee/models/tsm/mobilenet_v2.py
--rw-r--r--  2.0 unx     5815 b- defN 23-Mar-21 08:26 towhee/models/tsm/temporal_shift.py
--rw-r--r--  2.0 unx    13942 b- defN 23-Mar-21 08:26 towhee/models/tsm/tsm.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/uniformer/__init__.py
--rw-r--r--  2.0 unx     4337 b- defN 23-Mar-21 08:26 towhee/models/uniformer/config.py
--rw-r--r--  2.0 unx    21976 b- defN 23-Mar-21 08:26 towhee/models/uniformer/uniformer.py
--rw-r--r--  2.0 unx       89 b- defN 23-Mar-21 08:26 towhee/models/utils/__init__.py
--rw-r--r--  2.0 unx     4633 b- defN 23-Mar-21 08:26 towhee/models/utils/audio_preprocess.py
--rw-r--r--  2.0 unx     1488 b- defN 23-Mar-21 08:26 towhee/models/utils/basic_ops.py
--rw-r--r--  2.0 unx      808 b- defN 23-Mar-21 08:26 towhee/models/utils/causal_module.py
--rw-r--r--  2.0 unx     4368 b- defN 23-Mar-21 08:26 towhee/models/utils/create_act.py
--rw-r--r--  2.0 unx     2159 b- defN 23-Mar-21 08:26 towhee/models/utils/create_conv2d.py
--rw-r--r--  2.0 unx     1328 b- defN 23-Mar-21 08:26 towhee/models/utils/create_conv2d_pad.py
--rw-r--r--  2.0 unx     1664 b- defN 23-Mar-21 08:26 towhee/models/utils/create_model.py
--rw-r--r--  2.0 unx     3929 b- defN 23-Mar-21 08:26 towhee/models/utils/create_resnet_basic_3d_module.py
--rw-r--r--  2.0 unx     3658 b- defN 23-Mar-21 08:26 towhee/models/utils/download.py
--rw-r--r--  2.0 unx     1208 b- defN 23-Mar-21 08:26 towhee/models/utils/fuse_bn.py
--rw-r--r--  2.0 unx     1153 b- defN 23-Mar-21 08:26 towhee/models/utils/gelu_ignore_parameters.py
--rw-r--r--  2.0 unx     1370 b- defN 23-Mar-21 08:26 towhee/models/utils/general_utils.py
--rw-r--r--  2.0 unx     1720 b- defN 23-Mar-21 08:26 towhee/models/utils/get_relative_position_index.py
--rw-r--r--  2.0 unx      958 b- defN 23-Mar-21 08:26 towhee/models/utils/get_window_size.py
--rw-r--r--  2.0 unx     2642 b- defN 23-Mar-21 08:26 towhee/models/utils/init_vit_weights.py
--rw-r--r--  2.0 unx     1439 b- defN 23-Mar-21 08:26 towhee/models/utils/pretrained_utils.py
--rw-r--r--  2.0 unx     1556 b- defN 23-Mar-21 08:26 towhee/models/utils/round_width.py
--rw-r--r--  2.0 unx     8505 b- defN 23-Mar-21 08:26 towhee/models/utils/video_transforms.py
--rw-r--r--  2.0 unx     3957 b- defN 23-Mar-21 08:26 towhee/models/utils/weight_init.py
--rw-r--r--  2.0 unx      527 b- defN 23-Mar-21 08:26 towhee/models/utils/window_partition.py
--rw-r--r--  2.0 unx      891 b- defN 23-Mar-21 08:26 towhee/models/utils/window_partition3d.py
--rw-r--r--  2.0 unx      690 b- defN 23-Mar-21 08:26 towhee/models/utils/window_reverse.py
--rw-r--r--  2.0 unx      887 b- defN 23-Mar-21 08:26 towhee/models/utils/window_reverse3d.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/vggish/__init__.py
--rw-r--r--  2.0 unx     2049 b- defN 23-Mar-21 08:26 towhee/models/vggish/torch_vggish.py
--rw-r--r--  2.0 unx      824 b- defN 23-Mar-21 08:26 towhee/models/video_swin_transformer/__init__.py
--rw-r--r--  2.0 unx     1643 b- defN 23-Mar-21 08:26 towhee/models/video_swin_transformer/compute_mask.py
--rw-r--r--  2.0 unx     3992 b- defN 23-Mar-21 08:26 towhee/models/video_swin_transformer/get_configs.py
--rw-r--r--  2.0 unx    14573 b- defN 23-Mar-21 08:26 towhee/models/video_swin_transformer/video_swin_transformer.py
--rw-r--r--  2.0 unx     3925 b- defN 23-Mar-21 08:26 towhee/models/video_swin_transformer/video_swin_transformer_block.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/violet/__init__.py
--rw-r--r--  2.0 unx     3916 b- defN 23-Mar-21 08:26 towhee/models/violet/violet.py
--rw-r--r--  2.0 unx     1007 b- defN 23-Mar-21 08:26 towhee/models/vis4mer/__init__.py
--rw-r--r--  2.0 unx     1471 b- defN 23-Mar-21 08:26 towhee/models/vis4mer/activation.py
--rw-r--r--  2.0 unx     1944 b- defN 23-Mar-21 08:26 towhee/models/vis4mer/get_initializer.py
--rw-r--r--  2.0 unx     2347 b- defN 23-Mar-21 08:26 towhee/models/vis4mer/linearactivation.py
--rw-r--r--  2.0 unx     1662 b- defN 23-Mar-21 08:26 towhee/models/vis4mer/transposelinear.py
--rw-r--r--  2.0 unx    10305 b- defN 23-Mar-21 08:26 towhee/models/vis4mer/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/visualization/__init__.py
--rw-r--r--  2.0 unx     8524 b- defN 23-Mar-21 08:26 towhee/models/visualization/clip_visualization.py
--rw-r--r--  2.0 unx     3193 b- defN 23-Mar-21 08:26 towhee/models/visualization/embedding_visualization.py
--rw-r--r--  2.0 unx     7065 b- defN 23-Mar-21 08:26 towhee/models/visualization/transformer_visualization.py
--rw-r--r--  2.0 unx      612 b- defN 23-Mar-21 08:26 towhee/models/vit/__init__.py
--rw-r--r--  2.0 unx    10295 b- defN 23-Mar-21 08:26 towhee/models/vit/vit.py
--rw-r--r--  2.0 unx     3878 b- defN 23-Mar-21 08:26 towhee/models/vit/vit_block.py
--rw-r--r--  2.0 unx     1868 b- defN 23-Mar-21 08:26 towhee/models/vit/vit_utils.py
--rw-r--r--  2.0 unx      617 b- defN 23-Mar-21 08:26 towhee/models/wave_vit/__init__.py
--rw-r--r--  2.0 unx    10014 b- defN 23-Mar-21 08:26 towhee/models/wave_vit/wave_vit.py
--rw-r--r--  2.0 unx     9731 b- defN 23-Mar-21 08:26 towhee/models/wave_vit/wave_vit_block.py
--rw-r--r--  2.0 unx     6361 b- defN 23-Mar-21 08:26 towhee/models/wave_vit/wave_vit_utils.py
--rw-r--r--  2.0 unx    11357 b- defN 23-May-04 08:05 towhee.models-1.0.0rc1.dist-info/LICENSE
--rw-r--r--  2.0 unx    17255 b- defN 23-May-04 08:05 towhee.models-1.0.0rc1.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-May-04 08:05 towhee.models-1.0.0rc1.dist-info/WHEEL
--rw-r--r--  2.0 unx       55 b- defN 23-May-04 08:05 towhee.models-1.0.0rc1.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        7 b- defN 23-May-04 08:05 towhee.models-1.0.0rc1.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    26320 b- defN 23-May-04 08:05 towhee.models-1.0.0rc1.dist-info/RECORD
-280 files, 2679966 bytes uncompressed, 1758198 bytes compressed:  34.4%
+Zip file size: 1808311 bytes, number of entries: 280
+-rw-r--r--  2.0 unx     8596 b- defN 22-Oct-08 02:27 towhee/models/README.md
+-rw-r--r--  2.0 unx     8570 b- defN 22-Oct-08 02:27 towhee/models/README_CN.md
+-rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/__init__.py
+-rw-r--r--  2.0 unx      678 b- defN 22-Aug-19 02:52 towhee/models/acar_net/__init__.py
+-rw-r--r--  2.0 unx    10305 b- defN 22-Oct-08 02:27 towhee/models/acar_net/backbone.py
+-rw-r--r--  2.0 unx     9275 b- defN 22-Oct-08 02:27 towhee/models/acar_net/head.py
+-rw-r--r--  2.0 unx     3613 b- defN 22-Aug-19 02:52 towhee/models/acar_net/model.py
+-rw-r--r--  2.0 unx     4102 b- defN 22-Oct-08 02:27 towhee/models/acar_net/neck.py
+-rw-r--r--  2.0 unx     1618 b- defN 22-Aug-19 02:52 towhee/models/acar_net/utils.py
+-rw-r--r--  2.0 unx      709 b- defN 22-Aug-19 02:52 towhee/models/action_clip/__init__.py
+-rw-r--r--  2.0 unx     4305 b- defN 22-Oct-08 02:27 towhee/models/action_clip/action_clip.py
+-rw-r--r--  2.0 unx     2039 b- defN 22-Oct-08 02:27 towhee/models/action_clip/action_clip_utils.py
+-rw-r--r--  2.0 unx     1804 b- defN 22-Oct-08 02:27 towhee/models/action_clip/text_prompt.py
+-rw-r--r--  2.0 unx     8805 b- defN 22-Oct-08 02:27 towhee/models/action_clip/visual_prompt.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/allinone/__init__.py
+-rw-r--r--  2.0 unx     1289 b- defN 22-Aug-19 02:52 towhee/models/allinone/allinone.py
+-rw-r--r--  2.0 unx      622 b- defN 22-Aug-19 02:52 towhee/models/bridgeformer/__init__.py
+-rw-r--r--  2.0 unx     3300 b- defN 22-Oct-08 02:27 towhee/models/bridgeformer/bridge_former.py
+-rw-r--r--  2.0 unx    10421 b- defN 22-Oct-08 02:27 towhee/models/bridgeformer/bridge_former_training.py
+-rw-r--r--  2.0 unx    16155 b- defN 22-Oct-08 02:27 towhee/models/bridgeformer/bridge_former_training_block.py
+-rw-r--r--  2.0 unx     1560 b- defN 22-Aug-19 02:52 towhee/models/clip/README.md
+-rw-r--r--  2.0 unx      639 b- defN 22-Aug-19 02:52 towhee/models/clip/__init__.py
+-rw-r--r--  2.0 unx    20896 b- defN 23-May-17 03:38 towhee/models/clip/auxilary.py
+-rw-r--r--  2.0 unx  1356917 b- defN 22-Aug-19 02:52 towhee/models/clip/bpe_simple_vocab_16e6.txt.gz
+-rw-r--r--  2.0 unx    30031 b- defN 22-Nov-24 11:16 towhee/models/clip/clip.py
+-rw-r--r--  2.0 unx     9935 b- defN 22-Nov-11 10:02 towhee/models/clip/clip_utils.py
+-rw-r--r--  2.0 unx     5618 b- defN 22-Oct-08 02:27 towhee/models/clip/simple_tokenizer.py
+-rw-r--r--  2.0 unx       46 b- defN 22-Aug-19 02:52 towhee/models/clip4clip/__init__.py
+-rw-r--r--  2.0 unx    10156 b- defN 23-Feb-06 09:20 towhee/models/clip4clip/clip4clip.py
+-rw-r--r--  2.0 unx     2605 b- defN 22-Aug-19 02:52 towhee/models/clip4clip/until_module.py
+-rw-r--r--  2.0 unx     2536 b- defN 23-May-17 03:38 towhee/models/clip4clip/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/coca/__init__.py
+-rw-r--r--  2.0 unx    11514 b- defN 22-Oct-08 02:27 towhee/models/coca/coca.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/coformer/__init__.py
+-rw-r--r--  2.0 unx     5167 b- defN 22-Oct-08 02:27 towhee/models/coformer/backbone.py
+-rw-r--r--  2.0 unx    10222 b- defN 22-Oct-08 02:27 towhee/models/coformer/coformer.py
+-rw-r--r--  2.0 unx    10008 b- defN 22-Aug-19 02:52 towhee/models/coformer/config.py
+-rw-r--r--  2.0 unx    15471 b- defN 22-Oct-08 02:27 towhee/models/coformer/transformer.py
+-rw-r--r--  2.0 unx     2919 b- defN 22-Aug-19 02:52 towhee/models/coformer/utils.py
+-rw-r--r--  2.0 unx      630 b- defN 22-Aug-19 02:52 towhee/models/collaborative_experts/__init__.py
+-rw-r--r--  2.0 unx    60484 b- defN 22-Oct-08 02:27 towhee/models/collaborative_experts/collaborative_experts.py
+-rw-r--r--  2.0 unx     3587 b- defN 22-Oct-08 02:27 towhee/models/collaborative_experts/net_vlad.py
+-rw-r--r--  2.0 unx     1612 b- defN 22-Aug-19 02:52 towhee/models/collaborative_experts/util.py
+-rw-r--r--  2.0 unx      661 b- defN 22-Oct-08 02:27 towhee/models/convnext/__init__.py
+-rw-r--r--  2.0 unx     3245 b- defN 22-Oct-08 02:27 towhee/models/convnext/configs.py
+-rw-r--r--  2.0 unx     4425 b- defN 22-Nov-24 11:16 towhee/models/convnext/convnext.py
+-rw-r--r--  2.0 unx     4107 b- defN 22-Oct-08 02:27 towhee/models/convnext/utils.py
+-rw-r--r--  2.0 unx      614 b- defN 22-Aug-19 02:52 towhee/models/cvnet/__init__.py
+-rw-r--r--  2.0 unx     4665 b- defN 22-Nov-24 11:16 towhee/models/cvnet/cvnet.py
+-rw-r--r--  2.0 unx     6640 b- defN 22-Aug-19 02:52 towhee/models/cvnet/cvnet_block.py
+-rw-r--r--  2.0 unx     2439 b- defN 22-Oct-08 02:27 towhee/models/cvnet/cvnet_utils.py
+-rw-r--r--  2.0 unx     8106 b- defN 22-Oct-08 02:27 towhee/models/cvnet/resnet.py
+-rw-r--r--  2.0 unx       19 b- defN 22-Aug-19 02:52 towhee/models/drl/__init__.py
+-rw-r--r--  2.0 unx    35156 b- defN 23-May-17 03:38 towhee/models/drl/drl.py
+-rw-r--r--  2.0 unx     8633 b- defN 23-May-17 03:38 towhee/models/drl/module_cross.py
+-rw-r--r--  2.0 unx     4891 b- defN 22-Aug-19 02:52 towhee/models/drl/until_module.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/embedding/__init__.py
+-rw-r--r--  2.0 unx     2061 b- defN 22-Aug-19 02:52 towhee/models/embedding/embedding_extractor.py
+-rw-r--r--  2.0 unx      623 b- defN 22-Aug-19 02:52 towhee/models/frozen_in_time/__init__.py
+-rw-r--r--  2.0 unx    14921 b- defN 22-Oct-08 02:27 towhee/models/frozen_in_time/frozen_in_time.py
+-rw-r--r--  2.0 unx     2394 b- defN 22-Aug-19 02:52 towhee/models/frozen_in_time/frozen_utils.py
+-rw-r--r--  2.0 unx    15910 b- defN 22-Oct-08 02:27 towhee/models/frozen_in_time/frozen_video_transformer.py
+-rw-r--r--  2.0 unx      659 b- defN 22-Oct-08 02:27 towhee/models/hornet/__init__.py
+-rw-r--r--  2.0 unx     4309 b- defN 22-Oct-08 02:27 towhee/models/hornet/configs.py
+-rw-r--r--  2.0 unx     5036 b- defN 22-Nov-24 11:16 towhee/models/hornet/hornet.py
+-rw-r--r--  2.0 unx     5983 b- defN 22-Oct-08 02:27 towhee/models/hornet/utils.py
+-rw-r--r--  2.0 unx      612 b- defN 22-Oct-08 02:27 towhee/models/isc/__init__.py
+-rw-r--r--  2.0 unx     3470 b- defN 23-Feb-06 09:20 towhee/models/isc/isc.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/layers/__init__.py
+-rw-r--r--  2.0 unx     1638 b- defN 22-Oct-08 02:27 towhee/models/layers/aspp.py
+-rw-r--r--  2.0 unx     6693 b- defN 22-Aug-19 02:52 towhee/models/layers/attention.py
+-rw-r--r--  2.0 unx     5783 b- defN 22-Aug-19 02:52 towhee/models/layers/cond_conv2d.py
+-rw-r--r--  2.0 unx     2706 b- defN 22-Aug-19 02:52 towhee/models/layers/conv2d_same.py
+-rw-r--r--  2.0 unx     4180 b- defN 22-Aug-19 02:52 towhee/models/layers/conv2d_separable.py
+-rw-r--r--  2.0 unx     4354 b- defN 22-Oct-08 02:27 towhee/models/layers/conv4d.py
+-rw-r--r--  2.0 unx     5125 b- defN 22-Oct-12 06:36 towhee/models/layers/conv_bn_activation.py
+-rw-r--r--  2.0 unx     1918 b- defN 22-Oct-08 02:27 towhee/models/layers/convmlp.py
+-rw-r--r--  2.0 unx     4369 b- defN 22-Aug-19 02:52 towhee/models/layers/cross_attention.py
+-rw-r--r--  2.0 unx     5964 b- defN 22-Aug-19 02:52 towhee/models/layers/dropblock2d.py
+-rw-r--r--  2.0 unx     2200 b- defN 22-Aug-19 02:52 towhee/models/layers/droppath.py
+-rw-r--r--  2.0 unx     2175 b- defN 22-Oct-08 02:27 towhee/models/layers/ffn.py
+-rw-r--r--  2.0 unx     2052 b- defN 22-Oct-08 02:27 towhee/models/layers/gatedmlp.py
+-rw-r--r--  2.0 unx    10670 b- defN 22-Aug-19 02:52 towhee/models/layers/layers_with_relprop.py
+-rw-r--r--  2.0 unx     4347 b- defN 23-Jun-09 07:02 towhee/models/layers/mbconv.py
+-rw-r--r--  2.0 unx     2971 b- defN 22-Aug-19 02:52 towhee/models/layers/mixed_conv2d.py
+-rw-r--r--  2.0 unx     2566 b- defN 22-Aug-19 02:52 towhee/models/layers/mlp.py
+-rw-r--r--  2.0 unx    13234 b- defN 22-Oct-08 02:27 towhee/models/layers/multi_scale_attention.py
+-rw-r--r--  2.0 unx     6651 b- defN 22-Oct-08 02:27 towhee/models/layers/multi_scale_transformer_block.py
+-rw-r--r--  2.0 unx     3312 b- defN 22-Oct-08 02:27 towhee/models/layers/netvlad.py
+-rw-r--r--  2.0 unx     6047 b- defN 23-May-17 03:38 towhee/models/layers/non_local.py
+-rw-r--r--  2.0 unx     5759 b- defN 22-Aug-19 02:52 towhee/models/layers/padding_functions.py
+-rw-r--r--  2.0 unx     4015 b- defN 22-Aug-19 02:52 towhee/models/layers/patch_embed2d.py
+-rw-r--r--  2.0 unx     3620 b- defN 23-May-17 03:38 towhee/models/layers/patch_embed3d.py
+-rw-r--r--  2.0 unx     2458 b- defN 22-Oct-08 02:27 towhee/models/layers/patch_merging.py
+-rw-r--r--  2.0 unx     3009 b- defN 23-May-17 03:38 towhee/models/layers/patch_merging3d.py
+-rw-r--r--  2.0 unx     3343 b- defN 22-Oct-08 02:27 towhee/models/layers/pool_attention.py
+-rw-r--r--  2.0 unx     3084 b- defN 22-Aug-19 02:52 towhee/models/layers/position_encoding.py
+-rw-r--r--  2.0 unx     4749 b- defN 22-Oct-08 02:27 towhee/models/layers/relative_self_attention.py
+-rw-r--r--  2.0 unx     2028 b- defN 22-Aug-19 02:52 towhee/models/layers/resnet_basic_3d_module.py
+-rw-r--r--  2.0 unx     3273 b- defN 22-Aug-19 02:52 towhee/models/layers/sam.py
+-rw-r--r--  2.0 unx     1500 b- defN 22-Aug-19 02:52 towhee/models/layers/sequence_pool.py
+-rw-r--r--  2.0 unx     3663 b- defN 22-Aug-19 02:52 towhee/models/layers/spatial_temporal_cls_positional_encoding.py
+-rw-r--r--  2.0 unx      639 b- defN 22-Aug-19 02:52 towhee/models/layers/spp.py
+-rw-r--r--  2.0 unx     5934 b- defN 23-May-17 03:38 towhee/models/layers/swin_transformer_block3d.py
+-rw-r--r--  2.0 unx     1964 b- defN 22-Aug-19 02:52 towhee/models/layers/temporal_cg_avgpool3d.py
+-rw-r--r--  2.0 unx     1964 b- defN 22-Aug-19 02:52 towhee/models/layers/tf_avgpool3d.py
+-rw-r--r--  2.0 unx     2351 b- defN 22-Aug-19 02:52 towhee/models/layers/time2vec.py
+-rw-r--r--  2.0 unx     2054 b- defN 22-Aug-19 02:52 towhee/models/layers/transformer_encoder.py
+-rw-r--r--  2.0 unx     3223 b- defN 22-Aug-19 02:52 towhee/models/layers/vision_transformer_basic_head.py
+-rw-r--r--  2.0 unx     8064 b- defN 22-Oct-08 02:27 towhee/models/layers/window_attention.py
+-rw-r--r--  2.0 unx     5352 b- defN 23-May-17 03:38 towhee/models/layers/window_attention3d.py
+-rw-r--r--  2.0 unx     1202 b- defN 22-Aug-19 02:52 towhee/models/layers/activations/__init__.py
+-rw-r--r--  2.0 unx     1312 b- defN 22-Aug-19 02:52 towhee/models/layers/activations/gelu.py
+-rw-r--r--  2.0 unx     1393 b- defN 22-Aug-19 02:52 towhee/models/layers/activations/hardmish.py
+-rw-r--r--  2.0 unx     1482 b- defN 22-Aug-19 02:52 towhee/models/layers/activations/hardsigmoid.py
+-rw-r--r--  2.0 unx     1424 b- defN 22-Aug-19 02:52 towhee/models/layers/activations/hardswish.py
+-rw-r--r--  2.0 unx     1328 b- defN 22-Aug-19 02:52 towhee/models/layers/activations/mish.py
+-rw-r--r--  2.0 unx     1320 b- defN 22-Aug-19 02:52 towhee/models/layers/activations/prelu.py
+-rw-r--r--  2.0 unx     1378 b- defN 22-Aug-19 02:52 towhee/models/layers/activations/sigmoid.py
+-rw-r--r--  2.0 unx      815 b- defN 22-Aug-19 02:52 towhee/models/layers/activations/swiglu.py
+-rw-r--r--  2.0 unx     1287 b- defN 22-Aug-19 02:52 towhee/models/layers/activations/swish.py
+-rw-r--r--  2.0 unx     1290 b- defN 22-Aug-19 02:52 towhee/models/layers/activations/tanh.py
+-rw-r--r--  2.0 unx       26 b- defN 22-Aug-19 02:52 towhee/models/lightning_dot/__init__.py
+-rw-r--r--  2.0 unx     5800 b- defN 22-Aug-19 02:52 towhee/models/lightning_dot/bi_encoder.py
+-rw-r--r--  2.0 unx      593 b- defN 22-Aug-19 02:52 towhee/models/loss/__init__.py
+-rw-r--r--  2.0 unx     2467 b- defN 22-Aug-19 02:52 towhee/models/loss/focal_loss.py
+-rw-r--r--  2.0 unx      616 b- defN 22-Aug-19 02:52 towhee/models/max_vit/__init__.py
+-rw-r--r--  2.0 unx     1802 b- defN 22-Aug-19 02:52 towhee/models/max_vit/configs.py
+-rw-r--r--  2.0 unx     8066 b- defN 22-Nov-24 11:16 towhee/models/max_vit/max_vit.py
+-rw-r--r--  2.0 unx    11404 b- defN 22-Oct-08 02:27 towhee/models/max_vit/max_vit_block.py
+-rw-r--r--  2.0 unx     5382 b- defN 22-Aug-19 02:52 towhee/models/max_vit/max_vit_utils.py
+-rw-r--r--  2.0 unx      702 b- defN 22-Oct-08 02:27 towhee/models/mcprop/__init__.py
+-rw-r--r--  2.0 unx     3053 b- defN 22-Oct-08 02:27 towhee/models/mcprop/depthaggregator.py
+-rw-r--r--  2.0 unx     2388 b- defN 22-Oct-08 02:27 towhee/models/mcprop/featurefusion.py
+-rw-r--r--  2.0 unx     1413 b- defN 22-Oct-08 02:27 towhee/models/mcprop/imageextractor.py
+-rw-r--r--  2.0 unx     3070 b- defN 22-Oct-08 02:27 towhee/models/mcprop/loss.py
+-rw-r--r--  2.0 unx     6296 b- defN 22-Oct-08 02:27 towhee/models/mcprop/matching.py
+-rw-r--r--  2.0 unx     1399 b- defN 22-Oct-08 02:27 towhee/models/mcprop/textextractor.py
+-rw-r--r--  2.0 unx     2029 b- defN 22-Oct-08 02:27 towhee/models/mcprop/transformerpooling.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/mdmmt/__init__.py
+-rw-r--r--  2.0 unx    15492 b- defN 22-Aug-19 02:52 towhee/models/mdmmt/bert_mmt.py
+-rw-r--r--  2.0 unx    13512 b- defN 22-Oct-08 02:27 towhee/models/mdmmt/mmt.py
+-rw-r--r--  2.0 unx     1602 b- defN 22-Oct-08 02:27 towhee/models/metaformer/addpositionembed.py
+-rw-r--r--  2.0 unx     2453 b- defN 22-Oct-08 02:27 towhee/models/metaformer/attention.py
+-rw-r--r--  2.0 unx     1803 b- defN 22-Oct-08 02:27 towhee/models/metaformer/basicblocks.py
+-rw-r--r--  2.0 unx    10807 b- defN 22-Oct-08 02:27 towhee/models/metaformer/metaformer.py
+-rw-r--r--  2.0 unx     3109 b- defN 22-Oct-08 02:27 towhee/models/metaformer/metaformerblock.py
+-rw-r--r--  2.0 unx     1684 b- defN 22-Oct-08 02:27 towhee/models/metaformer/spatialfc.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/movinet/__init__.py
+-rw-r--r--  2.0 unx    27214 b- defN 22-Aug-19 02:52 towhee/models/movinet/config.py
+-rw-r--r--  2.0 unx     8063 b- defN 22-Aug-19 02:52 towhee/models/movinet/movinet.py
+-rw-r--r--  2.0 unx    13711 b- defN 22-Aug-19 02:52 towhee/models/movinet/movinet_block.py
+-rw-r--r--  2.0 unx       21 b- defN 22-Aug-19 02:52 towhee/models/mpvit/__init__.py
+-rw-r--r--  2.0 unx    28795 b- defN 22-Oct-08 02:27 towhee/models/mpvit/mpvit.py
+-rw-r--r--  2.0 unx       47 b- defN 22-Aug-19 02:52 towhee/models/multiscale_vision_transformers/__init__.py
+-rw-r--r--  2.0 unx     4509 b- defN 22-Aug-19 02:52 towhee/models/multiscale_vision_transformers/create_mvit.py
+-rw-r--r--  2.0 unx    34605 b- defN 22-Oct-08 02:27 towhee/models/multiscale_vision_transformers/mvit.py
+-rw-r--r--  2.0 unx      613 b- defN 22-Aug-19 02:52 towhee/models/nnfp/__init__.py
+-rw-r--r--  2.0 unx     5235 b- defN 22-Aug-19 02:52 towhee/models/nnfp/nnfp.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/omnivore/__init__.py
+-rw-r--r--  2.0 unx    15170 b- defN 22-Aug-19 02:52 towhee/models/omnivore/omnivore.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/perceiver/__init__.py
+-rw-r--r--  2.0 unx     1446 b- defN 22-Aug-19 02:52 towhee/models/perceiver/create_cross_attention.py
+-rw-r--r--  2.0 unx     1277 b- defN 22-Aug-19 02:52 towhee/models/perceiver/create_self_attention.py
+-rw-r--r--  2.0 unx      979 b- defN 22-Aug-19 02:52 towhee/models/perceiver/create_self_attention_block.py
+-rw-r--r--  2.0 unx     1233 b- defN 22-Aug-19 02:52 towhee/models/perceiver/cross_attention.py
+-rw-r--r--  2.0 unx      344 b- defN 22-Aug-19 02:52 towhee/models/perceiver/mlp.py
+-rw-r--r--  2.0 unx     1435 b- defN 22-Aug-19 02:52 towhee/models/perceiver/multi_head_attention.py
+-rw-r--r--  2.0 unx      667 b- defN 22-Aug-19 02:52 towhee/models/perceiver/residual.py
+-rw-r--r--  2.0 unx     1234 b- defN 22-Aug-19 02:52 towhee/models/perceiver/self_attention.py
+-rw-r--r--  2.0 unx      434 b- defN 22-Aug-19 02:52 towhee/models/perceiver/sequential.py
+-rw-r--r--  2.0 unx     1767 b- defN 22-Oct-08 02:27 towhee/models/poolformer/basic_blocks.py
+-rw-r--r--  2.0 unx      978 b- defN 22-Oct-08 02:27 towhee/models/poolformer/groupnorm.py
+-rw-r--r--  2.0 unx     1492 b- defN 22-Oct-08 02:27 towhee/models/poolformer/layernormchannel.py
+-rw-r--r--  2.0 unx     1839 b- defN 22-Oct-08 02:27 towhee/models/poolformer/mlp.py
+-rw-r--r--  2.0 unx     1558 b- defN 22-Oct-08 02:27 towhee/models/poolformer/patchembed.py
+-rw-r--r--  2.0 unx    10490 b- defN 22-Oct-08 02:27 towhee/models/poolformer/poolformer.py
+-rw-r--r--  2.0 unx     3034 b- defN 22-Oct-08 02:27 towhee/models/poolformer/poolformerblock.py
+-rw-r--r--  2.0 unx     1122 b- defN 22-Oct-08 02:27 towhee/models/poolformer/pooling.py
+-rw-r--r--  2.0 unx      661 b- defN 22-Oct-18 06:57 towhee/models/replknet/__init__.py
+-rw-r--r--  2.0 unx     2718 b- defN 22-Nov-24 11:16 towhee/models/replknet/configs.py
+-rw-r--r--  2.0 unx     8942 b- defN 22-Nov-24 11:16 towhee/models/replknet/replknet.py
+-rw-r--r--  2.0 unx    10184 b- defN 22-Oct-18 06:57 towhee/models/replknet/utils.py
+-rw-r--r--  2.0 unx      637 b- defN 22-Aug-19 02:52 towhee/models/repmlp/__init__.py
+-rw-r--r--  2.0 unx     8448 b- defN 22-Oct-08 02:27 towhee/models/repmlp/blocks.py
+-rw-r--r--  2.0 unx     2339 b- defN 22-Aug-19 02:52 towhee/models/repmlp/configs.py
+-rw-r--r--  2.0 unx     7259 b- defN 22-Nov-24 11:16 towhee/models/repmlp/repmlp.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/retina_face/__init__.py
+-rw-r--r--  2.0 unx     2633 b- defN 23-May-17 03:38 towhee/models/retina_face/configs.py
+-rw-r--r--  2.0 unx     3052 b- defN 23-May-17 03:38 towhee/models/retina_face/heads.py
+-rw-r--r--  2.0 unx     2557 b- defN 23-May-17 03:38 towhee/models/retina_face/mobilenet_v1.py
+-rw-r--r--  2.0 unx     2694 b- defN 23-May-17 03:38 towhee/models/retina_face/prior_box.py
+-rw-r--r--  2.0 unx     7031 b- defN 23-May-17 03:38 towhee/models/retina_face/retinaface.py
+-rw-r--r--  2.0 unx     2850 b- defN 23-May-17 03:38 towhee/models/retina_face/retinaface_fpn.py
+-rw-r--r--  2.0 unx     2640 b- defN 23-May-17 03:38 towhee/models/retina_face/ssh.py
+-rw-r--r--  2.0 unx     5694 b- defN 23-May-17 03:38 towhee/models/retina_face/utils.py
+-rw-r--r--  2.0 unx      672 b- defN 22-Nov-07 10:51 towhee/models/shunted_transformer/__init__.py
+-rw-r--r--  2.0 unx     2134 b- defN 22-Nov-07 10:51 towhee/models/shunted_transformer/configs.py
+-rw-r--r--  2.0 unx     5411 b- defN 22-Nov-24 11:16 towhee/models/shunted_transformer/shunted_transformer.py
+-rw-r--r--  2.0 unx    10283 b- defN 22-Nov-07 10:51 towhee/models/shunted_transformer/utils.py
+-rw-r--r--  2.0 unx      612 b- defN 22-Aug-19 02:52 towhee/models/svt/__init__.py
+-rw-r--r--  2.0 unx     2925 b- defN 22-Aug-19 02:52 towhee/models/svt/svt.py
+-rw-r--r--  2.0 unx     1375 b- defN 22-Aug-19 02:52 towhee/models/svt/svt_utils.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/swin_transformer/__init__.py
+-rw-r--r--  2.0 unx     3558 b- defN 22-Aug-19 02:52 towhee/models/swin_transformer/basic_layer.py
+-rw-r--r--  2.0 unx    10536 b- defN 23-May-17 03:38 towhee/models/swin_transformer/configs.py
+-rw-r--r--  2.0 unx     7134 b- defN 22-Aug-19 02:52 towhee/models/swin_transformer/model.py
+-rw-r--r--  2.0 unx     6114 b- defN 22-Aug-19 02:52 towhee/models/swin_transformer/swin_transformer_block.py
+-rw-r--r--  2.0 unx      620 b- defN 22-Aug-19 02:52 towhee/models/timesformer/__init__.py
+-rw-r--r--  2.0 unx     9537 b- defN 22-Oct-08 02:27 towhee/models/timesformer/timesformer.py
+-rw-r--r--  2.0 unx     6249 b- defN 22-Aug-19 02:52 towhee/models/timesformer/timesformer_block.py
+-rw-r--r--  2.0 unx     9090 b- defN 22-Aug-19 02:52 towhee/models/timesformer/timesformer_utils.py
+-rw-r--r--  2.0 unx      638 b- defN 22-Aug-19 02:52 towhee/models/transrac/__init__.py
+-rw-r--r--  2.0 unx     5724 b- defN 22-Oct-08 02:27 towhee/models/transrac/transrac.py
+-rw-r--r--  2.0 unx     3378 b- defN 22-Aug-19 02:52 towhee/models/transrac/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/tsm/__init__.py
+-rw-r--r--  2.0 unx     4683 b- defN 22-Aug-19 02:52 towhee/models/tsm/config.py
+-rw-r--r--  2.0 unx     5394 b- defN 22-Aug-19 02:52 towhee/models/tsm/mobilenet_v2.py
+-rw-r--r--  2.0 unx     5815 b- defN 22-Aug-19 02:52 towhee/models/tsm/temporal_shift.py
+-rw-r--r--  2.0 unx    13942 b- defN 22-Aug-19 02:52 towhee/models/tsm/tsm.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/uniformer/__init__.py
+-rw-r--r--  2.0 unx     4337 b- defN 22-Aug-19 02:52 towhee/models/uniformer/config.py
+-rw-r--r--  2.0 unx    21976 b- defN 22-Aug-19 02:52 towhee/models/uniformer/uniformer.py
+-rw-r--r--  2.0 unx       89 b- defN 22-Nov-24 11:16 towhee/models/utils/__init__.py
+-rw-r--r--  2.0 unx     4633 b- defN 22-Aug-19 02:52 towhee/models/utils/audio_preprocess.py
+-rw-r--r--  2.0 unx     1488 b- defN 22-Aug-19 02:52 towhee/models/utils/basic_ops.py
+-rw-r--r--  2.0 unx      808 b- defN 22-Aug-19 02:52 towhee/models/utils/causal_module.py
+-rw-r--r--  2.0 unx     4368 b- defN 22-Aug-19 02:52 towhee/models/utils/create_act.py
+-rw-r--r--  2.0 unx     2159 b- defN 22-Aug-19 02:52 towhee/models/utils/create_conv2d.py
+-rw-r--r--  2.0 unx     1328 b- defN 22-Aug-19 02:52 towhee/models/utils/create_conv2d_pad.py
+-rw-r--r--  2.0 unx     1664 b- defN 22-Nov-24 11:16 towhee/models/utils/create_model.py
+-rw-r--r--  2.0 unx     3929 b- defN 22-Aug-19 02:52 towhee/models/utils/create_resnet_basic_3d_module.py
+-rw-r--r--  2.0 unx     3658 b- defN 22-Nov-24 11:16 towhee/models/utils/download.py
+-rw-r--r--  2.0 unx     1208 b- defN 22-Aug-19 02:52 towhee/models/utils/fuse_bn.py
+-rw-r--r--  2.0 unx     1153 b- defN 22-Aug-19 02:52 towhee/models/utils/gelu_ignore_parameters.py
+-rw-r--r--  2.0 unx     1370 b- defN 22-Aug-19 02:52 towhee/models/utils/general_utils.py
+-rw-r--r--  2.0 unx     1720 b- defN 22-Aug-19 02:52 towhee/models/utils/get_relative_position_index.py
+-rw-r--r--  2.0 unx     1545 b- defN 23-May-17 03:38 towhee/models/utils/get_window_size.py
+-rw-r--r--  2.0 unx     2642 b- defN 22-Aug-19 02:52 towhee/models/utils/init_vit_weights.py
+-rw-r--r--  2.0 unx     2032 b- defN 23-May-17 03:38 towhee/models/utils/pretrained_utils.py
+-rw-r--r--  2.0 unx     1556 b- defN 22-Aug-19 02:52 towhee/models/utils/round_width.py
+-rw-r--r--  2.0 unx     8505 b- defN 22-Aug-19 02:52 towhee/models/utils/video_transforms.py
+-rw-r--r--  2.0 unx     3957 b- defN 22-Aug-19 02:52 towhee/models/utils/weight_init.py
+-rw-r--r--  2.0 unx     1114 b- defN 23-May-17 03:38 towhee/models/utils/window_partition.py
+-rw-r--r--  2.0 unx     1478 b- defN 23-May-17 03:38 towhee/models/utils/window_partition3d.py
+-rw-r--r--  2.0 unx     1277 b- defN 23-May-17 03:38 towhee/models/utils/window_reverse.py
+-rw-r--r--  2.0 unx     1474 b- defN 23-May-17 03:38 towhee/models/utils/window_reverse3d.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/vggish/__init__.py
+-rw-r--r--  2.0 unx     2049 b- defN 22-Aug-19 02:52 towhee/models/vggish/torch_vggish.py
+-rw-r--r--  2.0 unx      824 b- defN 22-Aug-19 02:52 towhee/models/video_swin_transformer/__init__.py
+-rw-r--r--  2.0 unx     1643 b- defN 22-Aug-19 02:52 towhee/models/video_swin_transformer/compute_mask.py
+-rw-r--r--  2.0 unx     3992 b- defN 22-Aug-19 02:52 towhee/models/video_swin_transformer/get_configs.py
+-rw-r--r--  2.0 unx    14573 b- defN 22-Aug-19 02:52 towhee/models/video_swin_transformer/video_swin_transformer.py
+-rw-r--r--  2.0 unx     3925 b- defN 22-Aug-19 02:52 towhee/models/video_swin_transformer/video_swin_transformer_block.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/violet/__init__.py
+-rw-r--r--  2.0 unx     3916 b- defN 22-Aug-19 02:52 towhee/models/violet/violet.py
+-rw-r--r--  2.0 unx     1007 b- defN 22-Oct-28 02:52 towhee/models/vis4mer/__init__.py
+-rw-r--r--  2.0 unx     1471 b- defN 22-Oct-08 02:27 towhee/models/vis4mer/activation.py
+-rw-r--r--  2.0 unx     1944 b- defN 22-Oct-12 06:36 towhee/models/vis4mer/get_initializer.py
+-rw-r--r--  2.0 unx     2347 b- defN 22-Oct-12 06:36 towhee/models/vis4mer/linearactivation.py
+-rw-r--r--  2.0 unx     1662 b- defN 22-Oct-08 02:27 towhee/models/vis4mer/transposelinear.py
+-rw-r--r--  2.0 unx    10305 b- defN 22-Oct-28 02:52 towhee/models/vis4mer/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/visualization/__init__.py
+-rw-r--r--  2.0 unx     8524 b- defN 22-Oct-08 02:27 towhee/models/visualization/clip_visualization.py
+-rw-r--r--  2.0 unx     3193 b- defN 22-Oct-08 02:27 towhee/models/visualization/embedding_visualization.py
+-rw-r--r--  2.0 unx     7065 b- defN 22-Oct-08 02:27 towhee/models/visualization/transformer_visualization.py
+-rw-r--r--  2.0 unx      612 b- defN 22-Aug-19 02:52 towhee/models/vit/__init__.py
+-rw-r--r--  2.0 unx    10295 b- defN 22-Oct-08 02:27 towhee/models/vit/vit.py
+-rw-r--r--  2.0 unx     3878 b- defN 22-Oct-08 02:27 towhee/models/vit/vit_block.py
+-rw-r--r--  2.0 unx     1868 b- defN 22-Aug-19 02:52 towhee/models/vit/vit_utils.py
+-rw-r--r--  2.0 unx      617 b- defN 22-Aug-19 02:52 towhee/models/wave_vit/__init__.py
+-rw-r--r--  2.0 unx    10014 b- defN 22-Oct-08 02:27 towhee/models/wave_vit/wave_vit.py
+-rw-r--r--  2.0 unx     9731 b- defN 22-Aug-19 02:52 towhee/models/wave_vit/wave_vit_block.py
+-rw-r--r--  2.0 unx     6361 b- defN 22-Aug-19 02:52 towhee/models/wave_vit/wave_vit_utils.py
+-rw-r--r--  2.0 unx    11357 b- defN 23-Jun-09 07:06 towhee.models-1.1.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx    13974 b- defN 23-Jun-09 07:06 towhee.models-1.1.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jun-09 07:06 towhee.models-1.1.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       55 b- defN 23-Jun-09 07:06 towhee.models-1.1.0.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx        7 b- defN 23-Jun-09 07:06 towhee.models-1.1.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    26308 b- defN 23-Jun-09 07:06 towhee.models-1.1.0.dist-info/RECORD
+280 files, 2691090 bytes uncompressed, 1766083 bytes compressed:  34.4%
```

## zipnote {}

```diff
@@ -816,26 +816,26 @@
 
 Filename: towhee/models/wave_vit/wave_vit_block.py
 Comment: 
 
 Filename: towhee/models/wave_vit/wave_vit_utils.py
 Comment: 
 
-Filename: towhee.models-1.0.0rc1.dist-info/LICENSE
+Filename: towhee.models-1.1.0.dist-info/LICENSE
 Comment: 
 
-Filename: towhee.models-1.0.0rc1.dist-info/METADATA
+Filename: towhee.models-1.1.0.dist-info/METADATA
 Comment: 
 
-Filename: towhee.models-1.0.0rc1.dist-info/WHEEL
+Filename: towhee.models-1.1.0.dist-info/WHEEL
 Comment: 
 
-Filename: towhee.models-1.0.0rc1.dist-info/entry_points.txt
+Filename: towhee.models-1.1.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: towhee.models-1.0.0rc1.dist-info/top_level.txt
+Filename: towhee.models-1.1.0.dist-info/top_level.txt
 Comment: 
 
-Filename: towhee.models-1.0.0rc1.dist-info/RECORD
+Filename: towhee.models-1.1.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## towhee/models/clip/auxilary.py

```diff
@@ -1,7 +1,23 @@
+# Built on top of the original implementation at https://github.com/openai/CLIP
+#
+# Modifications by Copyright 2022 Zilliz. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 import warnings
 from typing import Tuple, Optional
 
 import torch
 from torch import Tensor
 from torch.nn.init import xavier_uniform_
 from torch.nn.init import constant_
```

## towhee/models/clip4clip/utils.py

```diff
@@ -1,7 +1,23 @@
+# Built on top of the original implementation at https://github.com/ArrowLuo/CLIP4Clip
+#
+# Modifications by Copyright 2022 Zilliz. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 import regex as re
 import numpy as np
 from typing import List
 from towhee.models.clip.simple_tokenizer import SimpleTokenizer, whitespace_clean, basic_clean
 
 
 def tokenize(text: str) -> List:
```

## towhee/models/drl/drl.py

```diff
@@ -1,9 +1,21 @@
 # original code from https://github.com/foolwood/ddRL/blob/main/tvr/models/modeling.py
-# modified by Zilliz
+# Modifications by Copyright 2022 Zilliz. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from collections import OrderedDict
 from typing import Optional
 from types import SimpleNamespace
 import torch
 import logging
 from torch import nn
```

## towhee/models/drl/module_cross.py

```diff
@@ -1,7 +1,22 @@
+# original code from https://github.com/foolwood/ddRL/blob/main/tvr/models/modeling.py
+# Modifications by Copyright 2022 Zilliz. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
 import logging
 
 import torch
```

## towhee/models/layers/mbconv.py

```diff
@@ -13,16 +13,17 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from typing import Type
 import torch
+from timm.layers import SqueezeExcite
+from timm.models._efficientnet_blocks import DepthwiseSeparableConv
 from torch import nn
-from timm.models.efficientnet_blocks import SqueezeExcite, DepthwiseSeparableConv
 from towhee.models.layers.droppath import DropPath
 from towhee.models.utils.gelu_ignore_parameters import gelu_ignore_parameters
 
 
 class MBConv(nn.Module):
     """ MBConv block as described in: https://arxiv.org/pdf/2204.01697.pdf.
         Without downsampling:
@@ -69,15 +70,15 @@
         if act_layer == nn.GELU:
             act_layer = gelu_ignore_parameters
         # Make main path
         self.main_path = nn.Sequential(
             norm_layer(in_channels),
             DepthwiseSeparableConv(in_chs=in_channels, out_chs=out_channels, stride=2 if downscale else 1,
                                    act_layer=act_layer, norm_layer=norm_layer, drop_path_rate=drop_path),
-            SqueezeExcite(in_chs=out_channels, rd_ratio=0.25),
+            SqueezeExcite(out_channels, rd_ratio=0.25),
             nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=(1, 1))
         )
         # Make skip path
         self.skip_path = nn.Sequential(
             nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),
             nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(1, 1))
         ) if downscale else nn.Identity()
```

## towhee/models/layers/non_local.py

```diff
@@ -1,11 +1,23 @@
 # Non-local block using embedded gaussian
 # Code from
 # https://github.com/AlexHex7/Non-local_pytorch/blob/master/Non-Local_pytorch_0.3.1/lib/non_local_embedded_gaussian.py
-# modified by Zilliz.
+# Modifications by Copyright 2022 Zilliz. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import torch
 from torch import nn
 from torch.nn import functional as F
 import torchvision
 
 class _NonLocalBlockND(nn.Module):
```

## towhee/models/layers/patch_embed3d.py

```diff
@@ -1,9 +1,21 @@
 # original code from https://github.com/SwinTransformer/Video-Swin-Transformer
-# modified by Zilliz.
+# Modifications by Copyright 2022 Zilliz. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from torch import nn
 import torch.nn.functional as F
 
 
 class PatchEmbed3D(nn.Module):
     """
```

## towhee/models/layers/patch_merging3d.py

```diff
@@ -1,10 +1,21 @@
 # original code from https://github.com/SwinTransformer/Video-Swin-Transformer
-# modified by Zilliz.
-
+# Modifications by Copyright 2022 Zilliz. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 import torch
 from torch import nn
 import torch.nn.functional as F
 
 
 class PatchMerging3D(nn.Module):
     """
```

## towhee/models/layers/swin_transformer_block3d.py

```diff
@@ -1,9 +1,21 @@
 # original code from https://github.com/SwinTransformer/Video-Swin-Transformer
-# modified by Zilliz.
+# Modifications by Copyright 2022 Zilliz. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import torch
 from torch import nn
 from torch.utils import checkpoint
 import torch.nn.functional as F
 
 from towhee.models.layers.window_attention3d import WindowAttention3D
```

## towhee/models/layers/window_attention3d.py

```diff
@@ -1,9 +1,21 @@
 # original code from https://github.com/SwinTransformer/Video-Swin-Transformer
-# modified by Zilliz.
+# Modifications by Copyright 2022 Zilliz. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from torch import nn
 import torch
 from towhee.models.utils.weight_init import trunc_normal_
 
 
 class WindowAttention3D(nn.Module):
```

## towhee/models/retina_face/configs.py

```diff
@@ -1,7 +1,30 @@
+# MIT License
+#
+# Copyright (c) 2019
+#
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
+#
+# The above copyright notice and this permission notice shall be included in all
+# copies or substantial portions of the Software.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+# SOFTWARE.
+# This code is modified by Zilliz.
+
 model_cfgs = {
     'cfg_mnet' : {
         'name': 'mobilenet0.25',
         'min_sizes': [[16, 32], [64, 128], [256, 512]],
         'steps': [8, 16, 32],
         'variance': [0.1, 0.2],
         'clip': False,
```

## towhee/models/retina_face/heads.py

```diff
@@ -1,20 +1,28 @@
-# Copyright 2021 biubug6 . All rights reserved.
+# MIT License
 #
-# Licensed under the Apache License, Version 2.0 (the 'License');
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
+# Copyright (c) 2019
 #
-#     http://www.apache.org/licenses/LICENSE-2.0
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
 #
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an 'AS IS' BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+# The above copyright notice and this permission notice shall be included in all
+# copies or substantial portions of the Software.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+# SOFTWARE.
 # This code is modified by Zilliz.
 
 #adapted from https://github.com/biubug6/Pytorch_Retinaface
 
 import torch
 from torch import nn
```

## towhee/models/retina_face/mobilenet_v1.py

```diff
@@ -1,20 +1,28 @@
-# Copyright 2021 biubug6 . All rights reserved.
+# MIT License
 #
-# Licensed under the Apache License, Version 2.0 (the 'License');
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
+# Copyright (c) 2019
 #
-#     http://www.apache.org/licenses/LICENSE-2.0
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
 #
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an 'AS IS' BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+# The above copyright notice and this permission notice shall be included in all
+# copies or substantial portions of the Software.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+# SOFTWARE.
 # This code is modified by Zilliz.
 
 #adapted from https://github.com/biubug6/Pytorch_Retinaface
 import torch
 from torch import nn
 from towhee.models.retina_face.utils import conv_dw, conv_bn
```

## towhee/models/retina_face/prior_box.py

```diff
@@ -1,20 +1,28 @@
-# Copyright 2021 biubug6 . All rights reserved.
+# MIT License
 #
-# Licensed under the Apache License, Version 2.0 (the 'License');
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
+# Copyright (c) 2019
 #
-#     http://www.apache.org/licenses/LICENSE-2.0
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
 #
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an 'AS IS' BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+# The above copyright notice and this permission notice shall be included in all
+# copies or substantial portions of the Software.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+# SOFTWARE.
 # This code is modified by Zilliz.
 
 #adapted from https://github.com/biubug6/Pytorch_Retinaface
 from itertools import product
 from typing import Dict, List
 from math import ceil
```

## towhee/models/retina_face/retinaface.py

```diff
@@ -1,20 +1,28 @@
-# Copyright 2021 biubug6 . All rights reserved.
+# MIT License
 #
-# Licensed under the Apache License, Version 2.0 (the 'License');
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
+# Copyright (c) 2019
 #
-#     http://www.apache.org/licenses/LICENSE-2.0
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
 #
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an 'AS IS' BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+# The above copyright notice and this permission notice shall be included in all
+# copies or substantial portions of the Software.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+# SOFTWARE.
 # This code is modified by Zilliz.
 
 #adapted from https://github.com/biubug6/Pytorch_Retinaface
 #from collections import OrderedDict
 from typing import Tuple, Dict
 
 import numpy as np
```

## towhee/models/retina_face/retinaface_fpn.py

```diff
@@ -1,20 +1,28 @@
-# Copyright 2021 biubug6 . All rights reserved.
+# MIT License
 #
-# Licensed under the Apache License, Version 2.0 (the 'License');
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
+# Copyright (c) 2019
 #
-#     http://www.apache.org/licenses/LICENSE-2.0
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
 #
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an 'AS IS' BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+# The above copyright notice and this permission notice shall be included in all
+# copies or substantial portions of the Software.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+# SOFTWARE.
 # This code is modified by Zilliz.
 
 #adapted from https://github.com/biubug6/Pytorch_Retinaface
 from typing import List
 
 import torch
 import torch.nn.functional as F
```

## towhee/models/retina_face/ssh.py

```diff
@@ -1,20 +1,28 @@
-# Copyright 2021 biubug6 . All rights reserved.
+# MIT License
 #
-# Licensed under the Apache License, Version 2.0 (the 'License');
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
+# Copyright (c) 2019
 #
-#     http://www.apache.org/licenses/LICENSE-2.0
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
 #
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an 'AS IS' BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+# The above copyright notice and this permission notice shall be included in all
+# copies or substantial portions of the Software.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+# SOFTWARE.
 # This code is modified by Zilliz.
 
 #adapted from https://github.com/biubug6/Pytorch_Retinaface
 import torch
 import torch.nn.functional as F
 from torch import nn
```

## towhee/models/retina_face/utils.py

```diff
@@ -1,20 +1,28 @@
-# Copyright 2021 biubug6 . All rights reserved.
+# MIT License
 #
-# Licensed under the Apache License, Version 2.0 (the 'License');
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
+# Copyright (c) 2019
 #
-#     http://www.apache.org/licenses/LICENSE-2.0
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
 #
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an 'AS IS' BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+# The above copyright notice and this permission notice shall be included in all
+# copies or substantial portions of the Software.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+# SOFTWARE.
 # This code is modified by Zilliz.
 
 #adapted from https://github.com/biubug6/Pytorch_Retinaface
 from typing import Dict
 from collections import OrderedDict
 
 import torch
```

## towhee/models/swin_transformer/configs.py

```diff
@@ -1,7 +1,22 @@
+# Copyright 2021 Microsoft . All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# This code is modified by Zilliz.
+
 IMAGENET_DEFAULT_MEAN = [0.485, 0.456, 0.406]
 IMAGENET_DEFAULT_STD = [0.229, 0.224, 0.225]
 
 
 def _cfg(url='', **kwargs):
     return {
         'url': url,
```

## towhee/models/utils/get_window_size.py

```diff
@@ -1,9 +1,21 @@
 # original code from https://github.com/SwinTransformer/Video-Swin-Transformer
-# modified by Zilliz.
+# Modifications by Copyright 2022 Zilliz. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 
 def get_window_size(x_size, window_size, shift_size=None):
     """
     Args:
         x_size (`tuple[int]`):
             Tensor with size (B*num_windows, window_size, window_size, C)
```

## towhee/models/utils/pretrained_utils.py

```diff
@@ -1,7 +1,21 @@
+# Copyright 2023 Zilliz. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 from torch.utils import model_zoo
 import torch
 import logging
 
 
 def load_pretrained_weights(
     model,
```

## towhee/models/utils/window_partition.py

```diff
@@ -1,9 +1,21 @@
 # original code from https://github.com/SwinTransformer/Video-Swin-Transformer
-# modified by Zilliz.
+# Modifications by Copyright 2022 Zilliz. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 
 def window_partition(x, window_size: int):
     """
     Args:
        x: (b, h, w, c)
        window_size (int): window size
```

## towhee/models/utils/window_partition3d.py

```diff
@@ -1,9 +1,21 @@
 # original code from https://github.com/SwinTransformer/Video-Swin-Transformer
-# modified by Zilliz.
+# Modifications by Copyright 2022 Zilliz. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from functools import reduce
 from operator import mul
 
 
 def window_partition(x, window_size):
     """
```

## towhee/models/utils/window_reverse.py

```diff
@@ -1,9 +1,21 @@
 # original code from https://github.com/SwinTransformer/Video-Swin-Transformer
-# modified by Zilliz.
+# Modifications by Copyright 2022 Zilliz. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 
 def window_reverse(windows, window_size: int, h: int, w: int):
     """
     Args:
        windows (`tuple`):
             (num_windows*b, window_size, window_size, c)
```

## towhee/models/utils/window_reverse3d.py

```diff
@@ -1,9 +1,21 @@
 # original code from https://github.com/SwinTransformer/Video-Swin-Transformer
-# modified by Zilliz.
+# Modifications by Copyright 2022 Zilliz. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 
 def window_reverse(windows, window_size, b, d, h, w):
     """
     Args:
         windows (`torch.Tensor`):
             Tensor with size (B*num_windows, window_size, window_size, C)
```

## Comparing `towhee.models-1.0.0rc1.dist-info/LICENSE` & `towhee.models-1.1.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `towhee.models-1.0.0rc1.dist-info/METADATA` & `towhee.models-1.1.0.dist-info/METADATA`

 * *Files 22% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: towhee.models
-Version: 1.0.0rc1
+Version: 1.1.0
 Summary: Towhee is a framework that helps you encode your unstructured data into embeddings.
 Home-page: https://github.com/towhee-io/towhee
 Author: Towhee Team
 Author-email: towhee-team@zilliz.com
 License: http://www.apache.org/licenses/LICENSE-2.0
 Platform: unix
 Platform: linux
@@ -13,14 +13,16 @@
 Description-Content-Type: text/markdown
 License-File: LICENSE
 Requires-Dist: requests (>=2.12.5)
 Requires-Dist: tqdm (>=4.59.0)
 Requires-Dist: tabulate
 Requires-Dist: numpy
 Requires-Dist: twine
+Requires-Dist: tenacity
+Requires-Dist: pydantic
 Requires-Dist: contextvars ; python_version <= "3.6"
 Requires-Dist: importlib-resources ; python_version<'3.7'
 
 &nbsp;
 
 <p align="center">
     <img src="towhee_logo.png#gh-light-mode-only" width="60%"/>
@@ -45,202 +47,143 @@
   <a href="https://twitter.com/towheeio">
     <img src="https://img.shields.io/badge/follow-twitter-blue?style=flat" alt="twitter"/>
   </a>
   <a href="https://www.apache.org/licenses/LICENSE-2.0">
     <img src="https://img.shields.io/badge/license-apache2.0-green?style=flat" alt="license"/>
   </a>
   <a href="https://github.com/towhee-io/towhee/actions/workflows/pylint.yml">
-    <img src="https://img.shields.io/github/workflow/status/towhee-io/towhee/Workflow%20for%20pylint/main?label=pylint&style=flat" alt="github actions"/>
+    <img src="https://github.com/towhee-io/towhee/actions/workflows/pylint.yml/badge.svg" alt="github actions"/>
+  </a>
+  <a href="https://pypi.org/project/towhee/">
+    <img src="https://img.shields.io/pypi/v/towhee?label=Release&color&logo=Python" alt="github actions"/>
   </a>
   <a href="https://app.codecov.io/gh/towhee-io/towhee">
     <img src="https://img.shields.io/codecov/c/github/towhee-io/towhee?style=flat" alt="coverage"/>
   </a>
 </div>
 
 &nbsp;
 
-[Towhee](https://towhee.io) makes it easy to build neural data processing pipelines for AI applications.
-We provide hundreds of models, algorithms, and transformations that can be used as standard pipeline building blocks.
-You can use Towhee's Pythonic API to build a prototype of your pipeline and
-automatically optimize it for production-ready environments.
-
-:art:&emsp;**Various Modalities:** Towhee supports data processing on a variety of modalities, including images, videos, text, audio, molecular structures, etc.
-
-:mortar_board:&emsp;**SOTA Models:** Towhee provides SOTA models across 5 fields (CV, NLP, Multimodal, Audio, Medical), 15 tasks, and 140+ model architectures. These include BERT, CLIP, ViT, SwinTransformer, MAE, and data2vec, all pretrained and ready to use.
-
-:package:&emsp;**Data Processing:** Towhee also provides traditional methods alongside neural network models to help you build practical data processing pipelines. We have a rich pool of operators available, such as video decoding, audio slicing, frame sampling, feature vector dimension reduction, ensembling, and database operations.
-
-:snake:&emsp;**Pythonic API:** Towhee includes a Pythonic method-chaining API for describing custom data processing pipelines. We also support schemas, which makes processing unstructured data as easy as handling tabular data.
-
-## What's New
-
-**v0.9.0 Dec. 2, 2022**
-* Added one video classification model:
-[*Vis4mer*](https://github.com/towhee-io/towhee/tree/branch0.9.0/towhee/models/vis4mer)
-* Added three visual backbones:
-[*MCProp*](https://github.com/towhee-io/towhee/tree/branch0.9.0/towhee/models/mcprop), 
-[*RepLKNet*](https://github.com/towhee-io/towhee/tree/branch0.9.0/towhee/models/replknet), 
-[*Shunted Transformer*](https://github.com/towhee-io/towhee/tree/branch0.9.0/towhee/models/shunted_transformer)
-* Add two code search operators:
-[*code_search.codebert*](https://towhee.io/code-search/codebert), 
-[*code_search.unixcoder*](https://towhee.io/code-search/unixcoder)
-* Add five image captioning operators: 
-[*image_captioning.expansionnet-v2*](https://towhee.io/image-captioning/expansionnet-v2), 
-[*image_captioning.magic*](https://towhee.io/image-captioning/magic),
-[*image_captioning.clip_caption_reward*](https://towhee.io/image-captioning/clip-caption-reward), 
-[*image_captioning.blip*](https://towhee.io/image-captioning/blip), 
-[*image_captioning.clipcap*](https://towhee.io/image-captioning/clipcap)
-* Add five image-text embedding operators: 
-[*image_text_embedding.albef*](https://towhee.io/image-text-embedding/albef), 
-[*image_text_embedding.ru_clip*](https://towhee.io/image-text-embedding/ru-clip), 
-[*image_text_embedding.japanese_clip*](https://towhee.io/image-text-embedding/japanese-clip),
-[*image_text_embedding.taiyi*](https://towhee.io/image-text-embedding/taiyi),
-[*image_text_embedding.slip*](https://towhee.io/image-text-embedding/slip)
-* Add one machine-translation operator: 
-[*machine_translation.opus_mt*](https://towhee.io/machine-translation/opus-mt)
-* Add one filter-tiny-segments operator:
-[*video-copy-detection.filter-tiny-segments*](https://towhee.io/video-copy-detection/filter-tiny-segments)
-* Add an advanced tutorial for audio fingerprinting: 
-[*Audio Fingerprint II: Music Detection with Temporal Localization*](https://github.com/towhee-io/examples/blob/main/audio/audio_fingerprint/audio_fingerprint_advanced.ipynb) (increased accuracy from 84% to 90%)
-
-**v0.8.1 Sep. 30, 2022**
-
-* Added four visual backbones:
-[*ISC*](https://github.com/towhee-io/towhee/tree/branch0.8.1/towhee/models/isc),
-[*MetaFormer*](https://github.com/towhee-io/towhee/tree/branch0.8.1/towhee/models/metaformer),
-[*ConvNext*](https://github.com/towhee-io/towhee/tree/branch0.8.1/towhee/models/convnext),
-[*HorNet*](https://github.com/towhee-io/towhee/tree/branch0.8.1/towhee/models/hornet)
-* Add two video de-copy operators:
-[*select-video*](https://towhee.io/video-copy-detection/select-video), 
-[*temporal-network*](https://towhee.io/video-copy-detection/temporal-network)
-* Add one image embedding operator specifically designed for image retrieval and video de-copy with SOTA performance on VCSL dataset:
-[*isc*](https://towhee.io/image-embedding/isc)
-* Add one audio embedding operator specified for audio fingerprint:
-[*audio_embedding.nnfp*](https://towhee.io/audio-embedding/nnfp) (with pretrained weights)
-* Add one tutorial for video de-copy: 
-[*How to Build a Video Segment Copy Detection System*](https://github.com/towhee-io/examples/blob/main/video/video_deduplication/segment_level/video_deduplication_at_segment_level.ipynb)
-* Add one beginner tutorial for audio fingerprint:
-[*Audio Fingerprint I: Build a Demo with Towhee & Milvus*](https://github.com/towhee-io/examples/blob/main/audio/audio_fingerprint/audio_fingerprint_beginner.ipynb)
-
-
-**v0.8.0 Aug. 16, 2022**
-
-* Towhee now supports generating an Nvidia Triton Server from a Towhee pipeline, with aditional support for GPU image decoding.
-* Added one audio fingerprinting model: 
-[*nnfp*](https://github.com/towhee-io/towhee/tree/branch0.8.0/towhee/models/nnfp)
-* Added two image embedding models: 
-[*RepMLP*](https://github.com/towhee-io/towhee/tree/branch0.8.0/towhee/models/repmlp), [**WaveViT**](https://github.com/towhee-io/towhee/tree/branch0.8.0/towhee/models/wave_vit)
-
-**v0.7.3 Jul. 27, 2022**
-* Added one multimodal (text/image) model:
-[*CoCa*](https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/coca).
-* Added two video models for grounded situation recognition & repetitive action counting:
-[*CoFormer*](https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/coformer),
-[*TransRAC*](https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/transrac).
-* Added two SoTA models for image tasks (image retrieval, image classification, etc.):
-[*CVNet*](https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/cvnet),
-[*MaxViT*](https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/max_vit)
-
-**v0.7.1 Jul. 1, 2022**
-* Added one image embedding model:
-[*MPViT*](https://towhee.io/image-embedding/mpvit).
-* Added two video retrieval models:
-[*BridgeFormer*](https://towhee.io/video-text-embedding/bridge-former),
-[*collaborative-experts*](https://towhee.io/video-text-embedding/collaborative-experts).
-* Added FAISS-based ANNSearch operators: *to_faiss*, *faiss_search*.
-
-**v0.7.0 Jun. 24, 2022**
-
-* Added six video understanding/classification models:
-[*Video Swin Transformer*](https://towhee.io/action-classification/video-swin-transformer), 
-[*TSM*](https://towhee.io/action-classification/tsm), 
-[*Uniformer*](https://towhee.io/action-classification/uniformer), 
-[*OMNIVORE*](https://towhee.io/action-classification/omnivore), 
-[*TimeSformer*](https://towhee.io/action-classification/timesformer), 
-[*MoViNets*](https://towhee.io/action-classification/movinet).
-* Added four video retrieval models:
-[*CLIP4Clip*](https://towhee.io/video-text-embedding/clip4clip), 
-[*DRL*](https://towhee.io/video-text-embedding/drl), 
-[*Frozen in Time*](https://towhee.io/video-text-embedding/frozen-in-time), 
-[*MDMMT*](https://towhee.io/video-text-embedding/mdmmt).
-
-**v0.6.1  May. 13, 2022**
-
-* Added three text-image retrieval models:
-[*CLIP*](https://towhee.io/image-text-embedding/clip),
-[*BLIP*](https://towhee.io/image-text-embedding/blip),
-[*LightningDOT*](https://towhee.io/image-text-embedding/lightningdot).
-* Added six video understanding/classification models from PyTorchVideo:
-[*I3D*](https://towhee.io/action-classification/pytorchvideo),
-[*C2D*](https://towhee.io/action-classification/pytorchvideo),
-[*Slow*](https://towhee.io/action-classification/pytorchvideo),
-[*SlowFast*](https://towhee.io/action-classification/pytorchvideo),
-[*X3D*](https://towhee.io/action-classification/pytorchvideo),
-[*MViT*](https://towhee.io/action-classification/pytorchvideo).
+[Towhee](https://towhee.io) is a cutting-edge framework designed to streamline the processing of unstructured data through the use of Large Language Model (LLM) based pipeline orchestration. It is uniquely positioned to extract invaluable insights from diverse unstructured data types, including lengthy text, images, audio and video files. Leveraging the capabilities of generative AI and the SOTA deep learning models, Towhee is capable of transforming this unprocessed data into specific formats such as text, image, or embeddings. These can then be efficiently loaded into an appropriate storage system like a vector database. Developers can initially build an intuitive data processing pipeline prototype with user friendly Pythonic APU, then optimize it for production environments.
+
+Multi Modalities: Towhee is capable of handling a wide range of data types. Whether it's image data, video clips, text, audio files, or even molecular structures, Towhee can process them all. 
+
+    LLM Pipeline orchestration:  Towhee offers flexibility to adapt to different Large Language Models (LLMs). Additionally, it allows for hosting open-source large models locally. Moreover, Towhee provides features like prompt management and knowledge retrieval, making the interaction with these LLMs more efficient and effective.
+
+Rich Operators: Towhee provides a wide range of ready-to-use state-of-the-art models across five domains: CV, NLP, multimodal, audio, and medical. With over 140 models like BERT and CLIP and rich functionalities like video decoding, audio slicing, frame sampling,  and dimensionality reduction, it assists in efficiently building data processing pipelines. 
+
+   Prebuilt ETL Pipelines: Towhee offers ready-to-use ETL (Extract, Transform, Load) pipelines for common tasks such as Retrieval-Augmented Generation, Text Image search, and Video copy detection. This means you don't need to be an AI expert to build applications using these features. 
+  High performance backend: Leveraging the power of the Triton Inference Server, Towhee can speed up model serving on both CPU and GPU using platforms like TensorRT, Pytorch, and ONNX. Moreover, you can transform your Python pipeline into a high-performance docker container with just a few lines of code, enabling efficient deployment and scaling.
+
+Pythonic API: Towhee includes a Pythonic method-chaining API for describing custom data processing pipelines. We also support schemas, which makes processing unstructured data as easy as handling tabular data.
 
 ## Getting started
 
 Towhee requires Python 3.6+. You can install Towhee via `pip`:
 
 ```bash
 pip install towhee towhee.models
 ```
 
-If you run into any pip-related install problems, please try to upgrade pip with `pip install -U pip`.
+### Pipeline
+
+### Pre-defined Pipeline
 
-Let's try your first Towhee pipeline. Below is an example for how to create a CLIP-based cross modal retrieval pipeline with only 15 lines of code.
+Towhee provides some pre-defined pipelines to help users quickly implement some functions. 
+Currently implemented are: 
+- [Sentence Embedding](https://towhee.io/tasks/detail/pipeline/sentence-similarity)
+- [Image Embedding](https://towhee.io/tasks/detail/pipeline/text-image-search)
+- [Video deduplication](https://towhee.io/tasks/detail/pipeline/video-copy-detection)
+- [Question Answer with Docs](https://towhee.io/tasks/detail/pipeline/retrieval-augmented-generation)
 
+All pipelines can be found on Towhee Hub. Here is an example of using the sentence_embedding pipeline: 
+
+```python
+from towhee import AutoPipes, AutoConfig
+# get the built-in sentence_similarity pipeline
+config = AutoConfig.load_config('sentence_embedding')
+config.model = 'paraphrase-albert-small-v2'
+config.device = 0
+sentence_embedding = AutoPipes.pipeline('sentence_embedding', config=config)
+
+# generate embedding for one sentence
+embedding = sentence_embedding('how are you?').get()
+# batch generate embeddings for multi-sentences
+embeddings = sentence_embedding.batch(['how are you?', 'how old are you?'])
+embeddings = [e.get() for e in embeddings]
+```
+### Custom pipelines 
+
+If you can't find the pipeline you want in towhee hub, you can also implement custom pipelines through the towhee Python API. In the following example, we will create a cross-modal retrieval pipeline based on CLIP.
 ```python
-import towhee
 
+from towhee import ops, pipe, DataCollection
 # create image embeddings and build index
-(
-    towhee.glob['file_name']('./*.png')
-          .image_decode['file_name', 'img']()
-          .image_text_embedding.clip['img', 'vec'](model_name='clip_vit_base_patch32', modality='image')
-          .tensor_normalize['vec','vec']()
-          .to_faiss[('file_name', 'vec')](findex='./index.bin')
+p = (
+    pipe.input('file_name')
+    .map('file_name', 'img', ops.image_decode.cv2())
+    .map('img', 'vec', ops.image_text_embedding.clip(model_name='clip_vit_base_patch32', modality='image'))
+    .map('vec', 'vec', ops.towhee.np_normalize())
+    .map(('vec', 'file_name'), (), ops.ann_insert.faiss_index('./faiss', 512))
+    .output()
 )
 
-# search image by text
-results = (
-    towhee.dc['text'](['puppy Corgi'])
-          .image_text_embedding.clip['text', 'vec'](model_name='clip_vit_base_patch32', modality='text')
-          .tensor_normalize['vec', 'vec']()
-          .faiss_search['vec', 'results'](findex='./index.bin', k=3)
-          .select['text', 'results']()
+for f_name in ['https://raw.githubusercontent.com/towhee-io/towhee/main/assets/dog1.png',
+               'https://raw.githubusercontent.com/towhee-io/towhee/main/assets/dog2.png',
+               'https://raw.githubusercontent.com/towhee-io/towhee/main/assets/dog3.png']:
+    p(f_name)
+
+# Flush faiss data into disk. 
+p.flush()
+# search image by textdecode = ops.image_decode.cv2('rgb')
+p = (
+    pipe.input('text')
+    .map('text', 'vec', ops.image_text_embedding.clip(model_name='clip_vit_base_patch32', modality='text'))
+    .map('vec', 'vec', ops.towhee.np_normalize())
+    # faiss op result format:  [[id, score, [file_name], ...]
+    .map('vec', 'row', ops.ann_search.faiss_index('./faiss', 3))
+    .map('row', 'images', lambda x: [decode(item[2][0]) for item in x])
+    .output('text', 'images')
 )
+
+DataCollection(p('a cat')).show()
+
 ```
 <img src="assets/towhee_example.png" style="width: 60%; height: 60%">
 
-Learn more examples from the [Towhee Bootcamp](https://codelabs.towhee.io/).
 
 ## Core Concepts
 
 Towhee is composed of four main building blocks - `Operators`, `Pipelines`, `DataCollection API` and `Engine`.
 
 - __Operators__: An operator is a single building block of a neural data processing pipeline. Different implementations of operators are categorized by tasks, with each task having a standard interface. An operator can be a deep learning model, a data processing method, or a Python function.
 
 - __Pipelines__: A pipeline is composed of several operators interconnected in the form of a DAG (directed acyclic graph). This DAG can direct complex functionalities, such as embedding feature extraction, data tagging, and cross modal data analysis.
 
-- __DataCollection API__: A Pythonic and method-chaining style API for building custom pipelines. A pipeline defined by the DataColltion API can be run locally on a laptop for fast prototyping and then be converted to a docker image, with end-to-end optimizations, for production-ready environments.
+- __DataCollection API__: A Pythonic and method-chaining style API for building custom pipelines, providing multiple data conversion interfaces: map, filter, flat_map, concat, window, time_window, and window_all. Through these interfaces, complex data processing pipelines can be built quickly to process unstructured data such as video, audio, text, images, etc.
 
 - __Engine__: The engine sits at Towhee's core. Given a pipeline, the engine will drive dataflow among individual operators, schedule tasks, and monitor compute resource usage (CPU/GPU/etc). We provide a basic engine within Towhee to run pipelines on a single-instance machine and a Triton-based engine for docker containers.
 
+## Resource
+- TowheeHub: https://towhee.io/
+- docs: https://towhee.readthedocs.io/en/latest/
+- examples: https://github.com/towhee-io/examples
+
 ## Contributing
 
 Writing code is not the only way to contribute! Submitting issues, answering questions, and improving documentation are just some of the many ways you can help our growing community. Check out our [contributing page](https://github.com/towhee-io/towhee/blob/main/CONTRIBUTING.md) for more information.
 
 Special thanks goes to these folks for contributing to Towhee, either on Github, our Towhee Hub, or elsewhere:
 <br><!-- Do not remove start of hero-bot --><br>
 <img src="https://img.shields.io/badge/all--contributors-33-orange"><br>
 <a href="https://github.com/AniTho"><img src="https://avatars.githubusercontent.com/u/34787227?v=4" width="30px" /></a>
 <a href="https://github.com/Chiiizzzy"><img src="https://avatars.githubusercontent.com/u/72550076?v=4" width="30px" /></a>
 <a href="https://github.com/GuoRentong"><img src="https://avatars.githubusercontent.com/u/57477222?v=4" width="30px" /></a>
 <a href="https://github.com/NicoYuan1986"><img src="https://avatars.githubusercontent.com/u/109071306?v=4" width="30px" /></a>
+<a href="https://github.com/Opdoop"><img src="https://avatars.githubusercontent.com/u/21202514?v=4" width="30px" /></a>
 <a href="https://github.com/Tumao727"><img src="https://avatars.githubusercontent.com/u/20420181?v=4" width="30px" /></a>
 <a href="https://github.com/YuDongPan"><img src="https://avatars.githubusercontent.com/u/88148730?v=4" width="30px" /></a>
 <a href="https://github.com/binbinlv"><img src="https://avatars.githubusercontent.com/u/83755740?v=4" width="30px" /></a>
 <a href="https://github.com/derekdqc"><img src="https://avatars.githubusercontent.com/u/11754703?v=4" width="30px" /></a>
 <a href="https://github.com/dreamfireyu"><img src="https://avatars.githubusercontent.com/u/47691077?v=4" width="30px" /></a>
 <a href="https://github.com/filip-halt"><img src="https://avatars.githubusercontent.com/u/81822489?v=4" width="30px" /></a>
 <a href="https://github.com/fzliu"><img src="https://avatars.githubusercontent.com/u/6334158?v=4" width="30px" /></a>
@@ -261,11 +204,10 @@
 <a href="https://github.com/soulteary"><img src="https://avatars.githubusercontent.com/u/1500781?v=4" width="30px" /></a>
 <a href="https://github.com/sre-ci-robot"><img src="https://avatars.githubusercontent.com/u/56469371?v=4" width="30px" /></a>
 <a href="https://github.com/sutcalag"><img src="https://avatars.githubusercontent.com/u/83750738?v=4" width="30px" /></a>
 <a href="https://github.com/wxywb"><img src="https://avatars.githubusercontent.com/u/5432721?v=4" width="30px" /></a>
 <a href="https://github.com/zc277584121"><img src="https://avatars.githubusercontent.com/u/17022025?v=4" width="30px" /></a>
 <a href="https://github.com/zengxiang68"><img src="https://avatars.githubusercontent.com/u/68835157?v=4" width="30px" /></a>
 <a href="https://github.com/zhousicong"><img src="https://avatars.githubusercontent.com/u/7541863?v=4" width="30px" /></a>
-<a href="https://github.com/zhujiming"><img src="https://avatars.githubusercontent.com/u/18031320?v=4" width="30px" /></a>
 <br><!-- Do not remove end of hero-bot --><br>
 
 Looking for a database to store and index your embedding vectors? Check out [Milvus](https://github.com/milvus-io/milvus).
```

### html2text {}

```diff
@@ -1,168 +1,126 @@
-Metadata-Version: 2.1 Name: towhee.models Version: 1.0.0rc1 Summary: Towhee is
-a framework that helps you encode your unstructured data into embeddings. Home-
+Metadata-Version: 2.1 Name: towhee.models Version: 1.1.0 Summary: Towhee is a
+framework that helps you encode your unstructured data into embeddings. Home-
 page: https://github.com/towhee-io/towhee Author: Towhee Team Author-email:
 towhee-team@zilliz.com License: http://www.apache.org/licenses/LICENSE-2.0
 Platform: unix Platform: linux Platform: osx Platform: win32 Description-
 Content-Type: text/markdown License-File: LICENSE Requires-Dist: requests
 (>=2.12.5) Requires-Dist: tqdm (>=4.59.0) Requires-Dist: tabulate Requires-
-Dist: numpy Requires-Dist: twine Requires-Dist: contextvars ; python_version <=
-"3.6" Requires-Dist: importlib-resources ; python_version<'3.7' 
+Dist: numpy Requires-Dist: twine Requires-Dist: tenacity Requires-Dist:
+pydantic Requires-Dist: contextvars ; python_version <= "3.6" Requires-Dist:
+importlib-resources ; python_version<'3.7' 
 [towhee_logo.png#gh-light-mode-only] [assets/towhee_logo_dark.png#gh-dark-mode-
                                      only]
                    **** x2vec, Towhee is all you need! ****
                        **** ENGLISH |  ****
-[join-slack] [twitter] [license] [github_actions] [coverage]
- [Towhee](https://towhee.io) makes it easy to build neural data processing
-pipelines for AI applications. We provide hundreds of models, algorithms, and
-transformations that can be used as standard pipeline building blocks. You can
-use Towhee's Pythonic API to build a prototype of your pipeline and
-automatically optimize it for production-ready environments. :art:
-&emsp;**Various Modalities:** Towhee supports data processing on a variety of
-modalities, including images, videos, text, audio, molecular structures, etc. :
-mortar_board:&emsp;**SOTA Models:** Towhee provides SOTA models across 5 fields
-(CV, NLP, Multimodal, Audio, Medical), 15 tasks, and 140+ model architectures.
-These include BERT, CLIP, ViT, SwinTransformer, MAE, and data2vec, all
-pretrained and ready to use. :package:&emsp;**Data Processing:** Towhee also
-provides traditional methods alongside neural network models to help you build
-practical data processing pipelines. We have a rich pool of operators
-available, such as video decoding, audio slicing, frame sampling, feature
-vector dimension reduction, ensembling, and database operations. :snake:
-&emsp;**Pythonic API:** Towhee includes a Pythonic method-chaining API for
-describing custom data processing pipelines. We also support schemas, which
-makes processing unstructured data as easy as handling tabular data. ## What's
-New **v0.9.0 Dec. 2, 2022** * Added one video classification model: [*Vis4mer*]
-(https://github.com/towhee-io/towhee/tree/branch0.9.0/towhee/models/vis4mer) *
-Added three visual backbones: [*MCProp*](https://github.com/towhee-io/towhee/
-tree/branch0.9.0/towhee/models/mcprop), [*RepLKNet*](https://github.com/towhee-
-io/towhee/tree/branch0.9.0/towhee/models/replknet), [*Shunted Transformer*]
-(https://github.com/towhee-io/towhee/tree/branch0.9.0/towhee/models/
-shunted_transformer) * Add two code search operators: [*code_search.codebert*]
-(https://towhee.io/code-search/codebert), [*code_search.unixcoder*](https://
-towhee.io/code-search/unixcoder) * Add five image captioning operators:
-[*image_captioning.expansionnet-v2*](https://towhee.io/image-captioning/
-expansionnet-v2), [*image_captioning.magic*](https://towhee.io/image-
-captioning/magic), [*image_captioning.clip_caption_reward*](https://towhee.io/
-image-captioning/clip-caption-reward), [*image_captioning.blip*](https://
-towhee.io/image-captioning/blip), [*image_captioning.clipcap*](https://
-towhee.io/image-captioning/clipcap) * Add five image-text embedding operators:
-[*image_text_embedding.albef*](https://towhee.io/image-text-embedding/albef),
-[*image_text_embedding.ru_clip*](https://towhee.io/image-text-embedding/ru-
-clip), [*image_text_embedding.japanese_clip*](https://towhee.io/image-text-
-embedding/japanese-clip), [*image_text_embedding.taiyi*](https://towhee.io/
-image-text-embedding/taiyi), [*image_text_embedding.slip*](https://towhee.io/
-image-text-embedding/slip) * Add one machine-translation operator:
-[*machine_translation.opus_mt*](https://towhee.io/machine-translation/opus-mt)
-* Add one filter-tiny-segments operator: [*video-copy-detection.filter-tiny-
-segments*](https://towhee.io/video-copy-detection/filter-tiny-segments) * Add
-an advanced tutorial for audio fingerprinting: [*Audio Fingerprint II: Music
-Detection with Temporal Localization*](https://github.com/towhee-io/examples/
-blob/main/audio/audio_fingerprint/audio_fingerprint_advanced.ipynb) (increased
-accuracy from 84% to 90%) **v0.8.1 Sep. 30, 2022** * Added four visual
-backbones: [*ISC*](https://github.com/towhee-io/towhee/tree/branch0.8.1/towhee/
-models/isc), [*MetaFormer*](https://github.com/towhee-io/towhee/tree/
-branch0.8.1/towhee/models/metaformer), [*ConvNext*](https://github.com/towhee-
-io/towhee/tree/branch0.8.1/towhee/models/convnext), [*HorNet*](https://
-github.com/towhee-io/towhee/tree/branch0.8.1/towhee/models/hornet) * Add two
-video de-copy operators: [*select-video*](https://towhee.io/video-copy-
-detection/select-video), [*temporal-network*](https://towhee.io/video-copy-
-detection/temporal-network) * Add one image embedding operator specifically
-designed for image retrieval and video de-copy with SOTA performance on VCSL
-dataset: [*isc*](https://towhee.io/image-embedding/isc) * Add one audio
-embedding operator specified for audio fingerprint: [*audio_embedding.nnfp*]
-(https://towhee.io/audio-embedding/nnfp) (with pretrained weights) * Add one
-tutorial for video de-copy: [*How to Build a Video Segment Copy Detection
-System*](https://github.com/towhee-io/examples/blob/main/video/
-video_deduplication/segment_level/video_deduplication_at_segment_level.ipynb) *
-Add one beginner tutorial for audio fingerprint: [*Audio Fingerprint I: Build a
-Demo with Towhee & Milvus*](https://github.com/towhee-io/examples/blob/main/
-audio/audio_fingerprint/audio_fingerprint_beginner.ipynb) **v0.8.0 Aug. 16,
-2022** * Towhee now supports generating an Nvidia Triton Server from a Towhee
-pipeline, with aditional support for GPU image decoding. * Added one audio
-fingerprinting model: [*nnfp*](https://github.com/towhee-io/towhee/tree/
-branch0.8.0/towhee/models/nnfp) * Added two image embedding models: [*RepMLP*]
-(https://github.com/towhee-io/towhee/tree/branch0.8.0/towhee/models/repmlp),
-[**WaveViT**](https://github.com/towhee-io/towhee/tree/branch0.8.0/towhee/
-models/wave_vit) **v0.7.3 Jul. 27, 2022** * Added one multimodal (text/image)
-model: [*CoCa*](https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/
-models/coca). * Added two video models for grounded situation recognition &
-repetitive action counting: [*CoFormer*](https://github.com/towhee-io/towhee/
-tree/branch0.7.3/towhee/models/coformer), [*TransRAC*](https://github.com/
-towhee-io/towhee/tree/branch0.7.3/towhee/models/transrac). * Added two SoTA
-models for image tasks (image retrieval, image classification, etc.): [*CVNet*]
-(https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/cvnet),
-[*MaxViT*](https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/
-max_vit) **v0.7.1 Jul. 1, 2022** * Added one image embedding model: [*MPViT*]
-(https://towhee.io/image-embedding/mpvit). * Added two video retrieval models:
-[*BridgeFormer*](https://towhee.io/video-text-embedding/bridge-former),
-[*collaborative-experts*](https://towhee.io/video-text-embedding/collaborative-
-experts). * Added FAISS-based ANNSearch operators: *to_faiss*, *faiss_search*.
-**v0.7.0 Jun. 24, 2022** * Added six video understanding/classification models:
-[*Video Swin Transformer*](https://towhee.io/action-classification/video-swin-
-transformer), [*TSM*](https://towhee.io/action-classification/tsm),
-[*Uniformer*](https://towhee.io/action-classification/uniformer), [*OMNIVORE*]
-(https://towhee.io/action-classification/omnivore), [*TimeSformer*](https://
-towhee.io/action-classification/timesformer), [*MoViNets*](https://towhee.io/
-action-classification/movinet). * Added four video retrieval models:
-[*CLIP4Clip*](https://towhee.io/video-text-embedding/clip4clip), [*DRL*](https:
-//towhee.io/video-text-embedding/drl), [*Frozen in Time*](https://towhee.io/
-video-text-embedding/frozen-in-time), [*MDMMT*](https://towhee.io/video-text-
-embedding/mdmmt). **v0.6.1 May. 13, 2022** * Added three text-image retrieval
-models: [*CLIP*](https://towhee.io/image-text-embedding/clip), [*BLIP*](https:/
-/towhee.io/image-text-embedding/blip), [*LightningDOT*](https://towhee.io/
-image-text-embedding/lightningdot). * Added six video understanding/
-classification models from PyTorchVideo: [*I3D*](https://towhee.io/action-
-classification/pytorchvideo), [*C2D*](https://towhee.io/action-classification/
-pytorchvideo), [*Slow*](https://towhee.io/action-classification/pytorchvideo),
-[*SlowFast*](https://towhee.io/action-classification/pytorchvideo), [*X3D*]
-(https://towhee.io/action-classification/pytorchvideo), [*MViT*](https://
-towhee.io/action-classification/pytorchvideo). ## Getting started Towhee
-requires Python 3.6+. You can install Towhee via `pip`: ```bash pip install
-towhee towhee.models ``` If you run into any pip-related install problems,
-please try to upgrade pip with `pip install -U pip`. Let's try your first
-Towhee pipeline. Below is an example for how to create a CLIP-based cross modal
-retrieval pipeline with only 15 lines of code. ```python import towhee # create
-image embeddings and build index ( towhee.glob['file_name']('./*.png')
-.image_decode['file_name', 'img']() .image_text_embedding.clip['img', 'vec']
-(model_name='clip_vit_base_patch32', modality='image') .tensor_normalize
-['vec','vec']() .to_faiss[('file_name', 'vec')](findex='./index.bin') ) #
-search image by text results = ( towhee.dc['text'](['puppy Corgi'])
-.image_text_embedding.clip['text', 'vec'](model_name='clip_vit_base_patch32',
-modality='text') .tensor_normalize['vec', 'vec']() .faiss_search['vec',
-'results'](findex='./index.bin', k=3) .select['text', 'results']() ) ```
-[assets/towhee_example.png] Learn more examples from the [Towhee Bootcamp]
-(https://codelabs.towhee.io/). ## Core Concepts Towhee is composed of four main
-building blocks - `Operators`, `Pipelines`, `DataCollection API` and `Engine`.
-- __Operators__: An operator is a single building block of a neural data
-processing pipeline. Different implementations of operators are categorized by
-tasks, with each task having a standard interface. An operator can be a deep
-learning model, a data processing method, or a Python function. -
-__Pipelines__: A pipeline is composed of several operators interconnected in
-the form of a DAG (directed acyclic graph). This DAG can direct complex
-functionalities, such as embedding feature extraction, data tagging, and cross
-modal data analysis. - __DataCollection API__: A Pythonic and method-chaining
-style API for building custom pipelines. A pipeline defined by the DataColltion
-API can be run locally on a laptop for fast prototyping and then be converted
-to a docker image, with end-to-end optimizations, for production-ready
-environments. - __Engine__: The engine sits at Towhee's core. Given a pipeline,
-the engine will drive dataflow among individual operators, schedule tasks, and
-monitor compute resource usage (CPU/GPU/etc). We provide a basic engine within
-Towhee to run pipelines on a single-instance machine and a Triton-based engine
-for docker containers. ## Contributing Writing code is not the only way to
-contribute! Submitting issues, answering questions, and improving documentation
-are just some of the many ways you can help our growing community. Check out
-our [contributing page](https://github.com/towhee-io/towhee/blob/main/
-CONTRIBUTING.md) for more information. Special thanks goes to these folks for
-contributing to Towhee, either on Github, our Towhee Hub, or elsewhere:
+[join-slack] [twitter] [license] [github_actions] [github_actions] [coverage]
+ [Towhee](https://towhee.io) is a cutting-edge framework designed to
+streamline the processing of unstructured data through the use of Large
+Language Model (LLM) based pipeline orchestration. It is uniquely positioned to
+extract invaluable insights from diverse unstructured data types, including
+lengthy text, images, audio and video files. Leveraging the capabilities of
+generative AI and the SOTA deep learning models, Towhee is capable of
+transforming this unprocessed data into specific formats such as text, image,
+or embeddings. These can then be efficiently loaded into an appropriate storage
+system like a vector database. Developers can initially build an intuitive data
+processing pipeline prototype with user friendly Pythonic APU, then optimize it
+for production environments. Multi Modalities: Towhee is capable of
+handling a wide range of data types. Whether it's image data, video clips,
+text, audio files, or even molecular structures, Towhee can process them all.
+ LLM Pipeline orchestration: Towhee offers flexibility to adapt to
+different Large Language Models (LLMs). Additionally, it allows for hosting
+open-source large models locally. Moreover, Towhee provides features like
+prompt management and knowledge retrieval, making the interaction with these
+LLMs more efficient and effective. Rich Operators: Towhee provides a
+wide range of ready-to-use state-of-the-art models across five domains: CV,
+NLP, multimodal, audio, and medical. With over 140 models like BERT and CLIP
+and rich functionalities like video decoding, audio slicing, frame sampling,
+and dimensionality reduction, it assists in efficiently building data
+processing pipelines.  Prebuilt ETL Pipelines: Towhee offers ready-to-use
+ETL (Extract, Transform, Load) pipelines for common tasks such as Retrieval-
+Augmented Generation, Text Image search, and Video copy detection. This means
+you don't need to be an AI expert to build applications using these features.
+ High performance backend: Leveraging the power of the Triton Inference
+Server, Towhee can speed up model serving on both CPU and GPU using platforms
+like TensorRT, Pytorch, and ONNX. Moreover, you can transform your Python
+pipeline into a high-performance docker container with just a few lines of
+code, enabling efficient deployment and scaling. Pythonic API: Towhee
+includes a Pythonic method-chaining API for describing custom data processing
+pipelines. We also support schemas, which makes processing unstructured data as
+easy as handling tabular data. ## Getting started Towhee requires Python 3.6+.
+You can install Towhee via `pip`: ```bash pip install towhee towhee.models ```
+### Pipeline ### Pre-defined Pipeline Towhee provides some pre-defined
+pipelines to help users quickly implement some functions. Currently implemented
+are: - [Sentence Embedding](https://towhee.io/tasks/detail/pipeline/sentence-
+similarity) - [Image Embedding](https://towhee.io/tasks/detail/pipeline/text-
+image-search) - [Video deduplication](https://towhee.io/tasks/detail/pipeline/
+video-copy-detection) - [Question Answer with Docs](https://towhee.io/tasks/
+detail/pipeline/retrieval-augmented-generation) All pipelines can be found on
+Towhee Hub. Here is an example of using the sentence_embedding pipeline:
+```python from towhee import AutoPipes, AutoConfig # get the built-in
+sentence_similarity pipeline config = AutoConfig.load_config
+('sentence_embedding') config.model = 'paraphrase-albert-small-v2'
+config.device = 0 sentence_embedding = AutoPipes.pipeline('sentence_embedding',
+config=config) # generate embedding for one sentence embedding =
+sentence_embedding('how are you?').get() # batch generate embeddings for multi-
+sentences embeddings = sentence_embedding.batch(['how are you?', 'how old are
+you?']) embeddings = [e.get() for e in embeddings] ``` ### Custom pipelines If
+you can't find the pipeline you want in towhee hub, you can also implement
+custom pipelines through the towhee Python API. In the following example, we
+will create a cross-modal retrieval pipeline based on CLIP. ```python from
+towhee import ops, pipe, DataCollection # create image embeddings and build
+index p = ( pipe.input('file_name') .map('file_name', 'img',
+ops.image_decode.cv2()) .map('img', 'vec', ops.image_text_embedding.clip
+(model_name='clip_vit_base_patch32', modality='image')) .map('vec', 'vec',
+ops.towhee.np_normalize()) .map(('vec', 'file_name'), (),
+ops.ann_insert.faiss_index('./faiss', 512)) .output() ) for f_name in ['https:/
+/raw.githubusercontent.com/towhee-io/towhee/main/assets/dog1.png', 'https://
+raw.githubusercontent.com/towhee-io/towhee/main/assets/dog2.png', 'https://
+raw.githubusercontent.com/towhee-io/towhee/main/assets/dog3.png']: p(f_name) #
+Flush faiss data into disk. p.flush() # search image by textdecode =
+ops.image_decode.cv2('rgb') p = ( pipe.input('text') .map('text', 'vec',
+ops.image_text_embedding.clip(model_name='clip_vit_base_patch32',
+modality='text')) .map('vec', 'vec', ops.towhee.np_normalize()) # faiss op
+result format: [[id, score, [file_name], ...] .map('vec', 'row',
+ops.ann_search.faiss_index('./faiss', 3)) .map('row', 'images', lambda x:
+[decode(item[2][0]) for item in x]) .output('text', 'images') ) DataCollection
+(p('a cat')).show() ``` [assets/towhee_example.png] ## Core Concepts Towhee is
+composed of four main building blocks - `Operators`, `Pipelines`,
+`DataCollection API` and `Engine`. - __Operators__: An operator is a single
+building block of a neural data processing pipeline. Different implementations
+of operators are categorized by tasks, with each task having a standard
+interface. An operator can be a deep learning model, a data processing method,
+or a Python function. - __Pipelines__: A pipeline is composed of several
+operators interconnected in the form of a DAG (directed acyclic graph). This
+DAG can direct complex functionalities, such as embedding feature extraction,
+data tagging, and cross modal data analysis. - __DataCollection API__: A
+Pythonic and method-chaining style API for building custom pipelines, providing
+multiple data conversion interfaces: map, filter, flat_map, concat, window,
+time_window, and window_all. Through these interfaces, complex data processing
+pipelines can be built quickly to process unstructured data such as video,
+audio, text, images, etc. - __Engine__: The engine sits at Towhee's core. Given
+a pipeline, the engine will drive dataflow among individual operators, schedule
+tasks, and monitor compute resource usage (CPU/GPU/etc). We provide a basic
+engine within Towhee to run pipelines on a single-instance machine and a
+Triton-based engine for docker containers. ## Resource - TowheeHub: https://
+towhee.io/ - docs: https://towhee.readthedocs.io/en/latest/ - examples: https:/
+/github.com/towhee-io/examples ## Contributing Writing code is not the only way
+to contribute! Submitting issues, answering questions, and improving
+documentation are just some of the many ways you can help our growing
+community. Check out our [contributing page](https://github.com/towhee-io/
+towhee/blob/main/CONTRIBUTING.md) for more information. Special thanks goes to
+these folks for contributing to Towhee, either on Github, our Towhee Hub, or
+elsewhere:
 
 [https://img.shields.io/badge/all--contributors-33-orange]
 [https://avatars.githubusercontent.com/u/34787227?v=4] [https://
 avatars.githubusercontent.com/u/72550076?v=4] [https://
 avatars.githubusercontent.com/u/57477222?v=4] [https://
 avatars.githubusercontent.com/u/109071306?v=4] [https://
+avatars.githubusercontent.com/u/21202514?v=4] [https://
 avatars.githubusercontent.com/u/20420181?v=4] [https://
 avatars.githubusercontent.com/u/88148730?v=4] [https://
 avatars.githubusercontent.com/u/83755740?v=4] [https://
 avatars.githubusercontent.com/u/11754703?v=4] [https://
 avatars.githubusercontent.com/u/47691077?v=4] [https://
 avatars.githubusercontent.com/u/81822489?v=4] [https://
 avatars.githubusercontent.com/u/6334158?v=4] [https://
@@ -182,12 +140,11 @@
 avatars.githubusercontent.com/u/107831450?v=4] [https://
 avatars.githubusercontent.com/u/1500781?v=4] [https://
 avatars.githubusercontent.com/u/56469371?v=4] [https://
 avatars.githubusercontent.com/u/83750738?v=4] [https://
 avatars.githubusercontent.com/u/5432721?v=4] [https://
 avatars.githubusercontent.com/u/17022025?v=4] [https://
 avatars.githubusercontent.com/u/68835157?v=4] [https://
-avatars.githubusercontent.com/u/7541863?v=4] [https://
-avatars.githubusercontent.com/u/18031320?v=4]
+avatars.githubusercontent.com/u/7541863?v=4]
 
 Looking for a database to store and index your embedding vectors? Check out
 [Milvus](https://github.com/milvus-io/milvus).
```

## Comparing `towhee.models-1.0.0rc1.dist-info/RECORD` & `towhee.models-1.1.0.dist-info/RECORD`

 * *Files 6% similar despite different names*

```diff
@@ -16,23 +16,23 @@
 towhee/models/allinone/allinone.py,sha256=hP4gtH72FWgGD3p5CjKtP3CHFY5zvxDtzw-9iNe3wZ8,1289
 towhee/models/bridgeformer/__init__.py,sha256=h9OJzhiUEUCSBT1oOYi4nnuFAizmS53RvzuzVyAW7rg,622
 towhee/models/bridgeformer/bridge_former.py,sha256=8fHp_7hlg_GmRzsiRJEkjhTfP1hZRaacngtIkji0Py0,3300
 towhee/models/bridgeformer/bridge_former_training.py,sha256=II7mve21FBguaNNuTtenOMbBrarbLVrLnCOuUeftXJM,10421
 towhee/models/bridgeformer/bridge_former_training_block.py,sha256=erc-O7bOi-uoobxrHzLR_C03qKoq32S5vyCBGKqM9qc,16155
 towhee/models/clip/README.md,sha256=JkkWMVQDVR1AsUPBgO5qJ5ZnaAUhCFx5c9mZJzcb0V0,1560
 towhee/models/clip/__init__.py,sha256=IaeYF9bJeWL3q1X5GH_LJQx2m4yl7wYWKCVMHI7U3oU,639
-towhee/models/clip/auxilary.py,sha256=RaxNl99HbwiBlTDxjx1jQf0x-qeQbCEqJRaToT39HxI,20204
+towhee/models/clip/auxilary.py,sha256=v5_k8tcZCLpxOwZRij6ZYsg4flc01c-Ju6e2253s5NM,20896
 towhee/models/clip/bpe_simple_vocab_16e6.txt.gz,sha256=kkaRrCiOVECSNhFWUq1KolD0ggPeUKnkcipuzUjWgEo,1356917
 towhee/models/clip/clip.py,sha256=C5I9TBXO3h4L79Bnw6guH85qg5Fi7RSzbYIkd4DpCII,30031
 towhee/models/clip/clip_utils.py,sha256=_4rcQ0PRefv_AxAPWbahWRIHVdv8gaC7NpjIy15evhA,9935
 towhee/models/clip/simple_tokenizer.py,sha256=zTiYCOHcB94uoVyZdWzqJzI5GZMgE-vuFJmp9Qk458o,5618
 towhee/models/clip4clip/__init__.py,sha256=QhA4vIRpuETlUOVYywNc8nPe60gRxcl0r-tM0jM3Q-Q,46
 towhee/models/clip4clip/clip4clip.py,sha256=pjJuQQLsywQb5f5SNacsuS1NXSW4alo43l8FwkQ3lMM,10156
 towhee/models/clip4clip/until_module.py,sha256=I_mP9z0QQQzg3KxYcSa7g9hEIp_EPgx1EpEmjZpglaY,2605
-towhee/models/clip4clip/utils.py,sha256=oXXRWRIWFg7jt0t_y-ccC2-7gj-9o70zecdzueuKRDQ,1837
+towhee/models/clip4clip/utils.py,sha256=sZORWy6cdmqxEHo7THNbX2VIn52JyrgVNXJAthkyLrk,2536
 towhee/models/coca/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 towhee/models/coca/coca.py,sha256=19aMdsJj_zq2wjEFzJ4zqXPpJ8dZRnmtRuhtIUNvpW0,11514
 towhee/models/coformer/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 towhee/models/coformer/backbone.py,sha256=daSobWDRtjXXg-_v9Uzh3UssqlzfE4nakcr7lSOqPxU,5167
 towhee/models/coformer/coformer.py,sha256=tU_bRAB_jCmBZ13pqBaYg6TJhNMOuogos8IPNKWjnpU,10222
 towhee/models/coformer/config.py,sha256=cA-AEvqzluEqfmCZ3hy51oR7LPwmYLrBKktTR2DAFNk,10008
 towhee/models/coformer/transformer.py,sha256=_3nnNXJ5HD4wbcCc1Y4BOamcP7VYkQx6fncc-Jms2lQ,15471
@@ -47,16 +47,16 @@
 towhee/models/convnext/utils.py,sha256=nrm5J9agCfmn55h263sg0R6E9kP-agAWqpoIf4Hensk,4107
 towhee/models/cvnet/__init__.py,sha256=pHfteInPOBaVxm4c_-jetHWySrL2-WJ2I2Hzg-XhaVw,614
 towhee/models/cvnet/cvnet.py,sha256=CXghdOFn3heDbRcjmeEiXy0jiWTme__4i8HJxWA5Qdg,4665
 towhee/models/cvnet/cvnet_block.py,sha256=d-_LKK6oE9gQvGTYaK8D7mvnkowb8lV-D1DIDBJDWAM,6640
 towhee/models/cvnet/cvnet_utils.py,sha256=_dbGLWHSI14gvu7Wu4tyGAJLrPvpiPmAtzdyz47NE_I,2439
 towhee/models/cvnet/resnet.py,sha256=OiEo3KzbSPsPH_JCl1DMBhqZFxPMZiAVWZr-AO6Ba_w,8106
 towhee/models/drl/__init__.py,sha256=PgP1p-VWs-ePWvrYIxD5matG3lIz5muH_q7qP3MYrkk,19
-towhee/models/drl/drl.py,sha256=130rbEJYqTJV8TiXt9Tui2qj8fTBb4TDTlMzynkkY6E,34568
-towhee/models/drl/module_cross.py,sha256=g5xjqpUrp7M8wFBsK5Zk4lvVuwej_1xeVgpuINUYqIk,7936
+towhee/models/drl/drl.py,sha256=Xza7kjU4qxJvKAxW6-hA_cQt-Tf01Z-Kw_tGpRef01I,35156
+towhee/models/drl/module_cross.py,sha256=W4XCMbxJRQulGrBXXHQv1ehqfufzbp1HqIU4Mt8aiOY,8633
 towhee/models/drl/until_module.py,sha256=37QJUYt6Vy_69VYMhFEfwltq1XPY5M30L44kB_pkoc0,4891
 towhee/models/embedding/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 towhee/models/embedding/embedding_extractor.py,sha256=rIDHuT2Twi1NSbkw57t4A4vpLyACuDhRZThYv0FoT-E,2061
 towhee/models/frozen_in_time/__init__.py,sha256=kUP9-Oplu2lj1C4Xv9qYFN0CZcY7FZokSr2wtViaXDM,623
 towhee/models/frozen_in_time/frozen_in_time.py,sha256=MWK4kyzbOGxs2g2Q9O6HAJ8Kv2PGP7mtLtLICEqxzC8,14921
 towhee/models/frozen_in_time/frozen_utils.py,sha256=TpMVcCebdKlkR9-km_L-J6Spbm9FFiUMhpF5XBSo3i4,2394
 towhee/models/frozen_in_time/frozen_video_transformer.py,sha256=vJUF_X85nwaZKGqWbiKKL3XLk-8iaTBWujxZHQP1B0A,15910
@@ -77,42 +77,42 @@
 towhee/models/layers/convmlp.py,sha256=jnlz-ssDOYBPVmXn97kTgAlIYclFE2N4eTmKll8hoJM,1918
 towhee/models/layers/cross_attention.py,sha256=eUFv8d0RBZecBOoECFOfa7unzwv0KmXjeHMenYn44PU,4369
 towhee/models/layers/dropblock2d.py,sha256=6tMUuHN-x4LsXx2BXPZBbwi7vjVZCrKnnq3TCmouVEw,5964
 towhee/models/layers/droppath.py,sha256=A9ZBS2vmuRu84rbOlMeNi_LOCbGsTaZr0Dcd507xeUs,2200
 towhee/models/layers/ffn.py,sha256=mRGZxmxPe-IDfm4ZiN7z97Cm8ARp20-LccArcgkQbdU,2175
 towhee/models/layers/gatedmlp.py,sha256=bfRWzBsbZRU2lZ729tqZKqtilX9fZOJfCqi7WAcnjls,2052
 towhee/models/layers/layers_with_relprop.py,sha256=2yLFTbdyBF4re2a6jiMhK3Xdy9gFHJGTV20G7FZCrw8,10670
-towhee/models/layers/mbconv.py,sha256=IUfbV7IPr9v1PDvt3sagqed79uLiVwrddi-A9Sua8lM,4330
+towhee/models/layers/mbconv.py,sha256=wd-1IlywtBZkg7PwD9BKK5yjCnT7OB7jYZnG-FBlQ3s,4347
 towhee/models/layers/mixed_conv2d.py,sha256=j2BASNK70DBF3nvx7TCs4PHxCbu7tcE1hus_MJA26fY,2971
 towhee/models/layers/mlp.py,sha256=fnHJ0NkK_--SIQONjwOINuf7g9QckQSl0DZ-CIevtHk,2566
 towhee/models/layers/multi_scale_attention.py,sha256=p4aDkyazhS7Y4ub69Znm6A7ZuR_2m1evjGzmYq6vkcE,13234
 towhee/models/layers/multi_scale_transformer_block.py,sha256=r88UqixYETXnuSS9N6JDfEnxlESHwljfJP6Hv-gY-VI,6651
 towhee/models/layers/netvlad.py,sha256=KdDpg43AEytRN5GCIJfQcL-jcB5DwzVyxvKS9Ll73Zo,3312
-towhee/models/layers/non_local.py,sha256=ANY3NKTaEyq0I1FF5UJtQWeEUZRSr9rr5gfKoBIrI0c,5460
+towhee/models/layers/non_local.py,sha256=VGHG7BK7iav1myLa9lv7KeZfiHlEP4-B5s-6YHrD5r4,6047
 towhee/models/layers/padding_functions.py,sha256=5MDtYqAvEeha8Fd4jsr996tIg-QjR5RF-tNfd9Pny_o,5759
 towhee/models/layers/patch_embed2d.py,sha256=J9FOBxzjNXPCKxNRRlPiPD0bigAhYr8wD9Qb81185Mo,4015
-towhee/models/layers/patch_embed3d.py,sha256=oBtUEEy7VwLPhJJMjoClef4Ov1ZpLm_8m_fRQ1wFINE,3033
+towhee/models/layers/patch_embed3d.py,sha256=ZG-5YM-oCEqryTfbpSfVt3pA2I6IesxnYzrOpajCrtI,3620
 towhee/models/layers/patch_merging.py,sha256=3FK6sGdzIsi3gd7WhKwO6s5IKASM3tHyq9JMCbc5tPo,2458
-towhee/models/layers/patch_merging3d.py,sha256=edze-u7VNcxKU7jokgUCxJTdCAB0MWyRqeNVcRh9K8M,2423
+towhee/models/layers/patch_merging3d.py,sha256=wWjAdpdl5sBlOnH8Dy09XsErfq7AEomasVaFWJ-1jMA,3009
 towhee/models/layers/pool_attention.py,sha256=0FLseQrqI_r4nILARmMjY3_vfbcfmYbxCQTeXox2LOY,3343
 towhee/models/layers/position_encoding.py,sha256=3WPEgedMAbVKw3M6zKtSC0lhjRhofm0z5CFk6Zlur2o,3084
 towhee/models/layers/relative_self_attention.py,sha256=BiVkQlGwrtHhrFy3K3yiljEHdcvINqoprK2MxMMa5hE,4749
 towhee/models/layers/resnet_basic_3d_module.py,sha256=jPvB67qiFvpAGOQKCKJoOwPkRDRP7P7lks40-xBIL5A,2028
 towhee/models/layers/sam.py,sha256=EcH6645CtlLo41ms3pn3rfUzlu2LOTfA5iCddaVCmLo,3273
 towhee/models/layers/sequence_pool.py,sha256=UffEY2rITG3VHd7SspfNlP4MTT1Sub5XPf7K_nCas-0,1500
 towhee/models/layers/spatial_temporal_cls_positional_encoding.py,sha256=69gYjS8i6l3zKqjtff1ta9xHqnyKrbXoXz_haw7yeTA,3663
 towhee/models/layers/spp.py,sha256=8L39B8I_92Mg0haJvg14Lireuf-jTX94vWxI6rNm-X4,639
-towhee/models/layers/swin_transformer_block3d.py,sha256=DjggNXje8tNEfrmPWtdjl9eSauioW7zdGDuygD4Zqn8,5347
+towhee/models/layers/swin_transformer_block3d.py,sha256=gdW64Vu9lpJSLEsccNAyvbbfMtwQHKFBLoMsFJfFAIg,5934
 towhee/models/layers/temporal_cg_avgpool3d.py,sha256=QjTdGipUdPBGQfR7ni2KWOMkXWQx6hUnnkujDseM2PU,1964
 towhee/models/layers/tf_avgpool3d.py,sha256=RjTlcErKEIp63VqG0TNA338xH0U74Gwj2BprtWvi45A,1964
 towhee/models/layers/time2vec.py,sha256=VAB9f0gIM_Xo785iyT8TQew6NPtqtnUi_9BJD5q8vXo,2351
 towhee/models/layers/transformer_encoder.py,sha256=5YG01j38J4dIw1_nEs5-aPO5axhjZ4h4ttERm1D5FWI,2054
 towhee/models/layers/vision_transformer_basic_head.py,sha256=wLPS9vr3cW12028Tu0OrIeWJGVBK4A8jUGAnYm8JkyM,3223
 towhee/models/layers/window_attention.py,sha256=CsVxR3X6ffPZ3HJ_UW952ZA_MQmP8UUCJeMo5uwEGJ4,8064
-towhee/models/layers/window_attention3d.py,sha256=KVK4CGHY8QYy2a2p5x_StJA7jYofbVK9JHX_bJMk3RQ,4765
+towhee/models/layers/window_attention3d.py,sha256=Xg5TE49rTG9U_NQa_MSJye0EH6ZNqt2_XwaZElJ0SBo,5352
 towhee/models/layers/activations/__init__.py,sha256=mxenUeAqnWDmGzgHkWYhAfnmd0FINRpfdgtTykMzA0Q,1202
 towhee/models/layers/activations/gelu.py,sha256=Xgmvt0n1WweQyh-xI1iGGAzd5iGBDWwXvpmyZHJpohY,1312
 towhee/models/layers/activations/hardmish.py,sha256=EFSgWphSJXYwWP3Txos1JibZK_55zBld6RSWpUYW0xw,1393
 towhee/models/layers/activations/hardsigmoid.py,sha256=UDd60VxfxLMBiQvLgxKuzNoZQgFq2PzxPzbz-WBJvCY,1482
 towhee/models/layers/activations/hardswish.py,sha256=3B8RwCSRXfJUlEdMylpMpBrJR5cWJCFdChRGiP43fSg,1424
 towhee/models/layers/activations/mish.py,sha256=pU6QPlf2SmnnjNjCsiPO3C4KPIBT7ZcAlH2d2-VuJww,1328
 towhee/models/layers/activations/prelu.py,sha256=wZWiGrDyA82-54wtARVv7XAuU6ujBXsYjzJDWRz0QxM,1320
@@ -182,32 +182,32 @@
 towhee/models/replknet/replknet.py,sha256=d-cLaUNY40Ast1maZ7x0U28WfMwoqrYBRW3FG0DpFAM,8942
 towhee/models/replknet/utils.py,sha256=Hchocfoxrocd8ITlJPtezMkVsUQslRcEP31QSUJpmFw,10184
 towhee/models/repmlp/__init__.py,sha256=J60Eky7m7b0o-TxUEdB_52PMfwX7T3NwgMAJ5GzVyHs,637
 towhee/models/repmlp/blocks.py,sha256=HeOsvcLFBqlkRmOeQe5m--iJV06SRVY6t_j-4YIREFI,8448
 towhee/models/repmlp/configs.py,sha256=eGCDvRRT4ZGqUggYnAFTU5rhEJLf-EIf8KuT-tmBnL8,2339
 towhee/models/repmlp/repmlp.py,sha256=ykW7W42zWF00qaxnvWRvzWm9jAL8b9Pypj5FjPc7C6A,7259
 towhee/models/retina_face/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-towhee/models/retina_face/configs.py,sha256=A4A4kIRdQA2IV8kQ4FRJNGx0FTRbSq7xCEioWe_LNXk,1503
-towhee/models/retina_face/heads.py,sha256=sSg31BAmEI1w5BrpKRzMRvXsp3ktasSEPA9glipYgNc,2552
-towhee/models/retina_face/mobilenet_v1.py,sha256=0-m0fI5aAxtzLZUxzpjK6CDidO1S5NgsyIllg0Ar_tg,2057
-towhee/models/retina_face/prior_box.py,sha256=0c-ulN3jeUifJ5iHRloNnWcpXjXpfuNNuE8QPxqFDeI,2194
-towhee/models/retina_face/retinaface.py,sha256=JvGzzrWWudYpr0rwk1KiAaOE6wDI2h7mOdDLErNc9sk,6531
-towhee/models/retina_face/retinaface_fpn.py,sha256=4nBL-O2VonweoyEqQgf15HTjkIrlV_vbKuBj8tHF14c,2350
-towhee/models/retina_face/ssh.py,sha256=RnwRfs2AUd8N0wHFublnrOSzrUO6-3Ek7tDms5z4etE,2140
-towhee/models/retina_face/utils.py,sha256=_yQ0DH-oU3476FgquSJteKwYK_Oe5kuf5-9vE892qlg,5194
+towhee/models/retina_face/configs.py,sha256=nLccsIvQDgo-r78JBWvpwORTA5kv__ku6m_tp4JLo2E,2633
+towhee/models/retina_face/heads.py,sha256=NwzHBWVfnOh6saE_7_BjQTf48eg77GGXtsInokhcJBw,3052
+towhee/models/retina_face/mobilenet_v1.py,sha256=Xr3GC_C-hIKimS8oPWI6bpA_oZ3Ua6AdWjxJY-36CZE,2557
+towhee/models/retina_face/prior_box.py,sha256=o57TfQhm9OxuBymlH9LXtGOVsv0hPkP4MumtqVhJwnc,2694
+towhee/models/retina_face/retinaface.py,sha256=JhhYgr4v0B8J1FZYO04ewqqzXmkKyFUDwISeQXcV6Rc,7031
+towhee/models/retina_face/retinaface_fpn.py,sha256=H4TUpbQWF7jiiZD8wU-47qJSbvIZqqa8BdlvpQI0l9I,2850
+towhee/models/retina_face/ssh.py,sha256=EHKwc6HmQ5ZczWWiSDIhgjmCijbQsJd9xHKuutBpOlM,2640
+towhee/models/retina_face/utils.py,sha256=XlCic3Dp-2Uf9eFdMoAKqGowOS82phBSkBJnLhQSP8E,5694
 towhee/models/shunted_transformer/__init__.py,sha256=Z5SZfoRTDXwlw35W1A0Bb136qhUxCL4kEY0p54Ytrfo,672
 towhee/models/shunted_transformer/configs.py,sha256=hRbLJiBzGjwVlhUGd3XpEre5Ym_ZKqMQTF8rgNWzQwA,2134
 towhee/models/shunted_transformer/shunted_transformer.py,sha256=4gn6mmvW4J7B1fasCYUlo4Sw9iau_7Uduvt3rZ5oOgs,5411
 towhee/models/shunted_transformer/utils.py,sha256=jckB2vRVYIlGB7xJqAWQTmoMKqvCl9h59nTIU8eUYkY,10283
 towhee/models/svt/__init__.py,sha256=vnNhppuB2VQRKT-MkUDU-gkRCPGH7IDw67eyZpHVG8o,612
 towhee/models/svt/svt.py,sha256=iHSRDnfg-bBu6lttzgILaSzJTPDLaxNcJCiLtY0VV88,2925
 towhee/models/svt/svt_utils.py,sha256=t367VNH_aZmClB20Z1PEZNyUqe1TuNYZX41mnlnmS_g,1375
 towhee/models/swin_transformer/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 towhee/models/swin_transformer/basic_layer.py,sha256=70Y9V-4aYvRDm_2ZWqf1kALvN3z4RmbhajNsP8hdg4U,3558
-towhee/models/swin_transformer/configs.py,sha256=nUkM9R5wZXExGet6-WqxwCbF099P3QENF9xW0jmW--4,9904
+towhee/models/swin_transformer/configs.py,sha256=i_ibMhu7XAw5PDUPUp8HIsDOSkq-3fcc_Z6c1osg0fw,10536
 towhee/models/swin_transformer/model.py,sha256=M5_BRrSVbA0hIbNh6bZ4V5tlhOjpalLYa1ekymm_3bI,7134
 towhee/models/swin_transformer/swin_transformer_block.py,sha256=17KLxeOaxtoySWlBBgELmwaYuHrPGA_Er07UiK0HbBY,6114
 towhee/models/timesformer/__init__.py,sha256=fEDKuG1Eh6FN9C0_5PdS8x4aUADWs0pr3mg158E38fU,620
 towhee/models/timesformer/timesformer.py,sha256=Su5soEogKh0_ux801pljpPcACQ1ZzWgFOvhxAPUSd8A,9537
 towhee/models/timesformer/timesformer_block.py,sha256=71DJjqvLLr6LfkYWHhfxQSGr91F04Ctst3E5HTFXE1M,6249
 towhee/models/timesformer/timesformer_utils.py,sha256=YjPbKxeIA_PicYFNkZQGAjSRkejdLsamBaBYvVVZ44Q,9090
 towhee/models/transrac/__init__.py,sha256=h-7NsGXv-3LRNLsDOWOa3sOpCLkhREiIs49SBH_72YY,638
@@ -231,24 +231,24 @@
 towhee/models/utils/create_model.py,sha256=a5KGNZqJ9tts112aF0p9sY7q0tVKCmrPG638fPbmd3c,1664
 towhee/models/utils/create_resnet_basic_3d_module.py,sha256=7JEnHgidZDBk1fxRh8TENdQdEqUO35OIr9EEUIW5rLI,3929
 towhee/models/utils/download.py,sha256=2K6hvYBOcmKbRXOkE04eIyQQ_qOALGjCif0Q4CTV16c,3658
 towhee/models/utils/fuse_bn.py,sha256=GbpzHVfpS3FP3VExxkM57eTPkn7BfQFqS-dOSohZmXo,1208
 towhee/models/utils/gelu_ignore_parameters.py,sha256=2EC_t85IK1NVLvRxnHnxSMo9-CS_oKLV8crM6B-iMUA,1153
 towhee/models/utils/general_utils.py,sha256=dNFsR-gSLgW8MrbkXG20SSCyibvRaWjfc2-KXs_XnFg,1370
 towhee/models/utils/get_relative_position_index.py,sha256=rCuKguEGgN3FGn4TKa0jFQjAU0gVUf8fRyEr-qTUtjQ,1720
-towhee/models/utils/get_window_size.py,sha256=T6gtB0O8WrgXPj8iBBo2P6Q5H6M42AXT-eOJxP3Hw5Q,958
+towhee/models/utils/get_window_size.py,sha256=Ph2GKUvRq5L0KkEBlxt16FuyGdT5127rspQgltVvX14,1545
 towhee/models/utils/init_vit_weights.py,sha256=f0otLPFJNW-yILzoSINctOQTiANGKaCwGzqABE_LgqM,2642
-towhee/models/utils/pretrained_utils.py,sha256=O9bZPldWkr_g2vwZuqEDvMAs3qYKdyLC0odzCoaQcDc,1439
+towhee/models/utils/pretrained_utils.py,sha256=7hWQkYQC4zEhw3TOKyx11huW0Muctz6fp3QUiyKFhy4,2032
 towhee/models/utils/round_width.py,sha256=dIEkQbvvTU_hDasq8AzaEg2gErzE3qPJAaDiebvhOkU,1556
 towhee/models/utils/video_transforms.py,sha256=K0gt7QL7xwXkiW1nxYH-qL4ZM9KyOrv_0LbYXmXQkYE,8505
 towhee/models/utils/weight_init.py,sha256=3RYMXkDV5csIJMlpV6nMiMyf_0Z_EDw59hAjDCgN5qs,3957
-towhee/models/utils/window_partition.py,sha256=AJ4PUP4BXvnzMVPw_VwCrYpvfdt3d0OM7nZcGJcIGag,527
-towhee/models/utils/window_partition3d.py,sha256=-mBilv7KuJYnORgQqDDJufuiScZCrl39VkibB4yJksg,891
-towhee/models/utils/window_reverse.py,sha256=UL_cRNJWncNfBSecUOvkxxWMLbOprVWUHpd5hEBm0nc,690
-towhee/models/utils/window_reverse3d.py,sha256=oLFBUAfoHKwV1rlJOpp7ztpgZVAGnpNsoIgicTk3LWA,887
+towhee/models/utils/window_partition.py,sha256=FlFjoLsrrWFhd6dyW8PaRFTiWAc-gy8lTFIo4TOj_k8,1114
+towhee/models/utils/window_partition3d.py,sha256=dsYYCwuzS8g9sWmu5EsBFdL6D3vNvGKN-QwFmYoxlo0,1478
+towhee/models/utils/window_reverse.py,sha256=uMPTlYvfLDoJrmi8Fg8JIigVIg65e8h5cGrzuW-vS28,1277
+towhee/models/utils/window_reverse3d.py,sha256=Au0asxrw_OOQNwLIbnK_h-yxvylzXvtmjVZNazwqcog,1474
 towhee/models/vggish/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 towhee/models/vggish/torch_vggish.py,sha256=cuAIRk2TI5IxR3hkFOzZQgoW0XGUr-yamZ6oIyFRLBE,2049
 towhee/models/video_swin_transformer/__init__.py,sha256=V0d26HNa_p1PWdsj4qq_9_Zog1wr4Yf_3pjNa7u-o0Q,824
 towhee/models/video_swin_transformer/compute_mask.py,sha256=iAsRY_2l5zhz-nkG5b99zmFz3YZ7NAMQS-bXuUFS2VQ,1643
 towhee/models/video_swin_transformer/get_configs.py,sha256=2RA1BaCZTTu0qygLrfNS6aFo--pgX-HryWgeC2QWvAQ,3992
 towhee/models/video_swin_transformer/video_swin_transformer.py,sha256=9IDETk-T5KrMW5SjRj9IDZ-oE8hS5ROiWXQQFKND8Lw,14573
 towhee/models/video_swin_transformer/video_swin_transformer_block.py,sha256=QykEXDJKjiSLVUYHfBKuRTzrjNimNIenqmDpWCL8coQ,3925
@@ -268,13 +268,13 @@
 towhee/models/vit/vit.py,sha256=JCpAKp-e2U3ss45Ji-mJ9qRaAJpN7YBRdAHD98mTkaY,10295
 towhee/models/vit/vit_block.py,sha256=zprOJCOX2z273iMenk-ex9-Rr53SIvzQbCHCrJCqsm4,3878
 towhee/models/vit/vit_utils.py,sha256=y5PwJKR57TIwiO3ZssRO2r0esJLCiguYekhVkLUfZSI,1868
 towhee/models/wave_vit/__init__.py,sha256=oyVqphD9VR93Kj--l4TvVS-k-d3fqGuvEJcMJzohHaQ,617
 towhee/models/wave_vit/wave_vit.py,sha256=KqsLjc2DOQpjaIHvD909u6YNXSb7gGpv_VjbAYWC2aw,10014
 towhee/models/wave_vit/wave_vit_block.py,sha256=15DAbuYWb8SKr2crC4uS18W6bXmD_yClrPshaPYpriQ,9731
 towhee/models/wave_vit/wave_vit_utils.py,sha256=8nYx23xalB26swYnut2WlzWCj45md-1-hVHFoAydHPE,6361
-towhee.models-1.0.0rc1.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
-towhee.models-1.0.0rc1.dist-info/METADATA,sha256=vTD7XpAd9hJM6-FRP3SMdyLqRgoGX4w2_YSDcetpWY0,17255
-towhee.models-1.0.0rc1.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-towhee.models-1.0.0rc1.dist-info/entry_points.txt,sha256=tGMn2QCTr-tOrFxMYYTyubVJ-ZOXIP-qf6P5GXOJWxI,55
-towhee.models-1.0.0rc1.dist-info/top_level.txt,sha256=s8O0-CAA8lmENHKY-FR5ojkSAmqEOEkrpg6ITjplm4A,7
-towhee.models-1.0.0rc1.dist-info/RECORD,,
+towhee.models-1.1.0.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
+towhee.models-1.1.0.dist-info/METADATA,sha256=OWqBV3QZhOAJhjYIm64DQSHWc9ezweQuAuwGlz5Akms,13974
+towhee.models-1.1.0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+towhee.models-1.1.0.dist-info/entry_points.txt,sha256=tGMn2QCTr-tOrFxMYYTyubVJ-ZOXIP-qf6P5GXOJWxI,55
+towhee.models-1.1.0.dist-info/top_level.txt,sha256=s8O0-CAA8lmENHKY-FR5ojkSAmqEOEkrpg6ITjplm4A,7
+towhee.models-1.1.0.dist-info/RECORD,,
```

