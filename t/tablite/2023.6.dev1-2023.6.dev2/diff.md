# Comparing `tmp/tablite-2023.6.dev1-py3-none-any.whl.zip` & `tmp/tablite-2023.6.dev2-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,33 +1,33 @@
-Zip file size: 77208 bytes, number of entries: 31
--rw-r--r--  2.0 unx      244 b- defN 23-Jun-06 17:23 tablite/__init__.py
--rw-r--r--  2.0 unx    55667 b- defN 23-Jun-06 17:23 tablite/base.py
--rw-r--r--  2.0 unx     2262 b- defN 23-Jun-06 17:23 tablite/config.py
--rw-r--r--  2.0 unx    27578 b- defN 23-Jun-06 17:23 tablite/core.py
--rw-r--r--  2.0 unx     4504 b- defN 23-Jun-06 17:23 tablite/datasets.py
--rw-r--r--  2.0 unx    26976 b- defN 23-Jun-06 17:23 tablite/datatypes.py
--rw-r--r--  2.0 unx     2872 b- defN 23-Jun-06 17:23 tablite/diff.py
--rw-r--r--  2.0 unx     5340 b- defN 23-Jun-06 17:23 tablite/export_utils.py
--rw-r--r--  2.0 unx    10141 b- defN 23-Jun-06 17:23 tablite/file_reader_utils.py
--rw-r--r--  2.0 unx     4889 b- defN 23-Jun-06 17:23 tablite/groupby_utils.py
--rw-r--r--  2.0 unx     5652 b- defN 23-Jun-06 17:23 tablite/groupbys.py
--rw-r--r--  2.0 unx    17867 b- defN 23-Jun-06 17:23 tablite/import_utils.py
--rw-r--r--  2.0 unx     7589 b- defN 23-Jun-06 17:23 tablite/imputation.py
--rw-r--r--  2.0 unx    12182 b- defN 23-Jun-06 17:23 tablite/joins.py
--rw-r--r--  2.0 unx     6737 b- defN 23-Jun-06 17:23 tablite/lookup.py
--rw-r--r--  2.0 unx     3199 b- defN 23-Jun-06 17:23 tablite/mp_utils.py
--rw-r--r--  2.0 unx     8863 b- defN 23-Jun-06 17:23 tablite/pivots.py
--rw-r--r--  2.0 unx     9171 b- defN 23-Jun-06 17:23 tablite/redux.py
--rw-r--r--  2.0 unx     6098 b- defN 23-Jun-06 17:23 tablite/sort_utils.py
--rw-r--r--  2.0 unx     5299 b- defN 23-Jun-06 17:23 tablite/sortation.py
--rw-r--r--  2.0 unx     1125 b- defN 23-Jun-06 17:23 tablite/tools.py
--rw-r--r--  2.0 unx    11195 b- defN 23-Jun-06 17:23 tablite/utils.py
--rw-r--r--  2.0 unx      139 b- defN 23-Jun-06 17:23 tablite/version.py
--rw-r--r--  2.0 unx     1069 b- defN 23-Jun-06 17:23 tablite-2023.6.dev1.data/data/LICENSE
--rw-r--r--  2.0 unx     6960 b- defN 23-Jun-06 17:23 tablite-2023.6.dev1.data/data/README.md
--rw-r--r--  2.0 unx      246 b- defN 23-Jun-06 17:23 tablite-2023.6.dev1.data/data/requirements.txt
--rw-r--r--  2.0 unx     1069 b- defN 23-Jun-06 17:23 tablite-2023.6.dev1.dist-info/LICENSE
--rw-r--r--  2.0 unx     8677 b- defN 23-Jun-06 17:23 tablite-2023.6.dev1.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jun-06 17:23 tablite-2023.6.dev1.dist-info/WHEEL
--rw-r--r--  2.0 unx        8 b- defN 23-Jun-06 17:23 tablite-2023.6.dev1.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     2457 b- defN 23-Jun-06 17:23 tablite-2023.6.dev1.dist-info/RECORD
-31 files, 256167 bytes uncompressed, 73340 bytes compressed:  71.4%
+Zip file size: 79200 bytes, number of entries: 31
+-rw-r--r--  2.0 unx      244 b- defN 23-Jun-09 09:49 tablite/__init__.py
+-rw-r--r--  2.0 unx    56819 b- defN 23-Jun-09 09:49 tablite/base.py
+-rw-r--r--  2.0 unx     2323 b- defN 23-Jun-09 09:49 tablite/config.py
+-rw-r--r--  2.0 unx    28136 b- defN 23-Jun-09 09:49 tablite/core.py
+-rw-r--r--  2.0 unx     4504 b- defN 23-Jun-09 09:49 tablite/datasets.py
+-rw-r--r--  2.0 unx    26976 b- defN 23-Jun-09 09:49 tablite/datatypes.py
+-rw-r--r--  2.0 unx     2872 b- defN 23-Jun-09 09:49 tablite/diff.py
+-rw-r--r--  2.0 unx     7759 b- defN 23-Jun-09 09:49 tablite/export_utils.py
+-rw-r--r--  2.0 unx    10112 b- defN 23-Jun-09 09:49 tablite/file_reader_utils.py
+-rw-r--r--  2.0 unx     4889 b- defN 23-Jun-09 09:49 tablite/groupby_utils.py
+-rw-r--r--  2.0 unx     5621 b- defN 23-Jun-09 09:49 tablite/groupbys.py
+-rw-r--r--  2.0 unx    22204 b- defN 23-Jun-09 09:49 tablite/import_utils.py
+-rw-r--r--  2.0 unx     7589 b- defN 23-Jun-09 09:49 tablite/imputation.py
+-rw-r--r--  2.0 unx    12181 b- defN 23-Jun-09 09:49 tablite/joins.py
+-rw-r--r--  2.0 unx     6737 b- defN 23-Jun-09 09:49 tablite/lookup.py
+-rw-r--r--  2.0 unx     3199 b- defN 23-Jun-09 09:49 tablite/mp_utils.py
+-rw-r--r--  2.0 unx     8863 b- defN 23-Jun-09 09:49 tablite/pivots.py
+-rw-r--r--  2.0 unx     9171 b- defN 23-Jun-09 09:49 tablite/redux.py
+-rw-r--r--  2.0 unx     6098 b- defN 23-Jun-09 09:49 tablite/sort_utils.py
+-rw-r--r--  2.0 unx     5299 b- defN 23-Jun-09 09:49 tablite/sortation.py
+-rw-r--r--  2.0 unx     1125 b- defN 23-Jun-09 09:49 tablite/tools.py
+-rw-r--r--  2.0 unx    11195 b- defN 23-Jun-09 09:49 tablite/utils.py
+-rw-r--r--  2.0 unx      139 b- defN 23-Jun-09 09:49 tablite/version.py
+-rw-r--r--  2.0 unx     1069 b- defN 23-Jun-09 09:49 tablite-2023.6.dev2.data/data/LICENSE
+-rw-r--r--  2.0 unx     6960 b- defN 23-Jun-09 09:49 tablite-2023.6.dev2.data/data/README.md
+-rw-r--r--  2.0 unx      246 b- defN 23-Jun-09 09:49 tablite-2023.6.dev2.data/data/requirements.txt
+-rw-r--r--  2.0 unx     1069 b- defN 23-Jun-09 09:49 tablite-2023.6.dev2.dist-info/LICENSE
+-rw-r--r--  2.0 unx     8677 b- defN 23-Jun-09 09:49 tablite-2023.6.dev2.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jun-09 09:49 tablite-2023.6.dev2.dist-info/WHEEL
+-rw-r--r--  2.0 unx        8 b- defN 23-Jun-09 09:49 tablite-2023.6.dev2.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     2457 b- defN 23-Jun-09 09:49 tablite-2023.6.dev2.dist-info/RECORD
+31 files, 264633 bytes uncompressed, 75332 bytes compressed:  71.5%
```

## zipnote {}

```diff
@@ -63,32 +63,32 @@
 
 Filename: tablite/utils.py
 Comment: 
 
 Filename: tablite/version.py
 Comment: 
 
-Filename: tablite-2023.6.dev1.data/data/LICENSE
+Filename: tablite-2023.6.dev2.data/data/LICENSE
 Comment: 
 
-Filename: tablite-2023.6.dev1.data/data/README.md
+Filename: tablite-2023.6.dev2.data/data/README.md
 Comment: 
 
-Filename: tablite-2023.6.dev1.data/data/requirements.txt
+Filename: tablite-2023.6.dev2.data/data/requirements.txt
 Comment: 
 
-Filename: tablite-2023.6.dev1.dist-info/LICENSE
+Filename: tablite-2023.6.dev2.dist-info/LICENSE
 Comment: 
 
-Filename: tablite-2023.6.dev1.dist-info/METADATA
+Filename: tablite-2023.6.dev2.dist-info/METADATA
 Comment: 
 
-Filename: tablite-2023.6.dev1.dist-info/WHEEL
+Filename: tablite-2023.6.dev2.dist-info/WHEEL
 Comment: 
 
-Filename: tablite-2023.6.dev1.dist-info/top_level.txt
+Filename: tablite-2023.6.dev2.dist-info/top_level.txt
 Comment: 
 
-Filename: tablite-2023.6.dev1.dist-info/RECORD
+Filename: tablite-2023.6.dev2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## tablite/base.py

```diff
@@ -64,14 +64,32 @@
         """
         self.id = next(self.ids)
         type_check(path, Path)
         self.path = path / "pages" / f"{self.id}.npy"
 
         type_check(array, np.ndarray)
 
+        if Config.DISK_LIMIT <= 0:
+            pass
+        else:
+            _, _, free = shutil.disk_usage(path)
+            if free - array.nbytes < Config.DISK_LIMIT:
+                msg = "\n".join(
+                    [
+                        f"Disk limit reached: Config.DISK_LIMIT = {Config.DISK_LIMIT:,} bytes.",
+                        f"array requires {array.nbytes:,} bytes, but only {free:,} bytes are free.",
+                        "To disable this check, use:",
+                        ">>> from tablite.config import Config",
+                        ">>> Config.DISK_LIMIT = 0",
+                        "To free space, clean up Config.workdir:",
+                        f"{Config.workdir}",
+                    ]
+                )
+                raise OSError(msg)
+
         self.len = len(array)
         np.save(self.path, array, allow_pickle=True, fix_imports=False)
         log.debug(f"Page saved: {self.path}")
 
     def __len__(self):
         return self.len
 
@@ -84,17 +102,18 @@
     def __del__(self):
         """When python's reference count for an object is 0, python uses
         it's garbage collector to remove the object and free the memory.
         As tablite tables have columns and columns have page and pages have
         data stored on disk, the space on disk must be freed up as well.
         This __del__ override assures the cleanup of stored data.
         """
-        if self.path.exists():
-            os.remove(self.path)
-        log.debug(f"Page deleted: {self.path}")
+        if f"pid-{os.getpid()}" in self.path.parts:
+            if self.path.exists():
+                os.remove(self.path)
+            log.debug(f"Page deleted: {self.path}")
 
     def get(self):
         """loads stored data
 
         Returns:
             np.ndarray: stored data.
         """
@@ -148,16 +167,16 @@
 
     def repaginate(self):
         """resizes pages to Config.PAGE_SIZE"""
         new_pages = []
         start, end = 0, 0
         for _ in range(0, len(self) + 1, Config.PAGE_SIZE):
             start, end = end, end + Config.PAGE_SIZE
-            array = self[slice(start, end, step=1)]
-            new_pages.extend(Page(self.path.parent, array))
+            array = self[slice(start, end, 1)]
+            new_pages.append(Page(self.path, array))
         self.pages = new_pages
 
     def extend(self, value):  # USER FUNCTION.
         """extends the column.
 
         Args:
             value (np.ndarray): data
@@ -169,15 +188,15 @@
             pass
         elif isinstance(value, (list, tuple)):
             value = list_to_np_array(value)
         else:
             raise TypeError(f"Cannot extend Column with {type(value)}")
         type_check(value, np.ndarray)
         for array in self._paginate(value):
-            self.pages.append(Page(path=self.path.parent, array=array))
+            self.pages.append(Page(path=self.path, array=array))
 
     def clear(self):
         """
         clears the column. Like list().clear()
         """
         self.pages.clear()
 
@@ -329,15 +348,15 @@
         # now there's a positive key and a single value.
         start, end = 0, 0
         for index, page in enumerate(self.pages):
             start, end = end, end + page.len
             if start <= key < end:
                 data = page.get()
                 data[key - start] = value
-                new_page = Page(self.path.parent, data)
+                new_page = Page(self.path, data)
                 self.pages[index] = new_page
                 break
 
     def _setitem_replace_all(self, key, value):  # PRIVATE FUNCTION
         """handles the following cases:
         new = list(value)
         L[:] = [1,2,3]
@@ -357,15 +376,15 @@
                 data = page.get()
                 keep = data[: key.start - start]
                 new = np_type_unify([keep, value])
                 self.pages = self.pages[:index]
                 self.extend(new)
                 break
         else:
-            new = Page(self.path.parent, value)
+            new = Page(self.path, value)
             self.pages.append(new)
 
     def _setitem_prextend(self, key, value):  # PRIVATE FUNCTION
         """handles the following case:
         new = list(value) + old[key.stop:]
         example: L[:3] = [1,2,3]
         """
@@ -417,15 +436,15 @@
                 data = page.get() if data is None else data  # don't load again if on same page.
                 tail = data[key_stop - start :]
 
             if key_stop < start:
                 unchanged_tail.append(page)
 
         new_middle = np_type_unify([head, value, tail])
-        new_pages = [Page(self.path.parent, arr) for arr in self._paginate(new_middle)]
+        new_pages = [Page(self.path, arr) for arr in self._paginate(new_middle)]
         self.pages = unchanged_head + new_pages + unchanged_tail
 
     def _setitem_update(self, key, value):
         """
         See test_slice_rules.py/MyList for detailed behaviour
         """
         key_start, key_stop, key_step = key.indices(len(self))
@@ -453,15 +472,15 @@
 
         # determine changed pages.
         changed_pages = [p.get() for p in changed]
         new = np_type_unify(changed_pages)
 
         for index, val in zip(range(key_start, key_stop, key_step), value):
             new[index - starts_on] = val
-        new_pages = [Page(self.path.parent, arr) for arr in self._paginate(new)]
+        new_pages = [Page(self.path, arr) for arr in self._paginate(new)]
         # merge.
         self.pages = head + new_pages + tail
 
     def __delitem__(self, key):  # USER FUNCTION
         """deletes items selected by key
 
         Args:
@@ -515,15 +534,15 @@
         # create np array
         changed_pages = [p.get() for p in changed]
         new = np_type_unify(changed_pages)
         # create mask for np.delete.
         filter = [i - starts_on for i in seq]
         pruned = np.delete(new, filter)
         new_arrays = self._paginate(pruned)
-        self.pages = head + [Page(self.path.parent, arr) for arr in new_arrays] + tail
+        self.pages = head + [Page(self.path, arr) for arr in new_arrays] + tail
 
     def __iter__(self):  # USER FUNCTION.
         for page in self.pages:
             data = page.get()
             for value in data:
                 yield value
 
@@ -678,15 +697,15 @@
         to_remove = list_to_np_array(values)
         for index, page in enumerate(self.pages):
             data = page.get()
             bitmask = np.isin(data, to_remove)  # identify elements to remove.
             if bitmask.any():
                 bitmask = np.invert(bitmask)  # turn bitmask around to keep.
                 new_data = np.compress(bitmask, data)
-                new_page = Page(self.path.parent, new_data)
+                new_page = Page(self.path, new_data)
                 self.pages[index] = new_page
 
     def replace(self, mapping):
         """
         replaces values using mapping
 
         example:
@@ -701,15 +720,15 @@
             data = page.get()
             bitmask = np.isin(data, to_replace)  # identify elements to replace.
             if bitmask.any():
                 warray = np.compress(bitmask, data)
                 for ix, v in enumerate(warray):
                     warray[ix] = mapping[numpy_to_python(v)]
                 data[bitmask] = warray
-                self.pages[index] = Page(path=self.path.parent, array=data)
+                self.pages[index] = Page(path=self.path, array=data)
 
     def types(self):
         """
         returns dict with python datatypes: frequency of occurrence
         """
         d = defaultdict(int)
         for page in set(self.pages):
@@ -831,20 +850,18 @@
                 Example: t = Table(headers=["a", "b"], rows=[[1,3], [2,4]])
         """
         if _path is None:
             if self._pid_dir is None:
                 self._pid_dir = Path(Config.workdir) / f"pid-{os.getpid()}"
                 if not self._pid_dir.exists():
                     self._pid_dir.mkdir()
-                    (self._pid_dir / "tables").mkdir()  # NOT USED.
                     (self._pid_dir / "pages").mkdir()
-                    (self._pid_dir / "index").mkdir()  # NOT USED.
                 register(self._pid_dir)
 
-            _path = Path(self._pid_dir) / f"{next(self._ids)}.yml"
+            _path = Path(self._pid_dir)
             # if path exists under the given PID it will be overwritten.
             # this can only happen if the process previously was SIGKILLed.
         type_check(_path, Path)
         self.path = _path  # filename used during multiprocessing.
         self.columns = {}  # maps colunn names to instances of Column.
 
         # user friendly features.
@@ -862,14 +879,31 @@
 
     def __str__(self):  # USER FUNCTION.
         return f"{self.__class__.__name__}({len(self.columns):,} columns, {len(self):,} rows)"
 
     def __repr__(self):
         return self.__str__()
 
+    def nbytes(self):  # USER FUNCTION.
+        """finds the total bytes of the table on disk
+
+        Returns:
+            tuple:
+                int: real bytes used on disk
+                int: total bytes used if flattened
+        """
+        real = {}
+        total = 0
+        for column in self.columns.values():
+            for page in set(column.pages):
+                real[page] = page.path.stat().st_size
+            for page in column.pages:
+                total += real[page]
+        return sum(real.values()), total
+
     def items(self):  # USER FUNCTION.
         """returns table as dict."""
         return {name: column[:].tolist() for name, column in self.columns.items()}.items()
 
     def __delitem__(self, key):  # USER FUNCTION.
         """
         del table['a']  removes column 'a'
```

## tablite/config.py

```diff
@@ -31,16 +31,18 @@
     """
 
     workdir = pathlib.Path(os.environ.get("TABLITE_TMPDIR", f"{tempfile.gettempdir()}/tablite-tmp"))
     workdir.mkdir(parents=True, exist_ok=True)
 
     PAGE_SIZE = 1_000_000  # sets the page size limit.
     ENCODING = "UTF-8"  # sets the page encoding when using bytes
-    DISK_LIMIT = 10e9  # 10e9 (10Gb) on 100 Gb disk means raise at
+    
+    DISK_LIMIT = int(10e9)  # 10e9 (10Gb) on 100 Gb disk means raise at
     # 90 Gb disk usage.
+    # if DISK_LIMIT <= 0, the check is turned off.
 
     SINGLE_PROCESSING_LIMIT = 1_000_000
     # when the number of fields (rows x columns)
     # exceed this value, multiprocessing is used.
 
     AUTO = "auto"
     FALSE = "sp"
```

## tablite/core.py

```diff
@@ -92,15 +92,16 @@
             >>> from tablite.tools import head
             >>> head = head(path)
 
         first_row_has_headers: boolean
             True: (default) first row is used as column names.
             False: integers are used as column names.
 
-        encoding: str. Defaults to None (autodetect)
+        encoding: str. Defaults to None (autodetect using n bytes).
+            n is declared in filereader_utils as ENCODING_GUESS_BYTES
 
         start: the first line to be read (default: 0)
 
         limit: the number of lines to be read from start (default sys.maxint ~ 2**63)
 
         OPTIONAL FOR EXCEL AND ODS READERS
         ----------------------------------
@@ -170,14 +171,19 @@
             # here we inject tqdm, if tqdm is not provided, use generic iterator
             # fmt:off
             config = (path, columns, first_row_has_headers, encoding, start, limit, newline,
                       guess_datatypes, text_qualifier, strip_leading_and_tailing_whitespace,
                       delimiter, text_escape_openings, text_escape_closures)
             # fmt:on
 
+        elif reader == import_utils.from_html:
+            config = (path,)
+        elif reader == import_utils.from_hdf5:
+            config = (path, )
+
         elif reader == import_utils.excel_reader:
             # config = path, first_row_has_headers, sheet, columns, start, limit
             config = (
                 str(path),
                 first_row_has_headers,
                 sheet,
                 columns,
@@ -237,15 +243,15 @@
         imports an exported hdf5 table.
         """
         return import_utils.from_hdf5(cls, path)
 
     @classmethod
     def from_json(cls, jsn):
         """
-        Imports tables exported using .to_json
+        Imports table exported using .to_json
         """
         return import_utils.from_json(cls, jsn)
 
     def to_hdf5(self, path):
         """
         creates a copy of the table as hdf5
         """
@@ -265,48 +271,54 @@
 
     def to_json(self):
         """
         returns JSON
         """
         return export_utils.to_json(self)
 
-    def to_xls(self, path):
+    def to_xlsx(self, path):
         """
         exports table to path
         """
+        export_utils.path_suffix_check(path, ".xlsx")
         export_utils.excel_writer(self, path)
 
     def to_ods(self, path):
         """
         exports table to path
         """
+        export_utils.path_suffix_check(path, ".ods")
         export_utils.excel_writer(self, path)
 
     def to_csv(self, path):
         """
         exports table to path
         """
+        export_utils.path_suffix_check(path, ".csv")
         export_utils.text_writer(self, path)
 
     def to_tsv(self, path):
         """
         exports table to path
         """
+        export_utils.path_suffix_check(path, ".tsv")
         export_utils.text_writer(self, path)
 
     def to_text(self, path):
         """
         exports table to path
         """
+        export_utils.path_suffix_check(path, ".txt")
         export_utils.text_writer(self, path)
 
     def to_html(self, path):
         """
         exports table to path
         """
+        export_utils.path_suffix_check(path, ".html")
         export_utils.to_html(self, path)
 
     def expression(self, expression):
         """
         filters based on an expression, such as:
 
             "all((A==B, C!=4, 200<D))"
@@ -344,15 +356,15 @@
     def sort_index(self, sort_mode="excel", tqdm=_tqdm, pbar=None, **kwargs):
         """
         helper for methods `sort` and `is_sorted`
 
         param: sort_mode: str: "alphanumeric", "unix", or, "excel" (default)
         param: **kwargs: sort criteria. See Table.sort()
         """
-        return sortation.sort_index(self, sort_mode, tqdm=_tqdm, pbar=None, **kwargs)
+        return sortation.sort_index(self, sort_mode, tqdm=tqdm, pbar=pbar, **kwargs)
 
     def reindex(self, index):
         """
         index: list of integers that declare sort order.
 
         Examples:
 
@@ -383,16 +395,16 @@
         """Perform multi-pass sorting with precedence given order of column names.
         sort_mode: str: "alphanumeric", "unix", or, "excel"
         kwargs:
             keys: columns,
             values: 'reverse' as boolean.
 
         examples:
-        Table.sort('A'=False) means sort by 'A' in ascending order.
-        Table.sort('A'=True, 'B'=False) means sort 'A' in descending order, then (2nd priority)
+        Table.sort(**{'A':False}) means sort by 'A' in ascending order.
+        Table.sort(**{'A':True, 'B':False}) means sort 'A' in descending order, then (2nd priority)
         sort B in ascending order.
         """
         return sortation.sort(self, sort_mode, **kwargs)
 
     def is_sorted(self, **kwargs):
         """Performs multi-pass sorting check with precedence given order of column names.
         **kwargs: optional: sort criteria. See Table.sort()
@@ -560,15 +572,15 @@
         >>> t['A'].histogram()
         ([1,2,3], [4,4,4])
 
         for more exmaples see:
             https://github.com/root-11/tablite/blob/master/tests/test_groupby.py
 
         """
-        return groupbys.groupby(self, keys, functions, tqdm=_tqdm, pbar=None)
+        return groupbys.groupby(self, keys, functions, tqdm=tqdm, pbar=pbar)
 
     def pivot(self, rows, columns, functions, values_as_rows=True, tqdm=_tqdm, pbar=None):
         """
         param: rows: column names to keep as rows
         param: columns: column names to keep as columns
         param: functions: aggregation functions from the Groupby class as
 
@@ -604,15 +616,15 @@
         # |2  |  4|Sum(B)  |None |    6|None |
         # |3  |  3|Sum(B)  |None |    8|None |
         # |4  |  2|Sum(B)  |None |None |   10|
         # |5  |  1|Sum(B)  |None |None |   12|
         # +===+===+========+=====+=====+=====+
 
         """
-        return pivots.pivot(self, rows, columns, functions, values_as_rows, tqdm=_tqdm, pbar=None)
+        return pivots.pivot(self, rows, columns, functions, values_as_rows, tqdm=tqdm, pbar=pbar)
 
     def join(self, other, left_keys, right_keys, left_columns, right_columns, kind="inner", tqdm=_tqdm, pbar=None):
         """
         short-cut for all join functions.
         kind: 'inner', 'left', 'outer', 'cross'
         """
         kinds = {
@@ -636,15 +648,15 @@
         :return: new Table
         Example:
         SQL:   SELECT number, letter FROM numbers LEFT JOIN letters ON numbers.colour == letters.color
         Tablite: left_join = numbers.left_join(
             letters, left_keys=['colour'], right_keys=['color'], left_columns=['number'], right_columns=['letter']
         )
         """
-        return joins.left_join(self, other, left_keys, right_keys, left_columns, right_columns, tqdm=_tqdm, pbar=None)
+        return joins.left_join(self, other, left_keys, right_keys, left_columns, right_columns, tqdm=tqdm, pbar=pbar)
 
     def inner_join(self, other, left_keys, right_keys, left_columns=None, right_columns=None, tqdm=_tqdm, pbar=None):
         """
         :param other: self, other = (left, right)
         :param left_keys: list of keys for the join
         :param right_keys: list of keys for the join
         :param left_columns: list of left columns to retain, if None, all are retained.
@@ -652,15 +664,15 @@
         :return: new Table
         Example:
         SQL:   SELECT number, letter FROM numbers JOIN letters ON numbers.colour == letters.color
         Tablite: inner_join = numbers.inner_join(
             letters, left_keys=['colour'], right_keys=['color'], left_columns=['number'], right_columns=['letter']
             )
         """
-        return joins.inner_join(self, other, left_keys, right_keys, left_columns, right_columns, tqdm=_tqdm, pbar=None)
+        return joins.inner_join(self, other, left_keys, right_keys, left_columns, right_columns, tqdm=tqdm, pbar=pbar)
 
     def outer_join(self, other, left_keys, right_keys, left_columns=None, right_columns=None, tqdm=_tqdm, pbar=None):
         """
         :param other: self, other = (left, right)
         :param left_keys: list of keys for the join
         :param right_keys: list of keys for the join
         :param left_columns: list of left columns to retain, if None, all are retained.
@@ -668,23 +680,23 @@
         :return: new Table
         Example:
         SQL:   SELECT number, letter FROM numbers OUTER JOIN letters ON numbers.colour == letters.color
         Tablite: outer_join = numbers.outer_join(
             letters, left_keys=['colour'], right_keys=['color'], left_columns=['number'], right_columns=['letter']
             )
         """
-        return joins.outer_join(self, other, left_keys, right_keys, left_columns, right_columns, tqdm=_tqdm, pbar=None)
+        return joins.outer_join(self, other, left_keys, right_keys, left_columns, right_columns, tqdm=tqdm, pbar=pbar)
 
     def cross_join(self, other, left_keys, right_keys, left_columns=None, right_columns=None, tqdm=_tqdm, pbar=None):
         """
         CROSS JOIN returns the Cartesian product of rows from tables in the join.
         In other words, it will produce rows which combine each row from the first table
         with each row from the second table
         """
-        return joins.cross_join(self, other, left_keys, right_keys, left_columns, right_columns, tqdm=_tqdm, pbar=None)
+        return joins.cross_join(self, other, left_keys, right_keys, left_columns, right_columns, tqdm=tqdm, pbar=pbar)
 
     def lookup(self, other, *criteria, all=True, tqdm=_tqdm):
         """function for looking up values in `other` according to criteria in ascending order.
         :param: other: Table sorted in ascending search order.
         :param: criteria: Each criteria must be a tuple with value comparisons in the form:
             (LEFT, OPERATOR, RIGHT)
         :param: all: boolean: True=ALL, False=Any
@@ -748,15 +760,15 @@
             sources (list of strings): NEAREST NEIGHBOUR ONLY
                 column names to be used during imputation.
                 if None or empty, all columns will be used.
 
         Returns:
             table: table with replaced values.
         """
-        return imputation.imputation(self, targets, missing, method, sources, tqdm=_tqdm)
+        return imputation.imputation(self, targets, missing, method, sources, tqdm=tqdm)
 
     def transpose(self, tqdm=_tqdm):
         return pivots.transpose(self, tqdm)
 
     def pivot_transpose(self, columns, keep=None, column_name="transpose", value_name="value", tqdm=_tqdm):
         """Transpose a selection of columns to rows.
 
@@ -785,15 +797,15 @@
         |col1| col2| col3| transpose| value|
         |----|-----|-----|----------|------|
         |1234| 2345| 3456| sun      |   456|
         |1234| 2345| 3456| mon      |   567|
         |1244| 2445| 4456| mon      |     7|
 
         """
-        return pivots.pivot_transpose(self, columns, keep, column_name, value_name, tqdm=_tqdm)
+        return pivots.pivot_transpose(self, columns, keep, column_name, value_name, tqdm=tqdm)
 
     def diff(self, other, columns=None):
         """compares table self with table other
 
         Args:
             self (Table): Table
             other (Table): Table
```

## tablite/export_utils.py

```diff
@@ -55,34 +55,56 @@
         return pd.DataFrame(table.to_dict())  # noqa
     except ImportError:
         import pandas as pd  # noqa
     return pd.DataFrame(table.to_dict())  # noqa
 
 
 def to_hdf5(table, path):
+    # fmt: off
     """
     creates a copy of the table as hdf5
+
+    Note that some loss of type information is to be expected in columns of mixed type:
+    >>> t.show(dtype=True)
+    +===+===+=====+=====+====+=====+=====+===================+==========+========+===============+===+=========================+=====+===+
+    | # | A |  B  |  C  | D  |  E  |  F  |         G         |    H     |   I    |       J       | K |            L            |  M  | O |
+    |row|int|mixed|float|str |mixed| bool|      datetime     |   date   |  time  |   timedelta   |str|           int           |float|int|
+    +---+---+-----+-----+----+-----+-----+-------------------+----------+--------+---------------+---+-------------------------+-----+---+
+    | 0 | -1|None | -1.1|    |None |False|2023-06-09 09:12:06|2023-06-09|09:12:06| 1 day, 0:00:00|b  |-100000000000000000000000|  inf| 11|
+    | 1 |  1|    1|  1.1|1000|1    | True|2023-06-09 09:12:06|2023-06-09|09:12:06|2 days, 0:06:40|嗨  | 100000000000000000000000| -inf|-11|
+    +===+===+=====+=====+====+=====+=====+===================+==========+========+===============+===+=========================+=====+===+
+    >>> t.to_hdf5(filename)
+    >>> t2 = Table.from_hdf5(filename)
+    >>> t2.show(dtype=True)
+    +===+===+=====+=====+=====+=====+=====+===================+===================+========+===============+===+=========================+=====+===+
+    | # | A |  B  |  C  |  D  |  E  |  F  |         G         |         H         |   I    |       J       | K |            L            |  M  | O |
+    |row|int|mixed|float|mixed|mixed| bool|      datetime     |      datetime     |  time  |      str      |str|           int           |float|int|
+    +---+---+-----+-----+-----+-----+-----+-------------------+-------------------+--------+---------------+---+-------------------------+-----+---+
+    | 0 | -1|None | -1.1|None |None |False|2023-06-09 09:12:06|2023-06-09 00:00:00|09:12:06|1 day, 0:00:00 |b  |-100000000000000000000000|  inf| 11|
+    | 1 |  1|    1|  1.1| 1000|    1| True|2023-06-09 09:12:06|2023-06-09 00:00:00|09:12:06|2 days, 0:06:40|嗨  | 100000000000000000000000| -inf|-11|
+    +===+===+=====+=====+=====+=====+=====+===================+===================+========+===============+===+=========================+=====+===+
     """
+    # fmt: in
     import h5py
 
     sub_cls_check(table, Table)
     type_check(path, Path)
 
-    total = ":,".format(len(table.columns) * len(table))  # noqa
-    print(f"writing {total} records to {path}")
+    total = f"{len(table.columns) * len(table):,}"  # noqa
+    print(f"writing {total} records to {path}", end="")
 
-    with h5py.File(path, "a") as f:
+    with h5py.File(path, "w") as f:
         n = 0
         for name, col in table.items():
             try:
                 f.create_dataset(name, data=col[:])  # stored in hdf5 as '/name'
             except TypeError:
                 f.create_dataset(name, data=[str(i) for i in col[:]])  # stored in hdf5 as '/name'
             n += 1
-    print(f"writing {path} to HDF5 done")
+    print("... done")
 
 
 def excel_writer(table, path):
     """
     writer for excel files.
 
     This can create xlsx files beyond Excels.
@@ -115,14 +137,21 @@
 def to_json(table, *args, **kwargs):
     import json
 
     sub_cls_check(table, Table)
     return json.dumps(table.as_json_serializable())
 
 
+def path_suffix_check(path, kind):
+    if not path.suffix == kind:
+        raise ValueError(f"Suffix mismatch: Expected {kind}, got {path.suffix} in {path.name}")
+    if not path.parent.exists():
+        raise FileNotFoundError(f"directory {path.parent} not found.")
+
+
 def text_writer(table, path, tqdm=_tqdm):
     """exports table to csv, tsv or txt dependening on path suffix.
     follows the JSON norm. text escape is ON for all strings.
 
     Note:
     ----------------------
     If the delimiter is present in a string when the string is exported,
@@ -165,19 +194,12 @@
 def json_writer(table, path):
     type_check(table, Table)
     type_check(path, Path)
     with path.open("w") as fo:
         fo.write(to_json(table))
 
 
-def h5_writer(table, path):
-    type_check(table, Table)
-    type_check(path, Path)
-    to_hdf5(table, path)
-
-
 def to_html(table, path):
     type_check(table, Table)
     type_check(path, Path)
     with path.open("w", encoding="utf-8") as fo:
-        fo.write(table._repr_html_())
-
+        fo.write(table._repr_html_(slice(0, len(table))))
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## tablite/file_reader_utils.py

```diff
@@ -272,17 +272,15 @@
             d[path.name] = [line.split(delimiter) for line in lines]
         return d
     except Exception:
         raise ValueError(f"can't read {path.suffix}")
 
 
 def get_encoding(path, nbytes=ENCODING_GUESS_BYTES):
-    size = path.stat().st_size
-    if nbytes > size:
-        nbytes = size
+    nbytes = min(nbytes, path.stat().st_size)
     with path.open("rb") as fi:
         rawdata = fi.read(nbytes)
         encoding = chardet.detect(rawdata)["encoding"]
         if encoding == "ascii":  # utf-8 is backwards compatible with ascii
             return "utf-8"  # --   so should the first 10k chars not be enough,
         return encoding  # --      the utf-8 encoding will still get it right.
```

## tablite/groupbys.py

```diff
@@ -85,21 +85,19 @@
     for more exmaples see:
         https://github.com/root-11/tablite/blob/master/tests/test_groupby.py
 
     """
     if not isinstance(keys, list):
         raise TypeError("expected keys as a list of column names")
 
-    if not keys:
-        raise ValueError("Keys missing.")
-
-    if len(set(keys)) != len(keys):
-        duplicates = [k for k in keys if keys.count(k) > 1]
-        s = "" if len(duplicates) > 1 else "s"
-        raise ValueError(f"duplicate key{s} found across rows and columns: {duplicates}")
+    if keys:
+        if len(set(keys)) != len(keys):
+            duplicates = [k for k in keys if keys.count(k) > 1]
+            s = "" if len(duplicates) > 1 else "s"
+            raise ValueError(f"duplicate key{s} found across rows and columns: {duplicates}")
 
     if not isinstance(functions, list):
         raise TypeError(f"Expected functions to be a list of tuples. Got {type(functions)}")
 
     if not keys + functions:
         raise ValueError("No keys or functions?")
```

## tablite/import_utils.py

```diff
@@ -8,16 +8,16 @@
 import warnings
 import logging
 
 from mplite import TaskManager, Task
 
 from tablite.datatypes import DataTypes, list_to_np_array
 from tablite.config import Config
-from tablite.file_reader_utils import TextEscape, get_encoding, get_delimiter
-from tablite.utils import type_check, unique_name, sub_cls_check
+from tablite.file_reader_utils import TextEscape, get_encoding, get_delimiter, ENCODING_GUESS_BYTES
+from tablite.utils import type_check, unique_name
 from tablite.base import Table, Page, Column
 
 from tqdm import tqdm as _tqdm
 
 logging.getLogger("lml").propagate = False
 logging.getLogger("pyexcel_io").propagate = False
 logging.getLogger("pyexcel").propagate = False
@@ -51,29 +51,51 @@
     """
     if not issubclass(T, Table):
         raise TypeError("Expected subclass of Table")
 
     return T(columns=df.to_dict("list"))  # noqa
 
 
-def from_hdf5(T, path):
+def from_hdf5(T, path, tqdm=_tqdm, pbar=None):
     """
     imports an exported hdf5 table.
+
+    Note that some loss of type information is to be expected in columns of mixed type:
+    >>> t.show(dtype=True)
+    +===+===+=====+=====+====+=====+=====+===================+==========+========+===============+===+=========================+=====+===+
+    | # | A |  B  |  C  | D  |  E  |  F  |         G         |    H     |   I    |       J       | K |            L            |  M  | O |
+    |row|int|mixed|float|str |mixed| bool|      datetime     |   date   |  time  |   timedelta   |str|           int           |float|int|
+    +---+---+-----+-----+----+-----+-----+-------------------+----------+--------+---------------+---+-------------------------+-----+---+
+    | 0 | -1|None | -1.1|    |None |False|2023-06-09 09:12:06|2023-06-09|09:12:06| 1 day, 0:00:00|b  |-100000000000000000000000|  inf| 11|
+    | 1 |  1|    1|  1.1|1000|1    | True|2023-06-09 09:12:06|2023-06-09|09:12:06|2 days, 0:06:40|嗨 | 100000000000000000000000| -inf|-11|
+    +===+===+=====+=====+====+=====+=====+===================+==========+========+===============+===+=========================+=====+===+
+    >>> t.to_hdf5(filename)
+    >>> t2 = Table.from_hdf5(filename)
+    >>> t2.show(dtype=True)
+    +===+===+=====+=====+=====+=====+=====+===================+===================+========+===============+===+=========================+=====+===+
+    | # | A |  B  |  C  |  D  |  E  |  F  |         G         |         H         |   I    |       J       | K |            L            |  M  | O |
+    |row|int|mixed|float|mixed|mixed| bool|      datetime     |      datetime     |  time  |      str      |str|           int           |float|int|
+    +---+---+-----+-----+-----+-----+-----+-------------------+-------------------+--------+---------------+---+-------------------------+-----+---+
+    | 0 | -1|None | -1.1|None |None |False|2023-06-09 09:12:06|2023-06-09 00:00:00|09:12:06|1 day, 0:00:00 |b  |-100000000000000000000000|  inf| 11|
+    | 1 |  1|    1|  1.1| 1000|    1| True|2023-06-09 09:12:06|2023-06-09 00:00:00|09:12:06|2 days, 0:06:40|嗨 | 100000000000000000000000| -inf|-11|
+    +===+===+=====+=====+=====+=====+=====+===================+===================+========+===============+===+=========================+=====+===+
     """
     if not issubclass(T, Table):
         raise TypeError("Expected subclass of Table")
-
     import h5py
 
     type_check(path, Path)
     t = T()
     with h5py.File(path, "r") as h5:
         for col_name in h5.keys():
             dset = h5[col_name]
-            t[col_name] = dset[:]
+            arr = np.array(dset[:])
+            if arr.dtype == object:
+                arr = np.array(DataTypes.guess([v.decode('utf-8') for v in arr]))
+            t[col_name] = arr
     return t
 
 
 def from_json(T, jsn):
     """
     Imports tables exported using .to_json
     """
@@ -82,14 +104,66 @@
     import json
 
     type_check(jsn, str)
     d = json.loads(jsn)
     return T(columns=d["columns"])
 
 
+def from_html(T, path, tqdm=_tqdm, pbar=None):
+    if not issubclass(T, Table):
+        raise TypeError("Expected subclass of Table")
+    type_check(path, Path)
+
+    if pbar is None:
+        total = path.stat().st_size
+        pbar = tqdm(total=total, desc="from_html")
+
+    row_start, row_end = "<tr>", "</tr>"
+    value_start, value_end = "<th>", "</th>"
+    chunk = ""
+    t = None  # will be T()
+    start, end = 0, 0
+    data = {}
+    with path.open("r") as fi:
+        while True:
+            start = chunk.find(row_start, start)  # row tag start
+            end = chunk.find(row_end, end)  # row tag end
+            if start == -1 or end == -1:
+                new = fi.read(100_000)
+                pbar.update(len(new))
+                if new == "":
+                    break
+                chunk += new
+                continue
+            # get indices from chunk
+            row = chunk[start + len(row_start) : end]
+            fields = [v.rstrip(value_end) for v in row.split(value_start)]
+            if not data:
+                headers = fields[:]
+                data = {f: [] for f in headers}
+                continue
+            else:
+                for field, header in zip(fields, headers):
+                    data[header].append(field)
+
+            chunk = chunk[end + len(row_end) :]
+
+            if len(data[headers[0]]) == Config.PAGE_SIZE:
+                if t is None:
+                    t = T(columns=data)
+                else:
+                    for k, v in data.items():
+                        t[k].extend(DataTypes.guess(v))
+                data = {f: [] for f in headers}
+
+    for k, v in data.items():
+        t[k].extend(DataTypes.guess(v))
+    return t
+
+
 def excel_reader(T, path, first_row_has_headers=True, sheet=None, columns=None, start=0, limit=sys.maxsize, **kwargs):
     """
     returns Table from excel
 
     **kwargs are excess arguments that are ignored.
     """
     if not issubclass(T, Table):
@@ -253,15 +327,14 @@
     end: int: end of page.
     guess_datatypes: bool: if True datatypes will be inferred by datatypes.Datatypes.guess
     delimiter: ',' ';' or '|'
     text_qualifier: str: commonly \"
     text_escape_openings: str: default: "({[
     text_escape_closures: str: default: ]})"
     strip_leading_and_tailing_whitespace: bool
-
     encoding: chardet encoding ('utf-8, 'ascii', ..., 'ISO-22022-CN')
     """
     if isinstance(source, str):
         source = Path(source)
     type_check(source, Path)
     if not source.exists():
         raise FileNotFoundError(f"File not found: {source}")
@@ -277,15 +350,15 @@
         text_escape_openings,
         text_escape_closures,
         text_qualifier=text_qualifier,
         delimiter=delimiter,
         strip_leading_and_tailing_whitespace=strip_leading_and_tailing_whitespace,
     )
     values = []
-    with source.open("r", encoding=encoding) as fi:  # --READ
+    with source.open("r", encoding=encoding, errors="ignore") as fi:  # --READ
         for ix, line in enumerate(fi):
             if ix < start:
                 continue
             if ix >= end:
                 break
             L = text_escape(line.rstrip("\n"))
             try:
@@ -328,15 +401,15 @@
     if not path.exists():
         raise FileNotFoundError(str(path))
 
     if path.stat().st_size == 0:
         return T()  # NO DATA: EMPTY TABLE.
 
     if encoding is None:
-        encoding = get_encoding(path, nbytes=path.stat().st_size)
+        encoding = get_encoding(path, nbytes=ENCODING_GUESS_BYTES)
 
     if delimiter is None:
         try:
             delimiter = get_delimiter(path, encoding)
         except ValueError:
             return T()  # NO DELIMITER: EMPTY TABLE.
 
@@ -432,14 +505,20 @@
             for ix, name in enumerate(fields):
                 if name in columns:  # I may have to reduce to match user selection of columns.
                     unseen_name = unique_name(name, seen)
                     new_fields[ix] = unseen_name
                     seen.add(unseen_name)
             fields = {ix: name for ix, name in new_fields.items() if name in columns}
 
+        if not fields:
+            if columns is not None:
+                raise ValueError(f"Columns not found: {columns}")
+            else:
+                raise ValueError("No columns?")
+
         tasks = math.ceil(newlines / Config.PAGE_SIZE) * len(fields)
 
         task_config = TRconfig(
             source=str(path),
             destination=None,
             column_index=0,
             start=1,
@@ -494,14 +573,15 @@
         cpus_needed = min(len(tasks), cpus)  # 4 columns won't require 96 cpus ...!
         if cpus_needed < 2 or Config.MULTIPROCESSING_MODE == Config.FALSE:
             for task in tasks:
                 err = task.execute()
                 if err is not None:
                     raise Exception(err)
                 pbar.update(dump_size)
+
         else:
             with TaskManager(cpus_needed) as tm:
                 errors = tm.execute(tasks, pbar=PatchTqdm())  # I expects a list of None's if everything is ok.
                 if any(errors):
                     raise Exception("\n".join(e for e in errors if e))
 
         pbar.desc = f"importing: consolidating '{pbar_fname}'"
@@ -522,15 +602,16 @@
         pbar.update(100 - pbar.n)
         return t
 
 
 file_readers = {  # dict of file formats and functions used during Table.import_file
     "fods": excel_reader,
     "json": excel_reader,
-    "html": excel_reader,
+    "html": from_html,
+    "hdf5": from_hdf5,
     "simple": excel_reader,
     "rst": excel_reader,
     "mediawiki": excel_reader,
     "xlsx": excel_reader,
     "xls": excel_reader,
     "xlsm": excel_reader,
     "csv": text_reader,
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## tablite/joins.py

```diff
@@ -83,15 +83,15 @@
         revised_name = unique_name(col_name, result.columns)
         result[revised_name] = [col_data[k] if k != -1 else None for k in RIGHT]
         pbar.update(1)
     return result
 
 
 def _mp_join(T, other, LEFT, RIGHT, left_columns, right_columns, tqdm=_tqdm, pbar=None):
-    return _sp_join(T, other, LEFT, RIGHT, left_columns, right_columns, tqdm=_tqdm, pbar=None)
+    return _sp_join(T, other, LEFT, RIGHT, left_columns, right_columns, tqdm=tqdm, pbar=pbar)
 
     assert len(LEFT) == len(RIGHT)
     assert isinstance(LEFT, np.ndarray) and isinstance(RIGHT, np.ndarray)
 
     result = type(T)()
     cpus = max(psutil.cpu_count(logical=False), 2)
     step_size = math.ceil(len(LEFT) / cpus)
```

## tablite/version.py

```diff
@@ -1,3 +1,3 @@
-major, minor, patch = 2023, 6, "dev1"
+major, minor, patch = 2023, 6, "dev2"
 __version_info__ = (major, minor, patch)
 __version__ = ".".join(str(i) for i in __version_info__)
```

## Comparing `tablite-2023.6.dev1.data/data/LICENSE` & `tablite-2023.6.dev2.data/data/LICENSE`

 * *Files identical despite different names*

## Comparing `tablite-2023.6.dev1.data/data/README.md` & `tablite-2023.6.dev2.data/data/README.md`

 * *Files identical despite different names*

## Comparing `tablite-2023.6.dev1.dist-info/LICENSE` & `tablite-2023.6.dev2.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `tablite-2023.6.dev1.dist-info/METADATA` & `tablite-2023.6.dev2.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: tablite
-Version: 2023.6.dev1
+Version: 2023.6.dev2
 Summary: multiprocessing enabled out-of-memory data analysis library for tabular data.
 Home-page: https://github.com/root-11/tablite
 Author: https://github.com/root-11
 License: MIT
 Keywords: all,any,average,column,columns,count,csv,data imputation,date range,dict,excel,filter,first,from,from_pandas,groupby,guess,imputation,in-memory,index,indexing,inner join,is sorted,json,last,left join,list,list on disk,log,max,median,min,mode,numpy,ods,out-of-memory,outer join,pandas,pivot,pivot table,product,read csv,remove duplicates,replace,replace missing values,rows,show,sort,standard deviation,stored list,sum,table,tables,tablite,to,to_pandas,tools,transpose,txt,unique,use disk,xlsx,xround,zip
 Platform: any
 Classifier: Development Status :: 5 - Production/Stable
```

## Comparing `tablite-2023.6.dev1.dist-info/RECORD` & `tablite-2023.6.dev2.dist-info/RECORD`

 * *Files 14% similar despite different names*

```diff
@@ -1,31 +1,31 @@
 tablite/__init__.py,sha256=SH-lADLJJHz9XwBebXZMYNBsBneAGwSCXY5rJhYgrYg,244
-tablite/base.py,sha256=bwSxQyul-dserdKXy4I429peej1mkvOD96Lr1q9AQ74,55667
-tablite/config.py,sha256=wMqmk22lFeGCk80wn_Cv1JAU0qu4AL8oBlVkTVYuuAU,2262
-tablite/core.py,sha256=qsVYQ6WzSb_u1tosXJ7u01JizD738J9v-rvKZHiuTcs,27578
+tablite/base.py,sha256=IidDF2YGNWETD_m0gmC6j7s78tnIk5nXoEYkVa6uY78,56819
+tablite/config.py,sha256=Cf0U96l2hd3dQS3a28VxE3BxLrZUc_HsHOhi6z4mDE4,2323
+tablite/core.py,sha256=_jKOCMcAEq1l2h-oeRNrGXlMOSDyDKSyD49V9uFsxm0,28136
 tablite/datasets.py,sha256=WCCXy6nbLFokBGt3S9nd3IdYfeEqvvmHS90ZZVguV84,4504
 tablite/datatypes.py,sha256=X16D1b1Un9aXUoS8HH5-drHfo538258EYU2zKzGg6VQ,26976
 tablite/diff.py,sha256=NuxQGJLtXEat4utd1gyRDZBucV33nsMZeP9T02Mmofc,2872
-tablite/export_utils.py,sha256=_Z_XqbP3WZpna-oNpOANDR3pMmj182aeNcD-Ny5KOVA,5340
-tablite/file_reader_utils.py,sha256=GZ8rvzyr3PKMs6Sp2LG_LZLd75-IYn5Wt2eIbyE8Gcs,10141
+tablite/export_utils.py,sha256=cP_daDl-qXYqfVt2JVeFcNWYxMxTWiaA9IliYGRZ870,7759
+tablite/file_reader_utils.py,sha256=3a0XGUrmfM84BueFrP-wbTfL8_xpXmHp3RvknsYUf8k,10112
 tablite/groupby_utils.py,sha256=-GhV2q_4gFd7c1QKz0Oz6aiW5YLSMXXlIa_iNsJasBw,4889
-tablite/groupbys.py,sha256=t8zSkpk5S3bTx8d_AY7_FtMyl8IGyaIbmWL2BFvS6H8,5652
-tablite/import_utils.py,sha256=fMpIiCGglnPctNcnCwH9_hLZa6ThD0k0LanSX6sKD98,17867
+tablite/groupbys.py,sha256=ifu_EWHle0PkYiuPnNy11fRJmU4vQcBvZFfVW9b9_BY,5621
+tablite/import_utils.py,sha256=zGzrialo7V9ivJs7ebpec3eA7MALqpLeYBXSZJkGWjI,22204
 tablite/imputation.py,sha256=cBQ3k6_5LOFWyPC0-cum7IwIGk32PKcHb8kJ-WBK7a0,7589
-tablite/joins.py,sha256=fzt3OpeSHxgj8EcWcqQ0LB2zPm83xNC8bMXRLe-LYzk,12182
+tablite/joins.py,sha256=fD19iAX2Czu0mlheEd8oCrg7UryncbDMWX2git-eRO8,12181
 tablite/lookup.py,sha256=rRgwhWSDc1wTBHzoc7iRSWR0Ma0d7lWxfVHRoKE5dGU,6737
 tablite/mp_utils.py,sha256=hY1X3PodD4jEMJ04AahrnLbQztJrXEntDm_ftPFeYuw,3199
 tablite/pivots.py,sha256=Yg5-ZAHTNlndESK2k3NwNJ1YkbxYLlX9KS64hys8pcE,8863
 tablite/redux.py,sha256=yh_i7my5aBQRP95Lkt5El8lDLTV0y2aRvtcHoRfXHpQ,9171
 tablite/sort_utils.py,sha256=Wgl2X4NT59fc32y2mMOZQZlD7m41-vkgNVqR7q-Arqc,6098
 tablite/sortation.py,sha256=MAMTctJutH48GZryGe9iM1kqD9ZuDZxReK2PMOJWN04,5299
 tablite/tools.py,sha256=hEpqij7_2yc8yptUetk9Qjq1b7NxGoMamcCWOX3X010,1125
 tablite/utils.py,sha256=GXSuj7UfJzKa9a0t3FW5TnYacnvPD2_g1P-hDFP0ZWQ,11195
-tablite/version.py,sha256=rSmVDlUFU92fmrYT2ztnPHoHyV_Z9Sc3vOuSN6Xic2w,139
-tablite-2023.6.dev1.data/data/LICENSE,sha256=DzHDlst_HKcG2siTMmSx3QBYn1DfYj8Thz7d189Hd_M,1069
-tablite-2023.6.dev1.data/data/README.md,sha256=TlGniuVuNAT76KFExrlEPDOiUDnrRb6Qa7aahNmxZ3Y,6960
-tablite-2023.6.dev1.data/data/requirements.txt,sha256=AGNyHRtmn33IBZb4LVH8kqvOsoQCbzZTM1F9-VBUxVI,246
-tablite-2023.6.dev1.dist-info/LICENSE,sha256=DzHDlst_HKcG2siTMmSx3QBYn1DfYj8Thz7d189Hd_M,1069
-tablite-2023.6.dev1.dist-info/METADATA,sha256=agg4ij3bRzees1H-mZ1OX49QJUwv1zlyS_u3b1XPiZQ,8677
-tablite-2023.6.dev1.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-tablite-2023.6.dev1.dist-info/top_level.txt,sha256=wLPhlgUC10JlueE5ZhtCVSrs2VnvMQgNlhdCNsxPXPg,8
-tablite-2023.6.dev1.dist-info/RECORD,,
+tablite/version.py,sha256=bWk5fOpcjMVDxHWXDS4mu_AOpAAMx9WNRRVDOfHuS8Y,139
+tablite-2023.6.dev2.data/data/LICENSE,sha256=DzHDlst_HKcG2siTMmSx3QBYn1DfYj8Thz7d189Hd_M,1069
+tablite-2023.6.dev2.data/data/README.md,sha256=TlGniuVuNAT76KFExrlEPDOiUDnrRb6Qa7aahNmxZ3Y,6960
+tablite-2023.6.dev2.data/data/requirements.txt,sha256=AGNyHRtmn33IBZb4LVH8kqvOsoQCbzZTM1F9-VBUxVI,246
+tablite-2023.6.dev2.dist-info/LICENSE,sha256=DzHDlst_HKcG2siTMmSx3QBYn1DfYj8Thz7d189Hd_M,1069
+tablite-2023.6.dev2.dist-info/METADATA,sha256=3R4eCDP0voGUHiJM6o3C09fGdEwL53Q9W_AH_8xGj6o,8677
+tablite-2023.6.dev2.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+tablite-2023.6.dev2.dist-info/top_level.txt,sha256=wLPhlgUC10JlueE5ZhtCVSrs2VnvMQgNlhdCNsxPXPg,8
+tablite-2023.6.dev2.dist-info/RECORD,,
```

